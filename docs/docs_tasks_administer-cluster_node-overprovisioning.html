<div class="td-content"><h1 data-pagefind-weight="10">Overprovision Node Capacity For A Cluster</h1><p>This page guides you through configuring <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Node">Node</a>
overprovisioning in your Kubernetes cluster. Node overprovisioning is a strategy that proactively
reserves a portion of your cluster's compute resources. This reservation helps reduce the time
required to schedule new pods during scaling events, enhancing your cluster's responsiveness
to sudden spikes in traffic or workload demands.</p><p>By maintaining some unused capacity, you ensure that resources are immediately available when
new pods are created, preventing them from entering a pending state while the cluster scales up.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with
your cluster.</li><li>You should already have a basic understanding of
<a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>,
Pod <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank" aria-label="priority">priority</a>,
and <a class="glossary-tooltip" title="A mapping from a class name to the scheduling priority that a Pod should have." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass" target="_blank" aria-label="PriorityClasses">PriorityClasses</a>.</li><li>Your cluster must be set up with an <a href="/docs/concepts/cluster-administration/cluster-autoscaling/">autoscaler</a>
that manages nodes based on demand.</li></ul><h2 id="create-a-priorityclass">Create a PriorityClass</h2><p>Begin by defining a PriorityClass for the placeholder Pods. First, create a PriorityClass with a
negative priority value, that you will shortly assign to the placeholder pods.
Later, you will set up a Deployment that uses this PriorityClass</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priorityclass/low-priority-class.yaml" download="priorityclass/low-priority-class.yaml"><code>priorityclass/low-priority-class.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;priorityclass-low-priority-class-yaml&quot;)" title="Copy priorityclass/low-priority-class.yaml to clipboard"/></div><div class="includecode" id="priorityclass-low-priority-class-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>scheduling.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>placeholder<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># these Pods represent placeholder capacity</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>-<span style="color:#666">1000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">globalDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Negative priority for placeholder pods to enable overprovisioning."</span></span></span></code></pre></div></div></div><p>Then create the PriorityClass:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/priorityclass/low-priority-class.yaml
</span></span></code></pre></div><p>You will next define a Deployment that uses the negative-priority PriorityClass and runs a minimal container.
When you add this to your cluster, Kubernetes runs those placeholder pods to reserve capacity. Any time there
is a capacity shortage, the control plane will pick one these placeholder pods as the first candidate to
<a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank" aria-label="preempt">preempt</a>.</p><h2 id="run-pods-that-request-node-capacity">Run Pods that request node capacity</h2><p>Review the sample manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/deployments/deployment-with-capacity-reservation.yaml" download="deployments/deployment-with-capacity-reservation.yaml"><code>deployments/deployment-with-capacity-reservation.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;deployments-deployment-with-capacity-reservation-yaml&quot;)" title="Copy deployments/deployment-with-capacity-reservation.yaml to clipboard"/></div><div class="includecode" id="deployments-deployment-with-capacity-reservation-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>capacity-reservation<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># You should decide what namespace to deploy this into</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>capacity-placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>capacity-placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">kubernetes.io/description</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Capacity reservation"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">priorityClassName</span>:<span style="color:#bbb"> </span>placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Try to place these overhead Pods on different nodes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:#080;font-style:italic"># if possible</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAntiAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">podAffinityTerm</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                  </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>capacity-placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/hostname<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.6<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"50m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"512Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"512Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h3 id="pick-a-namespace-for-the-placeholder-pods">Pick a namespace for the placeholder pods</h3><p>You should select, or create, a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>
that the placeholder Pods will go into.</p><h3 id="create-the-placeholder-deployment">Create the placeholder deployment</h3><p>Create a Deployment based on that manifest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Change the namespace name "example"</span>
</span></span><span style="display:flex"><span>kubectl --namespace example apply -f https://k8s.io/examples/deployments/deployment-with-capacity-reservation.yaml
</span></span></code></pre></div><h2 id="adjust-placeholder-resource-requests">Adjust placeholder resource requests</h2><p>Configure the resource requests and limits for the placeholder pods to define the amount of overprovisioned resources you want to maintain. This reservation ensures that a specific amount of CPU and memory is kept available for new pods.</p><p>To edit the Deployment, modify the <code>resources</code> section in the Deployment manifest file
to set appropriate requests and limits. You can download that file locally and then edit it
with whichever text editor you prefer.</p><p>You can also edit the Deployment using kubectl:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit deployment capacity-reservation
</span></span></code></pre></div><p>For example, to reserve a total of a 0.5 CPU and 1GiB of memory across 5 placeholder pods,
define the resource requests and limits for a single placeholder pod as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100m"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="set-the-desired-replica-count">Set the desired replica count</h2><h3 id="calculate-the-total-reserved-resources">Calculate the total reserved resources</h3><p>For example, with 5 replicas each reserving 0.1 CPU and 200MiB of memory:<br/>Total CPU reserved: 5 × 0.1 = 0.5 (in the Pod specification, you'll write the quantity <code>500m</code>)<br/>Total memory reserved: 5 × 200MiB = 1GiB (in the Pod specification, you'll write <code>1 Gi</code>)</p><p>To scale the Deployment, adjust the number of replicas based on your cluster's size and expected workload:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale deployment capacity-reservation --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
</span></span></code></pre></div><p>Verify the scaling:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment capacity-reservation
</span></span></code></pre></div><p>The output should reflect the updated number of replicas:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
capacity-reservation   5/5     5            5           2m
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Some autoscalers, notably <a href="/docs/concepts/cluster-administration/cluster-autoscaling/#autoscaler-karpenter">Karpenter</a>,
treat preferred affinity rules as hard rules when considering node scaling.
If you use Karpenter or another node autoscaler that uses the same heuristic,
the replica count you set here also sets a minimum node count for your cluster.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClasses</a> and how they affect pod scheduling.</li><li>Explore <a href="/docs/concepts/cluster-administration/cluster-autoscaling/">node autoscaling</a> to dynamically adjust your cluster's size based on workload demands.</li><li>Understand <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod preemption</a>, a
key mechanism for Kubernetes to handle resource contention. The same page covers <em>eviction</em>,
which is less relevant to the placeholder Pod approach, but is also a mechanism for Kubernetes
to react when resources are contended.</li></ul></div>