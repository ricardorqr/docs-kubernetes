<div class="td-content"><h1 data-pagefind-weight="10">Operating etcd clusters for Kubernetes</h1><p><p>etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p></p><p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a
<a href="/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">back up</a> plan
for the data.</p><p>You can find in-depth information about etcd in the official <a href="https://etcd.io/docs/">documentation</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>Before you follow steps in this page to deploy, manage, back up or restore etcd,
you need to understand the typical expectations for operating an etcd cluster.
Refer to the <a href="https://etcd.io/docs/">etcd documentation</a> for more context.</p><p>Key details include:</p><ul><li><p>The minimum recommended etcd versions to run in production are <code>3.4.22+</code> and <code>3.5.6+</code>.</p></li><li><p>etcd is a leader-based distributed system. Ensure that the leader
periodically send heartbeats on time to all followers to keep the cluster
stable.</p></li><li><p>You should run etcd as a cluster with an odd number of members.</p></li><li><p>Aim to ensure that no resource starvation occurs.</p><p>Performance and stability of the cluster is sensitive to network and disk
I/O. Any resource starvation can lead to heartbeat timeout, causing instability
of the cluster. An unstable etcd indicates that no leader is elected. Under
such circumstances, a cluster cannot make any changes to its current state,
which implies no new pods can be scheduled.</p></li></ul><h3 id="resource-requirements-for-etcd">Resource requirements for etcd</h3><p>Operating etcd with limited resources is suitable only for testing purposes.
For deploying in production, advanced hardware configuration is required.
Before deploying etcd in production, see
<a href="https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations">resource requirement reference</a>.</p><p>Keeping etcd clusters stable is critical to the stability of Kubernetes
clusters. Therefore, run etcd clusters on dedicated machines or isolated
environments for <a href="https://etcd.io/docs/current/op-guide/hardware/">guaranteed resource requirements</a>.</p><h3 id="tools">Tools</h3><p>Depending on which specific outcome you're working on, you will need the <code>etcdctl</code> tool or the
<code>etcdutl</code> tool (you may need both).</p><h2 id="understanding-etcdctl-and-etcdutl">Understanding etcdctl and etcdutl</h2><p><code>etcdctl</code> and <code>etcdutl</code> are command-line tools used to interact with etcd clusters, but they serve different purposes:</p><ul><li><p><code>etcdctl</code>: This is the primary command-line client for interacting with etcd over a
network. It is used for day-to-day operations such as managing keys and values,
administering the cluster, checking health, and more.</p></li><li><p><code>etcdutl</code>: This is an administration utility designed to operate directly on etcd data
files, including migrating data between etcd versions, defragmenting the database,
restoring snapshots, and validating data consistency. For network operations, <code>etcdctl</code>
should be used.</p></li></ul><p>For more information on <code>etcdutl</code>, you can refer to the <a href="https://etcd.io/docs/v3.5/op-guide/recovery/">etcd recovery documentation</a>.</p><h2 id="starting-etcd-clusters">Starting etcd clusters</h2><p>This section covers starting a single-node and multi-node etcd cluster.</p><p>This guide assumes that <code>etcd</code> is already installed.</p><h3 id="single-node-etcd-cluster">Single-node etcd cluster</h3><p>Use a single-node etcd cluster only for testing purposes.</p><ol><li><p>Run the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379 <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379
</span></span></code></pre></div></li><li><p>Start the Kubernetes API server with the flag
<code>--etcd-servers=$PRIVATE_IP:2379</code>.</p><p>Make sure <code>PRIVATE_IP</code> is set to your etcd client IP.</p></li></ol><h3 id="multi-node-etcd-cluster">Multi-node etcd cluster</h3><p>For durability and high availability, run etcd as a multi-node cluster in
production and back it up periodically. A five-member cluster is recommended
in production. For more information, see
<a href="https://etcd.io/docs/current/faq/#what-is-failure-tolerance">FAQ documentation</a>.</p><p>As you're using Kubernetes, you have the option to run etcd as a container inside
one or more Pods. The <code>kubeadm</code> tool sets up etcd
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static pods">static pods</a> by default, or
you can deploy a
<a href="/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">separate cluster</a>
and instruct kubeadm to use that etcd cluster as the control plane's backing store.</p><p>You configure an etcd cluster either by static member information or by dynamic
discovery. For more information on clustering, see
<a href="https://etcd.io/docs/current/op-guide/clustering/">etcd clustering documentation</a>.</p><p>For an example, consider a five-member etcd cluster running with the following
client URLs: <code>http://$IP1:2379</code>, <code>http://$IP2:2379</code>, <code>http://$IP3:2379</code>,
<code>http://$IP4:2379</code>, and <code>http://$IP5:2379</code>. To start a Kubernetes API server:</p><ol><li><p>Run the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379 --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379
</span></span></code></pre></div></li><li><p>Start the Kubernetes API servers with the flag
<code>--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379</code>.</p><p>Make sure the <code>IP&lt;n&gt;</code> variables are set to your client IP addresses.</p></li></ol><h3 id="multi-node-etcd-cluster-with-load-balancer">Multi-node etcd cluster with load balancer</h3><p>To run a load balancing etcd cluster:</p><ol><li>Set up an etcd cluster.</li><li>Configure a load balancer in front of the etcd cluster.
For example, let the address of the load balancer be <code>$LB</code>.</li><li>Start Kubernetes API Servers with the flag <code>--etcd-servers=$LB:2379</code>.</li></ol><h2 id="securing-etcd-clusters">Securing etcd clusters</h2><p>Access to etcd is equivalent to root permission in the cluster so ideally only
the API server should have access to it. Considering the sensitivity of the
data, it is recommended to grant permission to only those nodes that require
access to etcd clusters.</p><p>To secure etcd, either set up firewall rules or use the security features
provided by etcd. etcd security features depend on x509 Public Key
Infrastructure (PKI). To begin, establish secure communication channels by
generating a key and certificate pair. For example, use key pairs <code>peer.key</code>
and <code>peer.cert</code> for securing communication between etcd members, and
<code>client.key</code> and <code>client.cert</code> for securing communication between etcd and its
clients. See the <a href="https://github.com/coreos/etcd/tree/master/hack/tls-setup">example scripts</a>
provided by the etcd project to generate key pairs and CA files for client
authentication.</p><h3 id="securing-communication">Securing communication</h3><p>To configure etcd with secure peer communication, specify flags
<code>--peer-key-file=peer.key</code> and <code>--peer-cert-file=peer.cert</code>, and use HTTPS as
the URL schema.</p><p>Similarly, to configure etcd with secure client communication, specify flags
<code>--key=k8sclient.key</code> and <code>--cert=k8sclient.cert</code>, and use HTTPS as
the URL schema. Here is an example on a client command that uses secure
communication:</p><pre tabindex="0"><code>ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
</code></pre><h3 id="limiting-access-of-etcd-clusters">Limiting access of etcd clusters</h3><p>After configuring secure communication, restrict the access of the etcd cluster to
only the Kubernetes API servers using TLS authentication.</p><p>For example, consider key pairs <code>k8sclient.key</code> and <code>k8sclient.cert</code> that are
trusted by the CA <code>etcd.ca</code>. When etcd is configured with <code>--client-cert-auth</code>
along with TLS, it verifies the certificates from clients by using system CAs
or the CA passed in by <code>--trusted-ca-file</code> flag. Specifying flags
<code>--client-cert-auth=true</code> and <code>--trusted-ca-file=etcd.ca</code> will restrict the
access to clients with the certificate <code>k8sclient.cert</code>.</p><p>Once etcd is configured correctly, only clients with valid certificates can
access it. To give Kubernetes API servers the access, configure them with the
flags <code>--etcd-certfile=k8sclient.cert</code>, <code>--etcd-keyfile=k8sclient.key</code> and
<code>--etcd-cafile=ca.cert</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>etcd authentication is not planned for Kubernetes.</div><h2 id="replacing-a-failed-etcd-member">Replacing a failed etcd member</h2><p>etcd cluster achieves high availability by tolerating minor member failures.
However, to improve the overall health of the cluster, replace failed members
immediately. When multiple members fail, replace them one by one. Replacing a
failed member involves two steps: removing the failed member and adding a new
member.</p><p>Though etcd keeps unique member IDs internally, it is recommended to use a
unique name for each member to avoid human errors. For example, consider a
three-member etcd cluster. Let the URLs be, <code>member1=http://10.0.0.1</code>,
<code>member2=http://10.0.0.2</code>, and <code>member3=http://10.0.0.3</code>. When <code>member1</code> fails,
replace it with <code>member4=http://10.0.0.4</code>.</p><ol><li><p>Get the member ID of the failed <code>member1</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdctl --endpoints<span style="color:#666">=</span>http://10.0.0.2,http://10.0.0.3 member list
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
</span></span></span><span style="display:flex"><span><span style="color:#888">91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
</span></span></span><span style="display:flex"><span><span style="color:#888">fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</span></span></span></code></pre></div></li><li><p>Do either of the following:</p><ol><li>If each Kubernetes API server is configured to communicate with all etcd
members, remove the failed member from the <code>--etcd-servers</code> flag, then
restart each Kubernetes API server.</li><li>If each Kubernetes API server communicates with a single etcd member,
then stop the Kubernetes API server that communicates with the failed
etcd.</li></ol></li><li><p>Stop the etcd server on the broken node. It is possible that other
clients besides the Kubernetes API server are causing traffic to etcd
and it is desirable to stop all traffic to prevent writes to the data
directory.</p></li><li><p>Remove the failed member:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdctl member remove 8211f1d0f64f3269
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Removed member 8211f1d0f64f3269 from cluster
</span></span></span></code></pre></div></li><li><p>Add the new member:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdctl member add member4 --peer-urls<span style="color:#666">=</span>http://10.0.0.4:2380
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</span></span></span></code></pre></div></li><li><p>Start the newly added member on a machine with the IP <code>10.0.0.4</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_NAME</span><span style="color:#666">=</span><span style="color:#b44">"member4"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER</span><span style="color:#666">=</span><span style="color:#b44">"member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER_STATE</span><span style="color:#666">=</span>existing
</span></span><span style="display:flex"><span>etcd <span style="color:#666">[</span>flags<span style="color:#666">]</span>
</span></span></code></pre></div></li><li><p>Do either of the following:</p><ol><li>If each Kubernetes API server is configured to communicate with all etcd
members, add the newly added member to the <code>--etcd-servers</code> flag, then
restart each Kubernetes API server.</li><li>If each Kubernetes API server communicates with a single etcd member,
start the Kubernetes API server that was stopped in step 2. Then
configure Kubernetes API server clients to again route requests to the
Kubernetes API server that was stopped. This can often be done by
configuring a load balancer.</li></ol></li></ol><p>For more information on cluster reconfiguration, see
<a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd reconfiguration documentation</a>.</p><h2 id="backing-up-an-etcd-cluster">Backing up an etcd cluster</h2><p>All Kubernetes objects are stored in etcd. Periodically backing up the etcd
cluster data is important to recover Kubernetes clusters under disaster
scenarios, such as losing all control plane nodes. The snapshot file contains
all the Kubernetes state and critical information. In order to keep the
sensitive Kubernetes data safe, encrypt the snapshot files.</p><p>Backing up an etcd cluster can be accomplished in two ways: etcd built-in
snapshot and volume snapshot.</p><h3 id="built-in-snapshot">Built-in snapshot</h3><p>etcd supports built-in snapshot. A snapshot may either be created from a live
member with the <code>etcdctl snapshot save</code> command or by copying the
<code>member/snap/db</code> file from an etcd
<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">data directory</a>
that is not currently used by an etcd process. Creating the snapshot will
not affect the performance of the member.</p><p>Below is an example for creating a snapshot of the keyspace served by
<code>$ENDPOINT</code> to the file <code>snapshot.db</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints <span style="color:#b8860b">$ENDPOINT</span> snapshot save snapshot.db
</span></span></code></pre></div><p>Verify the snapshot:</p><ul class="nav nav-tabs" id="etcd-verify-snapshot" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#etcd-verify-snapshot-0" role="tab" aria-controls="etcd-verify-snapshot-0" aria-selected="true">Use etcdutl</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#etcd-verify-snapshot-1" role="tab" aria-controls="etcd-verify-snapshot-1">Use etcdctl (Deprecated)</a></li></ul><div class="tab-content" id="etcd-verify-snapshot"><div id="etcd-verify-snapshot-0" class="tab-pane show active" role="tabpanel" aria-labelledby="etcd-verify-snapshot-0"><p><p>The below example depicts the usage of the <code>etcdutl</code> tool for verifying a snapshot:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdutl --write-out<span style="color:#666">=</span>table snapshot status snapshot.db 
</span></span></code></pre></div><p>This should generate an output resembling the example provided below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">| fe01cf57 |       10 |          7 | 2.1 MB     |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span></code></pre></div></p></div><div id="etcd-verify-snapshot-1" class="tab-pane" role="tabpanel" aria-labelledby="etcd-verify-snapshot-1"><p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The usage of <code>etcdctl snapshot status</code> has been <strong>deprecated</strong> since etcd v3.5.x and is slated for removal from etcd v3.6.
It is recommended to utilize <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a> instead.</div><p>The below example depicts the usage of the <code>etcdctl</code> tool for verifying a snapshot:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span>
</span></span><span style="display:flex"><span>etcdctl --write-out<span style="color:#666">=</span>table snapshot status snapshot.db
</span></span></code></pre></div><p>This should generate an output resembling the example provided below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Deprecated: Use `etcdutl snapshot status` instead.
</span></span></span><span style="display:flex"><span><span style="color:#888"/><span>
</span></span></span><span style="display:flex"><span><span/><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">| fe01cf57 |       10 |          7 | 2.1 MB     |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span></code></pre></div></p></div></div><h3 id="volume-snapshot">Volume snapshot</h3><p>If etcd is running on a storage volume that supports backup, such as Amazon
Elastic Block Store, back up etcd data by creating a snapshot of the storage
volume.</p><h3 id="snapshot-using-etcdctl-options">Snapshot using etcdctl options</h3><p>We can also create the snapshot using various options given by etcdctl. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl -h 
</span></span></code></pre></div><p>will list various options available from etcdctl. For example, you can create a snapshot by specifying
the endpoint, certificates and key as shown below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints<span style="color:#666">=</span>https://127.0.0.1:2379 <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --cacert<span style="color:#666">=</span>&lt;trusted-ca-file&gt; --cert<span style="color:#666">=</span>&lt;cert-file&gt; --key<span style="color:#666">=</span>&lt;key-file&gt; <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  snapshot save &lt;backup-file-location&gt;
</span></span></code></pre></div><p>where <code>trusted-ca-file</code>, <code>cert-file</code> and <code>key-file</code> can be obtained from the description of the etcd Pod.</p><h2 id="scaling-out-etcd-clusters">Scaling out etcd clusters</h2><p>Scaling out etcd clusters increases availability by trading off performance.
Scaling does not increase cluster performance nor capability. A general rule
is not to scale out or in etcd clusters. Do not configure any auto scaling
groups for etcd clusters. It is strongly recommended to always run a static
five-member etcd cluster for production Kubernetes clusters at any officially
supported scale.</p><p>A reasonable scaling is to upgrade a three-member cluster to a five-member
one, when more reliability is desired. See
<a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd reconfiguration documentation</a>
for information on how to add members into an existing cluster.</p><h2 id="restoring-an-etcd-cluster">Restoring an etcd cluster</h2><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>If any API servers are running in your cluster, you should not attempt to
restore instances of etcd. Instead, follow these steps to restore etcd:</p><ul><li>stop <em>all</em> API server instances</li><li>restore state in all etcd instances</li><li>restart all API server instances</li></ul><p>The Kubernetes project also recommends restarting Kubernetes components (<code>kube-scheduler</code>,
<code>kube-controller-manager</code>, <code>kubelet</code>) to ensure that they don't rely on some
stale data. In practice the restore takes a bit of time. During the
restoration, critical components will lose leader lock and restart themselves.</p></div><p>etcd supports restoring from snapshots that are taken from an etcd process of
the <a href="https://semver.org/">major.minor</a> version. Restoring a version from a
different patch version of etcd is also supported. A restore operation is
employed to recover the data of a failed cluster.</p><p>Before starting the restore operation, a snapshot file must be present. It can
either be a snapshot file from a previous backup operation, or from a remaining
<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">data directory</a>.</p><ul class="nav nav-tabs" id="etcd-restore" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#etcd-restore-0" role="tab" aria-controls="etcd-restore-0" aria-selected="true">Use etcdutl</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#etcd-restore-1" role="tab" aria-controls="etcd-restore-1">Use etcdctl (Deprecated)</a></li></ul><div class="tab-content" id="etcd-restore"><div id="etcd-restore-0" class="tab-pane show active" role="tabpanel" aria-labelledby="etcd-restore-0"><p><p>When restoring the cluster using <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a>,
use the <code>--data-dir</code> option to specify to which folder the cluster should be restored:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdutl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</span></span></code></pre></div><p>where <code>&lt;data-dir-location&gt;</code> is a directory that will be created during the restore process.</p></p></div><div id="etcd-restore-1" class="tab-pane" role="tabpanel" aria-labelledby="etcd-restore-1"><p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The usage of <code>etcdctl</code> for restoring has been <strong>deprecated</strong> since etcd v3.5.x and is slated for removal from etcd v3.6.
It is recommended to utilize <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a> instead.</div><p>The below example depicts the usage of the <code>etcdctl</code> tool for the restore operation:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span>
</span></span><span style="display:flex"><span>etcdctl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</span></span></code></pre></div><p>If <code>&lt;data-dir-location&gt;</code> is the same folder as before, delete it and stop the etcd process before restoring the cluster.
Otherwise, change etcd configuration and restart the etcd process after restoration to have it use the new data directory:
first change <code>/etc/kubernetes/manifests/etcd.yaml</code>'s <code>volumes.hostPath.path</code> for <code>name: etcd-data</code> to <code>&lt;data-dir-location&gt;</code>,
then execute <code>kubectl -n kube-system delete pod &lt;name-of-etcd-pod&gt;</code> or <code>systemctl restart kubelet.service</code> (or both).</p></p></div></div><p>For more information and examples on restoring a cluster from a snapshot file, see
<a href="https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster">etcd disaster recovery documentation</a>.</p><p>If the access URLs of the restored cluster are changed from the previous
cluster, the Kubernetes API server must be reconfigured accordingly. In this
case, restart Kubernetes API servers with the flag
<code>--etcd-servers=$NEW_ETCD_CLUSTER</code> instead of the flag
<code>--etcd-servers=$OLD_ETCD_CLUSTER</code>. Replace <code>$NEW_ETCD_CLUSTER</code> and
<code>$OLD_ETCD_CLUSTER</code> with the respective IP addresses. If a load balancer is
used in front of an etcd cluster, you might need to update the load balancer
instead.</p><p>If the majority of etcd members have permanently failed, the etcd cluster is
considered failed. In this scenario, Kubernetes cannot make any changes to its
current state. Although the scheduled pods might continue to run, no new pods
can be scheduled. In such cases, recover the etcd cluster and potentially
reconfigure Kubernetes API servers to fix the issue.</p><h2 id="upgrading-etcd-clusters">Upgrading etcd clusters</h2><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Before you start an upgrade, back up your etcd cluster first.</div><p>For details on etcd upgrade, refer to the <a href="https://etcd.io/docs/latest/upgrades/">etcd upgrades</a> documentation.</p><h2 id="maintaining-etcd-clusters">Maintaining etcd clusters</h2><p>For more details on etcd maintenance, please refer to the <a href="https://etcd.io/docs/latest/op-guide/maintenance/">etcd maintenance</a> documentation.</p><h3 id="cluster-defragmentation">Cluster defragmentation</h3><div class="alert alert-secondary callout third-party-content" role="alert">ðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>Defragmentation is an expensive operation, so it should be executed as infrequently
as possible. On the other hand, it's also necessary to make sure any etcd member
will not exceed the storage quota. The Kubernetes project recommends that when
you perform defragmentation, you use a tool such as <a href="https://github.com/ahrtr/etcd-defrag">etcd-defrag</a>.</p><p>You can also run the defragmentation tool as a Kubernetes CronJob, to make sure that
defragmentation happens regularly. See <a href="https://github.com/ahrtr/etcd-defrag/blob/main/doc/etcd-defrag-cronjob.yaml"><code>etcd-defrag-cronjob.yaml</code></a>
for details.</p></div>