<div class="td-content"><h1 data-pagefind-weight="10">Autoscaling Workloads</h1><div class="lead">With autoscaling, you can automatically update your workloads in one way or another. This allows your cluster to react to changes in resource demand more elastically and efficiently.</div><p>In Kubernetes, you can <em>scale</em> a workload depending on the current demand of resources.
This allows your cluster to react to changes in resource demand more elastically and efficiently.</p><p>When you scale a workload, you can either increase or decrease the number of replicas managed by
the workload, or adjust the resources available to the replicas in-place.</p><p>The first approach is referred to as <em>horizontal scaling</em>, while the second is referred to as
<em>vertical scaling</em>.</p><p>There are manual and automatic ways to scale your workloads, depending on your use case.</p><h2 id="scaling-workloads-manually">Scaling workloads manually</h2><p>Kubernetes supports <em>manual scaling</em> of workloads. Horizontal scaling can be done
using the <code>kubectl</code> CLI.
For vertical scaling, you need to <em>patch</em> the resource definition of your workload.</p><p>See below for examples of both strategies.</p><ul><li><strong>Horizontal scaling</strong>: <a href="/docs/tutorials/kubernetes-basics/scale/scale-intro/">Running multiple instances of your app</a></li><li><strong>Vertical scaling</strong>: <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resizing CPU and memory resources assigned to containers</a></li></ul><h2 id="scaling-workloads-automatically">Scaling workloads automatically</h2><p>Kubernetes also supports <em>automatic scaling</em> of workloads, which is the focus of this page.</p><p>The concept of <em>Autoscaling</em> in Kubernetes refers to the ability to automatically update an
object that manages a set of Pods (for example a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>).</p><h3 id="scaling-workloads-horizontally">Scaling workloads horizontally</h3><p>In Kubernetes, you can automatically scale a workload horizontally using a <em>HorizontalPodAutoscaler</em> (HPA).</p><p>It is implemented as a Kubernetes API resource and a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>
and periodically adjusts the number of <a class="glossary-tooltip" title="Replicas are copies of pods, ensuring availability, scalability, and fault tolerance by maintaining identical instances." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-replica" target="_blank" aria-label="replicas">replicas</a>
in a workload to match observed resource utilization such as CPU or memory usage.</p><p>There is a <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough tutorial</a> of configuring a HorizontalPodAutoscaler for a Deployment.</p><h3 id="scaling-workloads-vertically">Scaling workloads vertically</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You can automatically scale a workload vertically using a <em>VerticalPodAutoscaler</em> (VPA).
Unlike the HPA, the VPA doesn't come with Kubernetes by default, but is a separate project
that can be found <a href="https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler">on GitHub</a>.</p><p>Once installed, it allows you to create <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." data-toggle="tooltip" data-placement="top" href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank" aria-label="CustomResourceDefinitions">CustomResourceDefinitions</a>
(CRDs) for your workloads which define <em>how</em> and <em>when</em> to scale the resources of the managed replicas.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You will need to have the <a href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server</a>
installed to your cluster for the VPA to work.</div><p>At the moment, the VPA can operate in four different modes:</p><table><caption style="display:none">Different modes of the VPA</caption><thead><tr><th style="text-align:left">Mode</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>Auto</code></td><td style="text-align:left">Currently <code>Recreate</code>. This might change to in-place updates in the future.</td></tr><tr><td style="text-align:left"><code>Recreate</code></td><td style="text-align:left">The VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendation</td></tr><tr><td style="text-align:left"><code>Initial</code></td><td style="text-align:left">The VPA only assigns resource requests on pod creation and never changes them later.</td></tr><tr><td style="text-align:left"><code>Off</code></td><td style="text-align:left">The VPA does not automatically change the resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.</td></tr></tbody></table><h4 id="in-place-pod-vertical-scaling">In-place pod vertical scaling</h4><div class="feature-state-notice feature-beta" title="Feature Gate: InPlacePodVerticalScaling"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>As of Kubernetes 1.34, VPA does not support resizing pods in-place,
but this integration is being worked on.
For manually resizing pods in-place, see <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources In-Place</a>.</p><h3 id="autoscaling-based-on-cluster-size">Autoscaling based on cluster size</h3><p>For workloads that need to be scaled based on the size of the cluster (for example
<code>cluster-dns</code> or other system components), you can use the
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler"><em>Cluster Proportional Autoscaler</em></a>.
Just like the VPA, it is not part of the Kubernetes core, but hosted as its
own project on GitHub.</p><p>The Cluster Proportional Autoscaler watches the number of schedulable <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a>
and cores and scales the number of replicas of the target workload accordingly.</p><p>If the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using
the <a href="https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler"><em>Cluster Proportional Vertical Autoscaler</em></a>.
The project is <strong>currently in beta</strong> and can be found on GitHub.</p><p>While the Cluster Proportional Autoscaler scales the number of replicas of a workload,
the Cluster Proportional Vertical Autoscaler adjusts the resource requests for a workload
(for example a Deployment or DaemonSet) based on the number of nodes and/or cores in the cluster.</p><h3 id="event-driven-autoscaling">Event driven Autoscaling</h3><p>It is also possible to scale workloads based on events, for example using the
<a href="https://keda.sh/"><em>Kubernetes Event Driven Autoscaler</em> (<strong>KEDA</strong>)</a>.</p><p>KEDA is a CNCF-graduated project enabling you to scale your workloads based on the number
of events to be processed, for example the amount of messages in a queue. There exists
a wide range of adapters for different event sources to choose from.</p><h3 id="autoscaling-based-on-schedules">Autoscaling based on schedules</h3><p>Another strategy for scaling your workloads is to <strong>schedule</strong> the scaling operations, for example in order to
reduce resource consumption during off-peak hours.</p><p>Similar to event driven autoscaling, such behavior can be achieved using KEDA in conjunction with
its <a href="https://keda.sh/docs/latest/scalers/cron/"><code>Cron</code> scaler</a>.
The <code>Cron</code> scaler allows you to define schedules (and time zones) for scaling your workloads in or out.</p><h2 id="scaling-cluster-infrastructure">Scaling cluster infrastructure</h2><p>If scaling workloads isn't enough to meet your needs, you can also scale your cluster infrastructure itself.</p><p>Scaling the cluster infrastructure normally means adding or removing <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a>.
Read <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a>
for more information.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about scaling horizontally<ul><li><a href="/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a></li><li><a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">HorizontalPodAutoscaler Walkthrough</a></li></ul></li><li><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources In-Place</a></li><li><a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></li><li>Learn about <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a></li></ul></div>