<div class="td-content"><h1 data-pagefind-weight="10">Horizontal Pod Autoscaling</h1><p>In Kubernetes, a <em>HorizontalPodAutoscaler</em> automatically updates a workload resource (such as
a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a> or
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a>), with the
aim of automatically scaling the workload to match demand.</p><p>Horizontal scaling means that the response to increased load is to deploy more
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>.
This is different from <em>vertical</em> scaling, which for Kubernetes would mean
assigning more resources (for example: memory or CPU) to the Pods that are already
running for the workload.</p><p>If the load decreases, and the number of Pods is above the configured minimum,
the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet,
or other similar resource) to scale back down.</p><p>Horizontal pod autoscaling does not apply to objects that can't be scaled (for example:
a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/daemonset" target="_blank" aria-label="DaemonSet">DaemonSet</a>.)</p><p>The HorizontalPodAutoscaler is implemented as a Kubernetes API resource and a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>.
The resource determines the behavior of the controller.
The horizontal pod autoscaling controller, running within the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>, periodically adjusts the
desired scale of its target (for example, a Deployment) to match observed metrics such as average
CPU utilization, average memory utilization, or any other custom metric you specify.</p><p>There is <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough example</a> of using
horizontal pod autoscaling.</p><h2 id="how-does-a-horizontalpodautoscaler-work">How does a HorizontalPodAutoscaler work?</h2><figure><div class="mermaid">graph BT
hpa[HorizontalPodAutoscaler] --&gt; scale[Scale]
subgraph rc[RC / Deployment]
scale
end
scale -.-&gt; pod1[Pod 1]
scale -.-&gt; pod2[Pod 2]
scale -.-&gt; pod3[Pod N]
classDef hpa fill:#D5A6BD,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef rc fill:#F9CB9C,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef scale fill:#B6D7A8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef pod fill:#9FC5E8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
class hpa hpa;
class rc rc;
class scale scale;
class pod1,pod2,pod3 pod</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>Figure 1. HorizontalPodAutoscaler controls the scale of a Deployment and its ReplicaSet</p><p>Kubernetes implements horizontal pod autoscaling as a control loop that runs intermittently
(it is not a continuous process). The interval is set by the
<code>--horizontal-pod-autoscaler-sync-period</code> parameter to the
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a>
(and the default interval is 15 seconds).</p><p>Once during each period, the controller manager queries the resource utilization against the
metrics specified in each HorizontalPodAutoscaler definition. The controller manager
finds the target resource defined by the <code>scaleTargetRef</code>,
then selects the pods based on the target resource's <code>.spec.selector</code> labels,
and obtains the metrics from either the resource metrics API (for per-pod resource metrics),
or the custom metrics API (for all other metrics).</p><ul><li><p>For per-pod resource metrics (like CPU), the controller fetches the metrics
from the resource metrics API for each Pod targeted by the HorizontalPodAutoscaler.
Then, if a target utilization value is set, the controller calculates the utilization
value as a percentage of the equivalent
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">resource request</a>
on the containers in each Pod. If a target raw value is set, the raw metric values are used directly.
The controller then takes the mean of the utilization or the raw value (depending on the type
of target specified) across all targeted Pods, and produces a ratio used to scale
the number of desired replicas.</p><p>Please note that if some of the Pod's containers do not have the relevant resource request set,
CPU utilization for the Pod will not be defined and the autoscaler will
not take any action for that metric. See the <a href="#algorithm-details">algorithm details</a> section below
for more information about how the autoscaling algorithm works.</p></li><li><p>For per-pod custom metrics, the controller functions similarly to per-pod resource metrics,
except that it works with raw values, not utilization values.</p></li><li><p>For object metrics and external metrics, a single metric is fetched, which describes
the object in question. This metric is compared to the target
value, to produce a ratio as above. In the <code>autoscaling/v2</code> API
version, this value can optionally be divided by the number of Pods before the
comparison is made.</p></li></ul><p>The common use for HorizontalPodAutoscaler is to configure it to fetch metrics from
<a class="glossary-tooltip" title="The aggregation layer lets you install additional Kubernetes-style APIs in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/" target="_blank" aria-label="aggregated APIs">aggregated APIs</a>
(<code>metrics.k8s.io</code>, <code>custom.metrics.k8s.io</code>, or <code>external.metrics.k8s.io</code>). The <code>metrics.k8s.io</code> API is
usually provided by an add-on named Metrics Server, which needs to be launched separately.
For more information about resource metrics, see
<a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server">Metrics Server</a>.</p><p><a href="#support-for-metrics-apis">Support for metrics APIs</a> explains the stability guarantees and support status for these
different APIs.</p><p>The HorizontalPodAutoscaler controller accesses corresponding workload resources that support scaling (such as Deployments
and StatefulSet). These resources each have a subresource named <code>scale</code>, an interface that allows you to dynamically set the
number of replicas and examine each of their current states.
For general information about subresources in the Kubernetes API, see
<a href="/docs/reference/using-api/api-concepts/">Kubernetes API Concepts</a>.</p><h3 id="algorithm-details">Algorithm details</h3><p>From the most basic perspective, the HorizontalPodAutoscaler controller
operates on the ratio between desired metric value and current metric
value:</p><div class="math">$$\begin{equation*}
desiredReplicas = ceil\left\lceil currentReplicas \times \frac{currentMetricValue}{desiredMetricValue} \right\rceil
\end{equation*}$$</div><p>For example, if the current metric value is <code>200m</code>, and the desired value
is <code>100m</code>, the number of replicas will be doubled, since
\( { 200.0 \div 100.0 } = 2.0 \).<br/>If the current value is instead <code>50m</code>, you'll halve the number of
replicas, since \( { 50.0 \div 100.0 } = 0.5 \). The control plane skips any scaling
action if the ratio is sufficiently close to 1.0 (within a
<a href="#tolerance">configurable tolerance</a>, 0.1 by default).</p><p>When a <code>targetAverageValue</code> or <code>targetAverageUtilization</code> is specified,
the <code>currentMetricValue</code> is computed by taking the average of the given
metric across all Pods in the HorizontalPodAutoscaler's scale target.</p><p>Before checking the tolerance and deciding on the final values, the control
plane also considers whether any metrics are missing, and how many Pods
are <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions"><code>Ready</code></a>.
All Pods with a deletion timestamp set (objects with a deletion timestamp are
in the process of being shut down / removed) are ignored, and all failed Pods
are discarded.</p><p>If a particular Pod is missing metrics, it is set aside for later; Pods
with missing metrics will be used to adjust the final scaling amount.</p><p>When scaling on CPU, if any pod has yet to become ready (it's still
initializing, or possibly is unhealthy) <em>or</em> the most recent metric point for
the pod was before it became ready, that pod is set aside as well.</p><p>Due to technical constraints, the HorizontalPodAutoscaler controller
cannot exactly determine the first time a pod becomes ready when
determining whether to set aside certain CPU metrics. Instead, it
considers a Pod "not yet ready" if it's unready and transitioned to
ready within a short, configurable window of time since it started.
This value is configured with the <code>--horizontal-pod-autoscaler-initial-readiness-delay</code>
command line option, and its default is 30 seconds.
Once a pod has become ready, it considers any transition to
ready to be the first if it occurred within a longer, configurable time
since it started. This value is configured with the
<code>--horizontal-pod-autoscaler-cpu-initialization-period</code> command line option,
and its default is 5 minutes.</p><p>The \( currentMetricValue \over desiredMetricValue \) base scale ratio is then
calculated, using the remaining pods not set aside or discarded from above.</p><p>If there were any missing metrics, the control plane recomputes the average more
conservatively, assuming those pods were consuming 100% of the desired
value in case of a scale down, and 0% in case of a scale up. This dampens
the magnitude of any potential scale.</p><p>Furthermore, if any not-yet-ready pods were present, and the workload would have
scaled up without factoring in missing metrics or not-yet-ready pods,
the controller conservatively assumes that the not-yet-ready pods are consuming 0%
of the desired metric, further dampening the magnitude of a scale up.</p><p>After factoring in the not-yet-ready pods and missing metrics, the
controller recalculates the usage ratio. If the new ratio reverses the scale
direction, or is within the tolerance, the controller doesn't take any scaling
action. In other cases, the new ratio is used to decide any change to the
number of Pods.</p><p>Note that the <em>original</em> value for the average utilization is reported
back via the HorizontalPodAutoscaler status, without factoring in the
not-yet-ready pods or missing metrics, even when the new usage ratio is
used.</p><p>If multiple metrics are specified in a HorizontalPodAutoscaler, this
calculation is done for each metric, and then the largest of the desired
replica counts is chosen. If any of these metrics cannot be converted
into a desired replica count (e.g. due to an error fetching the metrics
from the metrics APIs) and a scale down is suggested by the metrics which
can be fetched, scaling is skipped. This means that the HPA is still capable
of scaling up if one or more metrics give a <code>desiredReplicas</code> greater than
the current value.</p><p>Finally, right before HPA scales the target, the scale recommendation is recorded. The
controller considers all recommendations within a configurable window choosing the
highest recommendation from within that window. You can configure this value using the
<code>--horizontal-pod-autoscaler-downscale-stabilization</code> command line option, which defaults to 5 minutes.
This means that scaledowns will occur gradually, smoothing out the impact of rapidly
fluctuating metric values.</p><h2 id="pod-readiness-and-autoscaling-metrics">Pod readiness and autoscaling metrics</h2><p>The HorizontalPodAutoscaler (HPA) controller includes two command line options that influence how CPU metrics are collected from Pods during startup:</p><ol><li><code>--horizontal-pod-autoscaler-cpu-initialization-period</code> (default: 5 minutes)</li></ol><p>This defines the time window after a Pod starts during which its <strong>CPU usage is ignored</strong> unless:
- The Pod is in a <code>Ready</code> state <strong>and</strong>
- The metric sample was taken entirely during the period it was <code>Ready</code>.</p><p>This command line option helps <strong>exclude misleading high CPU usage</strong> from initializing Pods (for example: Java apps warming up) in HPA scaling decisions.</p><ol><li><code>--horizontal-pod-autoscaler-initial-readiness-delay</code> (default: 30 seconds)</li></ol><p>This defines a short delay period after a Pod starts during which the HPA controller treats Pods that are currently <code>Unready</code> as still initializing, <strong>even if they have previously transitioned to <code>Ready</code> briefly</strong>.</p><p>It is designed to:
- Avoid including Pods that rapidly fluctuate between <code>Ready</code> and <code>Unready</code> during startup.
- Ensure stability in the initial readiness signal before HPA considers their metrics valid.</p><p>You can only set these command line options cluster-wide.</p><h3 id="pod-readiness-key-behaviors">Key behaviors for pod readiness</h3><ul><li>If a Pod is <code>Ready</code> and remains <code>Ready</code>, it can be counted as contributing metrics even within the delay.</li><li>If a Pod rapidly toggles between <code>Ready</code> and <code>Unready</code>, metrics are ignored until itâ€™s considered stably <code>Ready</code>.</li></ul><h3 id="pod-readiness-good-practices">Good practice for pod readiness</h3><ul><li>Configure a <code>startupProbe</code> that doesn't pass until the high CPU usage has passed, or</li><li>Ensure your <code>readinessProbe</code> only reports <code>Ready</code> <strong>after</strong> the CPU spike subsides, using <code>initialDelaySeconds</code>.</li></ul><p>And ideally also set <code>--horizontal-pod-autoscaler-cpu-initialization-period</code> to <strong>cover the startup duration</strong>.</p><h2 id="api-object">API object</h2><p>The HorizontalPodAutoscaler is an API kind in the Kubernetes
<code>autoscaling</code> API group. The current stable version can be found in
the <code>autoscaling/v2</code> API version which includes support for scaling on
memory and custom metrics. The new fields introduced in
<code>autoscaling/v2</code> are preserved as annotations when working with
<code>autoscaling/v1</code>.</p><p>When you create a HorizontalPodAutoscaler API object, make sure the name specified is a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.
More details about the API object can be found at
<a href="/docs/reference/generated/kubernetes-api/v1.34/#horizontalpodautoscaler-v2-autoscaling">HorizontalPodAutoscaler Object</a>.</p><h2 id="flapping">Stability of workload scale</h2><p>When managing the scale of a group of replicas using the HorizontalPodAutoscaler,
it is possible that the number of replicas keeps fluctuating frequently due to the
dynamic nature of the metrics evaluated. This is sometimes referred to as <em>thrashing</em>,
or <em>flapping</em>. It's similar to the concept of <em>hysteresis</em> in cybernetics.</p><h2 id="autoscaling-during-rolling-update">Autoscaling during rolling update</h2><p>Kubernetes lets you perform a rolling update on a Deployment. In that
case, the Deployment manages the underlying ReplicaSets for you.
When you configure autoscaling for a Deployment, you bind a
HorizontalPodAutoscaler to a single Deployment. The HorizontalPodAutoscaler
manages the <code>replicas</code> field of the Deployment. The deployment controller is responsible
for setting the <code>replicas</code> of the underlying ReplicaSets so that they add up to a suitable
number during the rollout and also afterwards.</p><p>If you perform a rolling update of a StatefulSet that has an autoscaled number of
replicas, the StatefulSet directly manages its set of Pods (there is no intermediate resource
similar to ReplicaSet).</p><h2 id="support-for-resource-metrics">Support for resource metrics</h2><p>Any HPA target can be scaled based on the resource usage of the pods in the scaling target.
When defining the pod specification the resource requests like <code>cpu</code> and <code>memory</code> should
be specified. This is used to determine the resource utilization and used by the HPA controller
to scale the target up or down. To use resource utilization based scaling specify a metric source
like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Resource<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">target</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Utilization<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>With this metric the HPA controller will keep the average utilization of the pods in the scaling
target at 60%. Utilization is the ratio between the current usage of resource to the requested
resources of the pod. See <a href="#algorithm-details">Algorithm</a> for more details about how the utilization
is calculated and averaged.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Since the resource usages of all the containers are summed up the total pod utilization may not
accurately represent the individual container resource usage. This could lead to situations where
a single container might be running with high usage and the HPA will not scale out because the overall
pod usage is still within acceptable limits.</div><h3 id="container-resource-metrics">Container resource metrics</h3><div class="feature-state-notice feature-stable" title="Feature Gate: HPAContainerMetrics"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [stable]</code> (enabled by default: true)</div><p>The HorizontalPodAutoscaler API also supports a container metric source where the HPA can track the
resource usage of individual containers across a set of Pods, in order to scale the target resource.
This lets you configure scaling thresholds for the containers that matter most in a particular Pod.
For example, if you have a web application and a sidecar container that provides logging, you can scale based on the resource
use of the web application, ignoring the sidecar container and its resource use.</p><p>If you revise the target resource to have a new Pod specification with a different set of containers,
you should revise the HPA spec if that newly added container should also be used for
scaling. If the specified container in the metric source is not present or only present in a subset
of the pods then those pods are ignored and the recommendation is recalculated. See <a href="#algorithm-details">Algorithm</a>
for more details about the calculation. To use container resources for autoscaling define a metric
source as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>ContainerResource<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">containerResource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">container</span>:<span style="color:#bbb"> </span>application<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">target</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Utilization<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In the above example the HPA controller scales the target such that the average utilization of the cpu
in the <code>application</code> container of all the pods is 60%.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you change the name of a container that a HorizontalPodAutoscaler is tracking, you can
make that change in a specific order to ensure scaling remains available and effective
whilst the change is being applied. Before you update the resource that defines the container
(such as a Deployment), you should update the associated HPA to track both the new and
old container names. This way, the HPA is able to calculate a scaling recommendation
throughout the update process.</p><p>Once you have rolled out the container name change to the workload resource, tidy up by removing
the old container name from the HPA specification.</p></div><h2 id="scaling-on-custom-metrics">Scaling on custom metrics</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>(the <code>autoscaling/v2beta2</code> API version previously provided this ability as a beta feature)</p><p>Provided that you use the <code>autoscaling/v2</code> API version, you can configure a HorizontalPodAutoscaler
to scale based on a custom metric (that is not built in to Kubernetes or any Kubernetes component).
The HorizontalPodAutoscaler controller then queries for these custom metrics from the Kubernetes
API.</p><p>See <a href="#support-for-metrics-apis">Support for metrics APIs</a> for the requirements.</p><h2 id="scaling-on-multiple-metrics">Scaling on multiple metrics</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>(the <code>autoscaling/v2beta2</code> API version previously provided this ability as a beta feature)</p><p>Provided that you use the <code>autoscaling/v2</code> API version, you can specify multiple metrics for a
HorizontalPodAutoscaler to scale on. Then, the HorizontalPodAutoscaler controller evaluates each metric,
and proposes a new scale based on that metric. The HorizontalPodAutoscaler takes the maximum scale
recommended for each metric and sets the workload to that size (provided that this isn't larger than the
overall maximum that you configured).</p><h2 id="support-for-metrics-apis">Support for metrics APIs</h2><p>By default, the HorizontalPodAutoscaler controller retrieves metrics from a series of APIs.
In order for it to access these APIs, cluster administrators must ensure that:</p><ul><li><p>The <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">API aggregation layer</a> is enabled.</p></li><li><p>The corresponding APIs are registered:</p><ul><li><p>For resource metrics, this is the <code>metrics.k8s.io</code> <a href="/docs/reference/external-api/metrics.v1beta1/">API</a>,
generally provided by <a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>.
It can be launched as a cluster add-on.</p></li><li><p>For custom metrics, this is the <code>custom.metrics.k8s.io</code> <a href="/docs/reference/external-api/custom-metrics.v1beta2/">API</a>.
It's provided by "adapter" API servers provided by metrics solution vendors.
Check with your metrics pipeline to see if there is a Kubernetes metrics adapter available.</p></li><li><p>For external metrics, this is the <code>external.metrics.k8s.io</code> <a href="/docs/reference/external-api/external-metrics.v1beta1/">API</a>.
It may be provided by the custom metrics adapters provided above.</p></li></ul></li></ul><p>For more information on these different metrics paths and how they differ please see the relevant design proposals for
<a href="https://git.k8s.io/design-proposals-archive/autoscaling/hpa-v2.md">the HPA V2</a>,
<a href="https://git.k8s.io/design-proposals-archive/instrumentation/custom-metrics-api.md">custom.metrics.k8s.io</a>
and <a href="https://git.k8s.io/design-proposals-archive/instrumentation/external-metrics-api.md">external.metrics.k8s.io</a>.</p><p>For examples of how to use them see
<a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">the walkthrough for using custom metrics</a>
and <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects">the walkthrough for using external metrics</a>.</p><h2 id="configurable-scaling-behavior">Configurable scaling behavior</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>(the <code>autoscaling/v2beta2</code> API version previously provided this ability as a beta feature)</p><p>If you use the <code>v2</code> HorizontalPodAutoscaler API, you can use the <code>behavior</code> field
(see the <a href="/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec">API reference</a>)
to configure separate scale-up and scale-down behaviors.
You specify these behaviors by setting <code>scaleUp</code> and / or <code>scaleDown</code>
under the <code>behavior</code> field.</p><p>Scaling policies let you control the rate of change of replicas while scaling.
Also two settings can be used to prevent <a href="#flapping">flapping</a>: you can specify a
<em>stabilization window</em> for smoothing replica counts, and a tolerance to ignore
minor metric fluctuations below a specified threshold.</p><h3 id="scaling-policies">Scaling policies</h3><p>One or more scaling policies can be specified in the <code>behavior</code> section of the spec.
When multiple policies are specified the policy which allows the highest amount of
change is the policy which is selected by default. The following example shows this behavior
while scaling down:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">policies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><p><code>periodSeconds</code> indicates the length of time in the past for which the policy must hold true.
The maximum value that you can set for <code>periodSeconds</code> is 1800 (half an hour).
The first policy <em>(Pods)</em> allows at most 4 replicas to be scaled down in one minute. The second policy
<em>(Percent)</em> allows at most 10% of the current replicas to be scaled down in one minute.</p><p>Since by default the policy which allows the highest amount of change is selected, the second policy will
only be used when the number of pod replicas is more than 40. With 40 or less replicas, the first policy will be applied.
For instance if there are 80 replicas and the target has to be scaled down to 10 replicas
then during the first step 8 replicas will be reduced. In the next iteration when the number
of replicas is 72, 10% of the pods is 7.2 but the number is rounded up to 8. On each loop of
the autoscaler controller the number of pods to be change is re-calculated based on the number
of current replicas. When the number of replicas falls below 40 the first policy <em>(Pods)</em> is applied
and 4 replicas will be reduced at a time.</p><p>The policy selection can be changed by specifying the <code>selectPolicy</code> field for a scaling
direction. By setting the value to <code>Min</code> which would select the policy which allows the
smallest change in the replica count. Setting the value to <code>Disabled</code> completely disables
scaling in that direction.</p><h3 id="stabilization-window">Stabilization window</h3><p>The stabilization window is used to restrict the <a href="#flapping">flapping</a> of
replica count when the metrics used for scaling keep fluctuating. The autoscaling algorithm
uses this window to infer a previous desired state and avoid unwanted changes to workload
scale.</p><p>For example, in the following example snippet, a stabilization window is specified for <code>scaleDown</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">300</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>When the metrics indicate that the target should be scaled down the algorithm looks
into previously computed desired states, and uses the highest value from the specified
interval. In the above example, all desired states from the past 5 minutes will be considered.</p><p>This approximates a rolling maximum, and avoids having the scaling algorithm frequently
remove Pods only to trigger recreating an equivalent Pod just moments later.</p><h3 id="tolerance">Tolerance</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: HPAConfigurableTolerance"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>The <code>tolerance</code> field configures a threshold for metric variations, preventing the
autoscaler from scaling for changes below that value.</p><p>This tolerance is defined as the amount of variation around the desired metric value under
which no scaling will occur. For example, consider a HorizontalPodAutoscaler configured
with a target memory consumption of 100MiB and a scale-up tolerance of 5%:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleUp</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tolerance</span>:<span style="color:#bbb"> </span><span style="color:#666">0.05</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 5% tolerance for scale up</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>With this configuration, the HPA algorithm will only consider scaling up if the memory
consumption is higher than 105MiB (that is: 5% above the target).</p><p>If you don't set this field, the HPA applies the default cluster-wide tolerance of 10%. This
default can be updated for both scale-up and scale-down using the
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a>
<code>--horizontal-pod-autoscaler-tolerance</code> command line argument. (You can't use the Kubernetes API
to configure this default value.)</p><h3 id="default-behavior">Default behavior</h3><p>To use the custom scaling not all fields have to be specified. Only values which need to be
customized can be specified. These custom values are merged with default values. The default values
match the existing behavior in the HPA algorithm.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">300</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">policies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleUp</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">policies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">selectPolicy</span>:<span style="color:#bbb"> </span>Max<span style="color:#bbb">
</span></span></span></code></pre></div><p>For scaling down the stabilization window is <em>300</em> seconds (or the value of the
<code>--horizontal-pod-autoscaler-downscale-stabilization</code> command line option, if provided). There is only a single policy
for scaling down which allows a 100% of the currently running replicas to be removed which
means the scaling target can be scaled down to the minimum allowed replicas.
For scaling up there is no stabilization window. When the metrics indicate that the target should be
scaled up the target is scaled up immediately. There are 2 policies where 4 pods or a 100% of the currently
running replicas may at most be added every 15 seconds till the HPA reaches its steady state.</p><h3 id="example-change-downscale-stabilization-window">Example: change downscale stabilization window</h3><p>To provide a custom downscale stabilization window of 1 minute, the following
behavior would be added to the HPA:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="example-limit-scale-down-rate">Example: limit scale down rate</h3><p>To limit the rate at which pods are removed by the HPA to 10% per minute, the
following behavior would be added to the HPA:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">policies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>To ensure that no more than 5 Pods are removed per minute, you can add a second scale-down
policy with a fixed size of 5, and set <code>selectPolicy</code> to minimum. Setting <code>selectPolicy</code> to <code>Min</code> means
that the autoscaler chooses the policy that affects the smallest number of Pods:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">policies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">selectPolicy</span>:<span style="color:#bbb"> </span>Min<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="example-disable-scale-down">Example: disable scale down</h3><p>The <code>selectPolicy</code> value of <code>Disabled</code> turns off scaling the given direction.
So to prevent downscaling the following policy would be used:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">behavior</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleDown</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">selectPolicy</span>:<span style="color:#bbb"> </span>Disabled<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="support-for-horizontalpodautoscaler-in-kubectl">Support for HorizontalPodAutoscaler in kubectl</h2><p>HorizontalPodAutoscaler, like every API resource, is supported in a standard way by <code>kubectl</code>.
You can create a new autoscaler using <code>kubectl create</code> command.
You can list autoscalers by <code>kubectl get hpa</code> or get detailed description by <code>kubectl describe hpa</code>.
Finally, you can delete an autoscaler using <code>kubectl delete hpa</code>.</p><p>In addition, there is a special <code>kubectl autoscale</code> command for creating a HorizontalPodAutoscaler object.
For instance, executing <code>kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80</code>
will create an autoscaler for ReplicaSet <em>foo</em>, with target CPU utilization set to <code>80%</code>
and the number of replicas between 2 and 5.</p><h2 id="implicit-maintenance-mode-deactivation">Implicit maintenance-mode deactivation</h2><p>You can implicitly deactivate the HPA for a target without the
need to change the HPA configuration itself. If the target's desired replica count
is set to 0, and the HPA's minimum replica count is greater than 0, the HPA
stops adjusting the target (and sets the <code>ScalingActive</code> Condition on itself
to <code>false</code>) until you reactivate it by manually adjusting the target's desired
replica count or HPA's minimum replica count.</p><h3 id="migrating-deployments-and-statefulsets-to-horizontal-autoscaling">Migrating Deployments and StatefulSets to horizontal autoscaling</h3><p>When an HPA is enabled, it is recommended that the value of <code>spec.replicas</code> of
the Deployment and / or StatefulSet be removed from their
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-manifest" target="_blank" aria-label="manifest(s)">manifest(s)</a>. If this isn't done, any time
a change to that object is applied, for example via <code>kubectl apply -f deployment.yaml</code>, this will instruct Kubernetes to scale the current number of Pods
to the value of the <code>spec.replicas</code> key. This may not be
desired and could be troublesome when an HPA is active, resulting in thrashing or flapping behavior.</p><p>Keep in mind that the removal of <code>spec.replicas</code> may incur a one-time
degradation of Pod counts as the default value of this key is 1 (reference
<a href="/docs/concepts/workloads/controllers/deployment/#replicas">Deployment Replicas</a>).
Upon the update, all Pods except 1 will begin their termination procedures. Any
deployment application afterwards will behave as normal and respect a rolling
update configuration as desired. You can avoid this degradation by choosing one of the following two
methods based on how you are modifying your deployments:</p><ul class="nav nav-tabs" id="fix-replicas-instructions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#fix-replicas-instructions-0" role="tab" aria-controls="fix-replicas-instructions-0" aria-selected="true">Client Side Apply (this is the default)</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#fix-replicas-instructions-1" role="tab" aria-controls="fix-replicas-instructions-1">Server Side Apply</a></li></ul><div class="tab-content" id="fix-replicas-instructions"><div id="fix-replicas-instructions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="fix-replicas-instructions-0"><p><ol><li><code>kubectl apply edit-last-applied deployment/&lt;deployment_name&gt;</code></li><li>In the editor, remove <code>spec.replicas</code>. When you save and exit the editor, <code>kubectl</code>
applies the update. No changes to Pod counts happen at this step.</li><li>You can now remove <code>spec.replicas</code> from the manifest. If you use source code management,
also commit your changes or take whatever other steps for revising the source code
are appropriate for how you track updates.</li><li>From here on out you can run <code>kubectl apply -f deployment.yaml</code></li></ol></p></div><div id="fix-replicas-instructions-1" class="tab-pane" role="tabpanel" aria-labelledby="fix-replicas-instructions-1"><p><p>When using the <a href="/docs/reference/using-api/server-side-apply/">Server-Side Apply</a>
you can follow the <a href="/docs/reference/using-api/server-side-apply/#transferring-ownership">transferring ownership</a>
guidelines, which cover this exact use case.</p></p></div></div><h2 id="what-s-next">What's next</h2><p>If you configure autoscaling in your cluster, you may also want to consider using
<a href="/docs/concepts/cluster-administration/node-autoscaling/">node autoscaling</a>
to ensure you are running the right number of nodes.</p><p>For more information on HorizontalPodAutoscaler:</p><ul><li>Read a <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough example</a> for horizontal pod autoscaling.</li><li>Read documentation for <a href="/docs/reference/generated/kubectl/kubectl-commands/#autoscale"><code>kubectl autoscale</code></a>.</li><li>If you would like to write your own custom metrics adapter, check out the
<a href="https://github.com/kubernetes-sigs/custom-metrics-apiserver">boilerplate</a> to get started.</li><li>Read the <a href="/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/">API reference</a> for HorizontalPodAutoscaler.</li></ul></div>