<div class="td-content"><h1 data-pagefind-weight="10">Running in multiple zones</h1><p>This page describes running Kubernetes across multiple zones.</p><h2 id="background">Background</h2><p>Kubernetes is designed so that a single Kubernetes cluster can run
across multiple failure zones, typically where these zones fit within
a logical grouping called a <em>region</em>. Major cloud providers define a region
as a set of failure zones (also called <em>availability zones</em>) that provide
a consistent set of features: within a region, each zone offers the same
APIs and services.</p><p>Typical cloud architectures aim to minimize the chance that a failure in
one zone also impairs services in another zone.</p><h2 id="control-plane-behavior">Control plane behavior</h2><p>All <a href="/docs/concepts/architecture/#control-plane-components">control plane components</a>
support running as a pool of interchangeable resources, replicated per
component.</p><p>When you deploy a cluster control plane, place replicas of
control plane components across multiple failure zones. If availability is
an important concern, select at least three failure zones and replicate
each individual control plane component (API server, scheduler, etcd,
cluster controller manager) across at least three failure zones.
If you are running a cloud controller manager then you should
also replicate this across all the failure zones you selected.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes does not provide cross-zone resilience for the API server
endpoints. You can use various techniques to improve availability for
the cluster API server, including DNS round-robin, SRV records, or
a third-party load balancing solution with health checking.</div><h2 id="node-behavior">Node behavior</h2><p>Kubernetes automatically spreads the Pods for
workload resources (such as <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>
or <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a>)
across different nodes in a cluster. This spreading helps
reduce the impact of failures.</p><p>When nodes start up, the kubelet on each node automatically adds
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="labels">labels</a> to the Node object
that represents that specific kubelet in the Kubernetes API.
These labels can include
<a href="/docs/reference/labels-annotations-taints/#topologykubernetesiozone">zone information</a>.</p><p>If your cluster spans multiple zones or regions, you can use node labels
in conjunction with
<a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a>
to control how Pods are spread across your cluster among fault domains:
regions, zones, and even specific nodes.
These hints enable the
<a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="scheduler">scheduler</a> to place
Pods for better expected availability, reducing the risk that a correlated
failure affects your whole workload.</p><p>For example, you can set a constraint to make sure that the
3 replicas of a StatefulSet are all running in different zones to each
other, whenever that is feasible. You can define this declaratively
without explicitly defining which availability zones are in use for
each workload.</p><h3 id="distributing-nodes-across-zones">Distributing nodes across zones</h3><p>Kubernetes' core does not create nodes for you; you need to do that yourself,
or use a tool such as the <a href="https://cluster-api.sigs.k8s.io/">Cluster API</a> to
manage nodes on your behalf.</p><p>Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.</p><h2 id="manual-zone-assignment-for-pods">Manual zone assignment for Pods</h2><p>You can apply <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">node selector constraints</a>
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.</p><h2 id="storage-access-for-zones">Storage access for zones</h2><p>When persistent volumes are created, Kubernetes automatically adds zone labels
to any PersistentVolumes that are linked to a specific zone.
The <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="scheduler">scheduler</a> then ensures,
through its <code>NoVolumeZoneConflict</code> predicate, that pods which claim a given PersistentVolume
are only placed into the same zone as that volume.</p><p>Please note that the method of adding zone labels can depend on your
cloud provider and the storage provisioner youâ€™re using. Always refer to the specific
documentation for your environment to ensure correct configuration.</p><p>You can specify a <a class="glossary-tooltip" title="A StorageClass provides a way for administrators to describe different available storage types." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/storage-classes" target="_blank" aria-label="StorageClass">StorageClass</a>
for PersistentVolumeClaims that specifies the failure domains (zones) that the
storage in that class may use.
To learn about configuring a StorageClass that is aware of failure domains or zones,
see <a href="/docs/concepts/storage/storage-classes/#allowed-topologies">Allowed topologies</a>.</p><h2 id="networking">Networking</h2><p>By itself, Kubernetes does not include zone-aware networking. You can use a
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a>
to configure cluster networking, and that network solution might have zone-specific
elements. For example, if your cloud provider supports Services with
<code>type=LoadBalancer</code>, the load balancer might only send traffic to Pods running in the
same zone as the load balancer element processing a given connection.
Check your cloud provider's documentation for details.</p><p>For custom or on-premises deployments, similar considerations apply.
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> and
<a class="glossary-tooltip" title="An API object that manages external access to the services in a cluster, typically HTTP." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/ingress/" target="_blank" aria-label="Ingress">Ingress</a> behavior, including handling
of different failure zones, does vary depending on exactly how your cluster is set up.</p><h2 id="fault-recovery">Fault recovery</h2><p>When you set up your cluster, you might also need to consider whether and how
your setup can restore service if all the failure zones in a region go
off-line at the same time. For example, do you rely on there being at least
one node able to run Pods in a zone?<br/>Make sure that any cluster-critical repair work does not rely
on there being at least one healthy node in your cluster. For example: if all nodes
are unhealthy, you might need to run a repair Job with a special
<a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="toleration">toleration</a> so that the repair
can complete enough to bring at least one node into service.</p><p>Kubernetes doesn't come with an answer for this challenge; however, it's
something to consider.</p><h2 id="what-s-next">What's next</h2><p>To learn how the scheduler places Pods in a cluster, honoring the configured constraints,
visit <a href="/docs/concepts/scheduling-eviction/">Scheduling and Eviction</a>.</p></div>