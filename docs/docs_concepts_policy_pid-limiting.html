<div class="td-content"><h1 data-pagefind-weight="10">Process ID Limits And Reservations</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>Kubernetes allow you to limit the number of process IDs (PIDs) that a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a> can use.
You can also reserve a number of allocatable PIDs for each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a>
for use by the operating system and daemons (rather than by Pods).</p><p>Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the
task limit without hitting any other resource limits, which can then cause
instability to a host machine.</p><p>Cluster administrators require mechanisms to ensure that Pods running in the
cluster cannot induce PID exhaustion that prevents host daemons (such as the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> or
<a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a>,
and potentially also the container runtime) from running.
In addition, it is important to ensure that PIDs are limited among Pods in order
to ensure they have limited impact on other workloads on the same node.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>On certain Linux installations, the operating system sets the PIDs limit to a low default,
such as <code>32768</code>. Consider raising the value of <code>/proc/sys/kernel/pid_max</code>.</div><p>You can configure a kubelet to limit the number of PIDs a given Pod can consume.
For example, if your node's host OS is set to use a maximum of <code>262144</code> PIDs and
expect to host less than <code>250</code> Pods, one can give each Pod a budget of <code>1000</code>
PIDs to prevent using up that node's overall number of available PIDs. If the
admin wants to overcommit PIDs similar to CPU or memory, they may do so as well
with some additional risks. Either way, a single Pod will not be able to bring
the whole machine down. This kind of resource limiting helps to prevent simple
fork bombs from affecting operation of an entire cluster.</p><p>Per-Pod PID limiting allows administrators to protect one Pod from another, but
does not ensure that all Pods scheduled onto that host are unable to impact the node overall.
Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.</p><p>You can also reserve an amount of PIDs for node overhead, separate from the
allocation to Pods. This is similar to how you can reserve CPU, memory, or other
resources for use by the operating system and other facilities outside of Pods
and their containers.</p><p>PID limiting is an important sibling to <a href="/docs/concepts/configuration/manage-resources-containers/">compute
resource</a> requests
and limits. However, you specify it in a different way: rather than defining a
Pod's resource limit in the <code>.spec</code> for a Pod, you configure the limit as a
setting on the kubelet. Pod-defined PID limits are not currently supported.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>This means that the limit that applies to a Pod may be different depending on
where the Pod is scheduled. To make things simple, it's easiest if all Nodes use
the same PID resource limits and reservations.</div><h2 id="node-pid-limits">Node PID limits</h2><p>Kubernetes allows you to reserve a number of process IDs for the system use. To
configure the reservation, use the parameter <code>pid=&lt;number&gt;</code> in the
<code>--system-reserved</code> and <code>--kube-reserved</code> command line options to the kubelet.
The value you specified declares that the specified number of process IDs will
be reserved for the system as a whole and for Kubernetes system daemons
respectively.</p><h2 id="pod-pid-limits">Pod PID limits</h2><p>Kubernetes allows you to limit the number of processes running in a Pod. You
specify this limit at the node level, rather than configuring it as a resource
limit for a particular Pod. Each Node can have a different PID limit.<br/>To configure the limit, you can specify the command line parameter <code>--pod-max-pids</code>
to the kubelet, or set <code>PodPidsLimit</code> in the kubelet
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">configuration file</a>.</p><h2 id="pid-based-eviction">PID based eviction</h2><p>You can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.
This feature is called eviction. You can
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Configure Out of Resource Handling</a>
for various eviction signals.
Use <code>pid.available</code> eviction signal to configure the threshold for number of PIDs used by Pod.
You can set soft and hard eviction policies.
However, even with the hard eviction policy, if the number of PIDs growing very fast,
node can still get into unstable state by hitting the node PIDs limit.
Eviction signal value is calculated periodically and does NOT enforce the limit.</p><p>PID limiting - per Pod and per Node sets the hard limit.
Once the limit is hit, workload will start experiencing failures when trying to get a new PID.
It may or may not lead to rescheduling of a Pod,
depending on how workload reacts on these failures and how liveness and readiness
probes are configured for the Pod. However, if limits were set correctly,
you can guarantee that other Pods workload and system processes will not run out of PIDs
when one Pod is misbehaving.</p><h2 id="what-s-next">What's next</h2><ul><li>Refer to the <a href="https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md">PID Limiting enhancement document</a> for more information.</li><li>For historical context, read
<a href="/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/">Process ID Limiting for Stability Improvements in Kubernetes 1.14</a>.</li><li>Read <a href="/docs/concepts/configuration/manage-resources-containers/">Managing Resources for Containers</a>.</li><li>Learn how to <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Configure Out of Resource Handling</a>.</li></ul></div>