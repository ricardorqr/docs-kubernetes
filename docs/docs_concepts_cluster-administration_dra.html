<div class="td-content"><h1 data-pagefind-weight="10">Good practices for Dynamic Resource Allocation as a Cluster Admin</h1><p>This page describes good practices when configuring a Kubernetes cluster
utilizing Dynamic Resource Allocation (DRA). These instructions are for cluster
administrators.</p><h2 id="separate-permissions-to-dra-related-apis">Separate permissions to DRA related APIs</h2><p>DRA is orchestrated through a number of different APIs. Use authorization tools
(like RBAC, or another solution) to control access to the right APIs depending
on the persona of your user.</p><p>In general, DeviceClasses and ResourceSlices should be restricted to admins and
the DRA drivers. Cluster operators that will be deploying Pods with claims will
need access to ResourceClaim and ResourceClaimTemplate APIs; both of these APIs
are namespace scoped.</p><h2 id="dra-driver-deployment-and-maintenance">DRA driver deployment and maintenance</h2><p>DRA drivers are third-party applications that run on each node of your cluster
to interface with the hardware of that node and Kubernetes' native DRA
components. The installation procedure depends on the driver you choose, but is
likely deployed as a DaemonSet to all or a selection of the nodes (using node
selectors or similar mechanisms) in your cluster.</p><h3 id="use-drivers-with-seamless-upgrade-if-available">Use drivers with seamless upgrade if available</h3><p>DRA drivers implement the <a href="https://pkg.go.dev/k8s.io/dynamic-resource-allocation/kubeletplugin"><code>kubeletplugin</code> package
interface</a>.
Your driver may support <em>seamless upgrades</em> by implementing a property of this
interface that allows two versions of the same DRA driver to coexist for a short
time. This is only available for kubelet versions 1.33 and above and may not be
supported by your driver for heterogeneous clusters with attached nodes running
older versions of Kubernetes - check your driver's documentation to be sure.</p><p>If seamless upgrades are available for your situation, consider using it to
minimize scheduling delays when your driver updates.</p><p>If you cannot use seamless upgrades, during driver downtime for upgrades you may
observe that:</p><ul><li>Pods cannot start unless the claims they depend on were already prepared for
use.</li><li>Cleanup after the last pod which used a claim gets delayed until the driver is
available again. The pod is not marked as terminated. This prevents reusing
the resources used by the pod for other pods.</li><li>Running pods will continue to run.</li></ul><h3 id="confirm-your-dra-driver-exposes-a-liveness-probe-and-utilize-it">Confirm your DRA driver exposes a liveness probe and utilize it</h3><p>Your DRA driver likely implements a gRPC socket for healthchecks as part of DRA
driver good practices. The easiest way to utilize this grpc socket is to
configure it as a liveness probe for the DaemonSet deploying your DRA driver.
Your driver's documentation or deployment tooling may already include this, but
if you are building your configuration separately or not running your DRA driver
as a Kubernetes pod, be sure that your orchestration tooling restarts the DRA
driver on failed healthchecks to this grpc socket. Doing so will minimize any
accidental downtime of the DRA driver and give it more opportunities to self
heal, reducing scheduling delays or troubleshooting time.</p><h3 id="when-draining-a-node-drain-the-dra-driver-as-late-as-possible">When draining a node, drain the DRA driver as late as possible</h3><p>The DRA driver is responsible for unpreparing any devices that were allocated to
Pods, and if the DRA driver is <a class="glossary-tooltip" title="Safely evicts Pods from a Node to prepare for maintenance or removal." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-drain" target="_blank" aria-label="drained">drained</a> before Pods with claims have been deleted, it will not be
able to finalize its cleanup. If you implement custom drain logic for nodes,
consider checking that there are no allocated/reserved ResourceClaim or
ResourceClaimTemplates before terminating the DRA driver itself.</p><h2 id="monitor-and-tune-components-for-higher-load-especially-in-high-scale-environments">Monitor and tune components for higher load, especially in high scale environments</h2><p>Control plane component <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a> and the internal ResourceClaim controller
orchestrated by the component <a class="glossary-tooltip" title="Control Plane component that runs controller processes." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" aria-label="kube-controller-manager">kube-controller-manager</a> do the
heavy lifting during scheduling of Pods with claims based on metadata stored in
the DRA APIs. Compared to non-DRA scheduled Pods, the number of API server
calls, memory, and CPU utilization needed by these components is increased for
Pods using DRA claims. In addition, node local components like the DRA driver
and kubelet utilize DRA APIs to allocated the hardware request at Pod sandbox
creation time. Especially in high scale environments where clusters have many
nodes, and/or deploy many workloads that heavily utilize DRA defined resource
claims, the cluster administrator should configure the relevant components to
anticipate the increased load.</p><p>The effects of mistuned components can have direct or snowballing affects
causing different symptoms during the Pod lifecycle. If the <code>kube-scheduler</code>
component's QPS and burst configurations are too low, the scheduler might
quickly identify a suitable node for a Pod but take longer to bind the Pod to
that node. With DRA, during Pod scheduling, the QPS and Burst parameters in the
client-go configuration within <code>kube-controller-manager</code> are critical.</p><p>The specific values to tune your cluster to depend on a variety of factors like
number of nodes/pods, rate of pod creation, churn, even in non-DRA environments;
see the <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">SIG Scalability README on Kubernetes scalability
thresholds</a>
for more information. In scale tests performed against a DRA enabled cluster
with 100 nodes, involving 720 long-lived pods (90% saturation) and 80 churn pods
(10% churn, 10 times), with a job creation QPS of 10, <code>kube-controller-manager</code>
QPS could be set to as low as 75 and Burst to 150 to meet equivalent metric
targets for non-DRA deployments. At this lower bound, it was observed that the
client side rate limiter was triggered enough to protect the API server from
explosive burst but was high enough that pod startup SLOs were not impacted.
While this is a good starting point, you can get a better idea of how to tune
the different components that have the biggest effect on DRA performance for
your deployment by monitoring the following metrics. For more information on all
the stable metrics in Kubernetes, see the <a href="/docs/reference/generated/metrics/">Kubernetes Metrics
Reference</a>.</p><h3 id="kube-controller-manager-metrics"><code>kube-controller-manager</code> metrics</h3><p>The following metrics look closely at the internal ResourceClaim controller
managed by the <code>kube-controller-manager</code> component.</p><ul><li>Workqueue Add Rate: Monitor <code class="code-inline language-promql"><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">workqueue_adds_total</span>{<span style="color:#a0a000">name</span><span style="color:#666">=</span>"<span style="color:#b44">resource_claim</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb"> </span></code>to gauge how quickly items are added to the ResourceClaim controller.</li><li>Workqueue Depth: Track
<code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#b8860b">workqueue_depth</span>{<span style="color:#a0a000">endpoint</span><span style="color:#666">=</span>"<span style="color:#b44">kube-controller-manager</span>",<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a0a000">name</span><span style="color:#666">=</span>"<span style="color:#b44">resource_claim</span>"}<span style="color:#666">)</span></code> to identify any backlogs in the ResourceClaim
controller.</li><li>Workqueue Work Duration: Observe <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">workqueue_work_duration_seconds_bucket</span>{<span style="color:#a0a000">name</span><span style="color:#666">=</span>"<span style="color:#b44">resource_claim</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code> to understand the speed at which the ResourceClaim controller
processes work.</li></ul><p>If you are experiencing low Workqueue Add Rate, high Workqueue Depth, and/or
high Workqueue Work Duration, this suggests the controller isn't performing
optimally. Consider tuning parameters like QPS, burst, and CPU/memory
configurations.</p><p>If you are experiencing high Workequeue Add Rate, high Workqueue Depth, but
reasonable Workqueue Work Duration, this indicates the controller is processing
work, but concurrency might be insufficient. Concurrency is hardcoded in the
controller, so as a cluster administrator, you can tune for this by reducing the
pod creation QPS, so the add rate to the resource claim workqueue is more
manageable.</p><h3 id="kube-scheduler-metrics"><code>kube-scheduler</code> metrics</h3><p>The following scheduler metrics are high level metrics aggregating performance
across all Pods scheduled, not just those using DRA. It is important to note
that the end-to-end metrics are ultimately influenced by the
<code>kube-controller-manager</code>'s performance in creating ResourceClaims from
ResourceClainTemplates in deployments that heavily use ResourceClainTemplates.</p><ul><li>Scheduler End-to-End Duration: Monitor <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">increase</span><span style="color:#666">(</span><span style="color:#b8860b">scheduler_pod_scheduling_sli_duration_seconds_bucket</span>[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li><li>Scheduler Algorithm Latency: Track <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">increase</span><span style="color:#666">(</span><span style="color:#b8860b">scheduler_scheduling_algorithm_duration_seconds_bucket</span>[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li></ul><h3 id="kubelet-metrics"><code>kubelet</code> metrics</h3><p>When a Pod bound to a node must have a ResourceClaim satisfied, kubelet calls
the <code>NodePrepareResources</code> and <code>NodeUnprepareResources</code> methods of the DRA
driver. You can observe this behavior from the kubelet's point of view with the
following metrics.</p><ul><li>Kubelet NodePrepareResources: Monitor <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">operation_name</span><span style="color:#666">=</span>"<span style="color:#b44">PrepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li><li>Kubelet NodeUnprepareResources: Track <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">operation_name</span><span style="color:#666">=</span>"<span style="color:#b44">UnprepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li></ul><h3 id="dra-kubeletplugin-operations">DRA kubeletplugin operations</h3><p>DRA drivers implement the <a href="https://pkg.go.dev/k8s.io/dynamic-resource-allocation/kubeletplugin"><code>kubeletplugin</code> package
interface</a>
which surfaces its own metric for the underlying gRPC operation
<code>NodePrepareResources</code> and <code>NodeUnprepareResources</code>. You can observe this
behavior from the point of view of the internal kubeletplugin with the following
metrics.</p><ul><li>DRA kubeletplugin gRPC NodePrepareResources operation: Observe <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_grpc_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">method_name</span><span style="color:#666">=~</span>"<span style="color:#b44">.*NodePrepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li><li>DRA kubeletplugin gRPC NodeUnprepareResources operation: Observe <code class="code-inline language-promql"><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_grpc_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">method_name</span><span style="color:#666">=~</span>"<span style="color:#b44">.*NodeUnprepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Learn more about
DRA</a></li><li>Read the <a href="/docs/reference/generated/metrics/">Kubernetes Metrics
Reference</a></li></ul></div>