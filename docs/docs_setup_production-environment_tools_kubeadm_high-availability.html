<div class="td-content"><h1 data-pagefind-weight="10">Creating Highly Available Clusters with kubeadm</h1><p>This page explains two different approaches to setting up a highly available Kubernetes
cluster using kubeadm:</p><ul><li>With stacked control plane nodes. This approach requires less infrastructure. The etcd members
and control plane nodes are co-located.</li><li>With an external etcd cluster. This approach requires more infrastructure. The
control plane nodes and etcd members are separated.</li></ul><p>Before proceeding, you should carefully consider which approach best meets the needs of your applications
and environment. <a href="/docs/setup/production-environment/tools/kubeadm/ha-topology/">Options for Highly Available topology</a>
outlines the advantages and disadvantages of each.</p><p>If you encounter issues with setting up the HA cluster, please report these
in the kubeadm <a href="https://github.com/kubernetes/kubeadm/issues/new">issue tracker</a>.</p><p>See also the <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">upgrade documentation</a>.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>This page does not address running your cluster on a cloud provider. In a cloud
environment, neither approach documented here works with Service objects of type
LoadBalancer, or with dynamic PersistentVolumes.</div><h2 id="before-you-begin">Before you begin</h2><p>The prerequisites depend on which topology you have selected for your cluster's
control plane:</p><ul class="nav nav-tabs" id="prerequisite-tabs" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#prerequisite-tabs-0" role="tab" aria-controls="prerequisite-tabs-0" aria-selected="true">Stacked etcd</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#prerequisite-tabs-1" role="tab" aria-controls="prerequisite-tabs-1">External etcd</a></li></ul><div class="tab-content" id="prerequisite-tabs"><div id="prerequisite-tabs-0" class="tab-pane show active" role="tabpanel" aria-labelledby="prerequisite-tabs-0"><p><p>You need:</p><ul><li>Three or more machines that meet <a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin">kubeadm's minimum requirements</a> for
the control-plane nodes. Having an odd number of control plane nodes can help
with leader selection in the case of machine or zone failure.<ul><li>including a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>, already set up and working</li></ul></li><li>Three or more machines that meet <a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin">kubeadm's minimum
requirements</a> for the workers<ul><li>including a container runtime, already set up and working</li></ul></li><li>Full network connectivity between all machines in the cluster (public or
private network)</li><li>Superuser privileges on all machines using <code>sudo</code><ul><li>You can use a different tool; this guide uses <code>sudo</code> in the examples.</li></ul></li><li>SSH access from one device to all nodes in the system</li><li><code>kubeadm</code> and <code>kubelet</code> already installed on all machines.</li></ul><p><em>See <a href="/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology">Stacked etcd topology</a> for context.</em></p></p></div><div id="prerequisite-tabs-1" class="tab-pane" role="tabpanel" aria-labelledby="prerequisite-tabs-1"><p><p>You need:</p><ul><li>Three or more machines that meet <a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin">kubeadm's minimum requirements</a> for
the control-plane nodes. Having an odd number of control plane nodes can help
with leader selection in the case of machine or zone failure.<ul><li>including a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>, already set up and working</li></ul></li><li>Three or more machines that meet <a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin">kubeadm's minimum
requirements</a> for the workers<ul><li>including a container runtime, already set up and working</li></ul></li><li>Full network connectivity between all machines in the cluster (public or
private network)</li><li>Superuser privileges on all machines using <code>sudo</code><ul><li>You can use a different tool; this guide uses <code>sudo</code> in the examples.</li></ul></li><li>SSH access from one device to all nodes in the system</li><li><code>kubeadm</code> and <code>kubelet</code> already installed on all machines.</li></ul><p>And you also need:</p><ul><li>Three or more additional machines, that will become etcd cluster members.
Having an odd number of members in the etcd cluster is a requirement for achieving
optimal voting quorum.<ul><li>These machines again need to have <code>kubeadm</code> and <code>kubelet</code> installed.</li><li>These machines also require a container runtime, that is already set up and working.</li></ul></li></ul><p><em>See <a href="/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology">External etcd topology</a> for context.</em></p></p></div></div><h3 id="container-images">Container images</h3><p>Each host should have access read and fetch images from the Kubernetes container image registry,
<code>registry.k8s.io</code>. If you want to deploy a highly-available cluster where the hosts do not have
access to pull images, this is possible. You must ensure by some other means that the correct
container images are already available on the relevant hosts.</p><h3 id="kubectl">Command line interface</h3><p>To manage Kubernetes once your cluster is set up, you should
<a href="/docs/tasks/tools/#kubectl">install kubectl</a> on your PC. It is also useful
to install the <code>kubectl</code> tool on each control plane node, as this can be
helpful for troubleshooting.</p><h2 id="first-steps-for-both-methods">First steps for both methods</h2><h3 id="create-load-balancer-for-kube-apiserver">Create load balancer for kube-apiserver</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There are many configurations for load balancers. The following example is only one
option. Your cluster requirements may need a different configuration.</div><ol><li><p>Create a kube-apiserver load balancer with a name that resolves to DNS.</p><ul><li><p>In a cloud environment you should place your control plane nodes behind a TCP
forwarding load balancer. This load balancer distributes traffic to all
healthy control plane nodes in its target list. The health check for
an apiserver is a TCP check on the port the kube-apiserver listens on
(default value <code>:6443</code>).</p></li><li><p>It is not recommended to use an IP address directly in a cloud environment.</p></li><li><p>The load balancer must be able to communicate with all control plane nodes
on the apiserver port. It must also allow incoming traffic on its
listening port.</p></li><li><p>Make sure the address of the load balancer always matches
the address of kubeadm's <code>ControlPlaneEndpoint</code>.</p></li><li><p>Read the <a href="https://git.k8s.io/kubeadm/docs/ha-considerations.md#options-for-software-load-balancing">Options for Software Load Balancing</a>
guide for more details.</p></li></ul></li><li><p>Add the first control plane node to the load balancer, and test the
connection:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>nc -zv -w <span style="color:#666">2</span> &lt;LOAD_BALANCER_IP&gt; &lt;PORT&gt;
</span></span></code></pre></div><p>A connection refused error is expected because the API server is not yet
running. A timeout, however, means the load balancer cannot communicate
with the control plane node. If a timeout occurs, reconfigure the load
balancer to communicate with the control plane node.</p></li><li><p>Add the remaining control plane nodes to the load balancer target group.</p></li></ol><h2 id="stacked-control-plane-and-etcd-nodes">Stacked control plane and etcd nodes</h2><h3 id="steps-for-the-first-control-plane-node">Steps for the first control plane node</h3><ol><li><p>Initialize the control plane:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>sudo kubeadm init --control-plane-endpoint <span style="color:#b44">"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"</span> --upload-certs
</span></span></code></pre></div><ul><li><p>You can use the <code>--kubernetes-version</code> flag to set the Kubernetes version to use.
It is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.</p></li><li><p>The <code>--control-plane-endpoint</code> flag should be set to the address or DNS and port of the load balancer.</p></li><li><p>The <code>--upload-certs</code> flag is used to upload the certificates that should be shared
across all the control-plane instances to the cluster. If instead, you prefer to copy certs across
control-plane nodes manually or using automation tools, please remove this flag and refer to <a href="#manual-certs">Manual
certificate distribution</a> section below.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>kubeadm init</code> flags <code>--config</code> and <code>--certificate-key</code> cannot be mixed, therefore if you want
to use the <a href="/docs/reference/config-api/kubeadm-config.v1beta4/">kubeadm configuration</a>
you must add the <code>certificateKey</code> field in the appropriate config locations
(under <code>InitConfiguration</code> and <code>JoinConfiguration: controlPlane</code>).</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Some CNI network plugins require additional configuration, for example specifying the pod IP CIDR, while others do not.
See the <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network">CNI network documentation</a>.
To add a pod CIDR pass the flag <code>--pod-network-cidr</code>, or if you are using a kubeadm configuration file
set the <code>podSubnet</code> field under the <code>networking</code> object of <code>ClusterConfiguration</code>.</div><p>The output looks similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>...
</span></span><span style="display:flex"><span>You can now join any number of control-plane node by running the following <span style="color:#a2f">command</span> on each as a root:
</span></span><span style="display:flex"><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style="display:flex"><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style="display:flex"><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><ul><li><p>Copy this output to a text file. You will need it later to join control plane and worker nodes to
the cluster.</p></li><li><p>When <code>--upload-certs</code> is used with <code>kubeadm init</code>, the certificates of the primary control plane
are encrypted and uploaded in the <code>kubeadm-certs</code> Secret.</p></li><li><p>To re-upload the certificates and generate a new decryption key, use the following command on a
control plane
node that is already joined to the cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>sudo kubeadm init phase upload-certs --upload-certs
</span></span></code></pre></div></li><li><p>You can also specify a custom <code>--certificate-key</code> during <code>init</code> that can later be used by <code>join</code>.
To generate such a key you can use the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>kubeadm certs certificate-key
</span></span></code></pre></div></li></ul><p>The certificate key is a hex encoded string that is an AES key of size 32 bytes.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>kubeadm-certs</code> Secret and the decryption key expire after two hours.</div><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>As stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!</div></li><li><p>Apply the CNI plugin of your choice:
<a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network">Follow these instructions</a>
to install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the
kubeadm configuration file (if applicable).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must pick a network plugin that suits your use case and deploy it before you move on to next step.
If you don't do this, you will not be able to launch your cluster properly.</div></li><li><p>Type the following and watch the pods of the control plane components get started:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div></li></ol><h3 id="steps-for-the-rest-of-the-control-plane-nodes">Steps for the rest of the control plane nodes</h3><p>For each additional control plane node you should:</p><ol><li><p>Execute the join command that was previously given to you by the <code>kubeadm init</code> output on the first node.
It should look something like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span></code></pre></div><ul><li>The <code>--control-plane</code> flag tells <code>kubeadm join</code> to create a new control plane.</li><li>The <code>--certificate-key ...</code> will cause the control plane certificates to be downloaded
from the <code>kubeadm-certs</code> Secret in the cluster and be decrypted using the given key.</li></ul></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run
on the first control plane node. To provide higher availability, please rebalance the CoreDNS Pods
with <code>kubectl -n kube-system rollout restart deployment coredns</code> after at least one new node is joined.</div><h2 id="external-etcd-nodes">External etcd nodes</h2><p>Setting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd
with the exception that you should setup etcd first, and you should pass the etcd information
in the kubeadm config file.</p><h3 id="set-up-the-etcd-cluster">Set up the etcd cluster</h3><ol><li><p>Follow these <a href="/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">instructions</a> to set up the etcd cluster.</p></li><li><p>Set up SSH as described <a href="#manual-certs">here</a>.</p></li><li><p>Copy the following files from any etcd node in the cluster to the first control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#666">=</span><span style="color:#b44">"ubuntu@10.0.0.7"</span>
</span></span><span style="display:flex"><span>scp /etc/kubernetes/pki/etcd/ca.crt <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>:
</span></span><span style="display:flex"><span>scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>:
</span></span><span style="display:flex"><span>scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>:
</span></span></code></pre></div><ul><li>Replace the value of <code>CONTROL_PLANE</code> with the <code>user@host</code> of the first control-plane node.</li></ul></li></ol><h3 id="set-up-the-first-control-plane-node">Set up the first control plane node</h3><ol><li><p>Create a file called <code>kubeadm-config.yaml</code> with the following contents:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kubernetesVersion</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controlPlaneEndpoint</span>:<span style="color:#bbb"> </span><span style="color:#b44">"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change this (see below)</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">etcd</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">external</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">endpoints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- https://ETCD_0_IP:2379<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change ETCD_0_IP appropriately</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- https://ETCD_1_IP:2379<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change ETCD_1_IP appropriately</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- https://ETCD_2_IP:2379<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change ETCD_2_IP appropriately</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">caFile</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/etcd/ca.crt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">certFile</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/apiserver-etcd-client.crt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">keyFile</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/apiserver-etcd-client.key<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The difference between stacked etcd and external etcd here is that the external etcd setup requires
a configuration file with the etcd endpoints under the <code>external</code> object for <code>etcd</code>.
In the case of the stacked etcd topology, this is managed automatically.</div><ul><li><p>Replace the following variables in the config template with the appropriate values for your cluster:</p><ul><li><code>LOAD_BALANCER_DNS</code></li><li><code>LOAD_BALANCER_PORT</code></li><li><code>ETCD_0_IP</code></li><li><code>ETCD_1_IP</code></li><li><code>ETCD_2_IP</code></li></ul></li></ul></li></ol><p>The following steps are similar to the stacked etcd setup:</p><ol><li><p>Run <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> on this node.</p></li><li><p>Write the output join commands that are returned to a text file for later use.</p></li><li><p>Apply the CNI plugin of your choice.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must pick a network plugin that suits your use case and deploy it before you move on to next step.
If you don't do this, you will not be able to launch your cluster properly.</div></li></ol><h3 id="steps-for-the-rest-of-the-control-plane-nodes-1">Steps for the rest of the control plane nodes</h3><p>The steps are the same as for the stacked etcd setup:</p><ul><li>Make sure the first control plane node is fully initialized.</li><li>Join each control plane node with the join command you saved to a text file. It's recommended
to join the control plane nodes one at a time.</li><li>Don't forget that the decryption key from <code>--certificate-key</code> expires after two hours, by default.</li></ul><h2 id="common-tasks-after-bootstrapping-control-plane">Common tasks after bootstrapping control plane</h2><h3 id="install-workers">Install workers</h3><p>Worker nodes can be joined to the cluster with the command you stored previously
as the output from the <code>kubeadm init</code> command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><h2 id="manual-certs">Manual certificate distribution</h2><p>If you choose to not use <code>kubeadm init</code> with the <code>--upload-certs</code> flag this means that
you are going to have to manually copy the certificates from the primary control plane node to the
joining control plane nodes.</p><p>There are many ways to do this. The following example uses <code>ssh</code> and <code>scp</code>:</p><p>SSH is required if you want to control all nodes from a single machine.</p><ol><li><p>Enable ssh-agent on your main device that has access to all other nodes in
the system:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">eval</span> <span style="color:#a2f;font-weight:700">$(</span>ssh-agent<span style="color:#a2f;font-weight:700">)</span>
</span></span></code></pre></div></li><li><p>Add your SSH identity to the session:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>ssh-add ~/.ssh/path_to_private_key
</span></span></code></pre></div></li><li><p>SSH between nodes to check that the connection is working correctly.</p><ul><li><p>When you SSH to any node, add the <code>-A</code> flag. This flag allows the node that you
have logged into via SSH to access the SSH agent on your PC. Consider alternative
methods if you do not fully trust the security of your user session on the node.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>ssh -A 10.0.0.7
</span></span></code></pre></div></li><li><p>When using sudo on any node, make sure to preserve the environment so SSH
forwarding works:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo -E -s
</span></span></code></pre></div></li></ul></li><li><p>After configuring SSH on all the nodes you should run the following script on the first
control plane node after running <code>kubeadm init</code>. This script will copy the certificates from
the first control plane node to the other control plane nodes:</p><p>In the following example, replace <code>CONTROL_PLANE_IPS</code> with the IP addresses of the
other control plane nodes.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span><span style="color:#b8860b">USER</span><span style="color:#666">=</span>ubuntu <span style="color:#080;font-style:italic"># customizable</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">CONTROL_PLANE_IPS</span><span style="color:#666">=</span><span style="color:#b44">"10.0.0.7 10.0.0.8"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">for</span> host in <span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">CONTROL_PLANE_IPS</span><span style="color:#b68;font-weight:700">}</span>; <span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/ca.crt <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/ca.key <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/sa.key <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/sa.pub <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/front-proxy-ca.key <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/etcd/ca.crt <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:etcd-ca.crt
</span></span><span style="display:flex"><span>    <span style="color:#080;font-style:italic"># Skip the next line if you are using external etcd</span>
</span></span><span style="display:flex"><span>    scp /etc/kubernetes/pki/etcd/ca.key <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>@<span style="color:#b8860b">$host</span>:etcd-ca.key
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span></code></pre></div><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Copy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates
with the required SANs for the joining control-plane instances. If you copy all the certificates by mistake,
the creation of additional nodes could fail due to a lack of required SANs.</div></li><li><p>Then on each joining control plane node you have to run the following script before running <code>kubeadm join</code>.
This script will move the previously copied certificates from the home directory to <code>/etc/kubernetes/pki</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span><span style="color:#b8860b">USER</span><span style="color:#666">=</span>ubuntu <span style="color:#080;font-style:italic"># customizable</span>
</span></span><span style="display:flex"><span>mkdir -p /etc/kubernetes/pki/etcd
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/ca.crt /etc/kubernetes/pki/
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/ca.key /etc/kubernetes/pki/
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/sa.pub /etc/kubernetes/pki/
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/sa.key /etc/kubernetes/pki/
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/front-proxy-ca.key /etc/kubernetes/pki/
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Skip the next line if you are using external etcd</span>
</span></span><span style="display:flex"><span>mv /home/<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</span></span></code></pre></div></li></ol></div>