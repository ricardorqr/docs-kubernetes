<div class="td-content"><h1 data-pagefind-weight="10">Resource Management for Windows nodes</h1><p>This page outlines the differences in how resources are managed between Linux and Windows.</p><p>On Linux nodes, <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank" aria-label="cgroups">cgroups</a> are used
as a pod boundary for resource control. Containers are created within that boundary
for network, process and file system isolation. The Linux cgroup APIs can be used to
gather CPU, I/O, and memory use statistics.</p><p>In contrast, Windows uses a <a href="https://docs.microsoft.com/windows/win32/procthread/job-objects"><em>job object</em></a> per container with a system namespace filter
to contain all processes in a container and provide logical isolation from the
host.
(Job objects are a Windows process isolation mechanism and are different from
what Kubernetes refers to as a <a class="glossary-tooltip" title="A finite or batch task that runs to completion." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/job/" target="_blank" aria-label="Job">Job</a>).</p><p>There is no way to run a Windows container without the namespace filtering in
place. This means that system privileges cannot be asserted in the context of the
host, and thus privileged containers are not available on Windows.
Containers cannot assume an identity from the host because the Security Account Manager
(SAM) is separate.</p><h2 id="resource-management-memory">Memory management</h2><p>Windows does not have an out-of-memory process killer as Linux does. Windows always
treats all user-mode memory allocations as virtual, and pagefiles are mandatory.</p><p>Windows nodes do not overcommit memory for processes. The
net effect is that Windows won't reach out of memory conditions the same way Linux
does, and processes page to disk instead of being subject to out of memory (OOM)
termination. If memory is over-provisioned and all physical memory is exhausted,
then paging can slow down performance.</p><h2 id="resource-management-cpu">CPU management</h2><p>Windows can limit the amount of CPU time allocated for different processes but cannot
guarantee a minimum amount of CPU time.</p><p>On Windows, the kubelet supports a command-line flag to set the
<a href="https://docs.microsoft.com/windows/win32/procthread/scheduling-priorities">scheduling priority</a> of the
kubelet process: <code>--windows-priorityclass</code>. This flag allows the kubelet process to get
more CPU time slices when compared to other processes running on the Windows host.
More information on the allowable values and their meaning is available at
<a href="https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class">Windows Priority Classes</a>.
To ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to <code>ABOVE_NORMAL_PRIORITY_CLASS</code> or above.</p><h2 id="resource-reservation">Resource reservation</h2><p>To account for memory and CPU used by the operating system, the container runtime, and by
Kubernetes host processes such as the kubelet, you can (and should) reserve
memory and CPU resources with the <code>--kube-reserved</code> and/or <code>--system-reserved</code> kubelet flags.
On Windows these values are only used to calculate the node's
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a> resources.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>As you deploy workloads, set resource memory and CPU limits on containers.
This also subtracts from <code>NodeAllocatable</code> and helps the cluster-wide scheduler in determining which pods to place on which nodes.</p><p>Scheduling pods without limits may over-provision the Windows nodes and in extreme
cases can cause the nodes to become unhealthy.</p></div><p>On Windows, a good practice is to reserve at least 2GiB of memory.</p><p>To determine how much CPU to reserve,
identify the maximum pod density for each node and monitor the CPU usage of
the system services running there, then choose a value that meets your workload needs.</p></div>