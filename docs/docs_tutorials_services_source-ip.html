<div class="td-content"><h1 data-pagefind-weight="10">Using Source IP</h1><p>Applications running in a Kubernetes cluster find and communicate with each
other, and the outside world, through the Service abstraction. This document
explains what happens to the source IP of packets sent to different types
of Services, and how you can toggle this behavior according to your needs.</p><h2 id="before-you-begin">Before you begin</h2><h3 id="terminology">Terminology</h3><p>This document makes use of the following terms:</p><dl><dt><a href="https://en.wikipedia.org/wiki/Network_address_translation">NAT</a></dt><dd>Network address translation</dd><dt><a href="https://en.wikipedia.org/wiki/Network_address_translation#SNAT">Source NAT</a></dt><dd>Replacing the source IP on a packet; in this page, that usually means replacing with the IP address of a node.</dd><dt><a href="https://en.wikipedia.org/wiki/Network_address_translation#DNAT">Destination NAT</a></dt><dd>Replacing the destination IP on a packet; in this page, that usually means replacing with the IP address of a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a></dd><dt><a href="/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">VIP</a></dt><dd>A virtual IP address, such as the one assigned to every <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> in Kubernetes</dd><dt><a href="/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">kube-proxy</a></dt><dd>A network daemon that orchestrates Service VIP management on every node</dd></dl><h3 id="prerequisites">Prerequisites</h3><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>The examples use a small nginx webserver that echoes back the source
IP of requests it receives through an HTTP header. You can create it as follows:</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The image in the following command only runs on AMD64 architectures.</div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create deployment source-ip-app --image<span style="color:#666">=</span>registry.k8s.io/echoserver:1.10
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>deployment.apps/source-ip-app created
</code></pre><h2 id="objectives">Objectives</h2><ul><li>Expose a simple application through various types of Services</li><li>Understand how each Service type handles source IP NAT</li><li>Understand the tradeoffs involved in preserving source IP</li></ul><h2 id="source-ip-for-services-with-type-clusterip">Source IP for Services with <code>Type=ClusterIP</code></h2><p>Packets sent to ClusterIP from within the cluster are never source NAT'd if
you're running kube-proxy in
<a href="/docs/reference/networking/virtual-ips/#proxy-mode-iptables">iptables mode</a>,
(the default). You can query the kube-proxy mode by fetching
<code>http://localhost:10249/proxyMode</code> on the node where kube-proxy is running.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl get nodes
</span></span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                           STATUS     ROLES    AGE     VERSION
kubernetes-node-6jst   Ready      &lt;none&gt;   2h      v1.13.0
kubernetes-node-cx31   Ready      &lt;none&gt;   2h      v1.13.0
kubernetes-node-jj1t   Ready      &lt;none&gt;   2h      v1.13.0
</code></pre><p>Get the proxy mode on one of the nodes (kube-proxy listens on port 10249):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this in a shell on the node you want to query.</span>
</span></span><span style="display:flex"><span>curl http://localhost:10249/proxyMode
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>iptables
</code></pre><p>You can test source IP preservation by creating a Service over the source IP app:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl expose deployment source-ip-app --name<span style="color:#666">=</span>clusterip --port<span style="color:#666">=</span><span style="color:#666">80</span> --target-port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>service/clusterip exposed
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get svc clusterip
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
clusterip    ClusterIP   10.0.170.92   &lt;none&gt;        80/TCP    51s
</code></pre><p>And hitting the <code>ClusterIP</code> from a pod in the same cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl run busybox -it --image<span style="color:#666">=</span>busybox:1.28 --restart<span style="color:#666">=</span>Never --rm
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for pod default/busybox to be running, status is Pending, pod ready: false
If you don't see a command prompt, try pressing enter.
</code></pre><p>You can then run a command inside that Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this inside the terminal from "kubectl run"</span>
</span></span><span style="display:flex"><span>ip addr
</span></span></code></pre></div><pre tabindex="0"><code>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1460 qdisc noqueue
    link/ether 0a:58:0a:f4:03:08 brd ff:ff:ff:ff:ff:ff
    inet 10.244.3.8/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::188a:84ff:feb0:26a5/64 scope link
       valid_lft forever preferred_lft forever
</code></pre><p>â€¦then use <code>wget</code> to query the local webserver</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Replace "10.0.170.92" with the IPv4 address of the Service named "clusterip"</span>
</span></span><span style="display:flex"><span>wget -qO - 10.0.170.92
</span></span></code></pre></div><pre tabindex="0"><code>CLIENT VALUES:
client_address=10.244.3.8
command=GET
...
</code></pre><p>The <code>client_address</code> is always the client pod's IP address, whether the client pod and server pod are in the same node or in different nodes.</p><h2 id="source-ip-for-services-with-type-nodeport">Source IP for Services with <code>Type=NodePort</code></h2><p>Packets sent to Services with
<a href="/docs/concepts/services-networking/service/#type-nodeport"><code>Type=NodePort</code></a>
are source NAT'd by default. You can test this by creating a <code>NodePort</code> Service:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl expose deployment source-ip-app --name<span style="color:#666">=</span>nodeport --port<span style="color:#666">=</span><span style="color:#666">80</span> --target-port<span style="color:#666">=</span><span style="color:#666">8080</span> --type<span style="color:#666">=</span>NodePort
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>service/nodeport exposed
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">NODEPORT</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl get -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">"{.spec.ports[0].nodePort}"</span> services nodeport<span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">NODES</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl get nodes -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{ $.items[*].status.addresses[?(@.type=="InternalIP")].address }'</span><span style="color:#a2f;font-weight:700">)</span>
</span></span></code></pre></div><p>If you're running on a cloud provider, you may need to open up a firewall-rule
for the <code>nodes:nodeport</code> reported above.
Now you can try reaching the Service from outside the cluster through the node
port allocated above.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">for</span> node in <span style="color:#b8860b">$NODES</span>; <span style="color:#a2f;font-weight:700">do</span> curl -s <span style="color:#b8860b">$node</span>:<span style="color:#b8860b">$NODEPORT</span> | grep -i client_address; <span style="color:#a2f;font-weight:700">done</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>client_address=10.180.1.1
client_address=10.240.0.5
client_address=10.240.0.3
</code></pre><p>Note that these are not the correct client IPs, they're cluster internal IPs. This is what happens:</p><ul><li>Client sends packet to <code>node2:nodePort</code></li><li><code>node2</code> replaces the source IP address (SNAT) in the packet with its own IP address</li><li><code>node2</code> replaces the destination IP on the packet with the pod IP</li><li>packet is routed to node 1, and then to the endpoint</li><li>the pod's reply is routed back to node2</li><li>the pod's reply is sent back to the client</li></ul><p>Visually:</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNkV9rwyAUxb-K3LysYEqS_WFYKAzat9GHdW9zDxKvi9RoMIZtlH732ZjSbE970cu5v3s86hFqJxEYfHjRNeT5ZcUtIbXRaMNN2hZ5vrYRqt52cSXV-4iMSuwkZiYtyX739EqWaahMQ-V1qPxDVLNOvkYrO6fj2dupWMR2iiT6foOKdEZoS5Q2hmVSStoH7w7IMqXUVOefWoaG3XVftHbGeZYVRbH6ZXJ47CeL2-qhxvt_ucTe1SUlpuMN6CX12XeGpLdJiaMMFFr0rdAyvvfxjHEIDbbIgcVSohKDCRy4PUV06KQIuJU6OA9MCdMjBTEEt_-2NbDgB7xAGy3i97VJPP0ABRmcqg"><img src="/docs/images/tutor-service-nodePort-fig01.svg" alt="source IP nodeport figure 01"/></a><figcaption><p>Figure. Source IP Type=NodePort using SNAT</p></figcaption></figure><p>To avoid this, Kubernetes has a feature to
<a href="/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">preserve the client source IP</a>.
If you set <code>service.spec.externalTrafficPolicy</code> to the value <code>Local</code>,
kube-proxy only proxies proxy requests to local endpoints, and does not
forward traffic to other nodes. This approach preserves the original
source IP address. If there are no local endpoints, packets sent to the
node are dropped, so you can rely on the correct source-ip in any packet
processing rules you might apply a packet that make it through to the
endpoint.</p><p>Set the <code>service.spec.externalTrafficPolicy</code> field as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch svc nodeport -p <span style="color:#b44">'{"spec":{"externalTrafficPolicy":"Local"}}'</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>service/nodeport patched
</code></pre><p>Now, re-run the test:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">for</span> node in <span style="color:#b8860b">$NODES</span>; <span style="color:#a2f;font-weight:700">do</span> curl --connect-timeout <span style="color:#666">1</span> -s <span style="color:#b8860b">$node</span>:<span style="color:#b8860b">$NODEPORT</span> | grep -i client_address; <span style="color:#a2f;font-weight:700">done</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>client_address=198.51.100.79
</code></pre><p>Note that you only got one reply, with the <em>right</em> client IP, from the one node on which the endpoint pod
is running.</p><p>This is what happens:</p><ul><li>client sends packet to <code>node2:nodePort</code>, which doesn't have any endpoints</li><li>packet is dropped</li><li>client sends packet to <code>node1:nodePort</code>, which <em>does</em> have endpoints</li><li>node1 routes packet to endpoint with the correct source IP</li></ul><p>Visually:</p><figure class="diagram-large"><img src="/docs/images/tutor-service-nodePort-fig02.svg" alt="source IP nodeport figure 02"/><figcaption><p>Figure. Source IP Type=NodePort preserves client source IP address</p></figcaption></figure><h2 id="source-ip-for-services-with-type-loadbalancer">Source IP for Services with <code>Type=LoadBalancer</code></h2><p>Packets sent to Services with
<a href="/docs/concepts/services-networking/service/#loadbalancer"><code>Type=LoadBalancer</code></a>
are source NAT'd by default, because all schedulable Kubernetes nodes in the
<code>Ready</code> state are eligible for load-balanced traffic. So if packets arrive
at a node without an endpoint, the system proxies it to a node <em>with</em> an
endpoint, replacing the source IP on the packet with the IP of the node (as
described in the previous section).</p><p>You can test this by exposing the source-ip-app through a load balancer:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl expose deployment source-ip-app --name<span style="color:#666">=</span>loadbalancer --port<span style="color:#666">=</span><span style="color:#666">80</span> --target-port<span style="color:#666">=</span><span style="color:#666">8080</span> --type<span style="color:#666">=</span>LoadBalancer
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>service/loadbalancer exposed
</code></pre><p>Print out the IP addresses of the Service:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl get svc loadbalancer
</span></span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME           TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)   AGE
loadbalancer   LoadBalancer   10.0.65.118   203.0.113.140     80/TCP    5m
</code></pre><p>Next, send a request to this Service's external-ip:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl 203.0.113.140
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>CLIENT VALUES:
client_address=10.240.0.5
...
</code></pre><p>However, if you're running on Google Kubernetes Engine/GCE, setting the same <code>service.spec.externalTrafficPolicy</code>
field to <code>Local</code> forces nodes <em>without</em> Service endpoints to remove
themselves from the list of nodes eligible for loadbalanced traffic by
deliberately failing health checks.</p><p>Visually:</p><p><img alt="Source IP with externalTrafficPolicy" src="/images/docs/sourceip-externaltrafficpolicy.svg"/></p><p>You can test this by setting the annotation:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch svc loadbalancer -p <span style="color:#b44">'{"spec":{"externalTrafficPolicy":"Local"}}'</span>
</span></span></code></pre></div><p>You should immediately see the <code>service.spec.healthCheckNodePort</code> field allocated
by Kubernetes:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get svc loadbalancer -o yaml | grep -i healthCheckNodePort
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">healthCheckNodePort</span>:<span style="color:#bbb"> </span><span style="color:#666">32122</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>service.spec.healthCheckNodePort</code> field points to a port on every node
serving the health check at <code>/healthz</code>. You can test this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod -o wide -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>source-ip-app
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                            READY     STATUS    RESTARTS   AGE       IP             NODE
source-ip-app-826191075-qehz4   1/1       Running   0          20h       10.180.1.136   kubernetes-node-6jst
</code></pre><p>Use <code>curl</code> to fetch the <code>/healthz</code> endpoint on various nodes:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this locally on a node you choose</span>
</span></span><span style="display:flex"><span>curl localhost:32122/healthz
</span></span></code></pre></div><pre tabindex="0"><code>1 Service Endpoints found
</code></pre><p>On a different node you might get a different result:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this locally on a node you choose</span>
</span></span><span style="display:flex"><span>curl localhost:32122/healthz
</span></span></code></pre></div><pre tabindex="0"><code>No Service Endpoints Found
</code></pre><p>A controller running on the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> is
responsible for allocating the cloud load balancer. The same controller also
allocates HTTP health checks pointing to this port/path on each node. Wait
about 10 seconds for the 2 nodes without endpoints to fail health checks,
then use <code>curl</code> to query the IPv4 address of the load balancer:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl 203.0.113.140
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>CLIENT VALUES:
client_address=198.51.100.79
...
</code></pre><h2 id="cross-platform-support">Cross-platform support</h2><p>Only some cloud providers offer support for source IP preservation through
Services with <code>Type=LoadBalancer</code>.
The cloud provider you're running on might fulfill the request for a loadbalancer
in a few different ways:</p><ol><li><p>With a proxy that terminates the client connection and opens a new connection
to your nodes/endpoints. In such cases the source IP will always be that of the
cloud LB, not that of the client.</p></li><li><p>With a packet forwarder, such that requests from the client sent to the
loadbalancer VIP end up at the node with the source IP of the client, not
an intermediate proxy.</p></li></ol><p>Load balancers in the first category must use an agreed upon
protocol between the loadbalancer and backend to communicate the true client IP
such as the HTTP <a href="https://tools.ietf.org/html/rfc7239#section-5.2">Forwarded</a>
or <a href="https://en.wikipedia.org/wiki/X-Forwarded-For">X-FORWARDED-FOR</a>
headers, or the
<a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">proxy protocol</a>.
Load balancers in the second category can leverage the feature described above
by creating an HTTP health check pointing at the port stored in
the <code>service.spec.healthCheckNodePort</code> field on the Service.</p><h2 id="cleaning-up">Cleaning up</h2><p>Delete the Services:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete svc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>source-ip-app
</span></span></code></pre></div><p>Delete the Deployment, ReplicaSet and Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment source-ip-app
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tutorials/services/connect-applications-service/">connecting applications via services</a></li><li>Read how to <a href="/docs/tasks/access-application-cluster/create-external-load-balancer/">Create an External Load Balancer</a></li></ul></div>