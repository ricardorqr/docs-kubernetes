<div><div class="td-content"><p>This section of the Kubernetes documentation contains pages that
show how to do individual tasks. A task page shows how to do a
single thing, typically by giving a short sequence of steps.</p><p>If you would like to write a task page, see
<a href="/docs/contribute/new-content/open-a-pr/">Creating a Documentation Pull Request</a>.</p><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/tools/">Install Tools</a></h5><p>Set up Kubernetes tools on your computer.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/">Administer a Cluster</a></h5><p>Learn common tasks for administering a cluster.</p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/">Configure Pods and Containers</a></h5><p>Perform common configuration tasks for Pods and containers.</p></div><div class="entry"><h5><a href="/docs/tasks/debug/">Monitoring, Logging, and Debugging</a></h5><p>Set up monitoring and logging to troubleshoot a cluster, or debug a containerized application.</p></div><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/">Manage Kubernetes Objects</a></h5><p>Declarative and imperative paradigms for interacting with the Kubernetes API.</p></div><div class="entry"><h5><a href="/docs/tasks/configmap-secret/">Managing Secrets</a></h5><p>Managing confidential settings data using Secrets.</p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/">Inject Data Into Applications</a></h5><p>Specify configuration and other data for the Pods that run your workload.</p></div><div class="entry"><h5><a href="/docs/tasks/run-application/">Run Applications</a></h5><p>Run and manage both stateless and stateful applications.</p></div><div class="entry"><h5><a href="/docs/tasks/job/">Run Jobs</a></h5><p>Run Jobs using parallel processing.</p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/">Access Applications in a Cluster</a></h5><p>Configure load balancing, port forwarding, or setup firewall or DNS configurations to access applications in a cluster.</p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/">Extend Kubernetes</a></h5><p>Understand advanced ways to adapt your Kubernetes cluster to the needs of your work environment.</p></div><div class="entry"><h5><a href="/docs/tasks/tls/">TLS</a></h5><p>Understand how to protect traffic within your cluster using Transport Layer Security (TLS).</p></div><div class="entry"><h5><a href="/docs/tasks/manage-daemon/">Manage Cluster Daemons</a></h5><p>Perform common tasks for managing a DaemonSet, such as performing a rolling update.</p></div><div class="entry"><h5><a href="/docs/tasks/network/">Networking</a></h5><p>Learn how to configure networking for your cluster.</p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubectl/kubectl-plugins/">Extend kubectl with plugins</a></h5><p>Extend kubectl by creating and installing kubectl plugins.</p></div><div class="entry"><h5><a href="/docs/tasks/manage-hugepages/scheduling-hugepages/">Manage HugePages</a></h5><p>Configure and manage huge pages as a schedulable resource in a cluster.</p></div><div class="entry"><h5><a href="/docs/tasks/manage-gpus/scheduling-gpus/">Schedule GPUs</a></h5><p>Configure and schedule GPUs for use as a resource by nodes in a cluster.</p></div></div></div></div><div><div class="td-content"><h1>Install Tools</h1><div class="lead">Set up Kubernetes tools on your computer.</div><h2 id="kubectl">kubectl</h2><p>The Kubernetes command-line tool, <a href="/docs/reference/kubectl/kubectl/">kubectl</a>, allows
you to run commands against Kubernetes clusters.
You can use kubectl to deploy applications, inspect and manage cluster resources,
and view logs. For more information including a complete list of kubectl operations, see the
<a href="/docs/reference/kubectl/"><code>kubectl</code> reference documentation</a>.</p><p>kubectl is installable on a variety of Linux platforms, macOS and Windows.
Find your preferred operating system below.</p><ul><li><a href="/docs/tasks/tools/install-kubectl-linux/">Install kubectl on Linux</a></li><li><a href="/docs/tasks/tools/install-kubectl-macos/">Install kubectl on macOS</a></li><li><a href="/docs/tasks/tools/install-kubectl-windows/">Install kubectl on Windows</a></li></ul><h2 id="kind">kind</h2><p><a href="https://kind.sigs.k8s.io/"><code>kind</code></a> lets you run Kubernetes on
your local computer. This tool requires that you have either
<a href="https://www.docker.com/">Docker</a> or <a href="https://podman.io/">Podman</a> installed.</p><p>The kind <a href="https://kind.sigs.k8s.io/docs/user/quick-start/">Quick Start</a> page
shows you what you need to do to get up and running with kind.</p><p><a class="btn btn-primary" href="https://kind.sigs.k8s.io/docs/user/quick-start/">View kind Quick Start Guide</a></p><h2 id="minikube">minikube</h2><p>Like <code>kind</code>, <a href="https://minikube.sigs.k8s.io/"><code>minikube</code></a> is a tool that lets you run Kubernetes
locally. <code>minikube</code> runs an all-in-one or a multi-node local Kubernetes cluster on your personal
computer (including Windows, macOS and Linux PCs) so that you can try out
Kubernetes, or for daily development work.</p><p>You can follow the official
<a href="https://minikube.sigs.k8s.io/docs/start/">Get Started!</a> guide if your focus is
on getting the tool installed.</p><p><a class="btn btn-primary" href="https://minikube.sigs.k8s.io/docs/start/">View minikube Get Started! Guide</a></p><p>Once you have <code>minikube</code> working, you can use it to
<a href="/docs/tutorials/hello-minikube/">run a sample application</a>.</p><h2 id="kubeadm">kubeadm</h2><p>You can use the <a class="glossary-tooltip" title="A tool for quickly installing Kubernetes and setting up a secure cluster." href="/docs/reference/setup-tools/kubeadm/" target="_blank">kubeadm</a> tool to create and manage Kubernetes clusters.
It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.</p><p><a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installing kubeadm</a> shows you how to install kubeadm.
Once installed, you can use it to <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">create a cluster</a>.</p><p><a class="btn btn-primary" href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">View kubeadm Install Guide</a></p><div class="section-index"></div></div></div><div><div class="td-content"><h1>Install and Set Up kubectl on Linux</h1><h2 id="before-you-begin">Before you begin</h2><p>You must use a kubectl version that is within one minor version difference of
your cluster. For example, a v1.34 client can communicate
with v1.33, v1.34,
and v1.35 control planes.
Using the latest compatible version of kubectl helps avoid unforeseen issues.</p><h2 id="install-kubectl-on-linux">Install kubectl on Linux</h2><p>The following methods exist for installing kubectl on Linux:</p><ul><li><a href="#install-kubectl-binary-with-curl-on-linux">Install kubectl binary with curl on Linux</a></li><li><a href="#install-using-native-package-management">Install using native package management</a></li><li><a href="#install-using-other-package-management">Install using other package management</a></li></ul><h3 id="install-kubectl-binary-with-curl-on-linux">Install kubectl binary with curl on Linux</h3><ol><li><p>Download the latest release with the command:</p><ul class="nav nav-tabs" id="download-binary-linux"><li class="nav-item"><a class="nav-link active" href="#download-binary-linux-0">x86-64</a></li><li class="nav-item"><a class="nav-link" href="#download-binary-linux-1">ARM64</a></li></ul><div class="tab-content" id="download-binary-linux"><div id="download-binary-linux-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/amd64/kubectl"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-binary-linux-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/arm64/kubectl"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>To download a specific version, replace the <code>$(curl -L -s https://dl.k8s.io/release/stable.txt)</code>
portion of the command with the specific version.</p><p>For example, to download version 1.34.0 on Linux x86-64, type:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>curl -LO https://dl.k8s.io/release/v1.34.0/bin/linux/amd64/kubectl
</span></span></code></pre></div><p>And for Linux ARM64, type:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>curl -LO https://dl.k8s.io/release/v1.34.0/bin/linux/arm64/kubectl
</span></span></code></pre></div></div></li><li><p>Validate the binary (optional)</p><p>Download the kubectl checksum file:</p><ul class="nav nav-tabs" id="download-checksum-linux"><li class="nav-item"><a class="nav-link active" href="#download-checksum-linux-0">x86-64</a></li><li class="nav-item"><a class="nav-link" href="#download-checksum-linux-1">ARM64</a></li></ul><div class="tab-content" id="download-checksum-linux"><div id="download-checksum-linux-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/amd64/kubectl.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-checksum-linux-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/arm64/kubectl.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div><p>Validate the kubectl binary against the checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>"</span><span>$(</span>cat kubectl.sha256<span>)</span><span>  kubectl"</span> | sha256sum --check
</span></span></code></pre></div><p>If valid, the output is:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl: OK
</span></span></span></code></pre></div><p>If the check fails, <code>sha256</code> exits with nonzero status and prints output similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl: FAILED
</span></span></span><span><span><span>sha256sum: WARNING: 1 computed checksum did NOT match
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Download the same version of the binary and checksum.</div></li><li><p>Install kubectl</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo install -o root -g root -m <span>0755</span> kubectl /usr/local/bin/kubectl
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you do not have root access on the target system, you can still install
kubectl to the <code>~/.local/bin</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>chmod +x kubectl
</span></span><span><span>mkdir -p ~/.local/bin
</span></span><span><span>mv ./kubectl ~/.local/bin/kubectl
</span></span><span><span><span># and then append (or prepend) ~/.local/bin to $PATH</span>
</span></span></code></pre></div></div></li><li><p>Test to ensure the version you installed is up-to-date:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl version --client
</span></span></code></pre></div><p>Or use this for detailed view of version:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>kubectl version --client --output=yaml
</span></span></code></pre></div></li></ol><h3 id="install-using-native-package-management">Install using native package management</h3><ul class="nav nav-tabs" id="kubectl-install"><li class="nav-item"><a class="nav-link active" href="#kubectl-install-0">Debian-based distributions</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-install-1">Red Hat-based distributions</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-install-2">SUSE-based distributions</a></li></ul><div class="tab-content" id="kubectl-install"><div id="kubectl-install-0" class="tab-pane show active"><p><ol><li><p>Update the <code>apt</code> package index and install packages needed to use the Kubernetes <code>apt</code> repository:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo apt-get update
</span></span><span><span><span># apt-transport-https may be a dummy package; if so, you can skip that package</span>
</span></span><span><span>sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
</span></span></code></pre></div></li><li><p>Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.</span>
</span></span><span><span><span># sudo mkdir -p -m 755 /etc/apt/keyrings</span>
</span></span><span><span>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
</span></span><span><span>sudo chmod <span>644</span> /etc/apt/keyrings/kubernetes-apt-keyring.gpg <span># allow unprivileged APT programs to read this keyring</span>
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In releases older than Debian 12 and Ubuntu 22.04, folder <code>/etc/apt/keyrings</code> does not exist by default, and it should be created before the curl command.</div><ol start="3"><li><p>Add the appropriate Kubernetes <code>apt</code> repository. If you want to use Kubernetes version different than v1.34,
replace v1.34 with the desired minor version in the command below:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list</span>
</span></span><span><span><span>echo</span> <span>'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /'</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span><span><span>sudo chmod <span>644</span> /etc/apt/sources.list.d/kubernetes.list   <span># helps tools such as command-not-found to work correctly</span>
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To upgrade kubectl to another minor release, you'll need to bump the version in <code>/etc/apt/sources.list.d/kubernetes.list</code> before running <code>apt-get update</code> and <code>apt-get upgrade</code>. This procedure is described in more detail in <a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing The Kubernetes Package Repository</a>.</div><ol start="4"><li><p>Update <code>apt</code> package index, then install kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo apt-get update
</span></span><span><span>sudo apt-get install -y kubectl
</span></span></code></pre></div></li></ol></p></div><div id="kubectl-install-1" class="tab-pane"><p><ol><li><p>Add the Kubernetes <code>yum</code> repository. If you want to use Kubernetes version
different than v1.34, replace v1.34 with
the desired minor version in the command below.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo</span>
</span></span><span><span>cat <span>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span></span></span><span><span><span>[kubernetes]
</span></span></span><span><span><span>name=Kubernetes
</span></span></span><span><span><span>baseurl=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/
</span></span></span><span><span><span>enabled=1
</span></span></span><span><span><span>gpgcheck=1
</span></span></span><span><span><span>gpgkey=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/repodata/repomd.xml.key
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To upgrade kubectl to another minor release, you'll need to bump the version in <code>/etc/yum.repos.d/kubernetes.repo</code> before running <code>yum update</code>. This procedure is described in more detail in <a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing The Kubernetes Package Repository</a>.</div><ol start="2"><li><p>Install kubectl using <code>yum</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo yum install -y kubectl
</span></span></code></pre></div></li></ol></p></div><div id="kubectl-install-2" class="tab-pane"><p><ol><li><p>Add the Kubernetes <code>zypper</code> repository. If you want to use Kubernetes version
different than v1.34, replace v1.34 with
the desired minor version in the command below.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># This overwrites any existing configuration in /etc/zypp/repos.d/kubernetes.repo</span>
</span></span><span><span>cat <span>&lt;&lt;EOF | sudo tee /etc/zypp/repos.d/kubernetes.repo
</span></span></span><span><span><span>[kubernetes]
</span></span></span><span><span><span>name=Kubernetes
</span></span></span><span><span><span>baseurl=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/
</span></span></span><span><span><span>enabled=1
</span></span></span><span><span><span>gpgcheck=1
</span></span></span><span><span><span>gpgkey=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/repodata/repomd.xml.key
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To upgrade kubectl to another minor release, you'll need to bump the version in <code>/etc/zypp/repos.d/kubernetes.repo</code>
before running <code>zypper update</code>. This procedure is described in more detail in
<a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing The Kubernetes Package Repository</a>.</div><ol start="2"><li><p>Update <code>zypper</code> and confirm the new repo addition:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo zypper update
</span></span></code></pre></div><p>When this message appears, press 't' or 'a':</p><pre tabindex="0"><code>New repository or package signing key received:

Repository:       Kubernetes
Key Fingerprint:  1111 2222 3333 4444 5555 6666 7777 8888 9999 AAAA
Key Name:         isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;
Key Algorithm:    RSA 2048
Key Created:      Thu 25 Aug 2022 01:21:11 PM -03
Key Expires:      Sat 02 Nov 2024 01:21:11 PM -03 (expires in 85 days)
Rpm Name:         gpg-pubkey-9a296436-6307a177

Note: Signing data enables the recipient to verify that no modifications occurred after the data
were signed. Accepting data with no, wrong or unknown signature can lead to a corrupted system
and in extreme cases even to a system compromise.

Note: A GPG pubkey is clearly identified by its fingerprint. Do not rely on the key's name. If
you are not sure whether the presented key is authentic, ask the repository provider or check
their web site. Many providers maintain a web page showing the fingerprints of the GPG keys they
are using.

Do you want to reject the key, trust temporarily, or trust always? [r/t/a/?] (r): a
</code></pre></li><li><p>Install kubectl using <code>zypper</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo zypper install -y kubectl
</span></span></code></pre></div></li></ol></p></div></div><h3 id="install-using-other-package-management">Install using other package management</h3><ul class="nav nav-tabs" id="other-kubectl-install"><li class="nav-item"><a class="nav-link active" href="#other-kubectl-install-0">Snap</a></li><li class="nav-item"><a class="nav-link" href="#other-kubectl-install-1">Homebrew</a></li></ul><div class="tab-content" id="other-kubectl-install"><div id="other-kubectl-install-0" class="tab-pane show active"><p><p>If you are on Ubuntu or another Linux distribution that supports the
<a href="https://snapcraft.io/docs/core/install">snap</a> package manager, kubectl
is available as a <a href="https://snapcraft.io/">snap</a> application.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>snap install kubectl --classic
</span></span><span><span>kubectl version --client
</span></span></code></pre></div></p></div><div id="other-kubectl-install-1" class="tab-pane"><p><p>If you are on Linux and using <a href="https://docs.brew.sh/Homebrew-on-Linux">Homebrew</a>
package manager, kubectl is available for <a href="https://docs.brew.sh/Homebrew-on-Linux#install">installation</a>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>brew install kubectl
</span></span><span><span>kubectl version --client
</span></span></code></pre></div></p></div></div><h2 id="verify-kubectl-configuration">Verify kubectl configuration</h2><p>In order for kubectl to find and access a Kubernetes cluster, it needs a
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>,
which is created automatically when you create a cluster using
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at <code>~/.kube/config</code>.</p><p>Check that kubectl is properly configured by getting the cluster state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info
</span></span></code></pre></div><p>If you see a URL response, kubectl is correctly configured to access your cluster.</p><p>If you see a message similar to the following, kubectl is not configured correctly
or is not able to connect to a Kubernetes cluster.</p><pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
</code></pre><p>For example, if you are intending to run a Kubernetes cluster on your laptop (locally),
you will need a tool like <a href="https://minikube.sigs.k8s.io/docs/start/">Minikube</a> to be installed first and then re-run the commands stated above.</p><p>If <code>kubectl cluster-info</code> returns the url response but you can't access your cluster,
to check whether it is configured properly, use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info dump
</span></span></code></pre></div><h3 id="no-auth-provider-found">Troubleshooting the 'No Auth Provider Found' error message</h3><p>In Kubernetes 1.26, kubectl removed the built-in authentication for the following cloud
providers' managed Kubernetes offerings. These providers have released kubectl plugins
to provide the cloud-specific authentication. For instructions, refer to the following provider documentation:</p><ul><li>Azure AKS: <a href="https://azure.github.io/kubelogin/">kubelogin plugin</a></li><li>Google Kubernetes Engine: <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#install_plugin">gke-gcloud-auth-plugin</a></li></ul><p>(There could also be other reasons to see the same error message, unrelated to that change.)</p><h2 id="optional-kubectl-configurations-and-plugins">Optional kubectl configurations and plugins</h2><h3 id="enable-shell-autocompletion">Enable shell autocompletion</h3><p>kubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell,
which can save you a lot of typing.</p><p>Below are the procedures to set up autocompletion for Bash, Fish, and Zsh.</p><ul class="nav nav-tabs" id="kubectl-autocompletion"><li class="nav-item"><a class="nav-link active" href="#kubectl-autocompletion-0">Bash</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-autocompletion-1">Fish</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-autocompletion-2">Zsh</a></li></ul><div class="tab-content" id="kubectl-autocompletion"><div id="kubectl-autocompletion-0" class="tab-pane show active"><p><h3 id="introduction">Introduction</h3><p>The kubectl completion script for Bash can be generated with the command <code>kubectl completion bash</code>.
Sourcing the completion script in your shell enables kubectl autocompletion.</p><p>However, the completion script depends on
<a href="https://github.com/scop/bash-completion"><strong>bash-completion</strong></a>,
which means that you have to install this software first
(you can test if you have bash-completion already installed by running <code>type _init_completion</code>).</p><h3 id="install-bash-completion">Install bash-completion</h3><p>bash-completion is provided by many package managers
(see <a href="https://github.com/scop/bash-completion#installation">here</a>).
You can install it with <code>apt-get install bash-completion</code> or <code>yum install bash-completion</code>, etc.</p><p>The above commands create <code>/usr/share/bash-completion/bash_completion</code>,
which is the main script of bash-completion. Depending on your package manager,
you have to manually source this file in your <code>~/.bashrc</code> file.</p><p>To find out, reload your shell and run <code>type _init_completion</code>.
If the command succeeds, you're already set, otherwise add the following to your <code>~/.bashrc</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>source</span> /usr/share/bash-completion/bash_completion
</span></span></code></pre></div><p>Reload your shell and verify that bash-completion is correctly installed by typing <code>type _init_completion</code>.</p><h3 id="enable-kubectl-autocompletion">Enable kubectl autocompletion</h3><h4 id="bash">Bash</h4><p>You now need to ensure that the kubectl completion script gets sourced in all
your shell sessions. There are two ways in which you can do this:</p><ul class="nav nav-tabs" id="kubectl-bash-autocompletion"><li class="nav-item"><a class="nav-link active" href="#kubectl-bash-autocompletion-0">User</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-bash-autocompletion-1">System</a></li></ul><div class="tab-content" id="kubectl-bash-autocompletion"><div id="kubectl-bash-autocompletion-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span><span>echo</span> <span>'source &lt;(kubectl completion bash)'</span> &gt;&gt;~/.bashrc
</span></span></code></pre></div></p></div><div id="kubectl-bash-autocompletion-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl &gt; /dev/null
</span></span><span><span>sudo chmod a+r /etc/bash_completion.d/kubectl
</span></span></code></pre></div></p></div></div><p>If you have an alias for kubectl, you can extend shell completion to work with that alias:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>'alias k=kubectl'</span> &gt;&gt;~/.bashrc
</span></span><span><span><span>echo</span> <span>'complete -o default -F __start_kubectl k'</span> &gt;&gt;~/.bashrc
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>bash-completion sources all completion scripts in <code>/etc/bash_completion.d</code>.</div><p>Both approaches are equivalent. After reloading your shell, kubectl autocompletion should be working.
To enable bash autocompletion in current session of shell, source the ~/.bashrc file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>source</span> ~/.bashrc
</span></span></code></pre></div></p></div><div id="kubectl-autocompletion-1" class="tab-pane"><p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Autocomplete for Fish requires kubectl 1.23 or later.</div><p>The kubectl completion script for Fish can be generated with the command <code>kubectl completion fish</code>. Sourcing the completion script in your shell enables kubectl autocompletion.</p><p>To do so in all your shell sessions, add the following line to your <code>~/.config/fish/config.fish</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl completion fish | <span>source</span>
</span></span></code></pre></div><p>After reloading your shell, kubectl autocompletion should be working.</p></p></div><div id="kubectl-autocompletion-2" class="tab-pane"><p><p>The kubectl completion script for Zsh can be generated with the command <code>kubectl completion zsh</code>. Sourcing the completion script in your shell enables kubectl autocompletion.</p><p>To do so in all your shell sessions, add the following to your <code>~/.zshrc</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-zsh"><span><span><span>source</span> &lt;<span>(</span>kubectl completion zsh<span>)</span>
</span></span></code></pre></div><p>If you have an alias for kubectl, kubectl autocompletion will automatically work with it.</p><p>After reloading your shell, kubectl autocompletion should be working.</p><p>If you get an error like <code>2: command not found: compdef</code>, then add the following to the beginning of your <code>~/.zshrc</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-zsh"><span><span>autoload -Uz compinit
</span></span><span><span>compinit
</span></span></code></pre></div></p></div></div><h3 id="configure-kuberc">Configure kuberc</h3><p>See <a href="/docs/reference/kubectl/kuberc/">kuberc</a> for more information.</p><h3 id="install-kubectl-convert-plugin">Install <code>kubectl convert</code> plugin</h3><p>A plugin for Kubernetes command-line tool <code>kubectl</code>, which allows you to convert manifests between different API
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit <a href="/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">migrate to non deprecated apis</a></p><ol><li><p>Download the latest release with the command:</p><ul class="nav nav-tabs" id="download-convert-binary-linux"><li class="nav-item"><a class="nav-link active" href="#download-convert-binary-linux-0">x86-64</a></li><li class="nav-item"><a class="nav-link" href="#download-convert-binary-linux-1">ARM64</a></li></ul><div class="tab-content" id="download-convert-binary-linux"><div id="download-convert-binary-linux-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/amd64/kubectl-convert"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-convert-binary-linux-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/arm64/kubectl-convert"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div></li><li><p>Validate the binary (optional)</p><p>Download the kubectl-convert checksum file:</p><ul class="nav nav-tabs" id="download-convert-checksum-linux"><li class="nav-item"><a class="nav-link active" href="#download-convert-checksum-linux-0">x86-64</a></li><li class="nav-item"><a class="nav-link" href="#download-convert-checksum-linux-1">ARM64</a></li></ul><div class="tab-content" id="download-convert-checksum-linux"><div id="download-convert-checksum-linux-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/amd64/kubectl-convert.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-convert-checksum-linux-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/linux/arm64/kubectl-convert.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div><p>Validate the kubectl-convert binary against the checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>"</span><span>$(</span>cat kubectl-convert.sha256<span>)</span><span> kubectl-convert"</span> | sha256sum --check
</span></span></code></pre></div><p>If valid, the output is:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl-convert: OK
</span></span></span></code></pre></div><p>If the check fails, <code>sha256</code> exits with nonzero status and prints output similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl-convert: FAILED
</span></span></span><span><span><span>sha256sum: WARNING: 1 computed checksum did NOT match
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Download the same version of the binary and checksum.</div></li><li><p>Install kubectl-convert</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo install -o root -g root -m <span>0755</span> kubectl-convert /usr/local/bin/kubectl-convert
</span></span></code></pre></div></li><li><p>Verify plugin is successfully installed</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl convert --help
</span></span></code></pre></div><p>If you do not see an error, it means the plugin is successfully installed.</p></li><li><p>After installing the plugin, clean up the installation files:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>rm kubectl-convert kubectl-convert.sha256
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li><a href="https://minikube.sigs.k8s.io/docs/start/">Install Minikube</a></li><li>See the <a href="/docs/setup/">getting started guides</a> for more about creating clusters.</li><li><a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Learn how to launch and expose your application.</a></li><li>If you need access to a cluster you didn't create, see the
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Sharing Cluster Access document</a>.</li><li>Read the <a href="/docs/reference/kubectl/kubectl/">kubectl reference docs</a></li></ul></div></div><div><div class="td-content"><h1>Install and Set Up kubectl on macOS</h1><h2 id="before-you-begin">Before you begin</h2><p>You must use a kubectl version that is within one minor version difference of
your cluster. For example, a v1.34 client can communicate
with v1.33, v1.34,
and v1.35 control planes.
Using the latest compatible version of kubectl helps avoid unforeseen issues.</p><h2 id="install-kubectl-on-macos">Install kubectl on macOS</h2><p>The following methods exist for installing kubectl on macOS:</p><ul><li><a href="#install-kubectl-on-macos">Install kubectl on macOS</a><ul><li><a href="#install-kubectl-binary-with-curl-on-macos">Install kubectl binary with curl on macOS</a></li><li><a href="#install-with-homebrew-on-macos">Install with Homebrew on macOS</a></li><li><a href="#install-with-macports-on-macos">Install with Macports on macOS</a></li></ul></li><li><a href="#verify-kubectl-configuration">Verify kubectl configuration</a></li><li><a href="#optional-kubectl-configurations-and-plugins">Optional kubectl configurations and plugins</a><ul><li><a href="#enable-shell-autocompletion">Enable shell autocompletion</a></li><li><a href="#install-kubectl-convert-plugin">Install <code>kubectl convert</code> plugin</a></li></ul></li></ul><h3 id="install-kubectl-binary-with-curl-on-macos">Install kubectl binary with curl on macOS</h3><ol><li><p>Download the latest release:</p><ul class="nav nav-tabs" id="download-binary-macos"><li class="nav-item"><a class="nav-link active" href="#download-binary-macos-0">Intel</a></li><li class="nav-item"><a class="nav-link" href="#download-binary-macos-1">Apple Silicon</a></li></ul><div class="tab-content" id="download-binary-macos"><div id="download-binary-macos-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/amd64/kubectl"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-binary-macos-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/arm64/kubectl"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>To download a specific version, replace the <code>$(curl -L -s https://dl.k8s.io/release/stable.txt)</code>
portion of the command with the specific version.</p><p>For example, to download version 1.34.0 on Intel macOS, type:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>curl -LO <span>"https://dl.k8s.io/release/v1.34.0/bin/darwin/amd64/kubectl"</span>
</span></span></code></pre></div><p>And for macOS on Apple Silicon, type:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>curl -LO <span>"https://dl.k8s.io/release/v1.34.0/bin/darwin/arm64/kubectl"</span>
</span></span></code></pre></div></div></li><li><p>Validate the binary (optional)</p><p>Download the kubectl checksum file:</p><ul class="nav nav-tabs" id="download-checksum-macos"><li class="nav-item"><a class="nav-link active" href="#download-checksum-macos-0">Intel</a></li><li class="nav-item"><a class="nav-link" href="#download-checksum-macos-1">Apple Silicon</a></li></ul><div class="tab-content" id="download-checksum-macos"><div id="download-checksum-macos-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/amd64/kubectl.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-checksum-macos-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/arm64/kubectl.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div><p>Validate the kubectl binary against the checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>"</span><span>$(</span>cat kubectl.sha256<span>)</span><span>  kubectl"</span> | shasum -a <span>256</span> --check
</span></span></code></pre></div><p>If valid, the output is:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl: OK
</span></span></span></code></pre></div><p>If the check fails, <code>shasum</code> exits with nonzero status and prints output similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl: FAILED
</span></span></span><span><span><span>shasum: WARNING: 1 computed checksum did NOT match
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Download the same version of the binary and checksum.</div></li><li><p>Make the kubectl binary executable.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>chmod +x ./kubectl
</span></span></code></pre></div></li><li><p>Move the kubectl binary to a file location on your system <code>PATH</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo mv ./kubectl /usr/local/bin/kubectl
</span></span><span><span>sudo chown root: /usr/local/bin/kubectl
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure <code>/usr/local/bin</code> is in your PATH environment variable.</div></li><li><p>Test to ensure the version you installed is up-to-date:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl version --client
</span></span></code></pre></div><p>Or use this for detailed view of version:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>kubectl version --client --output=yaml
</span></span></code></pre></div></li><li><p>After installing and validating kubectl, delete the checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>rm kubectl.sha256
</span></span></code></pre></div></li></ol><h3 id="install-with-homebrew-on-macos">Install with Homebrew on macOS</h3><p>If you are on macOS and using <a href="https://brew.sh/">Homebrew</a> package manager,
you can install kubectl with Homebrew.</p><ol><li><p>Run the installation command:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>brew install kubectl
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>brew install kubernetes-cli
</span></span></code></pre></div></li><li><p>Test to ensure the version you installed is up-to-date:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl version --client
</span></span></code></pre></div></li></ol><h3 id="install-with-macports-on-macos">Install with Macports on macOS</h3><p>If you are on macOS and using <a href="https://macports.org/">Macports</a> package manager,
you can install kubectl with Macports.</p><ol><li><p>Run the installation command:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo port selfupdate
</span></span><span><span>sudo port install kubectl
</span></span></code></pre></div></li><li><p>Test to ensure the version you installed is up-to-date:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl version --client
</span></span></code></pre></div></li></ol><h2 id="verify-kubectl-configuration">Verify kubectl configuration</h2><p>In order for kubectl to find and access a Kubernetes cluster, it needs a
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>,
which is created automatically when you create a cluster using
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at <code>~/.kube/config</code>.</p><p>Check that kubectl is properly configured by getting the cluster state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info
</span></span></code></pre></div><p>If you see a URL response, kubectl is correctly configured to access your cluster.</p><p>If you see a message similar to the following, kubectl is not configured correctly
or is not able to connect to a Kubernetes cluster.</p><pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
</code></pre><p>For example, if you are intending to run a Kubernetes cluster on your laptop (locally),
you will need a tool like <a href="https://minikube.sigs.k8s.io/docs/start/">Minikube</a> to be installed first and then re-run the commands stated above.</p><p>If <code>kubectl cluster-info</code> returns the url response but you can't access your cluster,
to check whether it is configured properly, use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info dump
</span></span></code></pre></div><h3 id="no-auth-provider-found">Troubleshooting the 'No Auth Provider Found' error message</h3><p>In Kubernetes 1.26, kubectl removed the built-in authentication for the following cloud
providers' managed Kubernetes offerings. These providers have released kubectl plugins
to provide the cloud-specific authentication. For instructions, refer to the following provider documentation:</p><ul><li>Azure AKS: <a href="https://azure.github.io/kubelogin/">kubelogin plugin</a></li><li>Google Kubernetes Engine: <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#install_plugin">gke-gcloud-auth-plugin</a></li></ul><p>(There could also be other reasons to see the same error message, unrelated to that change.)</p><h2 id="optional-kubectl-configurations-and-plugins">Optional kubectl configurations and plugins</h2><h3 id="enable-shell-autocompletion">Enable shell autocompletion</h3><p>kubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell
which can save you a lot of typing.</p><p>Below are the procedures to set up autocompletion for Bash, Fish, and Zsh.</p><ul class="nav nav-tabs" id="kubectl-autocompletion"><li class="nav-item"><a class="nav-link active" href="#kubectl-autocompletion-0">Bash</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-autocompletion-1">Fish</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-autocompletion-2">Zsh</a></li></ul><div class="tab-content" id="kubectl-autocompletion"><div id="kubectl-autocompletion-0" class="tab-pane show active"><p><h3 id="introduction">Introduction</h3><p>The kubectl completion script for Bash can be generated with <code>kubectl completion bash</code>.
Sourcing this script in your shell enables kubectl completion.</p><p>However, the kubectl completion script depends on
<a href="https://github.com/scop/bash-completion"><strong>bash-completion</strong></a> which you thus have to previously install.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>There are two versions of bash-completion, v1 and v2. V1 is for Bash 3.2
(which is the default on macOS), and v2 is for Bash 4.1+. The kubectl completion
script <strong>doesn't work</strong> correctly with bash-completion v1 and Bash 3.2.
It requires <strong>bash-completion v2</strong> and <strong>Bash 4.1+</strong>. Thus, to be able to
correctly use kubectl completion on macOS, you have to install and use
Bash 4.1+ (<a href="https://apple.stackexchange.com/a/292760"><em>instructions</em></a>).
The following instructions assume that you use Bash 4.1+
(that is, any Bash version of 4.1 or newer).</div><h3 id="upgrade-bash">Upgrade Bash</h3><p>The instructions here assume you use Bash 4.1+. You can check your Bash's version by running:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>$BASH_VERSION</span>
</span></span></code></pre></div><p>If it is too old, you can install/upgrade it using Homebrew:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>brew install bash
</span></span></code></pre></div><p>Reload your shell and verify that the desired version is being used:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>$BASH_VERSION</span> <span>$SHELL</span>
</span></span></code></pre></div><p>Homebrew usually installs it at <code>/usr/local/bin/bash</code>.</p><h3 id="install-bash-completion">Install bash-completion</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>As mentioned, these instructions assume you use Bash 4.1+, which means you will
install bash-completion v2 (in contrast to Bash 3.2 and bash-completion v1,
in which case kubectl completion won't work).</div><p>You can test if you have bash-completion v2 already installed with <code>type _init_completion</code>.
If not, you can install it with Homebrew:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>brew install bash-completion@2
</span></span></code></pre></div><p>As stated in the output of this command, add the following to your <code>~/.bash_profile</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>brew_etc</span><span>=</span><span>"</span><span>$(</span>brew --prefix<span>)</span><span>/etc"</span> <span>&amp;&amp;</span> <span>[[</span> -r <span>"</span><span>${</span><span>brew_etc</span><span>}</span><span>/profile.d/bash_completion.sh"</span> <span>]]</span> <span>&amp;&amp;</span> . <span>"</span><span>${</span><span>brew_etc</span><span>}</span><span>/profile.d/bash_completion.sh"</span>
</span></span></code></pre></div><p>Reload your shell and verify that bash-completion v2 is correctly installed with <code>type _init_completion</code>.</p><h3 id="enable-kubectl-autocompletion">Enable kubectl autocompletion</h3><p>You now have to ensure that the kubectl completion script gets sourced in all
your shell sessions. There are multiple ways to achieve this:</p><ul><li><p>Source the completion script in your <code>~/.bash_profile</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>'source &lt;(kubectl completion bash)'</span> &gt;&gt;~/.bash_profile
</span></span></code></pre></div></li><li><p>Add the completion script to the <code>/usr/local/etc/bash_completion.d</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl completion bash &gt;/usr/local/etc/bash_completion.d/kubectl
</span></span></code></pre></div></li><li><p>If you have an alias for kubectl, you can extend shell completion to work with that alias:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>'alias k=kubectl'</span> &gt;&gt;~/.bash_profile
</span></span><span><span><span>echo</span> <span>'complete -o default -F __start_kubectl k'</span> &gt;&gt;~/.bash_profile
</span></span></code></pre></div></li><li><p>If you installed kubectl with Homebrew (as explained
<a href="/docs/tasks/tools/install-kubectl-macos/#install-with-homebrew-on-macos">here</a>),
then the kubectl completion script should already be in <code>/usr/local/etc/bash_completion.d/kubectl</code>.
In that case, you don't need to do anything.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The Homebrew installation of bash-completion v2 sources all the files in the
<code>BASH_COMPLETION_COMPAT_DIR</code> directory, that's why the latter two methods work.</div></li></ul><p>In any case, after reloading your shell, kubectl completion should be working.</p></p></div><div id="kubectl-autocompletion-1" class="tab-pane"><p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Autocomplete for Fish requires kubectl 1.23 or later.</div><p>The kubectl completion script for Fish can be generated with the command <code>kubectl completion fish</code>. Sourcing the completion script in your shell enables kubectl autocompletion.</p><p>To do so in all your shell sessions, add the following line to your <code>~/.config/fish/config.fish</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl completion fish | <span>source</span>
</span></span></code></pre></div><p>After reloading your shell, kubectl autocompletion should be working.</p></p></div><div id="kubectl-autocompletion-2" class="tab-pane"><p><p>The kubectl completion script for Zsh can be generated with the command <code>kubectl completion zsh</code>. Sourcing the completion script in your shell enables kubectl autocompletion.</p><p>To do so in all your shell sessions, add the following to your <code>~/.zshrc</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-zsh"><span><span><span>source</span> &lt;<span>(</span>kubectl completion zsh<span>)</span>
</span></span></code></pre></div><p>If you have an alias for kubectl, kubectl autocompletion will automatically work with it.</p><p>After reloading your shell, kubectl autocompletion should be working.</p><p>If you get an error like <code>2: command not found: compdef</code>, then add the following to the beginning of your <code>~/.zshrc</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-zsh"><span><span>autoload -Uz compinit
</span></span><span><span>compinit
</span></span></code></pre></div></p></div></div><h3 id="configure-kuberc">Configure kuberc</h3><p>See <a href="/docs/reference/kubectl/kuberc/">kuberc</a> for more information.</p><h3 id="install-kubectl-convert-plugin">Install <code>kubectl convert</code> plugin</h3><p>A plugin for Kubernetes command-line tool <code>kubectl</code>, which allows you to convert manifests between different API
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit <a href="/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">migrate to non deprecated apis</a></p><ol><li><p>Download the latest release with the command:</p><ul class="nav nav-tabs" id="download-convert-binary-macos"><li class="nav-item"><a class="nav-link active" href="#download-convert-binary-macos-0">Intel</a></li><li class="nav-item"><a class="nav-link" href="#download-convert-binary-macos-1">Apple Silicon</a></li></ul><div class="tab-content" id="download-convert-binary-macos"><div id="download-convert-binary-macos-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/amd64/kubectl-convert"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-convert-binary-macos-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/arm64/kubectl-convert"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div></li><li><p>Validate the binary (optional)</p><p>Download the kubectl-convert checksum file:</p><ul class="nav nav-tabs" id="download-convert-checksum-macos"><li class="nav-item"><a class="nav-link active" href="#download-convert-checksum-macos-0">Intel</a></li><li class="nav-item"><a class="nav-link" href="#download-convert-checksum-macos-1">Apple Silicon</a></li></ul><div class="tab-content" id="download-convert-checksum-macos"><div id="download-convert-checksum-macos-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/amd64/kubectl-convert.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div><div id="download-convert-checksum-macos-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>   curl -LO <span>"https://dl.k8s.io/release/</span><span>$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span>)</span><span>/bin/darwin/arm64/kubectl-convert.sha256"</span>
</span></span><span><span>   </span></span></code></pre></div></p></div></div><p>Validate the kubectl-convert binary against the checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>echo</span> <span>"</span><span>$(</span>cat kubectl-convert.sha256<span>)</span><span>  kubectl-convert"</span> | shasum -a <span>256</span> --check
</span></span></code></pre></div><p>If valid, the output is:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl-convert: OK
</span></span></span></code></pre></div><p>If the check fails, <code>shasum</code> exits with nonzero status and prints output similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl-convert: FAILED
</span></span></span><span><span><span>shasum: WARNING: 1 computed checksum did NOT match
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Download the same version of the binary and checksum.</div></li><li><p>Make kubectl-convert binary executable</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>chmod +x ./kubectl-convert
</span></span></code></pre></div></li><li><p>Move the kubectl-convert binary to a file location on your system <code>PATH</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo mv ./kubectl-convert /usr/local/bin/kubectl-convert
</span></span><span><span>sudo chown root: /usr/local/bin/kubectl-convert
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure <code>/usr/local/bin</code> is in your PATH environment variable.</div></li><li><p>Verify plugin is successfully installed</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl convert --help
</span></span></code></pre></div><p>If you do not see an error, it means the plugin is successfully installed.</p></li><li><p>After installing the plugin, clean up the installation files:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>rm kubectl-convert kubectl-convert.sha256
</span></span></code></pre></div></li></ol><h3 id="uninstall-kubectl-on-macos">Uninstall kubectl on macOS</h3><p>Depending on how you installed <code>kubectl</code>, use one of the following methods.</p><h3 id="uninstall-kubectl-using-the-command-line">Uninstall kubectl using the command-line</h3><ol><li><p>Locate the <code>kubectl</code> binary on your system:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>which kubectl
</span></span></code></pre></div></li><li><p>Remove the <code>kubectl</code> binary:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo rm &lt;path&gt;
</span></span></code></pre></div><p>Replace <code>&lt;path&gt;</code> with the path to the <code>kubectl</code> binary from the previous step. For example, <code>sudo rm /usr/local/bin/kubectl</code>.</p></li></ol><h3 id="uninstall-kubectl-using-homebrew">Uninstall kubectl using homebrew</h3><p>If you installed <code>kubectl</code> using Homebrew, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>brew remove kubectl
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="https://minikube.sigs.k8s.io/docs/start/">Install Minikube</a></li><li>See the <a href="/docs/setup/">getting started guides</a> for more about creating clusters.</li><li><a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Learn how to launch and expose your application.</a></li><li>If you need access to a cluster you didn't create, see the
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Sharing Cluster Access document</a>.</li><li>Read the <a href="/docs/reference/kubectl/kubectl/">kubectl reference docs</a></li></ul></div></div><div><div class="td-content"><h1>Install and Set Up kubectl on Windows</h1><h2 id="before-you-begin">Before you begin</h2><p>You must use a kubectl version that is within one minor version difference of
your cluster. For example, a v1.34 client can communicate
with v1.33, v1.34,
and v1.35 control planes.
Using the latest compatible version of kubectl helps avoid unforeseen issues.</p><h2 id="install-kubectl-on-windows">Install kubectl on Windows</h2><p>The following methods exist for installing kubectl on Windows:</p><ul><li><a href="#install-kubectl-binary-on-windows-via-direct-download-or-curl">Install kubectl binary on Windows (via direct download or curl)</a></li><li><a href="#install-nonstandard-package-tools">Install on Windows using Chocolatey, Scoop, or winget</a></li></ul><h3 id="install-kubectl-binary-on-windows-via-direct-download-or-curl">Install kubectl binary on Windows (via direct download or curl)</h3><ol><li><p>You have two options for installing kubectl on your Windows device</p><ul><li><p>Direct download:</p><p>Download the latest 1.34 patch release binary directly for your specific architecture by visiting the <a href="https://kubernetes.io/releases/download/#binaries">Kubernetes release page</a>. Be sure to select the correct binary for your architecture (e.g., amd64, arm64, etc.).</p></li><li><p>Using curl:</p><p>If you have <code>curl</code> installed, use this command:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>curl.exe -LO <span>"https://dl.k8s.io/release/v1.34.0/bin/windows/amd64/kubectl.exe"</span>
</span></span></code></pre></div></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To find out the latest stable version (for example, for scripting), take a look at
<a href="https://dl.k8s.io/release/stable.txt">https://dl.k8s.io/release/stable.txt</a>.</div></li><li><p>Validate the binary (optional)</p><p>Download the <code>kubectl</code> checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>curl.exe -LO <span>"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kubectl.exe.sha256"</span>
</span></span></code></pre></div><p>Validate the <code>kubectl</code> binary against the checksum file:</p><ul><li><p>Using Command Prompt to manually compare <code>CertUtil</code>'s output to the checksum file downloaded:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>CertUtil -hashfile kubectl.exe SHA256
</span></span><span><span><span>type</span> kubectl.exe.sha256
</span></span></code></pre></div></li><li><p>Using PowerShell to automate the verification using the <code>-eq</code> operator to
get a <code>True</code> or <code>False</code> result:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span> $(<span>Get-FileHash</span> -Algorithm SHA256 .\kubectl.exe).Hash <span>-eq</span> $(<span>Get-Content</span> .\kubectl.exe.sha256)
</span></span></code></pre></div></li></ul></li><li><p>Append or prepend the <code>kubectl</code> binary folder to your <code>PATH</code> environment variable.</p></li><li><p>Test to ensure the version of <code>kubectl</code> is the same as downloaded:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>kubectl version --client
</span></span></code></pre></div><p>Or use this for detailed view of version:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>kubectl version --client --output=yaml
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><a href="https://docs.docker.com/docker-for-windows/#kubernetes">Docker Desktop for Windows</a>
adds its own version of <code>kubectl</code> to <code>PATH</code>. If you have installed Docker Desktop before,
you may need to place your <code>PATH</code> entry before the one added by the Docker Desktop
installer or remove the Docker Desktop's <code>kubectl</code>.</div><h3 id="install-nonstandard-package-tools">Install on Windows using Chocolatey, Scoop, or winget</h3><ol><li><p>To install kubectl on Windows you can use either <a href="https://chocolatey.org">Chocolatey</a>
package manager, <a href="https://scoop.sh">Scoop</a> command-line installer, or
<a href="https://learn.microsoft.com/en-us/windows/package-manager/winget/">winget</a> package manager.</p><ul class="nav nav-tabs" id="kubectl-win-install"><li class="nav-item"><a class="nav-link active" href="#kubectl-win-install-0">choco</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-win-install-1">scoop</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-win-install-2">winget</a></li></ul><div class="tab-content" id="kubectl-win-install"><div id="kubectl-win-install-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>choco install <span>kubernetes-cli</span>
</span></span></code></pre></div></p></div><div id="kubectl-win-install-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>scoop install kubectl
</span></span></code></pre></div></p></div><div id="kubectl-win-install-2" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>winget install -e --id Kubernetes.kubectl
</span></span></code></pre></div></p></div></div></li><li><p>Test to ensure the version you installed is up-to-date:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>kubectl version --client
</span></span></code></pre></div></li><li><p>Navigate to your home directory:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span># If you're using cmd.exe, run: cd %USERPROFILE%</span>
</span></span><span><span><span>cd </span>~
</span></span></code></pre></div></li><li><p>Create the <code>.kube</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>mkdir .kube
</span></span></code></pre></div></li><li><p>Change to the <code>.kube</code> directory you just created:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>cd </span>.kube
</span></span></code></pre></div></li><li><p>Configure kubectl to use a remote Kubernetes cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>New-Item</span> config -type file
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Edit the config file with a text editor of your choice, such as Notepad.</div><h2 id="verify-kubectl-configuration">Verify kubectl configuration</h2><p>In order for kubectl to find and access a Kubernetes cluster, it needs a
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>,
which is created automatically when you create a cluster using
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at <code>~/.kube/config</code>.</p><p>Check that kubectl is properly configured by getting the cluster state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info
</span></span></code></pre></div><p>If you see a URL response, kubectl is correctly configured to access your cluster.</p><p>If you see a message similar to the following, kubectl is not configured correctly
or is not able to connect to a Kubernetes cluster.</p><pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
</code></pre><p>For example, if you are intending to run a Kubernetes cluster on your laptop (locally),
you will need a tool like <a href="https://minikube.sigs.k8s.io/docs/start/">Minikube</a> to be installed first and then re-run the commands stated above.</p><p>If <code>kubectl cluster-info</code> returns the url response but you can't access your cluster,
to check whether it is configured properly, use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info dump
</span></span></code></pre></div><h3 id="no-auth-provider-found">Troubleshooting the 'No Auth Provider Found' error message</h3><p>In Kubernetes 1.26, kubectl removed the built-in authentication for the following cloud
providers' managed Kubernetes offerings. These providers have released kubectl plugins
to provide the cloud-specific authentication. For instructions, refer to the following provider documentation:</p><ul><li>Azure AKS: <a href="https://azure.github.io/kubelogin/">kubelogin plugin</a></li><li>Google Kubernetes Engine: <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#install_plugin">gke-gcloud-auth-plugin</a></li></ul><p>(There could also be other reasons to see the same error message, unrelated to that change.)</p><h2 id="optional-kubectl-configurations-and-plugins">Optional kubectl configurations and plugins</h2><h3 id="enable-shell-autocompletion">Enable shell autocompletion</h3><p>kubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell,
which can save you a lot of typing.</p><p>Below are the procedures to set up autocompletion for PowerShell.</p><p>The kubectl completion script for PowerShell can be generated with the command <code>kubectl completion powershell</code>.</p><p>To do so in all your shell sessions, add the following line to your <code>$PROFILE</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>kubectl completion powershell | <span>Out-String</span> | <span>Invoke-Expression</span>
</span></span></code></pre></div><p>This command will regenerate the auto-completion script on every PowerShell start up. You can also add the generated script directly to your <code>$PROFILE</code> file.</p><p>To add the generated script to your <code>$PROFILE</code> file, run the following line in your powershell prompt:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>kubectl completion powershell &gt;&gt; <span>$PROFILE</span>
</span></span></code></pre></div><p>After reloading your shell, kubectl autocompletion should be working.</p><h3 id="configure-kuberc">Configure kuberc</h3><p>See <a href="/docs/reference/kubectl/kuberc/">kuberc</a> for more information.</p><h3 id="install-kubectl-convert-plugin">Install <code>kubectl convert</code> plugin</h3><p>A plugin for Kubernetes command-line tool <code>kubectl</code>, which allows you to convert manifests between different API
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit <a href="/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">migrate to non deprecated apis</a></p><ol><li><p>Download the latest release with the command:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>curl.exe -LO <span>"https://dl.k8s.io/release/v1.34.0/bin/windows/amd64/kubectl-convert.exe"</span>
</span></span></code></pre></div></li><li><p>Validate the binary (optional).</p><p>Download the <code>kubectl-convert</code> checksum file:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>curl.exe -LO <span>"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kubectl-convert.exe.sha256"</span>
</span></span></code></pre></div><p>Validate the <code>kubectl-convert</code> binary against the checksum file:</p><ul><li><p>Using Command Prompt to manually compare <code>CertUtil</code>'s output to the checksum file downloaded:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>CertUtil -hashfile kubectl-convert.exe SHA256
</span></span><span><span><span>type</span> kubectl-convert.exe.sha256
</span></span></code></pre></div></li><li><p>Using PowerShell to automate the verification using the <code>-eq</code> operator to get
a <code>True</code> or <code>False</code> result:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>$($(CertUtil -hashfile .\<span>kubectl-convert</span>.exe SHA256)[<span>1</span>] <span>-replace</span> <span>" "</span>, <span>""</span>) <span>-eq</span> $(<span>type </span>.\<span>kubectl-convert</span>.exe.sha256)
</span></span></code></pre></div></li></ul></li><li><p>Append or prepend the <code>kubectl-convert</code> binary folder to your <code>PATH</code> environment variable.</p></li><li><p>Verify the plugin is successfully installed.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl convert --help
</span></span></code></pre></div><p>If you do not see an error, it means the plugin is successfully installed.</p></li><li><p>After installing the plugin, clean up the installation files:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>del kubectl-convert</span>.exe
</span></span><span><span><span>del kubectl-convert</span>.exe.sha256
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li><a href="https://minikube.sigs.k8s.io/docs/start/">Install Minikube</a></li><li>See the <a href="/docs/setup/">getting started guides</a> for more about creating clusters.</li><li><a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Learn how to launch and expose your application.</a></li><li>If you need access to a cluster you didn't create, see the
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Sharing Cluster Access document</a>.</li><li>Read the <a href="/docs/reference/kubectl/kubectl/">kubectl reference docs</a></li></ul></div></div><div><div class="td-content"><h1>Administer a Cluster</h1><div class="lead">Learn common tasks for administering a cluster.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubeadm/">Administration with kubeadm</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/node-overprovisioning/">Overprovision Node Capacity For A Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/certificates/">Generate Certificates Manually</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/">Manage Memory, CPU, and API Resources</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/">Install a Network Policy Provider</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/access-cluster-api/">Access Clusters Using the Kubernetes API</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/change-pv-access-mode-readwriteoncepod/">Change the Access Mode of a PersistentVolume to ReadWriteOncePod</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/change-default-storage-class/">Change the default StorageClass</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/switch-to-evented-pleg/">Switching from Polling to CRI Event-based Updates to Container Status</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/change-pv-reclaim-policy/">Change the Reclaim Policy of a PersistentVolume</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/running-cloud-controller/">Cloud Controller Manager Administration</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure a kubelet image credential provider</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/cpu-management-policies/">Control CPU Management Policies on the Node</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/topology-manager/">Control Topology Management Policies on a node</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/dns-debugging-resolution/">Debugging DNS Resolution</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/enable-disable-api/">Enable Or Disable A Kubernetes API</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/encrypt-data/">Encrypting Confidential Data at Rest</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/decrypt-data/">Decrypt Confidential Data that is Already Encrypted at Rest</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">Guaranteed Scheduling For Critical Add-On Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/ip-masq-agent/">IP Masquerade Agent User Guide</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/limit-storage-consumption/">Limit Storage Consumption</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/controller-manager-leader-migration/">Migrate Replicated Control Plane To Use Cloud Controller Manager</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/configure-upgrade-etcd/">Operating etcd clusters for Kubernetes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubelet-in-userns/">Running Kubernetes Node Components as a Non-root User</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/safely-drain-node/">Safely Drain a Node</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubelet-config-file/">Set Kubelet Parameters Via A Configuration File</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/namespaces/">Share a Cluster with Namespaces</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/cluster-upgrade/">Upgrade A Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/use-cascading-deletion/">Use Cascading Deletion in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/coredns/">Using CoreDNS for Service Discovery</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/nodelocaldns/">Using NodeLocal DNSCache in Kubernetes Clusters</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/sysctl-cluster/">Using sysctls in a Kubernetes Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/memory-manager/">Utilizing the NUMA-aware Memory Manager</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/verify-signed-artifacts/">Verify Signed Kubernetes Artifacts</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Administration with kubeadm</h1><p>If you don't yet have a cluster, visit
<a href="/docs/setup/production-environment/tools/kubeadm/">bootstrapping clusters with <code>kubeadm</code></a>.</p><p>The tasks in this section are aimed at people administering an existing cluster:</p><div class="section-index"><ul><li><a href="/docs/tasks/administer-cluster/kubeadm/adding-linux-nodes/">Adding Linux worker nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/">Adding Windows worker nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrading Linux nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrading Windows nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/">Configuring a cgroup driver</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">Certificate Management with kubeadm</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/">Reconfiguring a kubeadm cluster</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing The Kubernetes Package Repository</a></li></ul></div></div></div><div><div class="td-content"><h1>Adding Linux worker nodes</h1><p>This page explains how to add Linux worker nodes to a kubeadm cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Each joining worker node has installed the required components from
<a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installing kubeadm</a>, such as,
kubeadm, the kubelet and a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>.</li><li>A running kubeadm cluster created by <code>kubeadm init</code> and following the steps
in the document <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>.</li><li>You need superuser access to the node.</li></ul><h2 id="adding-linux-worker-nodes">Adding Linux worker nodes</h2><p>To add new Linux worker nodes to your cluster do the following for each machine:</p><ol><li>Connect to the machine by using SSH or another method.</li><li>Run the command that was output by <code>kubeadm init</code>. For example:</li></ol><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><h3 id="additional-information-for-kubeadm-join">Additional information for kubeadm join</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To specify an IPv6 tuple for <code>&lt;control-plane-host&gt;:&lt;control-plane-port&gt;</code>, IPv6 address must be enclosed in square brackets, for example: <code>[2001:db8::101]:2073</code>.</div><p>If you do not have the token, you can get it by running the following command on the control plane node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on a control plane node</span>
</span></span><span><span>sudo kubeadm token list
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span><span><span>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span><span><span>                                                   signing          token generated by     bootstrappers:
</span></span></span><span><span><span>                                                                    'kubeadm init'.        kubeadm:
</span></span></span><span><span><span>                                                                                           default-node-token
</span></span></span></code></pre></div><p>By default, node join tokens expire after 24 hours. If you are joining a node to the cluster after the
current token has expired, you can create a new token by running the following command on the
control plane node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on a control plane node</span>
</span></span><span><span>sudo kubeadm token create
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>To print a kubeadm join command while also generating a new token you can use:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo kubeadm token create --print-join-command
</span></span></code></pre></div><p>If you don't have the value of <code>--discovery-token-ca-cert-hash</code>, you can get it by running the
following commands on the control plane node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on a control plane node</span>
</span></span><span><span>sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey  | openssl rsa -pubin -outform der 2&gt;/dev/null | <span>\
</span></span></span><span><span><span></span>   openssl dgst -sha256 -hex | sed <span>'s/^.* //'</span>
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><p>The output of the <code>kubeadm join</code> command should look something like:</p><pre tabindex="0"><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><p>A few seconds later, you should notice this node in the output from <code>kubectl get nodes</code>.
(for example, run <code>kubectl</code> on a control plane node).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run
on the first control plane node. To provide higher availability, please rebalance the CoreDNS Pods
with <code>kubectl -n kube-system rollout restart deployment coredns</code> after at least one new node is joined.</div><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/">add Windows worker nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Adding Windows worker nodes</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [beta]</code></div><p>This page explains how to add Windows worker nodes to a kubeadm cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>A running <a href="https://www.microsoft.com/cloud-platform/windows-server-pricing">Windows Server 2022</a>
(or higher) instance with administrative access.</li><li>A running kubeadm cluster created by <code>kubeadm init</code> and following the steps
in the document <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>.</li></ul><h2 id="adding-windows-worker-nodes">Adding Windows worker nodes</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To facilitate the addition of Windows worker nodes to a cluster, PowerShell scripts from the repository
<a href="https://sigs.k8s.io/sig-windows-tools">https://sigs.k8s.io/sig-windows-tools</a> are used.</div><p>Do the following for each machine:</p><ol><li>Open a PowerShell session on the machine.</li><li>Make sure you are Administrator or a privileged user.</li></ol><p>Then proceed with the steps outlined below.</p><h3 id="install-containerd">Install containerd</h3><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>To install containerd, first run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>curl.exe -LO https<span>:</span>//raw.githubusercontent.com/<span>kubernetes-sigs</span>/<span>sig-windows</span>-tools/master/hostprocess/<span>Install-Containerd</span>.ps1
</span></span></code></pre></div><p>Then run the following command, but first replace <code>CONTAINERD_VERSION</code> with a recent release
from the <a href="https://github.com/containerd/containerd/releases">containerd repository</a>.
The version must not have a <code>v</code> prefix. For example, use <code>1.7.22</code> instead of <code>v1.7.22</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>.\<span>Install-Containerd</span>.ps1 -ContainerDVersion CONTAINERD_VERSION
</span></span></code></pre></div><ul><li>Adjust any other parameters for <code>Install-Containerd.ps1</code> such as <code>netAdapterName</code> as you need them.</li><li>Set <code>skipHypervisorSupportCheck</code> if your machine does not support Hyper-V and cannot host Hyper-V isolated
containers.</li><li>If you change the <code>Install-Containerd.ps1</code> optional parameters <code>CNIBinPath</code> and/or <code>CNIConfigPath</code> you will
need to configure the installed Windows CNI plugin with matching values.</li></ul><h3 id="install-kubeadm-and-kubelet">Install kubeadm and kubelet</h3><p>Run the following commands to install kubeadm and the kubelet:</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>curl.exe -LO https<span>:</span>//raw.githubusercontent.com/<span>kubernetes-sigs</span>/<span>sig-windows</span>-tools/master/hostprocess/PrepareNode.ps1
</span></span><span><span>.\PrepareNode.ps1 -KubernetesVersion v1.34
</span></span></code></pre></div><ul><li>Adjust the parameter <code>KubernetesVersion</code> of <code>PrepareNode.ps1</code> if needed.</li></ul><h3 id="run-kubeadm-join">Run <code>kubeadm join</code></h3><p>Run the command that was output by <code>kubeadm init</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><h4 id="additional-information-about-kubeadm-join">Additional information about kubeadm join</h4><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To specify an IPv6 tuple for <code>&lt;control-plane-host&gt;:&lt;control-plane-port&gt;</code>, IPv6 address must be enclosed in square brackets, for example: <code>[2001:db8::101]:2073</code>.</div><p>If you do not have the token, you can get it by running the following command on the control plane node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on a control plane node</span>
</span></span><span><span>sudo kubeadm token list
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span><span><span>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span><span><span>                                                   signing          token generated by     bootstrappers:
</span></span></span><span><span><span>                                                                    'kubeadm init'.        kubeadm:
</span></span></span><span><span><span>                                                                                           default-node-token
</span></span></span></code></pre></div><p>By default, node join tokens expire after 24 hours. If you are joining a node to the cluster after the
current token has expired, you can create a new token by running the following command on the
control plane node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on a control plane node</span>
</span></span><span><span>sudo kubeadm token create
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>If you don't have the value of <code>--discovery-token-ca-cert-hash</code>, you can get it by running the
following commands on the control plane node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey  | openssl rsa -pubin -outform der 2&gt;/dev/null | <span>\
</span></span></span><span><span><span></span>   openssl dgst -sha256 -hex | sed <span>'s/^.* //'</span>
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><p>The output of the <code>kubeadm join</code> command should look something like:</p><pre tabindex="0"><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><p>A few seconds later, you should notice this node in the output from <code>kubectl get nodes</code>.
(for example, run <code>kubectl</code> on a control plane node).</p><h3 id="network-configuration">Network configuration</h3><p>CNI setup on clusters mixed with Linux and Windows nodes requires more steps than just
running <code>kubectl apply</code> on a manifest file. Additionally, the CNI plugin running on control
plane nodes must be prepared to support the CNI plugin running on Windows worker nodes.</p><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Only a few CNI plugins currently support Windows. Below you can find individual setup instructions for them:</p><ul><li><a href="https://sigs.k8s.io/sig-windows-tools/guides/flannel.md">Flannel</a></li><li><a href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/windows-calico/">Calico</a></li></ul><h3 id="install-kubectl">Install kubectl for Windows (optional)</h3><p>See <a href="/docs/tasks/tools/install-kubectl-windows/">Install and Set Up kubectl on Windows</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/adding-linux-nodes/">add Linux worker nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Upgrading kubeadm clusters</h1><p>This page explains how to upgrade a Kubernetes cluster created with kubeadm from version
1.33.x to version 1.34.x, and from version
1.34.x to 1.34.y (where <code>y &gt; x</code>). Skipping MINOR versions
when upgrading is unsupported. For more details, please visit <a href="/releases/version-skew-policy/">Version Skew Policy</a>.</p><p>To see information about upgrading clusters created using older versions of kubeadm,
please refer to following pages instead:</p><ul><li><a href="https://v1-33.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.32 to 1.33</a></li><li><a href="https://v1-32.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.31 to 1.32</a></li><li><a href="https://v1-31.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.30 to 1.31</a></li><li><a href="https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.29 to 1.30</a></li></ul><p>The Kubernetes project recommends upgrading to the latest patch releases promptly, and
to ensure that you are running a supported minor release of Kubernetes.
Following this recommendation helps you to stay secure.</p><p>The upgrade workflow at high level is the following:</p><ol><li>Upgrade a primary control plane node.</li><li>Upgrade additional control plane nodes.</li><li>Upgrade worker nodes.</li></ol><h2 id="before-you-begin">Before you begin</h2><ul><li>Make sure you read the <a href="https://git.k8s.io/kubernetes/CHANGELOG">release notes</a> carefully.</li><li>The cluster should use a static control plane and etcd pods or external etcd.</li><li>Make sure to back up any important components, such as app-level state stored in a database.
<code>kubeadm upgrade</code> does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.</li><li><a href="https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux">Swap must be disabled</a>.</li></ul><h3 id="additional-information">Additional information</h3><ul><li>The instructions below outline when to drain each node during the upgrade process.
If you are performing a <strong>minor</strong> version upgrade for any kubelet, you <strong>must</strong>
first drain the node (or nodes) that you are upgrading. In the case of control plane nodes,
they could be running CoreDNS Pods or other critical workloads. For more information see
<a href="/docs/tasks/administer-cluster/safely-drain-node/">Draining nodes</a>.</li><li>The Kubernetes project recommends that you match your kubelet and kubeadm versions.
You can instead use a version of kubelet that is older than kubeadm, provided it is within the
range of supported versions.
For more details, please visit <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#kubeadm-s-skew-against-the-kubelet">kubeadm's skew against the kubelet</a>.</li><li>All containers are restarted after upgrade, because the container spec hash value is changed.</li><li>To verify that the kubelet service has successfully restarted after the kubelet has been upgraded,
you can execute <code>systemctl status kubelet</code> or view the service logs with <code>journalctl -xeu kubelet</code>.</li><li><code>kubeadm upgrade</code> supports <code>--config</code> with a
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/"><code>UpgradeConfiguration</code> API type</a> which can
be used to configure the upgrade process.</li><li><code>kubeadm upgrade</code> does not support reconfiguration of an existing cluster. Follow the steps in
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/">Reconfiguring a kubeadm cluster</a> instead.</li></ul><h3 id="considerations-when-upgrading-etcd">Considerations when upgrading etcd</h3><p>Because the <code>kube-apiserver</code> static pod is running at all times (even if you
have drained the node), when you perform a kubeadm upgrade which includes an
etcd upgrade, in-flight requests to the server will stall while the new etcd
static pod is restarting. As a workaround, it is possible to actively stop the
<code>kube-apiserver</code> process a few seconds before starting the <code>kubeadm upgrade apply</code> command. This permits to complete in-flight requests and close existing
connections, and minimizes the consequence of the etcd downtime. This can be
done as follows on control plane nodes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>killall -s SIGTERM kube-apiserver <span># trigger a graceful kube-apiserver shutdown</span>
</span></span><span><span>sleep <span>20</span> <span># wait a little bit to permit completing in-flight requests</span>
</span></span><span><span>kubeadm upgrade ... <span># execute a kubeadm upgrade command</span>
</span></span></code></pre></div><h2 id="changing-the-package-repository">Changing the package repository</h2><p>If you're using the community-owned package repositories (<code>pkgs.k8s.io</code>), you need to
enable the package repository for the desired Kubernetes minor release. This is explained in
<a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing the Kubernetes package repository</a>
document.</p><div class="alert alert-secondary callout note"><strong>Note:</strong> The legacy package repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) have been
<a href="/blog/2023/08/31/legacy-package-repository-deprecation/">deprecated and frozen starting from September 13, 2023</a>.
<strong>Using the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">new package repositories hosted at <code>pkgs.k8s.io</code></a>
is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.</strong>
The deprecated legacy repositories, and their contents, might be removed at any time in the future and without
a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.</div><h2 id="determine-which-version-to-upgrade-to">Determine which version to upgrade to</h2><p>Find the latest patch release for Kubernetes 1.34 using the OS package manager:</p><ul class="nav nav-tabs" id="k8s-install-versions"><li class="nav-item"><a class="nav-link active" href="#k8s-install-versions-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-install-versions-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-versions"><div id="k8s-install-versions-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Find the latest 1.34 version in the list.</span>
</span></span><span><span><span># It should look like 1.34.x-*, where x is the latest patch.</span>
</span></span><span><span>sudo apt update
</span></span><span><span>sudo apt-cache madison kubeadm
</span></span></code></pre></div></p></div><div id="k8s-install-versions-1" class="tab-pane"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Find the latest 1.34 version in the list.</span>
</span></span><span><span><span># It should look like 1.34.x-*, where x is the latest patch.</span>
</span></span><span><span>sudo yum list --showduplicates kubeadm --disableexcludes<span>=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Find the latest 1.34 version in the list.</span>
</span></span><span><span><span># It should look like 1.34.x-*, where x is the latest patch.</span>
</span></span><span><span>sudo yum list --showduplicates kubeadm --setopt<span>=</span><span>disable_excludes</span><span>=</span>kubernetes
</span></span></code></pre></div></p></div></div><p>If you don't see the version you expect to upgrade to, <a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/#verifying-if-the-kubernetes-package-repositories-are-used">verify if the Kubernetes package repositories are used.</a></p><h2 id="upgrading-control-plane-nodes">Upgrading control plane nodes</h2><p>The upgrade procedure on control plane nodes should be executed one node at a time.
Pick a control plane node that you wish to upgrade first. It must have the <code>/etc/kubernetes/admin.conf</code> file.</p><h3 id="call-kubeadm-upgrade">Call "kubeadm upgrade"</h3><p><strong>For the first control plane node</strong></p><ol><li><p>Upgrade kubeadm:</p><ul class="nav nav-tabs" id="k8s-install-kubeadm-first-cp"><li class="nav-item"><a class="nav-link active" href="#k8s-install-kubeadm-first-cp-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-install-kubeadm-first-cp-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-kubeadm-first-cp"><div id="k8s-install-kubeadm-first-cp-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo apt-mark unhold kubeadm <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-get update <span>&amp;&amp;</span> sudo apt-get install -y <span>kubeadm</span><span>=</span><span>'1.34.x-*'</span> <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-mark hold kubeadm
</span></span></code></pre></div></p></div><div id="k8s-install-kubeadm-first-cp-1" class="tab-pane"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubeadm-<span>'1.34.x-*'</span> --disableexcludes<span>=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubeadm-<span>'1.34.x-*'</span> --setopt<span>=</span><span>disable_excludes</span><span>=</span>kubernetes
</span></span></code></pre></div></p></div></div></li><li><p>Verify that the download works and has the expected version:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubeadm version
</span></span></code></pre></div></li><li><p>Verify the upgrade plan:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm upgrade plan
</span></span></code></pre></div><p>This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
It also shows a table with the component config version states.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>kubeadm upgrade</code> also automatically renews the certificates that it manages on this node.
To opt-out of certificate renewal the flag <code>--certificate-renewal=false</code> can be used.
For more information see the <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">certificate management guide</a>.</div></li><li><p>Choose a version to upgrade to, and run the appropriate command. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x with the patch version you picked for this upgrade</span>
</span></span><span><span>sudo kubeadm upgrade apply v1.34.x
</span></span></code></pre></div><p>Once the command finishes you should see:</p><pre tabindex="0"><code>[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.34.x". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For versions earlier than v1.28, kubeadm defaulted to a mode that upgrades the addons (including CoreDNS and kube-proxy)
immediately during <code>kubeadm upgrade apply</code>, regardless of whether there are other control plane instances that have not
been upgraded. This may cause compatibility problems. Since v1.28, kubeadm defaults to a mode that checks whether all
the control plane instances have been upgraded before starting to upgrade the addons. You must perform control plane
instances upgrade sequentially or at least ensure that the last control plane instance upgrade is not started until all
the other control plane instances have been upgraded completely, and the addons upgrade will be performed after the last
control plane instance is upgraded.</div></li><li><p>Manually upgrade your CNI provider plugin.</p><p>Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow.
Check the <a href="/docs/concepts/cluster-administration/addons/">addons</a> page to
find your CNI provider and see whether additional upgrade steps are required.</p><p>This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.</p></li></ol><p><strong>For the other control plane nodes</strong></p><p>Same as the first control plane node but use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm upgrade node
</span></span></code></pre></div><p>instead of:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm upgrade apply
</span></span></code></pre></div><p>Also calling <code>kubeadm upgrade plan</code> and upgrading the CNI provider plugin is no longer needed.</p><h3 id="drain-the-node">Drain the node</h3><p>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
</span></span><span><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><h3 id="upgrade-kubelet-and-kubectl">Upgrade kubelet and kubectl</h3><ol><li><p>Upgrade the kubelet and kubectl:</p><ul class="nav nav-tabs" id="k8s-install-kubelet"><li class="nav-item"><a class="nav-link active" href="#k8s-install-kubelet-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-install-kubelet-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-kubelet"><div id="k8s-install-kubelet-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo apt-mark unhold kubelet kubectl <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-get update <span>&amp;&amp;</span> sudo apt-get install -y <span>kubelet</span><span>=</span><span>'1.34.x-*'</span> <span>kubectl</span><span>=</span><span>'1.34.x-*'</span> <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-mark hold kubelet kubectl
</span></span></code></pre></div></p></div><div id="k8s-install-kubelet-1" class="tab-pane"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubelet-<span>'1.34.x-*'</span> kubectl-<span>'1.34.x-*'</span> --disableexcludes<span>=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubelet-<span>'1.34.x-*'</span> kubectl-<span>'1.34.x-*'</span> --setopt<span>=</span><span>disable_excludes</span><span>=</span>kubernetes
</span></span></code></pre></div></p></div></div></li><li><p>Restart the kubelet:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo systemctl daemon-reload
</span></span><span><span>sudo systemctl restart kubelet
</span></span></code></pre></div></li></ol><h3 id="uncordon-the-node">Uncordon the node</h3><p>Bring the node back online by marking it schedulable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace &lt;node-to-uncordon&gt; with the name of your node</span>
</span></span><span><span>kubectl uncordon &lt;node-to-uncordon&gt;
</span></span></code></pre></div><h2 id="upgrade-worker-nodes">Upgrade worker nodes</h2><p>The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time,
without compromising the minimum required capacity for running your workloads.</p><p>The following pages show how to upgrade Linux and Windows worker nodes:</p><ul><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrade Linux nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrade Windows nodes</a></li></ul><h2 id="verify-the-status-of-the-cluster">Verify the status of the cluster</h2><p>After the kubelet is upgraded on all nodes verify that all nodes are available again by running
the following command from anywhere kubectl can access the cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes
</span></span></code></pre></div><p>The <code>STATUS</code> column should show <code>Ready</code> for all your nodes, and the version number should be updated.</p><h2 id="recovering-from-a-failure-state">Recovering from a failure state</h2><p>If <code>kubeadm upgrade</code> fails and does not roll back, for example because of an unexpected shutdown during execution, you can run <code>kubeadm upgrade</code> again.
This command is idempotent and eventually makes sure that the actual state is the desired state you declare.</p><p>To recover from a bad state, you can also run <code>sudo kubeadm upgrade apply --force</code> without changing the version that your cluster is running.</p><p>During upgrade kubeadm writes the following backup folders under <code>/etc/kubernetes/tmp</code>:</p><ul><li><code>kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;</code></li><li><code>kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code></li></ul><p><code>kubeadm-backup-etcd</code> contains a backup of the local etcd member data for this control plane Node.
In case of an etcd upgrade failure and if the automatic rollback does not work, the contents of this folder
can be manually restored in <code>/var/lib/etcd</code>. In case external etcd is used this backup folder will be empty.</p><p><code>kubeadm-backup-manifests</code> contains a backup of the static Pod manifest files for this control plane Node.
In case of a upgrade failure and if the automatic rollback does not work, the contents of this folder can be
manually restored in <code>/etc/kubernetes/manifests</code>. If for some reason there is no difference between a pre-upgrade
and post-upgrade manifest file for a certain component, a backup file for it will not be written.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>After the cluster upgrade using kubeadm, the backup directory <code>/etc/kubernetes/tmp</code> will remain and
these backup files will need to be cleared manually.</div><h2 id="how-it-works">How it works</h2><p><code>kubeadm upgrade apply</code> does the following:</p><ul><li>Checks that your cluster is in an upgradeable state:<ul><li>The API server is reachable</li><li>All nodes are in the <code>Ready</code> state</li><li>The control plane is healthy</li></ul></li><li>Enforces the version skew policies.</li><li>Makes sure the control plane images are available or available to pull to the machine.</li><li>Generates replacements and/or uses user supplied overwrites if component configs require version upgrades.</li><li>Upgrades the control plane components or rollbacks if any of them fails to come up.</li><li>Applies the new <code>CoreDNS</code> and <code>kube-proxy</code> manifests and makes sure that all necessary RBAC rules are created.</li><li>Creates new certificate and key files of the API server and backs up old files if they're about to expire in 180 days.</li></ul><p><code>kubeadm upgrade node</code> does the following on additional control plane nodes:</p><ul><li>Fetches the kubeadm <code>ClusterConfiguration</code> from the cluster.</li><li>Optionally backups the kube-apiserver certificate.</li><li>Upgrades the static Pod manifests for the control plane components.</li><li>Upgrades the kubelet configuration for this node.</li></ul><p><code>kubeadm upgrade node</code> does the following on worker nodes:</p><ul><li>Fetches the kubeadm <code>ClusterConfiguration</code> from the cluster.</li><li>Upgrades the kubelet configuration for this node.</li></ul></div></div><div><div class="td-content"><h1>Upgrading Linux nodes</h1><p>This page explains how to upgrade a Linux Worker Nodes created with kubeadm.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have shell access to all the nodes, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial
on a cluster with at least two nodes that are not acting as control plane hosts.</p><p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>Familiarize yourself with <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">the process for upgrading the rest of your kubeadm
cluster</a>. You will want to
upgrade the control plane nodes before upgrading your Linux Worker nodes.</li></ul><h2 id="changing-the-package-repository">Changing the package repository</h2><p>If you're using the community-owned package repositories (<code>pkgs.k8s.io</code>), you need to
enable the package repository for the desired Kubernetes minor release. This is explained in
<a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing the Kubernetes package repository</a>
document.</p><div class="alert alert-secondary callout note"><strong>Note:</strong> The legacy package repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) have been
<a href="/blog/2023/08/31/legacy-package-repository-deprecation/">deprecated and frozen starting from September 13, 2023</a>.
<strong>Using the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">new package repositories hosted at <code>pkgs.k8s.io</code></a>
is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.</strong>
The deprecated legacy repositories, and their contents, might be removed at any time in the future and without
a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.</div><h2 id="upgrading-worker-nodes">Upgrading worker nodes</h2><h3 id="upgrade-kubeadm">Upgrade kubeadm</h3><p>Upgrade kubeadm:</p><ul class="nav nav-tabs" id="k8s-install-kubeadm-worker-nodes"><li class="nav-item"><a class="nav-link active" href="#k8s-install-kubeadm-worker-nodes-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-install-kubeadm-worker-nodes-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-kubeadm-worker-nodes"><div id="k8s-install-kubeadm-worker-nodes-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo apt-mark unhold kubeadm <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-get update <span>&amp;&amp;</span> sudo apt-get install -y <span>kubeadm</span><span>=</span><span>'1.34.x-*'</span> <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-mark hold kubeadm
</span></span></code></pre></div></p></div><div id="k8s-install-kubeadm-worker-nodes-1" class="tab-pane"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubeadm-<span>'1.34.x-*'</span> --disableexcludes<span>=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubeadm-<span>'1.34.x-*'</span> --setopt<span>=</span><span>disable_excludes</span><span>=</span>kubernetes
</span></span></code></pre></div></p></div></div><h3 id="call-kubeadm-upgrade">Call "kubeadm upgrade"</h3><p>For worker nodes this upgrades the local kubelet configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm upgrade node
</span></span></code></pre></div><h3 id="drain-the-node">Drain the node</h3><p>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># execute this command on a control plane node</span>
</span></span><span><span><span># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
</span></span><span><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><h3 id="upgrade-kubelet-and-kubectl">Upgrade kubelet and kubectl</h3><ol><li><p>Upgrade the kubelet and kubectl:</p><ul class="nav nav-tabs" id="k8s-kubelet-and-kubectl"><li class="nav-item"><a class="nav-link active" href="#k8s-kubelet-and-kubectl-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-kubelet-and-kubectl-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-kubelet-and-kubectl"><div id="k8s-kubelet-and-kubectl-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo apt-mark unhold kubelet kubectl <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-get update <span>&amp;&amp;</span> sudo apt-get install -y <span>kubelet</span><span>=</span><span>'1.34.x-*'</span> <span>kubectl</span><span>=</span><span>'1.34.x-*'</span> <span>&amp;&amp;</span> <span>\
</span></span></span><span><span><span></span>sudo apt-mark hold kubelet kubectl
</span></span></code></pre></div></p></div><div id="k8s-kubelet-and-kubectl-1" class="tab-pane"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubelet-<span>'1.34.x-*'</span> kubectl-<span>'1.34.x-*'</span> --disableexcludes<span>=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span><span>sudo yum install -y kubelet-<span>'1.34.x-*'</span> kubectl-<span>'1.34.x-*'</span> --setopt<span>=</span><span>disable_excludes</span><span>=</span>kubernetes
</span></span></code></pre></div></p></div></div></li><li><p>Restart the kubelet:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo systemctl daemon-reload
</span></span><span><span>sudo systemctl restart kubelet
</span></span></code></pre></div></li></ol><h3 id="uncordon-the-node">Uncordon the node</h3><p>Bring the node back online by marking it schedulable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># execute this command on a control plane node</span>
</span></span><span><span><span># replace &lt;node-to-uncordon&gt; with the name of your node</span>
</span></span><span><span>kubectl uncordon &lt;node-to-uncordon&gt;
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrade Windows nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Upgrading Windows nodes</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [beta]</code></div><p>This page explains how to upgrade a Windows node created with kubeadm.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have shell access to all the nodes, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial
on a cluster with at least two nodes that are not acting as control plane hosts.</p>Your Kubernetes server must be at or later than version 1.17.<p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>Familiarize yourself with <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">the process for upgrading the rest of your kubeadm
cluster</a>. You will want to
upgrade the control plane nodes before upgrading your Windows nodes.</li></ul><h2 id="upgrading-worker-nodes">Upgrading worker nodes</h2><h3 id="upgrade-kubeadm">Upgrade kubeadm</h3><ol><li><p>From the Windows node, upgrade kubeadm:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span># replace 1.34.0 with your desired version</span>
</span></span><span><span>curl.exe -Lo &lt;<span>path-to</span>-kubeadm.exe&gt;  <span>"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kubeadm.exe"</span>
</span></span></code></pre></div></li></ol><h3 id="drain-the-node">Drain the node</h3><ol><li><p>From a machine with access to the Kubernetes API,
prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
</span></span><span><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><p>You should see output similar to this:</p><pre tabindex="0"><code>node/ip-172-31-85-18 cordoned
node/ip-172-31-85-18 drained
</code></pre></li></ol><h3 id="upgrade-the-kubelet-configuration">Upgrade the kubelet configuration</h3><ol><li><p>From the Windows node, call the following command to sync new kubelet configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>kubeadm upgrade node
</span></span></code></pre></div></li></ol><h3 id="upgrade-kubelet-and-kube-proxy">Upgrade kubelet and kube-proxy</h3><ol><li><p>From the Windows node, upgrade and restart the kubelet:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>stop-service</span> kubelet
</span></span><span><span>curl.exe -Lo &lt;<span>path-to</span>-kubelet.exe&gt; <span>"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kubelet.exe"</span>
</span></span><span><span><span>restart-service</span> kubelet
</span></span></code></pre></div></li><li><p>From the Windows node, upgrade and restart the kube-proxy.</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>stop-service</span> <span>kube-proxy</span>
</span></span><span><span>curl.exe -Lo &lt;<span>path-to</span>-kube-proxy.exe&gt; <span>"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kube-proxy.exe"</span>
</span></span><span><span><span>restart-service</span> <span>kube-proxy</span>
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you are running kube-proxy in a HostProcess container within a Pod, and not as a Windows Service,
you can upgrade kube-proxy by applying a newer version of your kube-proxy manifests.</div><h3 id="uncordon-the-node">Uncordon the node</h3><ol><li><p>From a machine with access to the Kubernetes API,
bring the node back online by marking it schedulable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># replace &lt;node-to-drain&gt; with the name of your node</span>
</span></span><span><span>kubectl uncordon &lt;node-to-drain&gt;
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrade Linux nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Configuring a cgroup driver</h1><p>This page explains how to configure the kubelet's cgroup driver to match the container
runtime cgroup driver for kubeadm clusters.</p><h2 id="before-you-begin">Before you begin</h2><p>You should be familiar with the Kubernetes
<a href="/docs/setup/production-environment/container-runtimes/">container runtime requirements</a>.</p><h2 id="configuring-the-container-runtime-cgroup-driver">Configuring the container runtime cgroup driver</h2><p>The <a href="/docs/setup/production-environment/container-runtimes/">Container runtimes</a> page
explains that the <code>systemd</code> driver is recommended for kubeadm based setups instead
of the kubelet's <a href="/docs/reference/config-api/kubelet-config.v1beta1/">default</a> <code>cgroupfs</code> driver,
because kubeadm manages the kubelet as a
<a href="/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">systemd service</a>.</p><p>The page also provides details on how to set up a number of different container runtimes with the
<code>systemd</code> driver by default.</p><h2 id="configuring-the-kubelet-cgroup-driver">Configuring the kubelet cgroup driver</h2><p>kubeadm allows you to pass a <code>KubeletConfiguration</code> structure during <code>kubeadm init</code>.
This <code>KubeletConfiguration</code> can include the <code>cgroupDriver</code> field which controls the cgroup
driver of the kubelet.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In v1.22 and later, if the user does not set the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code>,
kubeadm defaults it to <code>systemd</code>.</p><p>In Kubernetes v1.28, you can enable automatic detection of the
cgroup driver as an alpha feature.
See <a href="/docs/setup/production-environment/container-runtimes/#systemd-cgroup-driver">systemd cgroup driver</a>
for more details.</p></div><p>A minimal example of configuring the field explicitly:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># kubeadm-config.yaml</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>kubeadm.k8s.io/v1beta4<span>
</span></span></span><span><span><span></span><span>kubernetesVersion</span>:<span> </span>v1.21.0<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>cgroupDriver</span>:<span> </span>systemd<span>
</span></span></span></code></pre></div><p>Such a configuration file can then be passed to the kubeadm command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubeadm init --config kubeadm-config.yaml
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Kubeadm uses the same <code>KubeletConfiguration</code> for all nodes in the cluster.
The <code>KubeletConfiguration</code> is stored in a <a href="/docs/concepts/configuration/configmap/">ConfigMap</a>
object under the <code>kube-system</code> namespace.</p><p>Executing the sub commands <code>init</code>, <code>join</code> and <code>upgrade</code> would result in kubeadm
writing the <code>KubeletConfiguration</code> as a file under <code>/var/lib/kubelet/config.yaml</code>
and passing it to the local node kubelet.</p><p>On each node, kubeadm detects the CRI socket and stores its details into the <code>/var/lib/kubelet/instance-config.yaml</code> file.
When executing the <code>init</code>, <code>join</code>, or <code>upgrade</code> subcommands,
kubeadm patches the <code>containerRuntimeEndpoint</code> value from this instance configuration into <code>/var/lib/kubelet/config.yaml</code>.</p></div><h2 id="using-the-cgroupfs-driver">Using the <code>cgroupfs</code> driver</h2><p>To use <code>cgroupfs</code> and to prevent <code>kubeadm upgrade</code> from modifying the
<code>KubeletConfiguration</code> cgroup driver on existing setups, you must be explicit
about its value. This applies to a case where you do not wish future versions
of kubeadm to apply the <code>systemd</code> driver by default.</p><p>See the below section on "<a href="#modify-the-kubelet-configmap">Modify the kubelet ConfigMap</a>" for details on
how to be explicit about the value.</p><p>If you wish to configure a container runtime to use the <code>cgroupfs</code> driver,
you must refer to the documentation of the container runtime of your choice.</p><h2 id="migrating-to-the-systemd-driver">Migrating to the <code>systemd</code> driver</h2><p>To change the cgroup driver of an existing kubeadm cluster from <code>cgroupfs</code> to <code>systemd</code> in-place,
a similar procedure to a kubelet upgrade is required. This must include both
steps outlined below.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Alternatively, it is possible to replace the old nodes in the cluster with new ones
that use the <code>systemd</code> driver. This requires executing only the first step below
before joining the new nodes and ensuring the workloads can safely move to the new
nodes before deleting the old nodes.</div><h3 id="modify-the-kubelet-configmap">Modify the kubelet ConfigMap</h3><ul><li><p>Call <code>kubectl edit cm kubelet-config -n kube-system</code>.</p></li><li><p>Either modify the existing <code>cgroupDriver</code> value or add a new field that looks like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>cgroupDriver</span>:<span> </span>systemd<span>
</span></span></span></code></pre></div><p>This field must be present under the <code>kubelet:</code> section of the ConfigMap.</p></li></ul><h3 id="update-the-cgroup-driver-on-all-nodes">Update the cgroup driver on all nodes</h3><p>For each node in the cluster:</p><ul><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Drain the node</a> using <code>kubectl drain &lt;node-name&gt; --ignore-daemonsets</code></li><li>Stop the kubelet using <code>systemctl stop kubelet</code></li><li>Stop the container runtime</li><li>Modify the container runtime cgroup driver to <code>systemd</code></li><li>Set <code>cgroupDriver: systemd</code> in <code>/var/lib/kubelet/config.yaml</code></li><li>Start the container runtime</li><li>Start the kubelet using <code>systemctl start kubelet</code></li><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Uncordon the node</a> using <code>kubectl uncordon &lt;node-name&gt;</code></li></ul><p>Execute these steps on nodes one at a time to ensure workloads
have sufficient time to schedule on different nodes.</p><p>Once the process is complete ensure that all nodes and workloads are healthy.</p></div></div><div><div class="td-content"><h1>Certificate Management with kubeadm</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.15 [stable]</code></div><p>Client certificates generated by <a href="/docs/reference/setup-tools/kubeadm/">kubeadm</a> expire after 1 year.
This page explains how to manage certificate renewals with kubeadm. It also covers other tasks related
to kubeadm certificate management.</p><p>The Kubernetes project recommends upgrading to the latest patch releases promptly, and
to ensure that you are running a supported minor release of Kubernetes.
Following this recommendation helps you to stay secure.</p><h2 id="before-you-begin">Before you begin</h2><p>You should be familiar with <a href="/docs/setup/best-practices/certificates/">PKI certificates and requirements in Kubernetes</a>.</p><p>You should be familiar with how to pass a <a href="/docs/reference/config-api/kubeadm-config.v1beta4/">configuration</a> file to the kubeadm commands.</p><p>This guide covers the usage of the <code>openssl</code> command (used for manual certificate signing,
if you choose that approach), but you can use your preferred tools.</p><p>Some of the steps here use <code>sudo</code> for administrator access. You can use any equivalent tool.</p><h2 id="custom-certificates">Using custom certificates</h2><p>By default, kubeadm generates all the certificates needed for a cluster to run.
You can override this behavior by providing your own certificates.</p><p>To do so, you must place them in whatever directory is specified by the
<code>--cert-dir</code> flag or the <code>certificatesDir</code> field of kubeadm's <code>ClusterConfiguration</code>.
By default this is <code>/etc/kubernetes/pki</code>.</p><p>If a given certificate and private key pair exists before running <code>kubeadm init</code>,
kubeadm does not overwrite them. This means you can, for example, copy an existing
CA into <code>/etc/kubernetes/pki/ca.crt</code> and <code>/etc/kubernetes/pki/ca.key</code>,
and kubeadm will use this CA for signing the rest of the certificates.</p><h2 id="choosing-encryption-algorithm">Choosing an encryption algorithm</h2><p>kubeadm allows you to choose an encryption algorithm that is used for creating
public and private keys. That can be done by using the <code>encryptionAlgorithm</code> field of the
kubeadm configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubeadm.k8s.io/v1beta4<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterConfiguration<span>
</span></span></span><span><span><span></span><span>encryptionAlgorithm</span>:<span> </span>&lt;ALGORITHM&gt;<span>
</span></span></span></code></pre></div><p><code>&lt;ALGORITHM&gt;</code> can be one of <code>RSA-2048</code> (default), <code>RSA-3072</code>, <code>RSA-4096</code> or <code>ECDSA-P256</code>.</p><h2 id="choosing-cert-validity-period">Choosing certificate validity period</h2><p>kubeadm allows you to choose the validity period of CA and leaf certificates.
That can be done by using the <code>certificateValidityPeriod</code> and <code>caCertificateValidityPeriod</code>
fields of the kubeadm configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubeadm.k8s.io/v1beta4<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterConfiguration<span>
</span></span></span><span><span><span></span><span>certificateValidityPeriod: 8760h # Default</span>:<span> </span><span>365</span><span> </span>days &#215; 24 hours = 1 year<span>
</span></span></span><span><span><span></span><span>caCertificateValidityPeriod: 87600h # Default</span>:<span> </span><span>365</span><span> </span>days &#215; 24 hours * 10 = 10 years<span>
</span></span></span></code></pre></div><p>The values of the fields follow the accepted format for
<a href="https://pkg.go.dev/time#ParseDuration">Go's <code>time.Duration</code> values</a>, with the longest supported
unit being <code>h</code> (hours).</p><h2 id="external-ca-mode">External CA mode</h2><p>It is also possible to provide only the <code>ca.crt</code> file and not the
<code>ca.key</code> file (this is only available for the root CA file, not other cert pairs).
If all other certificates and kubeconfig files are in place, kubeadm recognizes
this condition and activates the "External CA" mode. kubeadm will proceed without the
CA key on disk.</p><p>Instead, run the controller-manager standalone with <code>--controllers=csrsigner</code> and
point to the CA certificate and key.</p><p>There are various ways to prepare the component credentials when using external CA mode.</p><h3 id="manual-preparation-of-component-credentials">Manual preparation of component credentials</h3><p><a href="/docs/setup/best-practices/certificates/">PKI certificates and requirements</a> includes information
on how to prepare all the required by kubeadm component credentials manually.</p><p>This guide covers the usage of the <code>openssl</code> command (used for manual certificate signing,
if you choose that approach), but you can use your preferred tools.</p><h3 id="preparation-of-credentials-by-signing-csrs-generated-by-kubeadm">Preparation of credentials by signing CSRs generated by kubeadm</h3><p>kubeadm can <a href="#signing-csr">generate CSR files</a> that you can sign manually with tools like
<code>openssl</code> and your external CA. These CSR files will include all the specification for credentials
that components deployed by kubeadm require.</p><h3 id="automated-preparation-of-component-credentials-by-using-kubeadm-phases">Automated preparation of component credentials by using kubeadm phases</h3><p>Alternatively, it is possible to use kubeadm phase commands to automate this process.</p><ul><li>Go to a host that you want to prepare as a kubeadm control plane node with external CA.</li><li>Copy the external CA files <code>ca.crt</code> and <code>ca.key</code> that you have into <code>/etc/kubernetes/pki</code> on the node.</li><li>Prepare a temporary <a href="/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file">kubeadm configuration file</a>
called <code>config.yaml</code> that can be used with <code>kubeadm init</code>. Make sure that this file includes
any relevant cluster wide or host-specific information that could be included in certificates, such as,
<code>ClusterConfiguration.controlPlaneEndpoint</code>, <code>ClusterConfiguration.certSANs</code> and <code>InitConfiguration.APIEndpoint</code>.</li><li>On the same host execute the commands <code>kubeadm init phase kubeconfig all --config config.yaml</code> and
<code>kubeadm init phase certs all --config config.yaml</code>. This will generate all required kubeconfig
files and certificates under <code>/etc/kubernetes/</code> and its <code>pki</code> sub directory.</li><li>Inspect the generated files. Delete <code>/etc/kubernetes/pki/ca.key</code>, delete or move to a safe location
the file <code>/etc/kubernetes/super-admin.conf</code>.</li><li>On nodes where <code>kubeadm join</code> will be called also delete <code>/etc/kubernetes/kubelet.conf</code>.
This file is only required on the first node where <code>kubeadm init</code> will be called.</li><li>Note that some files such <code>pki/sa.*</code>, <code>pki/front-proxy-ca.*</code> and <code>pki/etc/ca.*</code> are
shared between control plane nodes, You can generate them once and
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/#manual-certs">distribute them manually</a>
to nodes where <code>kubeadm join</code> will be called, or you can use the
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes"><code>--upload-certs</code></a>
functionality of <code>kubeadm init</code> and <code>--certificate-key</code> of <code>kubeadm join</code> to automate this distribution.</li></ul><p>Once the credentials are prepared on all nodes, call <code>kubeadm init</code> and <code>kubeadm join</code> for these nodes to
join the cluster. kubeadm will use the existing kubeconfig and certificate files under <code>/etc/kubernetes/</code>
and its <code>pki</code> sub directory.</p><h2 id="check-certificate-expiration">Certificate expiry and management</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>kubeadm</code> cannot manage certificates signed by an external CA.</div><p>You can use the <code>check-expiration</code> subcommand to check when certificates expire:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubeadm certs check-expiration
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
</span></span></span><span><span><span>admin.conf                 Dec 30, 2020 23:36 UTC   364d                                    no
</span></span></span><span><span><span>apiserver                  Dec 30, 2020 23:36 UTC   364d            ca                      no
</span></span></span><span><span><span>apiserver-etcd-client      Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span><span><span>apiserver-kubelet-client   Dec 30, 2020 23:36 UTC   364d            ca                      no
</span></span></span><span><span><span>controller-manager.conf    Dec 30, 2020 23:36 UTC   364d                                    no
</span></span></span><span><span><span>etcd-healthcheck-client    Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span><span><span>etcd-peer                  Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span><span><span>etcd-server                Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span><span><span>front-proxy-client         Dec 30, 2020 23:36 UTC   364d            front-proxy-ca          no
</span></span></span><span><span><span>scheduler.conf             Dec 30, 2020 23:36 UTC   364d                                    no
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
</span></span></span><span><span><span>ca                      Dec 28, 2029 23:36 UTC   9y              no
</span></span></span><span><span><span>etcd-ca                 Dec 28, 2029 23:36 UTC   9y              no
</span></span></span><span><span><span>front-proxy-ca          Dec 28, 2029 23:36 UTC   9y              no
</span></span></span></code></pre></div><p>The command shows expiration/residual time for the client certificates in the
<code>/etc/kubernetes/pki</code> folder and for the client certificate embedded in the kubeconfig files used
by kubeadm (<code>admin.conf</code>, <code>controller-manager.conf</code> and <code>scheduler.conf</code>).</p><p>Additionally, kubeadm informs the user if the certificate is externally managed; in this case, the
user should take care of managing certificate renewal manually/using other tools.</p><p>The <code>kubelet.conf</code> configuration file is not included in the list above because kubeadm
configures kubelet
for <a href="/docs/tasks/tls/certificate-rotation/">automatic certificate renewal</a>
with rotatable certificates under <code>/var/lib/kubelet/pki</code>.
To repair an expired kubelet client certificate see
<a href="/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert">Kubelet client certificate rotation fails</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>On nodes created with <code>kubeadm init</code> from versions prior to kubeadm version 1.17, there is a
<a href="https://github.com/kubernetes/kubeadm/issues/1753">bug</a> where you manually have to modify the
contents of <code>kubelet.conf</code>. After <code>kubeadm init</code> finishes, you should update <code>kubelet.conf</code> to
point to the rotated kubelet client certificates, by replacing <code>client-certificate-data</code> and
<code>client-key-data</code> with:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>client-certificate</span>:<span> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span>
</span></span></span><span><span><span></span><span>client-key</span>:<span> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span>
</span></span></span></code></pre></div></div><h2 id="automatic-certificate-renewal">Automatic certificate renewal</h2><p>kubeadm renews all the certificates during control plane
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">upgrade</a>.</p><p>This feature is designed for addressing the simplest use cases;
if you don't have specific requirements on certificate renewal and perform Kubernetes version
upgrades regularly (less than 1 year in between each upgrade), kubeadm will take care of keeping
your cluster up to date and reasonably secure.</p><p>If you have more complex requirements for certificate renewal, you can opt out from the default
behavior by passing <code>--certificate-renewal=false</code> to <code>kubeadm upgrade apply</code> or to <code>kubeadm upgrade node</code>.</p><h2 id="manual-certificate-renewal">Manual certificate renewal</h2><p>You can renew your certificates manually at any time with the <code>kubeadm certs renew</code> command,
with the appropriate command line options. If you are running cluster with a replicated control
plane, this command needs to be executed on all the control-plane nodes.</p><p>This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in <code>/etc/kubernetes/pki</code>.</p><p><code>kubeadm certs renew</code> uses the existing certificates as the authoritative source for attributes
(Common Name, Organization, subject alternative name) and does not rely on the <code>kubeadm-config</code>
ConfigMap.
Even so, the Kubernetes project recommends keeping the served certificate and the associated
values in that ConfigMap synchronized, to avoid any risk of confusion.</p><p>After running the command you should restart the control plane Pods. This is required since
dynamic certificate reload is currently not supported for all components and certificates.
<a href="/docs/tasks/configure-pod-container/static-pod/">Static Pods</a> are managed by the local kubelet
and not by the API Server, thus kubectl cannot be used to delete and restart them.
To restart a static Pod you can temporarily remove its manifest file from <code>/etc/kubernetes/manifests/</code>
and wait for 20 seconds (see the <code>fileCheckFrequency</code> value in <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration struct</a>).
The kubelet will terminate the Pod if it's no longer in the manifest directory.
You can then move the file back and after another <code>fileCheckFrequency</code> period, the kubelet will recreate
the Pod and the certificate renewal for the component can complete.</p><p><code>kubeadm certs renew</code> can renew any specific certificate or, with the subcommand <code>all</code>, it can renew all of them:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If you are running cluster with a replicated control plane, this command</span>
</span></span><span><span><span># needs to be executed on all the control-plane nodes.</span>
</span></span><span><span>kubeadm certs renew all
</span></span></code></pre></div><h3 id="admin-certificate-copy">Copying the administrator certificate (optional)</h3><p>Clusters built with kubeadm often copy the <code>admin.conf</code> certificate into
<code>$HOME/.kube/config</code>, as instructed in <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>.
On such a system, to update the contents of <code>$HOME/.kube/config</code>
after renewing the <code>admin.conf</code>, you could run the following commands:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo cp -i /etc/kubernetes/admin.conf <span>$HOME</span>/.kube/config
</span></span><span><span>sudo chown <span>$(</span>id -u<span>)</span>:<span>$(</span>id -g<span>)</span> <span>$HOME</span>/.kube/config
</span></span></code></pre></div><h2 id="renew-certificates-with-the-kubernetes-certificates-api">Renew certificates with the Kubernetes certificates API</h2><p>This section provides more details about how to execute manual certificate renewal using the Kubernetes certificates API.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>These are advanced topics for users who need to integrate their organization's certificate
infrastructure into a kubeadm-built cluster. If the default kubeadm configuration satisfies your
needs, you should let kubeadm manage certificates instead.</div><h3 id="set-up-a-signer">Set up a signer</h3><p>The Kubernetes Certificate Authority does not work out of the box.
You can configure an external signer such as <a href="https://cert-manager.io/docs/configuration/ca/">cert-manager</a>,
or you can use the built-in signer.</p><p>The built-in signer is part of <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a>.</p><p>To activate the built-in signer, you must pass the <code>--cluster-signing-cert-file</code> and
<code>--cluster-signing-key-file</code> flags.</p><p>If you're creating a new cluster, you can use a kubeadm
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/">configuration file</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubeadm.k8s.io/v1beta4<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterConfiguration<span>
</span></span></span><span><span><span></span><span>controllerManager</span>:<span>
</span></span></span><span><span><span>  </span><span>extraArgs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span><span>"cluster-signing-cert-file"</span><span>
</span></span></span><span><span><span>    </span><span>value</span>:<span> </span><span>"/etc/kubernetes/pki/ca.crt"</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span><span>"cluster-signing-key-file"</span><span>
</span></span></span><span><span><span>    </span><span>value</span>:<span> </span><span>"/etc/kubernetes/pki/ca.key"</span><span>
</span></span></span></code></pre></div><h3 id="create-certificate-signing-requests-csr">Create certificate signing requests (CSR)</h3><p>See <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest">Create CertificateSigningRequest</a>
for creating CSRs with the Kubernetes API.</p><h2 id="renew-certificates-with-external-ca">Renew certificates with external CA</h2><p>This section provide more details about how to execute manual certificate renewal using an external CA.</p><p>To better integrate with external CAs, kubeadm can also produce certificate signing requests (CSRs).
A CSR represents a request to a CA for a signed certificate for a client.
In kubeadm terms, any certificate that would normally be signed by an on-disk CA can be produced
as a CSR instead. A CA, however, cannot be produced as a CSR.</p><h3 id="renewal-by-using-certificate-signing-requests-csr">Renewal by using certificate signing requests (CSR)</h3><p>Renewal of ceritficates is possible by generating new CSRs and signing them with the external CA.
For more details about working with CSRs generated by kubeadm see the section
<a href="#signing-csr">Signing certificate signing requests (CSR) generated by kubeadm</a>.</p><h2 id="certificate-authority-rotation">Certificate authority (CA) rotation</h2><p>Kubeadm does not support rotation or replacement of CA certificates out of the box.</p><p>For more information about manual rotation or replacement of CA, see <a href="/docs/tasks/tls/manual-rotation-of-ca-certificates/">manual rotation of CA certificates</a>.</p><h2 id="kubelet-serving-certs">Enabling signed kubelet serving certificates</h2><p>By default the kubelet serving certificate deployed by kubeadm is self-signed.
This means a connection from external services like the
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a> to a
kubelet cannot be secured with TLS.</p><p>To configure the kubelets in a new kubeadm cluster to obtain properly signed serving
certificates you must pass the following minimal configuration to <code>kubeadm init</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubeadm.k8s.io/v1beta4<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterConfiguration<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>serverTLSBootstrap</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>If you have already created the cluster you must adapt it by doing the following:</p><ul><li>Find and edit the <code>kubelet-config</code> ConfigMap in the <code>kube-system</code> namespace.
In that ConfigMap, the <code>kubelet</code> key has a
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration</a>
document as its value. Edit the KubeletConfiguration document to set <code>serverTLSBootstrap: true</code>.</li><li>On each node, add the <code>serverTLSBootstrap: true</code> field in <code>/var/lib/kubelet/config.yaml</code>
and restart the kubelet with <code>systemctl restart kubelet</code></li></ul><p>The field <code>serverTLSBootstrap: true</code> will enable the bootstrap of kubelet serving
certificates by requesting them from the <code>certificates.k8s.io</code> API. One known limitation
is that the CSRs (Certificate Signing Requests) for these certificates cannot be automatically
approved by the default signer in the kube-controller-manager -
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers"><code>kubernetes.io/kubelet-serving</code></a>.
This will require action from the user or a third party controller.</p><p>These CSRs can be viewed using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME        AGE     SIGNERNAME                        REQUESTOR                      CONDITION
</span></span></span><span><span><span>csr-9wvgt   112s    kubernetes.io/kubelet-serving     system:node:worker-1           Pending
</span></span></span><span><span><span>csr-lz97v   1m58s   kubernetes.io/kubelet-serving     system:node:control-plane-1    Pending
</span></span></span></code></pre></div><p>To approve them you can do the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl certificate approve &lt;CSR-name&gt;
</span></span></code></pre></div><p>By default, these serving certificate will expire after one year. Kubeadm sets the
<code>KubeletConfiguration</code> field <code>rotateCertificates</code> to <code>true</code>, which means that close
to expiration a new set of CSRs for the serving certificates will be created and must
be approved to complete the rotation. To understand more see
<a href="/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#certificate-rotation">Certificate Rotation</a>.</p><p>If you are looking for a solution for automatic approval of these CSRs it is recommended
that you contact your cloud provider and ask if they have a CSR signer that verifies
the node identity with an out of band mechanism.</p><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Third party custom controllers can be used:</p><ul><li><a href="https://github.com/postfinance/kubelet-csr-approver">kubelet-csr-approver</a></li></ul><p>Such a controller is not a secure mechanism unless it not only verifies the CommonName
in the CSR but also verifies the requested IPs and domain names. This would prevent
a malicious actor that has access to a kubelet client certificate to create
CSRs requesting serving certificates for any IP or domain name.</p><h2 id="kubeconfig-additional-users">Generating kubeconfig files for additional users</h2><p>During cluster creation, <code>kubeadm init</code> signs the certificate in the <code>super-admin.conf</code>
to have <code>Subject: O = system:masters, CN = kubernetes-super-admin</code>.
<a href="/docs/reference/access-authn-authz/rbac/#user-facing-roles"><code>system:masters</code></a>
is a break-glass, super user group that bypasses the authorization layer (for example,
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>). The file <code>admin.conf</code> is also created
by kubeadm on control plane nodes and it contains a certificate with
<code>Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin</code>. <code>kubeadm:cluster-admins</code>
is a group logically belonging to kubeadm. If your cluster uses RBAC
(the kubeadm default), the <code>kubeadm:cluster-admins</code> group is bound to the
<a href="/docs/reference/access-authn-authz/rbac/#user-facing-roles"><code>cluster-admin</code></a> ClusterRole.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Avoid sharing the <code>super-admin.conf</code> or <code>admin.conf</code> files. Instead, create least
privileged access even for people who work as administrators and use that least
privilege alternative for anything other than break-glass (emergency) access.</div><p>You can use the <a href="/docs/reference/setup-tools/kubeadm/kubeadm-kubeconfig/"><code>kubeadm kubeconfig user</code></a>
command to generate kubeconfig files for additional users.
The command accepts a mixture of command line flags and
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/">kubeadm configuration</a> options.
The generated kubeconfig will be written to stdout and can be piped to a file using
<code>kubeadm kubeconfig user ... &gt; somefile.conf</code>.</p><p>Example configuration file that can be used with <code>--config</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># example.yaml</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>kubeadm.k8s.io/v1beta4<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterConfiguration<span>
</span></span></span><span><span><span></span><span># Will be used as the target "cluster" in the kubeconfig</span><span>
</span></span></span><span><span><span></span><span>clusterName</span>:<span> </span><span>"kubernetes"</span><span>
</span></span></span><span><span><span></span><span># Will be used as the "server" (IP or DNS name) of this cluster in the kubeconfig</span><span>
</span></span></span><span><span><span></span><span>controlPlaneEndpoint</span>:<span> </span><span>"some-dns-address:6443"</span><span>
</span></span></span><span><span><span></span><span># The cluster CA key and certificate will be loaded from this local directory</span><span>
</span></span></span><span><span><span></span><span>certificatesDir</span>:<span> </span><span>"/etc/kubernetes/pki"</span><span>
</span></span></span></code></pre></div><p>Make sure that these settings match the desired target cluster settings.
To see the settings of an existing cluster use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get cm kubeadm-config -n kube-system -o<span>=</span><span>jsonpath</span><span>=</span><span>"{.data.ClusterConfiguration}"</span>
</span></span></code></pre></div><p>The following example will generate a kubeconfig file with credentials valid for 24 hours
for a new user <code>johndoe</code> that is part of the <code>appdevs</code> group:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubeadm kubeconfig user --config example.yaml --org appdevs --client-name johndoe --validity-period 24h
</span></span></code></pre></div><p>The following example will generate a kubeconfig file with administrator credentials valid for 1 week:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubeadm kubeconfig user --config example.yaml --client-name admin --validity-period 168h
</span></span></code></pre></div><h2 id="signing-csr">Signing certificate signing requests (CSR) generated by kubeadm</h2><p>You can create certificate signing requests with <code>kubeadm certs generate-csr</code>.
Calling this command will generate <code>.csr</code> / <code>.key</code> file pairs for regular
certificates. For certificates embedded in kubeconfig files, the command will
generate a <code>.csr</code> / <code>.conf</code> pair where the key is already embedded in the <code>.conf</code> file.</p><p>A CSR file contains all relevant information for a CA to sign a certificate.
kubeadm uses a
<a href="/docs/setup/best-practices/certificates/#all-certificates">well defined specification</a>
for all its certificates and CSRs.</p><p>The default certificate directory is <code>/etc/kubernetes/pki</code>, while the default
directory for kubeconfig files is <code>/etc/kubernetes</code>. These defaults can be
overridden with the flags <code>--cert-dir</code> and <code>--kubeconfig-dir</code>, respectively.</p><p>To pass custom options to <code>kubeadm certs generate-csr</code> use the <code>--config</code> flag,
which accepts a <a href="/docs/reference/config-api/kubeadm-config.v1beta4/">kubeadm configuration</a>
file, similarly to commands such as <code>kubeadm init</code>. Any specification such
as extra SANs and custom IP addresses must be stored in the same configuration
file and used for all relevant kubeadm commands by passing it as <code>--config</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>This guide uses the default Kubernetes directory <code>/etc/kubernetes</code>, which requires
a super user. If you are following this guide and are using directories that you can
write to (typically, this means running <code>kubeadm</code> with <code>--cert-dir</code> and <code>--kubeconfig-dir</code>)
then you can omit the <code>sudo</code> command.</p><p>You must then copy the files that you produced over to within the <code>/etc/kubernetes</code>
directory so that <code>kubeadm init</code> or <code>kubeadm join</code> will find them.</p></div><h3 id="preparing-ca-and-service-account-files">Preparing CA and service account files</h3><p>On the primary control plane node, where <code>kubeadm init</code> will be executed, call the following
commands:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm init phase certs ca
</span></span><span><span>sudo kubeadm init phase certs etcd-ca
</span></span><span><span>sudo kubeadm init phase certs front-proxy-ca
</span></span><span><span>sudo kubeadm init phase certs sa
</span></span></code></pre></div><p>This will populate the folders <code>/etc/kubernetes/pki</code> and <code>/etc/kubernetes/pki/etcd</code>
with all self-signed CA files (certificates and keys) and service account (public and
private keys) that kubeadm needs for a control plane node.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you are using an external CA, you must generate the same files out of band and manually
copy them to the primary control plane node in <code>/etc/kubernetes</code>.</p><p>Once all CSRs are signed, you can delete the root CA key (<code>ca.key</code>) as noted in the
<a href="#external-ca-mode">External CA mode</a> section.</p></div><p>For secondary control plane nodes (<code>kubeadm join --control-plane</code>) there is no need to call
the above commands. Depending on how you setup the
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/">High Availability</a>
cluster, you either have to manually copy the same files from the primary
control plane node, or use the automated <code>--upload-certs</code> functionality of <code>kubeadm init</code>.</p><h3 id="generate-csrs">Generate CSRs</h3><p>The <code>kubeadm certs generate-csr</code> command generates CSRs for all known certificates
managed by kubeadm. Once the command is done you must manually delete <code>.csr</code>, <code>.conf</code>
or <code>.key</code> files that you don't need.</p><h4 id="considerations-kubelet-conf">Considerations for kubelet.conf</h4><p>This section applies to both control plane and worker nodes.</p><p>If you have deleted the <code>ca.key</code> file from control plane nodes
(<a href="#external-ca-mode">External CA mode</a>), the active kube-controller-manager in
this cluster will not be able to sign kubelet client certificates. If no external
method for signing these certificates exists in your setup (such as an
<a href="#set-up-a-signer">external signer</a>), you could manually sign the <code>kubelet.conf.csr</code>
as explained in this guide.</p><p>Note that this also means that the automatic
<a href="/docs/tasks/tls/certificate-rotation/#enabling-client-certificate-rotation">kubelet client certificate rotation</a>
will be disabled. If so, close to certificate expiration, you must generate
a new <code>kubelet.conf.csr</code>, sign the certificate, embed it in <code>kubelet.conf</code>
and restart the kubelet.</p><p>If this does not apply to your setup, you can skip processing the <code>kubelet.conf.csr</code>
on secondary control plane and on workers nodes (all nodes that call <code>kubeadm join ...</code>).
That is because the active kube-controller-manager will be responsible
for signing new kubelet client certificates.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You must process the <code>kubelet.conf.csr</code> file on the primary control plane node
(the host where you originally ran <code>kubeadm init</code>). This is because <code>kubeadm</code>
considers that as the node that bootstraps the cluster, and a pre-populated
<code>kubelet.conf</code> is needed.</div><h4 id="control-plane-nodes">Control plane nodes</h4><p>Execute the following command on primary (<code>kubeadm init</code>) and secondary
(<code>kubeadm join --control-plane</code>) control plane nodes to generate all CSR files:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm certs generate-csr
</span></span></code></pre></div><p>If external etcd is to be used, follow the
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/#external-etcd-nodes">External etcd with kubeadm</a>
guide to understand what CSR files are needed on the kubeadm and etcd nodes. Other
<code>.csr</code> and <code>.key</code> files under <code>/etc/kubernetes/pki/etcd</code> can be removed.</p><p>Based on the explanation in
<a href="#considerations-kubelet-conf">Considerations for kubelet.conf</a> keep or delete
the <code>kubelet.conf</code> and <code>kubelet.conf.csr</code> files.</p><h4 id="worker-nodes">Worker nodes</h4><p>Based on the explanation in
<a href="#considerations-kubelet-conf">Considerations for kubelet.conf</a>, optionally call:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo kubeadm certs generate-csr
</span></span></code></pre></div><p>and keep only the <code>kubelet.conf</code> and <code>kubelet.conf.csr</code> files. Alternatively skip
the steps for worker nodes entirely.</p><h3 id="signing-csrs-for-all-certificates">Signing CSRs for all certificates</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you are using external CA and already have CA serial number files (<code>.srl</code>) for
<code>openssl</code>, you can copy such files to a kubeadm node where CSRs will be processed.
The <code>.srl</code> files to copy are <code>/etc/kubernetes/pki/ca.srl</code>,
<code>/etc/kubernetes/pki/front-proxy-ca.srl</code> and <code>/etc/kubernetes/pki/etcd/ca.srl</code>.
The files can be then moved to a new node where CSR files will be processed.</p><p>If a <code>.srl</code> file is missing for a CA on a node, the script below will generate a new SRL file
with a random starting serial number.</p><p>To read more about <code>.srl</code> files see the
<a href="https://www.openssl.org/docs/man3.0/man1/openssl-x509.html"><code>openssl</code></a>
documentation for the <code>--CAserial</code> flag.</p></div><p>Repeat this step for all nodes that have CSR files.</p><p>Write the following script in the <code>/etc/kubernetes</code> directory, navigate to the directory
and execute the script. The script will generate certificates for all CSR files that are
present in the <code>/etc/kubernetes</code> tree.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>#!/bin/bash
</span></span></span><span><span><span></span>
</span></span><span><span><span># Set certificate expiration time in days</span>
</span></span><span><span><span>DAYS</span><span>=</span><span>365</span>
</span></span><span><span>
</span></span><span><span><span># Process all CSR files except those for front-proxy and etcd</span>
</span></span><span><span>find ./ -name <span>"*.csr"</span> | grep -v <span>"pki/etcd"</span> | grep -v <span>"front-proxy"</span> | <span>while</span> <span>read</span> -r FILE;
</span></span><span><span><span>do</span>
</span></span><span><span>    <span>echo</span> <span>"* Processing </span><span>${</span><span>FILE</span><span>}</span><span> ..."</span>
</span></span><span><span>    <span>FILE</span><span>=</span><span>${</span><span>FILE</span>%.*<span>}</span> <span># Trim the extension</span>
</span></span><span><span>    <span>if</span> <span>[</span> -f <span>"./pki/ca.srl"</span> <span>]</span>; <span>then</span>
</span></span><span><span>        <span>SERIAL_FLAG</span><span>=</span><span>"-CAserial ./pki/ca.srl"</span>
</span></span><span><span>    <span>else</span>
</span></span><span><span>        <span>SERIAL_FLAG</span><span>=</span><span>"-CAcreateserial"</span>
</span></span><span><span>    <span>fi</span>
</span></span><span><span>    openssl x509 -req -days <span>"</span><span>${</span><span>DAYS</span><span>}</span><span>"</span> -CA ./pki/ca.crt -CAkey ./pki/ca.key <span>${</span><span>SERIAL_FLAG</span><span>}</span> <span>\
</span></span></span><span><span><span></span>        -in <span>"</span><span>${</span><span>FILE</span><span>}</span><span>.csr"</span> -out <span>"</span><span>${</span><span>FILE</span><span>}</span><span>.crt"</span>
</span></span><span><span>    sleep <span>2</span>
</span></span><span><span><span>done</span>
</span></span><span><span>
</span></span><span><span><span># Process all etcd CSRs</span>
</span></span><span><span>find ./pki/etcd -name <span>"*.csr"</span> | <span>while</span> <span>read</span> -r FILE;
</span></span><span><span><span>do</span>
</span></span><span><span>    <span>echo</span> <span>"* Processing </span><span>${</span><span>FILE</span><span>}</span><span> ..."</span>
</span></span><span><span>    <span>FILE</span><span>=</span><span>${</span><span>FILE</span>%.*<span>}</span> <span># Trim the extension</span>
</span></span><span><span>    <span>if</span> <span>[</span> -f <span>"./pki/etcd/ca.srl"</span> <span>]</span>; <span>then</span>
</span></span><span><span>        <span>SERIAL_FLAG</span><span>=</span>-CAserial ./pki/etcd/ca.srl
</span></span><span><span>    <span>else</span>
</span></span><span><span>        <span>SERIAL_FLAG</span><span>=</span>-CAcreateserial
</span></span><span><span>    <span>fi</span>
</span></span><span><span>    openssl x509 -req -days <span>"</span><span>${</span><span>DAYS</span><span>}</span><span>"</span> -CA ./pki/etcd/ca.crt -CAkey ./pki/etcd/ca.key <span>${</span><span>SERIAL_FLAG</span><span>}</span> <span>\
</span></span></span><span><span><span></span>        -in <span>"</span><span>${</span><span>FILE</span><span>}</span><span>.csr"</span> -out <span>"</span><span>${</span><span>FILE</span><span>}</span><span>.crt"</span>
</span></span><span><span><span>done</span>
</span></span><span><span>
</span></span><span><span><span># Process front-proxy CSRs</span>
</span></span><span><span><span>echo</span> <span>"* Processing ./pki/front-proxy-client.csr ..."</span>
</span></span><span><span>openssl x509 -req -days <span>"</span><span>${</span><span>DAYS</span><span>}</span><span>"</span> -CA ./pki/front-proxy-ca.crt -CAkey ./pki/front-proxy-ca.key -CAcreateserial <span>\
</span></span></span><span><span><span></span>    -in ./pki/front-proxy-client.csr -out ./pki/front-proxy-client.crt
</span></span></code></pre></div><h3 id="embedding-certificates-in-kubeconfig-files">Embedding certificates in kubeconfig files</h3><p>Repeat this step for all nodes that have CSR files.</p><p>Write the following script in the <code>/etc/kubernetes</code> directory, navigate to the directory
and execute the script. The script will take the <code>.crt</code> files that were signed for
kubeconfig files from CSRs in the previous step and will embed them in the kubeconfig files.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>#!/bin/bash
</span></span></span><span><span><span></span>
</span></span><span><span><span>CLUSTER</span><span>=</span>kubernetes
</span></span><span><span>find ./ -name <span>"*.conf"</span> | <span>while</span> <span>read</span> -r FILE;
</span></span><span><span><span>do</span>
</span></span><span><span>    <span>echo</span> <span>"* Processing </span><span>${</span><span>FILE</span><span>}</span><span> ..."</span>
</span></span><span><span>    <span>KUBECONFIG</span><span>=</span><span>"</span><span>${</span><span>FILE</span><span>}</span><span>"</span> kubectl config set-cluster <span>"</span><span>${</span><span>CLUSTER</span><span>}</span><span>"</span> --certificate-authority ./pki/ca.crt --embed-certs
</span></span><span><span>    <span>USER</span><span>=</span><span>$(</span><span>KUBECONFIG</span><span>=</span><span>"</span><span>${</span><span>FILE</span><span>}</span><span>"</span> kubectl config view -o <span>jsonpath</span><span>=</span><span>'{.users[0].name}'</span><span>)</span>
</span></span><span><span>    <span>KUBECONFIG</span><span>=</span><span>"</span><span>${</span><span>FILE</span><span>}</span><span>"</span> kubectl config set-credentials <span>"</span><span>${</span><span>USER</span><span>}</span><span>"</span> --client-certificate <span>"</span><span>${</span><span>FILE</span><span>}</span><span>.crt"</span> --embed-certs
</span></span><span><span><span>done</span>
</span></span></code></pre></div><h3 id="post-csr-cleanup">Performing cleanup</h3><p>Perform this step on all nodes that have CSR files.</p><p>Write the following script in the <code>/etc/kubernetes</code> directory, navigate to the directory
and execute the script.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>#!/bin/bash
</span></span></span><span><span><span></span>
</span></span><span><span><span># Cleanup CSR files</span>
</span></span><span><span>rm -f ./*.csr ./pki/*.csr ./pki/etcd/*.csr <span># Clean all CSR files</span>
</span></span><span><span>
</span></span><span><span><span># Cleanup CRT files that were already embedded in kubeconfig files</span>
</span></span><span><span>rm -f ./*.crt
</span></span></code></pre></div><p>Optionally, move <code>.srl</code> files to the next node to be processed.</p><p>Optionally, if using external CA remove the <code>/etc/kubernetes/pki/ca.key</code> file,
as explained in the <a href="#external-ca-mode">External CA node</a> section.</p><h3 id="kubeadm-node-initialization">kubeadm node initialization</h3><p>Once CSR files have been signed and required certificates are in place on the hosts
you want to use as nodes, you can use the commands <code>kubeadm init</code> and <code>kubeadm join</code>
to create a Kubernetes cluster from these nodes. During <code>init</code> and <code>join</code>, kubeadm
uses existing certificates, encryption keys and kubeconfig files that it finds in the
<code>/etc/kubernetes</code> tree on the host's local filesystem.</p></div></div><div><div class="td-content"><h1>Reconfiguring a kubeadm cluster</h1><p>kubeadm does not support automated ways of reconfiguring components that
were deployed on managed nodes. One way of automating this would be
by using a custom <a href="/docs/concepts/extend-kubernetes/operator/">operator</a>.</p><p>To modify the components configuration you must manually edit associated cluster
objects and files on disk.</p><p>This guide shows the correct sequence of steps that need to be performed
to achieve kubeadm cluster reconfiguration.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need a cluster that was deployed using kubeadm</li><li>Have administrator credentials (<code>/etc/kubernetes/admin.conf</code>) and network connectivity
to a running kube-apiserver in the cluster from a host that has kubectl installed</li><li>Have a text editor installed on all hosts</li></ul><h2 id="reconfiguring-the-cluster">Reconfiguring the cluster</h2><p>kubeadm writes a set of cluster wide component configuration options in
ConfigMaps and other objects. These objects must be manually edited. The command <code>kubectl edit</code>
can be used for that.</p><p>The <code>kubectl edit</code> command will open a text editor where you can edit and save the object directly.</p><p>You can use the environment variables <code>KUBECONFIG</code> and <code>KUBE_EDITOR</code> to specify the location of
the kubectl consumed kubeconfig file and preferred text editor.</p><p>For example:</p><pre tabindex="0"><code>KUBECONFIG=/etc/kubernetes/admin.conf KUBE_EDITOR=nano kubectl edit &lt;parameters&gt;
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Upon saving any changes to these cluster objects, components running on nodes may not be
automatically updated. The steps below instruct you on how to perform that manually.</div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Component configuration in ConfigMaps is stored as unstructured data (YAML string).
This means that validation will not be performed upon updating the contents of a ConfigMap.
You have to be careful to follow the documented API format for a particular
component configuration and avoid introducing typos and YAML indentation mistakes.</div><h3 id="applying-cluster-configuration-changes">Applying cluster configuration changes</h3><h4 id="updating-the-clusterconfiguration">Updating the <code>ClusterConfiguration</code></h4><p>During cluster creation and upgrade, kubeadm writes its
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/"><code>ClusterConfiguration</code></a>
in a ConfigMap called <code>kubeadm-config</code> in the <code>kube-system</code> namespace.</p><p>To change a particular option in the <code>ClusterConfiguration</code> you can edit the ConfigMap with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit cm -n kube-system kubeadm-config
</span></span></code></pre></div><p>The configuration is located under the <code>data.ClusterConfiguration</code> key.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>ClusterConfiguration</code> includes a variety of options that affect the configuration of individual
components such as kube-apiserver, kube-scheduler, kube-controller-manager, CoreDNS, etcd and kube-proxy.
Changes to the configuration must be reflected on node components manually.</div><h4 id="reflecting-clusterconfiguration-changes-on-control-plane-nodes">Reflecting <code>ClusterConfiguration</code> changes on control plane nodes</h4><p>kubeadm manages the control plane components as static Pod manifests located in
the directory <code>/etc/kubernetes/manifests</code>.
Any changes to the <code>ClusterConfiguration</code> under the <code>apiServer</code>, <code>controllerManager</code>, <code>scheduler</code> or <code>etcd</code>
keys must be reflected in the associated files in the manifests directory on a control plane node.</p><p>Such changes may include:</p><ul><li><code>extraArgs</code> - requires updating the list of flags passed to a component container</li><li><code>extraVolumes</code> - requires updating the volume mounts for a component container</li><li><code>*SANs</code> - requires writing new certificates with updated Subject Alternative Names</li></ul><p>Before proceeding with these changes, make sure you have backed up the directory <code>/etc/kubernetes/</code>.</p><p>To write new certificates you can use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubeadm init phase certs &lt;component-name&gt; --config &lt;config-file&gt;
</span></span></code></pre></div><p>To write new manifest files in <code>/etc/kubernetes/manifests</code> you can use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># For Kubernetes control plane components</span>
</span></span><span><span>kubeadm init phase control-plane &lt;component-name&gt; --config &lt;config-file&gt;
</span></span><span><span><span># For local etcd</span>
</span></span><span><span>kubeadm init phase etcd <span>local</span> --config &lt;config-file&gt;
</span></span></code></pre></div><p>The <code>&lt;config-file&gt;</code> contents must match the updated <code>ClusterConfiguration</code>.
The <code>&lt;component-name&gt;</code> value must be a name of a Kubernetes control plane component (<code>apiserver</code>, <code>controller-manager</code> or <code>scheduler</code>).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Updating a file in <code>/etc/kubernetes/manifests</code> will tell the kubelet to restart the static Pod for the corresponding component.
Try doing these changes one node at a time to leave the cluster without downtime.</div><h3 id="applying-kubelet-configuration-changes">Applying kubelet configuration changes</h3><h4 id="updating-the-kubeletconfiguration">Updating the <code>KubeletConfiguration</code></h4><p>During cluster creation and upgrade, kubeadm writes its
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
in a ConfigMap called <code>kubelet-config</code> in the <code>kube-system</code> namespace.</p><p>You can edit the ConfigMap with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit cm -n kube-system kubelet-config
</span></span></code></pre></div><p>The configuration is located under the <code>data.kubelet</code> key.</p><h4 id="reflecting-the-kubelet-changes">Reflecting the kubelet changes</h4><p>To reflect the change on kubeadm nodes you must do the following:</p><ul><li>Log in to a kubeadm node</li><li>Run <code>kubeadm upgrade node phase kubelet-config</code> to download the latest <code>kubelet-config</code>
ConfigMap contents into the local file <code>/var/lib/kubelet/config.yaml</code></li><li>Edit the file <code>/var/lib/kubelet/kubeadm-flags.env</code> to apply additional configuration with
flags</li><li>Restart the kubelet service with <code>systemctl restart kubelet</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Do these changes one node at a time to allow workloads to be rescheduled properly.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>During <code>kubeadm upgrade</code>, kubeadm downloads the <code>KubeletConfiguration</code> from the
<code>kubelet-config</code> ConfigMap and overwrite the contents of <code>/var/lib/kubelet/config.yaml</code>.
This means that node local configuration must be applied either by flags in
<code>/var/lib/kubelet/kubeadm-flags.env</code> or by manually updating the contents of
<code>/var/lib/kubelet/config.yaml</code> after <code>kubeadm upgrade</code>, and then restarting the kubelet.</div><h3 id="applying-kube-proxy-configuration-changes">Applying kube-proxy configuration changes</h3><h4 id="updating-the-kubeproxyconfiguration">Updating the <code>KubeProxyConfiguration</code></h4><p>During cluster creation and upgrade, kubeadm writes its
<a href="/docs/reference/config-api/kube-proxy-config.v1alpha1/"><code>KubeProxyConfiguration</code></a>
in a ConfigMap in the <code>kube-system</code> namespace called <code>kube-proxy</code>.</p><p>This ConfigMap is used by the <code>kube-proxy</code> DaemonSet in the <code>kube-system</code> namespace.</p><p>To change a particular option in the <code>KubeProxyConfiguration</code>, you can edit the ConfigMap with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit cm -n kube-system kube-proxy
</span></span></code></pre></div><p>The configuration is located under the <code>data.config.conf</code> key.</p><h4 id="reflecting-the-kube-proxy-changes">Reflecting the kube-proxy changes</h4><p>Once the <code>kube-proxy</code> ConfigMap is updated, you can restart all kube-proxy Pods:</p><p>Delete the Pods with:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete po -n kube-system -l k8s-app<span>=</span>kube-proxy
</span></span></code></pre></div><p>New Pods that use the updated ConfigMap will be created.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Because kubeadm deploys kube-proxy as a DaemonSet, node specific configuration is unsupported.</div><h3 id="applying-coredns-configuration-changes">Applying CoreDNS configuration changes</h3><h4 id="updating-the-coredns-deployment-and-service">Updating the CoreDNS Deployment and Service</h4><p>kubeadm deploys CoreDNS as a Deployment called <code>coredns</code> and with a Service <code>kube-dns</code>,
both in the <code>kube-system</code> namespace.</p><p>To update any of the CoreDNS settings, you can edit the Deployment and
Service objects:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit deployment -n kube-system coredns
</span></span><span><span>kubectl edit service -n kube-system kube-dns
</span></span></code></pre></div><h4 id="reflecting-the-coredns-changes">Reflecting the CoreDNS changes</h4><p>Once the CoreDNS changes are applied you can restart the CoreDNS deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout restart deployment -n kube-system coredns
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>kubeadm does not allow CoreDNS configuration during cluster creation and upgrade.
This means that if you execute <code>kubeadm upgrade apply</code>, your changes to the CoreDNS
objects will be lost and must be reapplied.</div><h2 id="persisting-the-reconfiguration">Persisting the reconfiguration</h2><p>During the execution of <code>kubeadm upgrade</code> on a managed node, kubeadm might overwrite configuration
that was applied after the cluster was created (reconfiguration).</p><h3 id="persisting-node-object-reconfiguration">Persisting Node object reconfiguration</h3><p>kubeadm writes Labels, Taints, CRI socket and other information on the Node object for a particular
Kubernetes node. To change any of the contents of this Node object you can use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit no &lt;node-name&gt;
</span></span></code></pre></div><p>During <code>kubeadm upgrade</code> the contents of such a Node might get overwritten.
If you would like to persist your modifications to the Node object after upgrade,
you can prepare a <a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">kubectl patch</a>
and apply it to the Node object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch no &lt;node-name&gt; --patch-file &lt;patch-file&gt;
</span></span></code></pre></div><h4 id="persisting-control-plane-component-reconfiguration">Persisting control plane component reconfiguration</h4><p>The main source of control plane configuration is the <code>ClusterConfiguration</code>
object stored in the cluster. To extend the static Pod manifests configuration,
<a href="/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches">patches</a> can be used.</p><p>These patch files must remain as files on the control plane nodes to ensure that
they can be used by the <code>kubeadm upgrade ... --patches &lt;directory&gt;</code>.</p><p>If reconfiguration is done to the <code>ClusterConfiguration</code> and static Pod manifests on disk,
the set of node specific patches must be updated accordingly.</p><h4 id="persisting-kubelet-reconfiguration">Persisting kubelet reconfiguration</h4><p>Any changes to the <code>KubeletConfiguration</code> stored in <code>/var/lib/kubelet/config.yaml</code> will be overwritten on
<code>kubeadm upgrade</code> by downloading the contents of the cluster wide <code>kubelet-config</code> ConfigMap.
To persist kubelet node specific configuration either the file <code>/var/lib/kubelet/config.yaml</code>
has to be updated manually post-upgrade or the file <code>/var/lib/kubelet/kubeadm-flags.env</code> can include flags.
The kubelet flags override the associated <code>KubeletConfiguration</code> options, but note that
some of the flags are deprecated.</p><p>A kubelet restart will be required after changing <code>/var/lib/kubelet/config.yaml</code> or
<code>/var/lib/kubelet/kubeadm-flags.env</code>.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a></li><li><a href="/docs/setup/production-environment/tools/kubeadm/control-plane-flags/">Customizing components with the kubeadm API</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">Certificate management with kubeadm</a></li><li><a href="/docs/reference/setup-tools/kubeadm/">Find more about kubeadm set-up</a></li></ul></div></div><div><div class="td-content"><h1>Changing The Kubernetes Package Repository</h1><p>This page explains how to enable a package repository for the desired
Kubernetes minor release upon upgrading a cluster. This is only needed
for users of the community-owned package repositories hosted at <code>pkgs.k8s.io</code>.
Unlike the legacy package repositories, the community-owned package
repositories are structured in a way that there's a dedicated package
repository for each Kubernetes minor version.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This guide only covers a part of the Kubernetes upgrade process. Please see the
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">upgrade guide</a> for
more information about upgrading Kubernetes clusters.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This step is only needed upon upgrading a cluster to another <strong>minor</strong> release.
If you're upgrading to another patch release within the same minor release (e.g.
v1.34.5 to v1.34.7), you don't
need to follow this guide. However, if you're still using the legacy package
repositories, you'll need to migrate to the new community-owned package
repositories before upgrading (see the next section for more details on how to
do this).</div><h2 id="before-you-begin">Before you begin</h2><p>This document assumes that you're already using the community-owned
package repositories (<code>pkgs.k8s.io</code>). If that's not the case, it's strongly
recommended to migrate to the community-owned package repositories as described
in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p><div class="alert alert-secondary callout note"><strong>Note:</strong> The legacy package repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) have been
<a href="/blog/2023/08/31/legacy-package-repository-deprecation/">deprecated and frozen starting from September 13, 2023</a>.
<strong>Using the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">new package repositories hosted at <code>pkgs.k8s.io</code></a>
is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.</strong>
The deprecated legacy repositories, and their contents, might be removed at any time in the future and without
a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.</div><h3 id="verifying-if-the-kubernetes-package-repositories-are-used">Verifying if the Kubernetes package repositories are used</h3><p>If you're unsure whether you're using the community-owned package repositories or the
legacy package repositories, take the following steps to verify:</p><ul class="nav nav-tabs" id="k8s-install-versions"><li class="nav-item"><a class="nav-link active" href="#k8s-install-versions-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-install-versions-1">CentOS, RHEL or Fedora</a></li><li class="nav-item"><a class="nav-link" href="#k8s-install-versions-2">openSUSE or SLES</a></li></ul><div class="tab-content" id="k8s-install-versions"><div id="k8s-install-versions-0" class="tab-pane show active"><p><p>Print the contents of the file that defines the Kubernetes <code>apt</code> repository:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># On your system, this configuration file could have a different name</span>
</span></span><span><span>pager /etc/apt/sources.list.d/kubernetes.list
</span></span></code></pre></div><p>If you see a line similar to:</p><pre tabindex="0"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
</code></pre><p><strong>You're using the Kubernetes package repositories and this guide applies to you.</strong>
Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories
as described in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p></p></div><div id="k8s-install-versions-1" class="tab-pane"><p><p>Print the contents of the file that defines the Kubernetes <code>yum</code> repository:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># On your system, this configuration file could have a different name</span>
</span></span><span><span>cat /etc/yum.repos.d/kubernetes.repo
</span></span></code></pre></div><p>If you see a <code>baseurl</code> similar to the <code>baseurl</code> in the output below:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl
</code></pre><p><strong>You're using the Kubernetes package repositories and this guide applies to you.</strong>
Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories
as described in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p></p></div><div id="k8s-install-versions-2" class="tab-pane"><p><p>Print the contents of the file that defines the Kubernetes <code>zypper</code> repository:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># On your system, this configuration file could have a different name</span>
</span></span><span><span>cat /etc/zypp/repos.d/kubernetes.repo
</span></span></code></pre></div><p>If you see a <code>baseurl</code> similar to the <code>baseurl</code> in the output below:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl
</code></pre><p><strong>You're using the Kubernetes package repositories and this guide applies to you.</strong>
Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories
as described in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p></p></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The URL used for the Kubernetes package repositories is not limited to <code>pkgs.k8s.io</code>,
it can also be one of:</p><ul><li><code>pkgs.k8s.io</code></li><li><code>pkgs.kubernetes.io</code></li><li><code>packages.kubernetes.io</code></li></ul></div><h2 id="switching-to-another-kubernetes-package-repository">Switching to another Kubernetes package repository</h2><p>This step should be done upon upgrading from one to another Kubernetes minor
release in order to get access to the packages of the desired Kubernetes minor
version.</p><ul class="nav nav-tabs" id="k8s-upgrade-versions"><li class="nav-item"><a class="nav-link active" href="#k8s-upgrade-versions-0">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a class="nav-link" href="#k8s-upgrade-versions-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-upgrade-versions"><div id="k8s-upgrade-versions-0" class="tab-pane show active"><p><ol><li><p>Open the file that defines the Kubernetes <code>apt</code> repository using a text editor of your choice:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nano /etc/apt/sources.list.d/kubernetes.list
</span></span></code></pre></div><p>You should see a single line with the URL that contains your current Kubernetes
minor version. For example, if you're using v1.33,
you should see this:</p><pre tabindex="0"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
</code></pre></li><li><p>Change the version in the URL to <strong>the next available minor release</strong>, for example:</p><pre tabindex="0"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /
</code></pre></li><li><p>Save the file and exit your text editor. Continue following the relevant upgrade instructions.</p></li></ol></p></div><div id="k8s-upgrade-versions-1" class="tab-pane"><p><ol><li><p>Open the file that defines the Kubernetes <code>yum</code> repository using a text editor of your choice:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nano /etc/yum.repos.d/kubernetes.repo
</span></span></code></pre></div><p>You should see a file with two URLs that contain your current Kubernetes
minor version. For example, if you're using v1.33,
you should see this:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
</code></pre></li><li><p>Change the version in these URLs to <strong>the next available minor release</strong>, for example:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
</code></pre></li><li><p>Save the file and exit your text editor. Continue following the relevant upgrade instructions.</p></li></ol></p></div></div><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrade Linux nodes</a>.</li><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrade Windows nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Overprovision Node Capacity For A Cluster</h1><p>This page guides you through configuring <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Node</a>
overprovisioning in your Kubernetes cluster. Node overprovisioning is a strategy that proactively
reserves a portion of your cluster's compute resources. This reservation helps reduce the time
required to schedule new pods during scaling events, enhancing your cluster's responsiveness
to sudden spikes in traffic or workload demands.</p><p>By maintaining some unused capacity, you ensure that resources are immediately available when
new pods are created, preventing them from entering a pending state while the cluster scales up.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with
your cluster.</li><li>You should already have a basic understanding of
<a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>,
Pod <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank">priority</a>,
and <a class="glossary-tooltip" title="A mapping from a class name to the scheduling priority that a Pod should have." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass" target="_blank">PriorityClasses</a>.</li><li>Your cluster must be set up with an <a href="/docs/concepts/cluster-administration/cluster-autoscaling/">autoscaler</a>
that manages nodes based on demand.</li></ul><h2 id="create-a-priorityclass">Create a PriorityClass</h2><p>Begin by defining a PriorityClass for the placeholder Pods. First, create a PriorityClass with a
negative priority value, that you will shortly assign to the placeholder pods.
Later, you will set up a Deployment that uses this PriorityClass</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priorityclass/low-priority-class.yaml"><code>priorityclass/low-priority-class.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy priorityclass/low-priority-class.yaml to clipboard"></div><div class="includecode" id="priorityclass-low-priority-class-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>scheduling.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>placeholder<span> </span><span># these Pods represent placeholder capacity</span><span>
</span></span></span><span><span><span></span><span>value</span>:<span> </span>-<span>1000</span><span>
</span></span></span><span><span><span></span><span>globalDefault</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span></span><span>description</span>:<span> </span><span>"Negative priority for placeholder pods to enable overprovisioning."</span></span></span></code></pre></div></div></div><p>Then create the PriorityClass:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/priorityclass/low-priority-class.yaml
</span></span></code></pre></div><p>You will next define a Deployment that uses the negative-priority PriorityClass and runs a minimal container.
When you add this to your cluster, Kubernetes runs those placeholder pods to reserve capacity. Any time there
is a capacity shortage, the control plane will pick one these placeholder pods as the first candidate to
<a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank">preempt</a>.</p><h2 id="run-pods-that-request-node-capacity">Run Pods that request node capacity</h2><p>Review the sample manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/deployments/deployment-with-capacity-reservation.yaml"><code>deployments/deployment-with-capacity-reservation.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy deployments/deployment-with-capacity-reservation.yaml to clipboard"></div><div class="includecode" id="deployments-deployment-with-capacity-reservation-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>capacity-reservation<span>
</span></span></span><span><span><span>  </span><span># You should decide what namespace to deploy this into</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app.kubernetes.io/name</span>:<span> </span>capacity-placeholder<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app.kubernetes.io/name</span>:<span> </span>capacity-placeholder<span>
</span></span></span><span><span><span>      </span><span>annotations</span>:<span>
</span></span></span><span><span><span>        </span><span>kubernetes.io/description</span>:<span> </span><span>"Capacity reservation"</span><span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>priorityClassName</span>:<span> </span>placeholder<span>
</span></span></span><span><span><span>      </span><span>affinity</span>:<span> </span><span># Try to place these overhead Pods on different nodes</span><span>
</span></span></span><span><span><span>                </span><span># if possible</span><span>
</span></span></span><span><span><span>        </span><span>podAntiAffinity</span>:<span>
</span></span></span><span><span><span>          </span><span>preferredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>          </span>- <span>weight</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>            </span><span>podAffinityTerm</span>:<span>
</span></span></span><span><span><span>              </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>                </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>                  </span><span>app.kubernetes.io/name</span>:<span> </span>capacity-placeholder<span>
</span></span></span><span><span><span>              </span><span>topologyKey</span>:<span> </span>topology.kubernetes.io/hostname<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.6<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>"50m"</span><span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"512Mi"</span><span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"512Mi"</span><span>
</span></span></span></code></pre></div></div></div><h3 id="pick-a-namespace-for-the-placeholder-pods">Pick a namespace for the placeholder pods</h3><p>You should select, or create, a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>
that the placeholder Pods will go into.</p><h3 id="create-the-placeholder-deployment">Create the placeholder deployment</h3><p>Create a Deployment based on that manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Change the namespace name "example"</span>
</span></span><span><span>kubectl --namespace example apply -f https://k8s.io/examples/deployments/deployment-with-capacity-reservation.yaml
</span></span></code></pre></div><h2 id="adjust-placeholder-resource-requests">Adjust placeholder resource requests</h2><p>Configure the resource requests and limits for the placeholder pods to define the amount of overprovisioned resources you want to maintain. This reservation ensures that a specific amount of CPU and memory is kept available for new pods.</p><p>To edit the Deployment, modify the <code>resources</code> section in the Deployment manifest file
to set appropriate requests and limits. You can download that file locally and then edit it
with whichever text editor you prefer.</p><p>You can also edit the Deployment using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit deployment capacity-reservation
</span></span></code></pre></div><p>For example, to reserve a total of a 0.5 CPU and 1GiB of memory across 5 placeholder pods,
define the resource requests and limits for a single placeholder pod as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"100m"</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"100m"</span><span>
</span></span></span></code></pre></div><h2 id="set-the-desired-replica-count">Set the desired replica count</h2><h3 id="calculate-the-total-reserved-resources">Calculate the total reserved resources</h3><p>For example, with 5 replicas each reserving 0.1 CPU and 200MiB of memory:<br>Total CPU reserved: 5 &#215; 0.1 = 0.5 (in the Pod specification, you'll write the quantity <code>500m</code>)<br>Total memory reserved: 5 &#215; 200MiB = 1GiB (in the Pod specification, you'll write <code>1 Gi</code>)</p><p>To scale the Deployment, adjust the number of replicas based on your cluster's size and expected workload:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale deployment capacity-reservation --replicas<span>=</span><span>5</span>
</span></span></code></pre></div><p>Verify the scaling:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment capacity-reservation
</span></span></code></pre></div><p>The output should reflect the updated number of replicas:</p><pre tabindex="0"><code class="language-none">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
capacity-reservation   5/5     5            5           2m
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Some autoscalers, notably <a href="/docs/concepts/cluster-administration/cluster-autoscaling/#autoscaler-karpenter">Karpenter</a>,
treat preferred affinity rules as hard rules when considering node scaling.
If you use Karpenter or another node autoscaler that uses the same heuristic,
the replica count you set here also sets a minimum node count for your cluster.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClasses</a> and how they affect pod scheduling.</li><li>Explore <a href="/docs/concepts/cluster-administration/cluster-autoscaling/">node autoscaling</a> to dynamically adjust your cluster's size based on workload demands.</li><li>Understand <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod preemption</a>, a
key mechanism for Kubernetes to handle resource contention. The same page covers <em>eviction</em>,
which is less relevant to the placeholder Pod approach, but is also a mechanism for Kubernetes
to react when resources are contended.</li></ul></div></div><div><div class="td-content"><h1>Migrating from dockershim</h1><p>This section presents information you need to know when migrating from
dockershim to other container runtimes.</p><p>Since the announcement of <a href="/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">dockershim deprecation</a>
in Kubernetes 1.20, there were questions on how this will affect various workloads and Kubernetes
installations. Our <a href="/blog/2022/02/17/dockershim-faq/">Dockershim Removal FAQ</a> is there to help you
to understand the problem better.</p><p>Dockershim was removed from Kubernetes with the release of v1.24.
If you use Docker Engine via dockershim as your container runtime and wish to upgrade to v1.24,
it is recommended that you either migrate to another runtime or find an alternative means to obtain Docker Engine support.
Check out the <a href="/docs/setup/production-environment/container-runtimes/">container runtimes</a>
section to know your options.</p><p>The version of Kubernetes with dockershim (1.23) is out of support and the v1.24
will run out of support <a href="/releases/#release-v1-24">soon</a>. Make sure to
<a href="https://github.com/kubernetes/kubernetes/issues">report issues</a> you encountered
with the migration so the issues can be fixed in a timely manner and your cluster would be
ready for dockershim removal. After v1.24 running out of support, you will need
to contact your Kubernetes provider for support or upgrade multiple versions at a time
if there are critical issues affecting your cluster.</p><p>Your cluster might have more than one kind of node, although this is not a common
configuration.</p><p>These tasks will help you to migrate:</p><ul><li><a href="/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether Dockershim removal affects you</a></li><li><a href="/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">Migrating telemetry and security agents from dockershim</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Check out <a href="/docs/setup/production-environment/container-runtimes/">container runtimes</a>
to understand your options for an alternative.</li><li>If you find a defect or other technical concern relating to migrating away from dockershim,
you can <a href="https://github.com/kubernetes/kubernetes/issues/new/choose">report an issue</a>
to the Kubernetes project.</li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Changing the Container Runtime on a Node from Docker Engine to containerd</h1><p>This task outlines the steps needed to update your container runtime to containerd from Docker. It
is applicable for cluster operators running Kubernetes 1.23 or earlier. This also covers an
example scenario for migrating from dockershim to containerd. Alternative container runtimes
can be picked from this <a href="/docs/setup/production-environment/container-runtimes/">page</a>.</p><h2 id="before-you-begin">Before you begin</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Install containerd. For more information see
<a href="https://containerd.io/docs/getting-started/">containerd's installation documentation</a>
and for specific prerequisite follow
<a href="/docs/setup/production-environment/container-runtimes/#containerd">the containerd guide</a>.</p><h2 id="drain-the-node">Drain the node</h2><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><p>Replace <code>&lt;node-to-drain&gt;</code> with the name of your node you are draining.</p><h2 id="stop-the-docker-daemon">Stop the Docker daemon</h2><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>systemctl stop kubelet
</span></span><span><span>systemctl disable docker.service --now
</span></span></code></pre></div><h2 id="install-containerd">Install Containerd</h2><p>Follow the <a href="/docs/setup/production-environment/container-runtimes/#containerd">guide</a>
for detailed steps to install containerd.</p><ul class="nav nav-tabs" id="tab-cri-containerd-installation"><li class="nav-item"><a class="nav-link active" href="#tab-cri-containerd-installation-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#tab-cri-containerd-installation-1">Windows (PowerShell)</a></li></ul><div class="tab-content" id="tab-cri-containerd-installation"><div id="tab-cri-containerd-installation-0" class="tab-pane show active"><p><ol><li><p>Install the <code>containerd.io</code> package from the official Docker repositories.
Instructions for setting up the Docker repository for your respective Linux distribution and
installing the <code>containerd.io</code> package can be found at
<a href="https://github.com/containerd/containerd/blob/main/docs/getting-started.md">Getting started with containerd</a>.</p></li><li><p>Configure containerd:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo mkdir -p /etc/containerd
</span></span><span><span>containerd config default | sudo tee /etc/containerd/config.toml
</span></span></code></pre></div></li><li><p>Restart containerd:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo systemctl restart containerd
</span></span></code></pre></div></li></ol></p></div><div id="tab-cri-containerd-installation-1" class="tab-pane"><p><p>Start a Powershell session, set <code>$Version</code> to the desired version (ex: <code>$Version="1.4.3"</code>), and
then run the following commands:</p><ol><li><p>Download containerd:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>curl.exe -L https<span>:</span>//github.com/containerd/containerd/releases/download/v<span>$Version</span>/containerd-<span>$Version</span>-windows-amd64.tar.gz -o <span>containerd-windows</span>-amd64.tar.gz
</span></span><span><span>tar.exe xvf .\<span>containerd-windows</span>-amd64.tar.gz
</span></span></code></pre></div></li><li><p>Extract and configure:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>Copy-Item</span> -Path <span>".\bin\"</span> -Destination <span>"</span><span>$Env:ProgramFiles</span><span>\containerd"</span> -Recurse -Force
</span></span><span><span><span>cd </span><span>$Env:ProgramFiles</span>\containerd\
</span></span><span><span>.\containerd.exe config <span>default</span> | <span>Out-File</span> config.toml -Encoding ascii
</span></span><span><span>
</span></span><span><span><span># Review the configuration. Depending on setup you may want to adjust:</span>
</span></span><span><span><span># - the sandbox_image (Kubernetes pause image)</span>
</span></span><span><span><span># - cni bin_dir and conf_dir locations</span>
</span></span><span><span><span>Get-Content</span> config.toml
</span></span><span><span>
</span></span><span><span><span># (Optional - but highly recommended) Exclude containerd from Windows Defender Scans</span>
</span></span><span><span><span>Add-MpPreference</span> -ExclusionProcess <span>"</span><span>$Env:ProgramFiles</span><span>\containerd\containerd.exe"</span>
</span></span></code></pre></div></li><li><p>Start containerd:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>.\containerd.exe --register-service
</span></span><span><span><span>Start-Service</span> containerd
</span></span></code></pre></div></li></ol></p></div></div><h2 id="configure-the-kubelet-to-use-containerd-as-its-container-runtime">Configure the kubelet to use containerd as its container runtime</h2><p>Edit the file <code>/var/lib/kubelet/kubeadm-flags.env</code> and add the containerd runtime to the flags;
<code>--container-runtime-endpoint=unix:///run/containerd/containerd.sock</code>.</p><p>Users using kubeadm should be aware that the kubeadm tool stores the host's CRI socket in the</p><p><code>/var/lib/kubelet/instance-config.yaml</code> file on each node. You can create this <code>/var/lib/kubelet/instance-config.yaml</code> file on the node.</p><p>The <code>/var/lib/kubelet/instance-config.yaml</code> file allows setting the <code>containerRuntimeEndpoint</code> parameter.</p><p>You can set this parameter's value to the path of your chosen CRI socket (for example <code>unix:///run/containerd/containerd.sock</code>).</p><h2 id="restart-the-kubelet">Restart the kubelet</h2><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>systemctl start kubelet
</span></span></code></pre></div><h2 id="verify-that-the-node-is-healthy">Verify that the node is healthy</h2><p>Run <code>kubectl get nodes -o wide</code> and containerd appears as the runtime for the node we just changed.</p><h2 id="remove-docker-engine">Remove Docker Engine</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>If the node appears healthy, remove Docker.</p><ul class="nav nav-tabs" id="tab-remove-docker-engine"><li class="nav-item"><a class="nav-link active" href="#tab-remove-docker-engine-0">CentOS</a></li><li class="nav-item"><a class="nav-link" href="#tab-remove-docker-engine-1">Debian</a></li><li class="nav-item"><a class="nav-link" href="#tab-remove-docker-engine-2">Fedora</a></li><li class="nav-item"><a class="nav-link" href="#tab-remove-docker-engine-3">Ubuntu</a></li></ul><div class="tab-content" id="tab-remove-docker-engine"><div id="tab-remove-docker-engine-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo yum remove docker-ce docker-ce-cli
</span></span></code></pre></div></p></div><div id="tab-remove-docker-engine-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo apt-get purge docker-ce docker-ce-cli
</span></span></code></pre></div></p></div><div id="tab-remove-docker-engine-2" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo dnf remove docker-ce docker-ce-cli
</span></span></code></pre></div></p></div><div id="tab-remove-docker-engine-3" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo apt-get purge docker-ce docker-ce-cli
</span></span></code></pre></div></p></div></div><p>The preceding commands don't remove images, containers, volumes, or customized configuration files on your host.
To delete them, follow Docker's instructions to <a href="https://docs.docker.com/engine/install/ubuntu/#uninstall-docker-engine">Uninstall Docker Engine</a>.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Docker's instructions for uninstalling Docker Engine create a risk of deleting containerd. Be careful when executing commands.</div><h2 id="uncordon-the-node">Uncordon the node</h2><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl uncordon &lt;node-to-uncordon&gt;
</span></span></code></pre></div><p>Replace <code>&lt;node-to-uncordon&gt;</code> with the name of your node you previously drained.</p></div></div><div><div class="td-content"><h1>Find Out What Container Runtime is Used on a Node</h1><p>This page outlines steps to find out what <a href="/docs/setup/production-environment/container-runtimes/">container runtime</a>
the nodes in your cluster use.</p><p>Depending on the way you run your cluster, the container runtime for the nodes may
have been pre-configured or you need to configure it. If you're using a managed
Kubernetes service, there might be vendor-specific ways to check what container runtime is
configured for the nodes. The method described on this page should work whenever
the execution of <code>kubectl</code> is allowed.</p><h2 id="before-you-begin">Before you begin</h2><p>Install and configure <code>kubectl</code>. See <a href="/docs/tasks/tools/#kubectl">Install Tools</a> section for details.</p><h2 id="find-out-the-container-runtime-used-on-a-node">Find out the container runtime used on a Node</h2><p>Use <code>kubectl</code> to fetch and show node information:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes -o wide
</span></span></code></pre></div><p>The output is similar to the following. The column <code>CONTAINER-RUNTIME</code> outputs
the runtime and its version.</p><p>For Docker Engine, the output is similar to this:</p><pre tabindex="0"><code class="language-none">NAME         STATUS   VERSION    CONTAINER-RUNTIME
node-1       Ready    v1.16.15   docker://19.3.1
node-2       Ready    v1.16.15   docker://19.3.1
node-3       Ready    v1.16.15   docker://19.3.1
</code></pre><p>If your runtime shows as Docker Engine, you still might not be affected by the
removal of dockershim in Kubernetes v1.24.
<a href="#which-endpoint">Check the runtime endpoint</a> to see if you use dockershim.
If you don't use dockershim, you aren't affected.</p><p>For containerd, the output is similar to this:</p><pre tabindex="0"><code class="language-none">NAME         STATUS   VERSION   CONTAINER-RUNTIME
node-1       Ready    v1.19.6   containerd://1.4.1
node-2       Ready    v1.19.6   containerd://1.4.1
node-3       Ready    v1.19.6   containerd://1.4.1
</code></pre><p>Find out more information about container runtimes
on <a href="/docs/setup/production-environment/container-runtimes/">Container Runtimes</a>
page.</p><h2 id="which-endpoint">Find out what container runtime endpoint you use</h2><p>The container runtime talks to the kubelet over a Unix socket using the <a href="/docs/concepts/architecture/cri/">CRI
protocol</a>, which is based on the gRPC
framework. The kubelet acts as a client, and the runtime acts as the server.
In some cases, you might find it useful to know which socket your nodes use. For
example, with the removal of dockershim in Kubernetes v1.24 and later, you might
want to know whether you use Docker Engine with dockershim.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you currently use Docker Engine in your nodes with <code>cri-dockerd</code>, you aren't
affected by the dockershim removal.</div><p>You can check which socket you use by checking the kubelet configuration on your
nodes.</p><ol><li><p>Read the starting commands for the kubelet process:</p><pre tabindex="0"><code>tr \\0 ' ' &lt; /proc/"$(pgrep kubelet)"/cmdline
</code></pre><p>If you don't have <code>tr</code> or <code>pgrep</code>, check the command line for the kubelet
process manually.</p></li><li><p>In the output, look for the <code>--container-runtime</code> flag and the
<code>--container-runtime-endpoint</code> flag.</p><ul><li>If your nodes use Kubernetes v1.23 and earlier and these flags aren't
present or if the <code>--container-runtime</code> flag is not <code>remote</code>,
you use the dockershim socket with Docker Engine. The <code>--container-runtime</code> command line
argument is not available in Kubernetes v1.27 and later.</li><li>If the <code>--container-runtime-endpoint</code> flag is present, check the socket
name to find out which runtime you use. For example,
<code>unix:///run/containerd/containerd.sock</code> is the containerd endpoint.</li></ul></li></ol><p>If you want to change the Container Runtime on a Node from Docker Engine to containerd,
you can find out more information on <a href="/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">migrating from Docker Engine to containerd</a>,
or, if you want to continue using Docker Engine in Kubernetes v1.24 and later, migrate to a
CRI-compatible adapter like <a href="https://github.com/Mirantis/cri-dockerd"><code>cri-dockerd</code></a>.</p></div></div><div><div class="td-content"><h1>Troubleshooting CNI plugin-related errors</h1><p>To avoid CNI plugin-related errors, verify that you are using or upgrading to a
container runtime that has been tested to work correctly with your version of
Kubernetes.</p><h2 id="about-the-incompatible-cni-versions-and-failed-to-destroy-network-for-sandbox-errors">About the "Incompatible CNI versions" and "Failed to destroy network for sandbox" errors</h2><p>Service issues exist for pod CNI network setup and tear down in containerd
v1.6.0-v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config
version is not declared in the CNI config files. The containerd team reports,
"these issues are resolved in containerd v1.6.4."</p><p>With containerd v1.6.0-v1.6.3, if you do not upgrade the CNI plugins and/or
declare the CNI config version, you might encounter the following "Incompatible
CNI versions" or "Failed to destroy network for sandbox" error conditions.</p><h3 id="incompatible-cni-versions-error">Incompatible CNI versions error</h3><p>If the version of your CNI plugin does not correctly match the plugin version in
the config because the config version is later than the plugin version, the
containerd log will likely show an error message on startup of a pod similar
to:</p><pre tabindex="0"><code>incompatible CNI versions; config is \"1.0.0\", plugin supports [\"0.1.0\" \"0.2.0\" \"0.3.0\" \"0.3.1\" \"0.4.0\"]"
</code></pre><p>To fix this issue, <a href="#updating-your-cni-plugins-and-cni-config-files">update your CNI plugins and CNI config files</a>.</p><h3 id="failed-to-destroy-network-for-sandbox-error">Failed to destroy network for sandbox error</h3><p>If the version of the plugin is missing in the CNI plugin config, the pod may
run. However, stopping the pod generates an error similar to:</p><pre tabindex="0"><code>ERROR[2022-04-26T00:43:24.518165483Z] StopPodSandbox for "b" failed
error="failed to destroy network for sandbox \"bbc85f891eaf060c5a879e27bba9b6b06450210161dfdecfbb2732959fb6500a\": invalid version \"\": the version is empty"
</code></pre><p>This error leaves the pod in the not-ready state with a network namespace still
attached. To recover from this problem, <a href="#updating-your-cni-plugins-and-cni-config-files">edit the CNI config file</a> to add
the missing version information. The next attempt to stop the pod should
be successful.</p><h3 id="updating-your-cni-plugins-and-cni-config-files">Updating your CNI plugins and CNI config files</h3><p>If you're using containerd v1.6.0-v1.6.3 and encountered "Incompatible CNI
versions" or "Failed to destroy network for sandbox" errors, consider updating
your CNI plugins and editing the CNI config files.</p><p>Here's an overview of the typical steps for each node:</p><ol><li><p><a href="/docs/tasks/administer-cluster/safely-drain-node/">Safely drain and cordon the node</a>.</p></li><li><p>After stopping your container runtime and kubelet services, perform the
following upgrade operations:</p><ul><li>If you're running CNI plugins, upgrade them to the latest version.</li><li>If you're using non-CNI plugins, replace them with CNI plugins. Use the
latest version of the plugins.</li><li>Update the plugin configuration file to specify or match a version of the
CNI specification that the plugin supports, as shown in the following
<a href="#an-example-containerd-configuration-file">"An example containerd configuration file"</a> section.</li><li>For <code>containerd</code>, ensure that you have installed the latest version (v1.0.0 or later)
of the CNI loopback plugin.</li><li>Upgrade node components (for example, the kubelet) to Kubernetes v1.24</li><li>Upgrade to or install the most current version of the container runtime.</li></ul></li><li><p>Bring the node back into your cluster by restarting your container runtime
and kubelet. Uncordon the node (<code>kubectl uncordon &lt;nodename&gt;</code>).</p></li></ol><h2 id="an-example-containerd-configuration-file">An example containerd configuration file</h2><p>The following example shows a configuration for <code>containerd</code> runtime v1.6.x,
which supports a recent version of the CNI specification (v1.0.0).</p><p>Please see the documentation from your plugin and networking provider for
further instructions on configuring your system.</p><p>On Kubernetes, containerd runtime adds a loopback interface, <code>lo</code>, to pods as a
default behavior. The containerd runtime configures the loopback interface via a
CNI plugin, <code>loopback</code>. The <code>loopback</code> plugin is distributed as part of the
<code>containerd</code> release packages that have the <code>cni</code> designation. <code>containerd</code>
v1.6.0 and later includes a CNI v1.0.0-compatible loopback plugin as well as
other default CNI plugins. The configuration for the loopback plugin is done
internally by containerd, and is set to use CNI v1.0.0. This also means that the
version of the <code>loopback</code> plugin must be v1.0.0 or later when this newer version
<code>containerd</code> is started.</p><p>The following bash command generates an example CNI config. Here, the 1.0.0
value for the config version is assigned to the <code>cniVersion</code> field for use when
<code>containerd</code> invokes the CNI bridge plugin.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>cat <span>&lt;&lt; EOF | tee /etc/cni/net.d/10-containerd-net.conflist
</span></span></span><span><span><span>{
</span></span></span><span><span><span> "cniVersion": "1.0.0",
</span></span></span><span><span><span> "name": "containerd-net",
</span></span></span><span><span><span> "plugins": [
</span></span></span><span><span><span>   {
</span></span></span><span><span><span>     "type": "bridge",
</span></span></span><span><span><span>     "bridge": "cni0",
</span></span></span><span><span><span>     "isGateway": true,
</span></span></span><span><span><span>     "ipMasq": true,
</span></span></span><span><span><span>     "promiscMode": true,
</span></span></span><span><span><span>     "ipam": {
</span></span></span><span><span><span>       "type": "host-local",
</span></span></span><span><span><span>       "ranges": [
</span></span></span><span><span><span>         [{
</span></span></span><span><span><span>           "subnet": "10.88.0.0/16"
</span></span></span><span><span><span>         }],
</span></span></span><span><span><span>         [{
</span></span></span><span><span><span>           "subnet": "2001:db8:4860::/64"
</span></span></span><span><span><span>         }]
</span></span></span><span><span><span>       ],
</span></span></span><span><span><span>       "routes": [
</span></span></span><span><span><span>         { "dst": "0.0.0.0/0" },
</span></span></span><span><span><span>         { "dst": "::/0" }
</span></span></span><span><span><span>       ]
</span></span></span><span><span><span>     }
</span></span></span><span><span><span>   },
</span></span></span><span><span><span>   {
</span></span></span><span><span><span>     "type": "portmap",
</span></span></span><span><span><span>     "capabilities": {"portMappings": true},
</span></span></span><span><span><span>     "externalSetMarkChain": "KUBE-MARK-MASQ"
</span></span></span><span><span><span>   }
</span></span></span><span><span><span> ]
</span></span></span><span><span><span>}
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Update the IP address ranges in the preceding example with ones that are based
on your use case and network addressing plan.</p></div></div><div><div class="td-content"><h1>Check whether dockershim removal affects you</h1><p>The <code>dockershim</code> component of Kubernetes allows the use of Docker as a Kubernetes's
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>.
Kubernetes' built-in <code>dockershim</code> component was removed in release v1.24.</p><p>This page explains how your cluster could be using Docker as a container runtime,
provides details on the role that <code>dockershim</code> plays when in use, and shows steps
you can take to check whether any workloads could be affected by <code>dockershim</code> removal.</p><h2 id="find-docker-dependencies">Finding if your app has a dependencies on Docker</h2><p>If you are using Docker for building your application containers, you can still
run these containers on any container runtime. This use of Docker does not count
as a dependency on Docker as a container runtime.</p><p>When alternative container runtime is used, executing Docker commands may either
not work or yield unexpected output. This is how you can find whether you have a
dependency on Docker:</p><ol><li>Make sure no privileged Pods execute Docker commands (like <code>docker ps</code>),
restart the Docker service (commands such as <code>systemctl restart docker.service</code>),
or modify Docker-specific files such as <code>/etc/docker/daemon.json</code>.</li><li>Check for any private registries or image mirror settings in the Docker
configuration file (like <code>/etc/docker/daemon.json</code>). Those typically need to
be reconfigured for another container runtime.</li><li>Check that scripts and apps running on nodes outside of your Kubernetes
infrastructure do not execute Docker commands. It might be:<ul><li>SSH to nodes to troubleshoot;</li><li>Node startup scripts;</li><li>Monitoring and security agents installed on nodes directly.</li></ul></li><li>Third-party tools that perform above mentioned privileged operations. See
<a href="/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">Migrating telemetry and security agents from dockershim</a>
for more information.</li><li>Make sure there are no indirect dependencies on dockershim behavior.
This is an edge case and unlikely to affect your application. Some tooling may be configured
to react to Docker-specific behaviors, for example, raise alert on specific metrics or search for
a specific log message as part of troubleshooting instructions.
If you have such tooling configured, test the behavior on a test
cluster before migration.</li></ol><h2 id="role-of-dockershim">Dependency on Docker explained</h2><p>A <a href="/docs/concepts/containers/#container-runtimes">container runtime</a> is software that can
execute the containers that make up a Kubernetes pod. Kubernetes is responsible for orchestration
and scheduling of Pods; on each node, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a>
uses the container runtime interface as an abstraction so that you can use any compatible
container runtime.</p><p>In its earliest releases, Kubernetes offered compatibility with one container runtime: Docker.
Later in the Kubernetes project's history, cluster operators wanted to adopt additional container runtimes.
The CRI was designed to allow this kind of flexibility - and the kubelet began supporting CRI. However,
because Docker existed before the CRI specification was invented, the Kubernetes project created an
adapter component, <code>dockershim</code>. The dockershim adapter allows the kubelet to interact with Docker as
if Docker were a CRI compatible runtime.</p><p>You can read about it in <a href="/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/">Kubernetes Containerd integration goes GA</a> blog post.</p><p><img alt="Dockershim vs. CRI with Containerd" src="/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png"></p><p>Switching to Containerd as a container runtime eliminates the middleman. All the
same containers can be run by container runtimes like Containerd as before. But
now, since containers schedule directly with the container runtime, they are not visible to Docker.
So any Docker tooling or fancy UI you might have used
before to check on these containers is no longer available.</p><p>You cannot get container information using <code>docker ps</code> or <code>docker inspect</code>
commands. As you cannot list containers, you cannot get logs, stop containers,
or execute something inside a container using <code>docker exec</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you're running workloads via Kubernetes, the best way to stop a container is through
the Kubernetes API rather than directly through the container runtime (this advice applies
for all container runtimes, not only Docker).</div><p>You can still pull images or build them using <code>docker build</code> command. But images
built or pulled by Docker would not be visible to container runtime and
Kubernetes. They needed to be pushed to some registry to allow them to be used
by Kubernetes.</p><h2 id="known-issues">Known issues</h2><h3 id="some-filesystem-metrics-are-missing-and-the-metrics-format-is-different">Some filesystem metrics are missing and the metrics format is different</h3><p>The Kubelet <code>/metrics/cadvisor</code> endpoint provides Prometheus metrics,
as documented in <a href="/docs/concepts/cluster-administration/system-metrics/">Metrics for Kubernetes system components</a>.
If you install a metrics collector that depends on that endpoint, you might see the following issues:</p><ul><li>The metrics format on the Docker node is <code>k8s_&lt;container-name&gt;_&lt;pod-name&gt;_&lt;namespace&gt;_&lt;pod-uid&gt;_&lt;restart-count&gt;</code>
but the format on other runtime is different. For example, on containerd node it is <code>&lt;container-id&gt;</code>.</li><li>Some filesystem metrics are missing, as follows:<pre tabindex="0"><code>container_fs_inodes_free
container_fs_inodes_total
container_fs_io_current
container_fs_io_time_seconds_total
container_fs_io_time_weighted_seconds_total
container_fs_limit_bytes
container_fs_read_seconds_total
container_fs_reads_merged_total
container_fs_sector_reads_total
container_fs_sector_writes_total
container_fs_usage_bytes
container_fs_write_seconds_total
container_fs_writes_merged_total
</code></pre></li></ul><h4 id="workaround">Workaround</h4><p>You can mitigate this issue by using <a href="https://github.com/google/cadvisor">cAdvisor</a> as a standalone daemonset.</p><ol><li>Find the latest <a href="https://github.com/google/cadvisor/releases">cAdvisor release</a>
with the name pattern <code>vX.Y.Z-containerd-cri</code> (for example, <code>v0.42.0-containerd-cri</code>).</li><li>Follow the steps in <a href="https://github.com/google/cadvisor/tree/master/deploy/kubernetes">cAdvisor Kubernetes Daemonset</a> to create the daemonset.</li><li>Point the installed metrics collector to use the cAdvisor <code>/metrics</code> endpoint
which provides the full set of
<a href="https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md">Prometheus container metrics</a>.</li></ol><p>Alternatives:</p><ul><li>Use alternative third party metrics collection solution.</li><li>Collect metrics from the Kubelet summary API that is served at <code>/stats/summary</code>.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim</a> to understand your next steps</li><li>Read the <a href="/blog/2020/12/02/dockershim-faq/">dockershim deprecation FAQ</a> article for more information.</li></ul></div></div><div><div class="td-content"><h1>Migrating telemetry and security agents from dockershim</h1><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Kubernetes' support for direct integration with Docker Engine is deprecated and
has been removed. Most apps do not have a direct dependency on runtime hosting
containers. However, there are still a lot of telemetry and monitoring agents
that have a dependency on Docker to collect containers metadata, logs, and
metrics. This document aggregates information on how to detect these
dependencies as well as links on how to migrate these agents to use generic tools or
alternative runtimes.</p><h2 id="telemetry-and-security-agents">Telemetry and security agents</h2><p>Within a Kubernetes cluster there are a few different ways to run telemetry or
security agents. Some agents have a direct dependency on Docker Engine when
they run as DaemonSets or directly on nodes.</p><h3 id="why-do-some-telemetry-agents-communicate-with-docker-engine">Why do some telemetry agents communicate with Docker Engine?</h3><p>Historically, Kubernetes was written to work specifically with Docker Engine.
Kubernetes took care of networking and scheduling, relying on Docker Engine for
launching and running containers (within Pods) on a node. Some information that
is relevant to telemetry, such as a pod name, is only available from Kubernetes
components. Other data, such as container metrics, is not the responsibility of
the container runtime. Early telemetry agents needed to query the container
runtime <em>and</em> Kubernetes to report an accurate picture. Over time, Kubernetes
gained the ability to support multiple runtimes, and now supports any runtime
that is compatible with the <a href="/docs/concepts/architecture/cri/">container runtime interface</a>.</p><p>Some telemetry agents rely specifically on Docker Engine tooling. For example, an agent
might run a command such as
<a href="https://docs.docker.com/engine/reference/commandline/ps/"><code>docker ps</code></a>
or <a href="https://docs.docker.com/engine/reference/commandline/top/"><code>docker top</code></a> to list
containers and processes or <a href="https://docs.docker.com/engine/reference/commandline/logs/"><code>docker logs</code></a>
to receive streamed logs. If nodes in your existing cluster use
Docker Engine, and you switch to a different container runtime,
these commands will not work any longer.</p><h3 id="identify-docker-dependency">Identify DaemonSets that depend on Docker Engine</h3><p>If a pod wants to make calls to the <code>dockerd</code> running on the node, the pod must either:</p><ul><li>mount the filesystem containing the Docker daemon's privileged socket, as a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volume</a>; or</li><li>mount the specific path of the Docker daemon's privileged socket directly, also as a volume.</li></ul><p>For example: on COS images, Docker exposes its Unix domain socket at
<code>/var/run/docker.sock</code> This means that the pod spec will include a
<code>hostPath</code> volume mount of <code>/var/run/docker.sock</code>.</p><p>Here's a sample shell script to find Pods that have a mount directly mapping the
Docker socket. This script outputs the namespace and name of the pod. You can
remove the <code>grep '/var/run/docker.sock'</code> to review other mounts.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pods --all-namespaces <span>\
</span></span></span><span><span><span></span>-o<span>=</span><span>jsonpath</span><span>=</span><span>'{range .items[*]}{"\n"}{.metadata.namespace}{":\t"}{.metadata.name}{":\t"}{range .spec.volumes[*]}{.hostPath.path}{", "}{end}{end}'</span> <span>\
</span></span></span><span><span><span></span>| sort <span>\
</span></span></span><span><span><span></span>| grep <span>'/var/run/docker.sock'</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There are alternative ways for a pod to access Docker on the host. For instance, the parent
directory <code>/var/run</code> may be mounted instead of the full path (like in <a href="https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd">this
example</a>).
The script above only detects the most common uses.</div><h3 id="detecting-docker-dependency-from-node-agents">Detecting Docker dependency from node agents</h3><p>If your cluster nodes are customized and install additional security and
telemetry agents on the node, check with the agent vendor
to verify whether it has any dependency on Docker.</p><h3 id="telemetry-and-security-agent-vendors">Telemetry and security agent vendors</h3><p>This section is intended to aggregate information about various telemetry and
security agents that may have a dependency on container runtimes.</p><p>We keep the work in progress version of migration instructions for various telemetry and security agent vendors
in <a href="https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit">Google doc</a>.
Please contact the vendor to get up to date instructions for migrating from dockershim.</p><h2 id="migration-from-dockershim">Migration from dockershim</h2><h3 id="aqua-https-www-aquasec-com"><a href="https://www.aquasec.com">Aqua</a></h3><p>No changes are needed: everything should work seamlessly on the runtime switch.</p><h3 id="datadog-https-www-datadoghq-com-product"><a href="https://www.datadoghq.com/product/">Datadog</a></h3><p>How to migrate:
<a href="https://docs.datadoghq.com/agent/guide/docker-deprecation/">Docker deprecation in Kubernetes</a>
The pod that accesses Docker Engine may have a name containing any of:</p><ul><li><code>datadog-agent</code></li><li><code>datadog</code></li><li><code>dd-agent</code></li></ul><h3 id="dynatrace-https-www-dynatrace-com"><a href="https://www.dynatrace.com/">Dynatrace</a></h3><p>How to migrate:
<a href="https://community.dynatrace.com/t5/Best-practices/Migrating-from-Docker-only-to-generic-container-metrics-in/m-p/167030#M49">Migrating from Docker-only to generic container metrics in Dynatrace</a></p><p>Containerd support announcement: <a href="https://www.dynatrace.com/news/blog/get-automated-full-stack-visibility-into-containerd-based-kubernetes-environments/">Get automated full-stack visibility into
containerd-based Kubernetes
environments</a></p><p>CRI-O support announcement: <a href="https://www.dynatrace.com/news/blog/get-automated-full-stack-visibility-into-your-cri-o-kubernetes-containers-beta/">Get automated full-stack visibility into your CRI-O Kubernetes containers (Beta)</a></p><p>The pod accessing Docker may have name containing:</p><ul><li><code>dynatrace-oneagent</code></li></ul><h3 id="falco-https-falco-org"><a href="https://falco.org">Falco</a></h3><p>How to migrate:</p><p><a href="https://falco.org/docs/getting-started/deployment/#docker-deprecation-in-kubernetes">Migrate Falco from dockershim</a>
Falco supports any CRI-compatible runtime (containerd is used in the default configuration); the documentation explains all details.
The pod accessing Docker may have name containing:</p><ul><li><code>falco</code></li></ul><h3 id="prisma-cloud-compute-https-docs-paloaltonetworks-com-prisma-prisma-cloud-html"><a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud.html">Prisma Cloud Compute</a></h3><p>Check <a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/install/install_kubernetes.html">documentation for Prisma Cloud</a>,
under the "Install Prisma Cloud on a CRI (non-Docker) cluster" section.
The pod accessing Docker may be named like:</p><ul><li><code>twistlock-defender-ds</code></li></ul><h3 id="signalfx-splunk-https-www-splunk-com-en-us-investor-relations-acquisitions-signalfx-html"><a href="https://www.splunk.com/en_us/investor-relations/acquisitions/signalfx.html">SignalFx (Splunk)</a></h3><p>The SignalFx Smart Agent (deprecated) uses several different monitors for Kubernetes including
<code>kubernetes-cluster</code>, <code>kubelet-stats/kubelet-metrics</code>, and <code>docker-container-stats</code>.
The <code>kubelet-stats</code> monitor was previously deprecated by the vendor, in favor of <code>kubelet-metrics</code>.
The <code>docker-container-stats</code> monitor is the one affected by dockershim removal.
Do not use the <code>docker-container-stats</code> with container runtimes other than Docker Engine.</p><p>How to migrate from dockershim-dependent agent:</p><ol><li>Remove <code>docker-container-stats</code> from the list of <a href="https://github.com/signalfx/signalfx-agent/blob/main/docs/monitor-config.md">configured monitors</a>.
Note, keeping this monitor enabled with non-dockershim runtime will result in incorrect metrics
being reported when docker is installed on node and no metrics when docker is not installed.</li><li><a href="https://github.com/signalfx/signalfx-agent/blob/main/docs/monitors/kubelet-metrics.md">Enable and configure <code>kubelet-metrics</code></a> monitor.</li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The set of collected metrics will change. Review your alerting rules and dashboards.</div><p>The Pod accessing Docker may be named something like:</p><ul><li><code>signalfx-agent</code></li></ul><h3 id="yahoo-kubectl-flame">Yahoo Kubectl Flame</h3><p>Flame does not support container runtimes other than Docker. See
<a href="https://github.com/yahoo/kubectl-flame/issues/51">https://github.com/yahoo/kubectl-flame/issues/51</a></p></div></div><div><div class="td-content"><h1>Generate Certificates Manually</h1><p>When using client certificate authentication, you can generate certificates
manually through <a href="https://github.com/OpenVPN/easy-rsa"><code>easyrsa</code></a>, <a href="https://github.com/openssl/openssl"><code>openssl</code></a> or <a href="https://github.com/cloudflare/cfssl"><code>cfssl</code></a>.</p><h3 id="easyrsa">easyrsa</h3><p><strong>easyrsa</strong> can manually generate certificates for your cluster.</p><ol><li><p>Download, unpack, and initialize the patched version of <code>easyrsa3</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -LO https://dl.k8s.io/easy-rsa/easy-rsa.tar.gz
</span></span><span><span>tar xzf easy-rsa.tar.gz
</span></span><span><span><span>cd</span> easy-rsa-master/easyrsa3
</span></span><span><span>./easyrsa init-pki
</span></span></code></pre></div></li><li><p>Generate a new certificate authority (CA). <code>--batch</code> sets automatic mode;
<code>--req-cn</code> specifies the Common Name (CN) for the CA's new root certificate.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>./easyrsa --batch <span>"--req-cn=</span><span>${</span><span>MASTER_IP</span><span>}</span><span>@`date +%s`"</span> build-ca nopass
</span></span></code></pre></div></li><li><p>Generate server certificate and key.</p><p>The argument <code>--subject-alt-name</code> sets the possible IPs and DNS names the API server will
be accessed with. The <code>MASTER_CLUSTER_IP</code> is usually the first IP from the service CIDR
that is specified as the <code>--service-cluster-ip-range</code> argument for both the API server and
the controller manager component. The argument <code>--days</code> is used to set the number of days
after which the certificate expires.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>./easyrsa --subject-alt-name<span>=</span><span>"IP:</span><span>${</span><span>MASTER_IP</span><span>}</span><span>,"</span><span>\
</span></span></span><span><span><span></span><span>"IP:</span><span>${</span><span>MASTER_CLUSTER_IP</span><span>}</span><span>,"</span><span>\
</span></span></span><span><span><span></span><span>"DNS:kubernetes,"</span><span>\
</span></span></span><span><span><span></span><span>"DNS:kubernetes.default,"</span><span>\
</span></span></span><span><span><span></span><span>"DNS:kubernetes.default.svc,"</span><span>\
</span></span></span><span><span><span></span><span>"DNS:kubernetes.default.svc.cluster,"</span><span>\
</span></span></span><span><span><span></span><span>"DNS:kubernetes.default.svc.cluster.local"</span> <span>\
</span></span></span><span><span><span></span>--days<span>=</span><span>10000</span> <span>\
</span></span></span><span><span><span></span>build-server-full server nopass
</span></span></code></pre></div></li><li><p>Copy <code>pki/ca.crt</code>, <code>pki/issued/server.crt</code>, and <code>pki/private/server.key</code> to your directory.</p></li><li><p>Fill in and add the following parameters into the API server start parameters:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>--client-ca-file<span>=</span>/yourdirectory/ca.crt
</span></span><span><span>--tls-cert-file<span>=</span>/yourdirectory/server.crt
</span></span><span><span>--tls-private-key-file<span>=</span>/yourdirectory/server.key
</span></span></code></pre></div></li></ol><h3 id="openssl">openssl</h3><p><strong>openssl</strong> can manually generate certificates for your cluster.</p><ol><li><p>Generate a ca.key with 2048bit:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl genrsa -out ca.key <span>2048</span>
</span></span></code></pre></div></li><li><p>According to the ca.key generate a ca.crt (use <code>-days</code> to set the certificate effective time):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl req -x509 -new -nodes -key ca.key -subj <span>"/CN=</span><span>${</span><span>MASTER_IP</span><span>}</span><span>"</span> -days <span>10000</span> -out ca.crt
</span></span></code></pre></div></li><li><p>Generate a server.key with 2048bit:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl genrsa -out server.key <span>2048</span>
</span></span></code></pre></div></li><li><p>Create a config file for generating a Certificate Signing Request (CSR).</p><p>Be sure to substitute the values marked with angle brackets (e.g. <code>&lt;MASTER_IP&gt;</code>)
with real values before saving this to a file (e.g. <code>csr.conf</code>).
Note that the value for <code>MASTER_CLUSTER_IP</code> is the service cluster IP for the
API server as described in previous subsection.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p><div class="highlight"><pre tabindex="0"><code class="language-ini"><span><span><span>[ req ]</span>
</span></span><span><span><span>default_bits</span> <span>=</span> <span>2048</span>
</span></span><span><span><span>prompt</span> <span>=</span> <span>no</span>
</span></span><span><span><span>default_md</span> <span>=</span> <span>sha256</span>
</span></span><span><span><span>req_extensions</span> <span>=</span> <span>req_ext</span>
</span></span><span><span><span>distinguished_name</span> <span>=</span> <span>dn</span>
</span></span><span><span>
</span></span><span><span><span>[ dn ]</span>
</span></span><span><span><span>C</span> <span>=</span> <span>&lt;country&gt;</span>
</span></span><span><span><span>ST</span> <span>=</span> <span>&lt;state&gt;</span>
</span></span><span><span><span>L</span> <span>=</span> <span>&lt;city&gt;</span>
</span></span><span><span><span>O</span> <span>=</span> <span>&lt;organization&gt;</span>
</span></span><span><span><span>OU</span> <span>=</span> <span>&lt;organization unit&gt;</span>
</span></span><span><span><span>CN</span> <span>=</span> <span>&lt;MASTER_IP&gt;</span>
</span></span><span><span>
</span></span><span><span><span>[ req_ext ]</span>
</span></span><span><span><span>subjectAltName</span> <span>=</span> <span>@alt_names</span>
</span></span><span><span>
</span></span><span><span><span>[ alt_names ]</span>
</span></span><span><span><span>DNS.1</span> <span>=</span> <span>kubernetes</span>
</span></span><span><span><span>DNS.2</span> <span>=</span> <span>kubernetes.default</span>
</span></span><span><span><span>DNS.3</span> <span>=</span> <span>kubernetes.default.svc</span>
</span></span><span><span><span>DNS.4</span> <span>=</span> <span>kubernetes.default.svc.cluster</span>
</span></span><span><span><span>DNS.5</span> <span>=</span> <span>kubernetes.default.svc.cluster.local</span>
</span></span><span><span><span>IP.1</span> <span>=</span> <span>&lt;MASTER_IP&gt;</span>
</span></span><span><span><span>IP.2</span> <span>=</span> <span>&lt;MASTER_CLUSTER_IP&gt;</span>
</span></span><span><span>
</span></span><span><span><span>[ v3_ext ]</span>
</span></span><span><span><span>authorityKeyIdentifier</span><span>=</span><span>keyid,issuer:always</span>
</span></span><span><span><span>basicConstraints</span><span>=</span><span>CA:FALSE</span>
</span></span><span><span><span>keyUsage</span><span>=</span><span>keyEncipherment,dataEncipherment</span>
</span></span><span><span><span>extendedKeyUsage</span><span>=</span><span>serverAuth,clientAuth</span>
</span></span><span><span><span>subjectAltName</span><span>=</span><span>@alt_names</span>
</span></span></code></pre></div></li><li><p>Generate the certificate signing request based on the config file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl req -new -key server.key -out server.csr -config csr.conf
</span></span></code></pre></div></li><li><p>Generate the server certificate using the ca.key, ca.crt and server.csr:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key <span>\
</span></span></span><span><span><span></span>    -CAcreateserial -out server.crt -days <span>10000</span> <span>\
</span></span></span><span><span><span></span>    -extensions v3_ext -extfile csr.conf -sha256
</span></span></code></pre></div></li><li><p>View the certificate signing request:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl req  -noout -text -in ./server.csr
</span></span></code></pre></div></li><li><p>View the certificate:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>openssl x509  -noout -text -in ./server.crt
</span></span></code></pre></div></li></ol><p>Finally, add the same parameters into the API server start parameters.</p><h3 id="cfssl">cfssl</h3><p><strong>cfssl</strong> is another tool for certificate generation.</p><ol><li><p>Download, unpack and prepare the command line tools as shown below.</p><p>Note that you may need to adapt the sample commands based on the hardware
architecture and cfssl version you are using.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
</span></span><span><span>chmod +x cfssl
</span></span><span><span>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
</span></span><span><span>chmod +x cfssljson
</span></span><span><span>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
</span></span><span><span>chmod +x cfssl-certinfo
</span></span></code></pre></div></li><li><p>Create a directory to hold the artifacts and initialize cfssl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>mkdir cert
</span></span><span><span><span>cd</span> cert
</span></span><span><span>../cfssl print-defaults config &gt; config.json
</span></span><span><span>../cfssl print-defaults csr &gt; csr.json
</span></span></code></pre></div></li><li><p>Create a JSON config file for generating the CA file, for example, <code>ca-config.json</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"signing"</span>: {
</span></span><span><span>    <span>"default"</span>: {
</span></span><span><span>      <span>"expiry"</span>: <span>"8760h"</span>
</span></span><span><span>    },
</span></span><span><span>    <span>"profiles"</span>: {
</span></span><span><span>      <span>"kubernetes"</span>: {
</span></span><span><span>        <span>"usages"</span>: [
</span></span><span><span>          <span>"signing"</span>,
</span></span><span><span>          <span>"key encipherment"</span>,
</span></span><span><span>          <span>"server auth"</span>,
</span></span><span><span>          <span>"client auth"</span>
</span></span><span><span>        ],
</span></span><span><span>        <span>"expiry"</span>: <span>"8760h"</span>
</span></span><span><span>      }
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div></li><li><p>Create a JSON config file for CA certificate signing request (CSR), for example,
<code>ca-csr.json</code>. Be sure to replace the values marked with angle brackets with
real values you want to use.</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"CN"</span>: <span>"kubernetes"</span>,
</span></span><span><span>  <span>"key"</span>: {
</span></span><span><span>    <span>"algo"</span>: <span>"rsa"</span>,
</span></span><span><span>    <span>"size"</span>: <span>2048</span>
</span></span><span><span>  },
</span></span><span><span>  <span>"names"</span>:[{
</span></span><span><span>    <span>"C"</span>: <span>"&lt;country&gt;"</span>,
</span></span><span><span>    <span>"ST"</span>: <span>"&lt;state&gt;"</span>,
</span></span><span><span>    <span>"L"</span>: <span>"&lt;city&gt;"</span>,
</span></span><span><span>    <span>"O"</span>: <span>"&lt;organization&gt;"</span>,
</span></span><span><span>    <span>"OU"</span>: <span>"&lt;organization unit&gt;"</span>
</span></span><span><span>  }]
</span></span><span><span>}
</span></span></code></pre></div></li><li><p>Generate CA key (<code>ca-key.pem</code>) and certificate (<code>ca.pem</code>):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</span></span></code></pre></div></li><li><p>Create a JSON config file for generating keys and certificates for the API
server, for example, <code>server-csr.json</code>. Be sure to replace the values in angle brackets with
real values you want to use. The <code>&lt;MASTER_CLUSTER_IP&gt;</code> is the service cluster
IP for the API server as described in previous subsection.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"CN"</span>: <span>"kubernetes"</span>,
</span></span><span><span>  <span>"hosts"</span>: [
</span></span><span><span>    <span>"127.0.0.1"</span>,
</span></span><span><span>    <span>"&lt;MASTER_IP&gt;"</span>,
</span></span><span><span>    <span>"&lt;MASTER_CLUSTER_IP&gt;"</span>,
</span></span><span><span>    <span>"kubernetes"</span>,
</span></span><span><span>    <span>"kubernetes.default"</span>,
</span></span><span><span>    <span>"kubernetes.default.svc"</span>,
</span></span><span><span>    <span>"kubernetes.default.svc.cluster"</span>,
</span></span><span><span>    <span>"kubernetes.default.svc.cluster.local"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"key"</span>: {
</span></span><span><span>    <span>"algo"</span>: <span>"rsa"</span>,
</span></span><span><span>    <span>"size"</span>: <span>2048</span>
</span></span><span><span>  },
</span></span><span><span>  <span>"names"</span>: [{
</span></span><span><span>    <span>"C"</span>: <span>"&lt;country&gt;"</span>,
</span></span><span><span>    <span>"ST"</span>: <span>"&lt;state&gt;"</span>,
</span></span><span><span>    <span>"L"</span>: <span>"&lt;city&gt;"</span>,
</span></span><span><span>    <span>"O"</span>: <span>"&lt;organization&gt;"</span>,
</span></span><span><span>    <span>"OU"</span>: <span>"&lt;organization unit&gt;"</span>
</span></span><span><span>  }]
</span></span><span><span>}
</span></span></code></pre></div></li><li><p>Generate the key and certificate for the API server, which are by default
saved into file <code>server-key.pem</code> and <code>server.pem</code> respectively:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>../cfssl gencert -ca<span>=</span>ca.pem -ca-key<span>=</span>ca-key.pem <span>\
</span></span></span><span><span><span></span>     --config<span>=</span>ca-config.json -profile<span>=</span>kubernetes <span>\
</span></span></span><span><span><span></span>     server-csr.json | ../cfssljson -bare server
</span></span></code></pre></div></li></ol><h2 id="distributing-self-signed-ca-certificate">Distributing Self-Signed CA Certificate</h2><p>A client node may refuse to recognize a self-signed CA certificate as valid.
For a non-production deployment, or for a deployment that runs behind a company
firewall, you can distribute a self-signed CA certificate to all clients and
refresh the local list for valid certificates.</p><p>On each client, perform the following operations:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
</span></span><span><span>sudo update-ca-certificates
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id="certificates-api">Certificates API</h2><p>You can use the <code>certificates.k8s.io</code> API to provision
x509 certificates to use for authentication as documented
in the <a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Managing TLS in a cluster</a>
task page.</p></div></div><div><div class="td-content"><h1>Manage Memory, CPU, and API Resources</h1><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></h5><p>Define a default memory resource limit for a namespace, so that every new Pod in that namespace has a memory resource limit configured.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></h5><p>Define a default CPU resource limits for a namespace, so that every new Pod in that namespace has a CPU resource limit configured.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></h5><p>Define a range of valid memory resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></h5><p>Define a range of valid CPU resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></h5><p>Define overall memory and CPU resource limits for a namespace.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></h5><p>Restrict how many Pods you can create within a namespace.</p></div></div></div></div><div><div class="td-content"><h1>Configure Default Memory Requests and Limits for a Namespace</h1><div class="lead">Define a default memory resource limit for a namespace, so that every new Pod in that namespace has a memory resource limit configured.</div><p>This page shows how to configure default memory requests and limits for a
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.</p><p>A Kubernetes cluster can be divided into namespaces. Once you have a namespace that
has a default memory
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">limit</a>,
and you then try to create a Pod with a container that does not specify its own memory
limit, then the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> assigns the default
memory limit to that container.</p><p>Kubernetes assigns a default memory request under certain conditions that are explained later in this topic.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 2 GiB of memory.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace default-mem-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's a manifest for an example <a class="glossary-tooltip" title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." href="/docs/concepts/policy/limit-range/" target="_blank">LimitRange</a>.
The manifest specifies a default memory
request and a default memory limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults.yaml"><code>admin/resource/memory-defaults.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-defaults.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-defaults-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LimitRange<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mem-limit-range<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>  </span>- <span>default</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>512Mi<span>
</span></span></span><span><span><span>    </span><span>defaultRequest</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>256Mi<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Container<span>
</span></span></span></code></pre></div></div></div><p>Create the LimitRange in the default-mem-example namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>Now if you create a Pod in the default-mem-example namespace, and any container
within that Pod does not specify its own values for memory request and memory limit,
then the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
applies default values: a memory request of 256MiB and a memory limit of 512MiB.</p><p>Here's an example manifest for a Pod that has one container. The container
does not specify a memory request and limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults-pod.yaml"><code>admin/resource/memory-defaults-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-defaults-pod.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-defaults-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-mem-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default-mem-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod default-mem-demo --output<span>=</span>yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>The output shows that the Pod's container has a memory request of 256 MiB and
a memory limit of 512 MiB. These are the default values specified by the LimitRange.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>containers:
</span></span><span><span>- image: nginx
</span></span><span><span>  imagePullPolicy: Always
</span></span><span><span>  name: default-mem-demo-ctr
</span></span><span><span>  resources:
</span></span><span><span>    limits:
</span></span><span><span>      memory: 512Mi
</span></span><span><span>    requests:
</span></span><span><span>      memory: 256Mi
</span></span></code></pre></div><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod default-mem-demo --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><h2 id="what-if-you-specify-a-container-s-limit-but-not-its-request">What if you specify a container's limit, but not its request?</h2><p>Here's a manifest for a Pod that has one container. The container
specifies a memory limit, but not a request:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults-pod-2.yaml"><code>admin/resource/memory-defaults-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-defaults-pod-2.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-defaults-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-mem-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default-mem-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"1Gi"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod default-mem-demo-2 --output<span>=</span>yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>The output shows that the container's memory request is set to match its memory limit.
Notice that the container was not assigned the default memory request value of 256Mi.</p><pre tabindex="0"><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><h2 id="what-if-you-specify-a-container-s-request-but-not-its-limit">What if you specify a container's request, but not its limit?</h2><p>Here's a manifest for a Pod that has one container. The container
specifies a memory request, but not a limit:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults-pod-3.yaml"><code>admin/resource/memory-defaults-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-defaults-pod-3.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-defaults-pod-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-mem-demo-3<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default-mem-demo-3-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"128Mi"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>View the Pod's specification:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod default-mem-demo-3 --output<span>=</span>yaml --namespace<span>=</span>default-mem-example
</span></span></code></pre></div><p>The output shows that the container's memory request is set to the value specified in the
container's manifest. The container is limited to use no more than 512MiB of
memory, which matches the default memory limit for the namespace.</p><pre tabindex="0"><code>resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A <code>LimitRange</code> does <strong>not</strong> check the consistency of the default values it applies. This means that a default value for the <em>limit</em> that is set by <code>LimitRange</code> may be less than the <em>request</em> value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be scheduleable.
See <a href="/docs/concepts/policy/limit-range/#constraints-on-resource-limits-and-requests">Constraints on resource limits and requests</a> for more details.</div><h2 id="motivation-for-default-memory-limits-and-requests">Motivation for default memory limits and requests</h2><p>If your namespace has a memory <a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." href="/docs/concepts/policy/resource-quotas/" target="_blank">resource quota</a>
configured,
it is helpful to have a default value in place for memory limit.
Here are three of the restrictions that a resource quota imposes on a namespace:</p><ul><li>For every Pod that runs in the namespace, the Pod and each of its containers must have a memory limit.
(If you specify a memory limit for every container in a Pod, Kubernetes can infer the Pod-level memory
limit by adding up the limits for its containers).</li><li>Memory limits apply a resource reservation on the node where the Pod in question is scheduled.
The total amount of memory reserved for all Pods in the namespace must not exceed a specified limit.</li><li>The total amount of memory actually used by all Pods in the namespace must also not exceed a specified limit.</li></ul><p>When you add a LimitRange:</p><p>If any Pod in that namespace that includes a container does not specify its own memory limit,
the control plane applies the default memory limit to that container, and the Pod can be
allowed to run in a namespace that is restricted by a memory ResourceQuota.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace default-mem-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure Default CPU Requests and Limits for a Namespace</h1><div class="lead">Define a default CPU resource limits for a namespace, so that every new Pod in that namespace has a CPU resource limit configured.</div><p>This page shows how to configure default CPU requests and limits for a
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.</p><p>A Kubernetes cluster can be divided into namespaces. If you create a Pod within a
namespace that has a default CPU
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">limit</a>, and any container in that Pod does not specify
its own CPU limit, then the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> assigns the default
CPU limit to that container.</p><p>Kubernetes assigns a default CPU
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">request</a>,
but only under certain conditions that are explained later in this page.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>If you're not already familiar with what Kubernetes means by 1.0 CPU,
read <a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace default-cpu-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's a manifest for an example <a class="glossary-tooltip" title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." href="/docs/concepts/policy/limit-range/" target="_blank">LimitRange</a>.
The manifest specifies a default CPU request and a default CPU limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults.yaml"><code>admin/resource/cpu-defaults.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-defaults.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-defaults-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LimitRange<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu-limit-range<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>  </span>- <span>default</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>defaultRequest</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>0.5</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Container<span>
</span></span></span></code></pre></div></div></div><p>Create the LimitRange in the default-cpu-example namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace<span>=</span>default-cpu-example
</span></span></code></pre></div><p>Now if you create a Pod in the default-cpu-example namespace, and any container
in that Pod does not specify its own values for CPU request and CPU limit,
then the control plane applies default values: a CPU request of 0.5 and a default
CPU limit of 1.</p><p>Here's a manifest for a Pod that has one container. The container
does not specify a CPU request and limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults-pod.yaml"><code>admin/resource/cpu-defaults-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-defaults-pod.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-defaults-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-cpu-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default-cpu-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace<span>=</span>default-cpu-example
</span></span></code></pre></div><p>View the Pod's specification:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod default-cpu-demo --output<span>=</span>yaml --namespace<span>=</span>default-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod's only container has a CPU request of 500m <code>cpu</code>
(which you can read as &#8220;500 millicpu&#8221;), and a CPU limit of 1 <code>cpu</code>.
These are the default values specified by the LimitRange.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>containers:
</span></span><span><span>- image: nginx
</span></span><span><span>  imagePullPolicy: Always
</span></span><span><span>  name: default-cpu-demo-ctr
</span></span><span><span>  resources:
</span></span><span><span>    limits:
</span></span><span><span>      cpu: <span>"1"</span>
</span></span><span><span>    requests:
</span></span><span><span>      cpu: 500m
</span></span></code></pre></div><h2 id="what-if-you-specify-a-container-s-limit-but-not-its-request">What if you specify a container's limit, but not its request?</h2><p>Here's a manifest for a Pod that has one container. The container
specifies a CPU limit, but not a request:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults-pod-2.yaml"><code>admin/resource/cpu-defaults-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-defaults-pod-2.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-defaults-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-cpu-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default-cpu-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace<span>=</span>default-cpu-example
</span></span></code></pre></div><p>View the <a href="/docs/concepts/overview/working-with-objects/#object-spec-and-status">specification</a>
of the Pod that you created:</p><pre tabindex="0"><code>kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example
</code></pre><p>The output shows that the container's CPU request is set to match its CPU limit.
Notice that the container was not assigned the default CPU request value of 0.5 <code>cpu</code>:</p><pre tabindex="0"><code>resources:
  limits:
    cpu: "1"
  requests:
    cpu: "1"
</code></pre><h2 id="what-if-you-specify-a-container-s-request-but-not-its-limit">What if you specify a container's request, but not its limit?</h2><p>Here's an example manifest for a Pod that has one container. The container
specifies a CPU request, but not a limit:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults-pod-3.yaml"><code>admin/resource/cpu-defaults-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-defaults-pod-3.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-defaults-pod-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-cpu-demo-3<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default-cpu-demo-3-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"0.75"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace<span>=</span>default-cpu-example
</span></span></code></pre></div><p>View the specification of the Pod that you created:</p><pre tabindex="0"><code>kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example
</code></pre><p>The output shows that the container's CPU request is set to the value you specified at
the time you created the Pod (in other words: it matches the manifest).
However, the same container's CPU limit is set to 1 <code>cpu</code>, which is the default CPU limit
for that namespace.</p><pre tabindex="0"><code>resources:
  limits:
    cpu: "1"
  requests:
    cpu: 750m
</code></pre><h2 id="motivation-for-default-cpu-limits-and-requests">Motivation for default CPU limits and requests</h2><p>If your namespace has a CPU <a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." href="/docs/concepts/policy/resource-quotas/" target="_blank">resource quota</a>
configured,
it is helpful to have a default value in place for CPU limit.
Here are two of the restrictions that a CPU resource quota imposes on a namespace:</p><ul><li>For every Pod that runs in the namespace, each of its containers must have a CPU limit.</li><li>CPU limits apply a resource reservation on the node where the Pod in question is scheduled.
The total amount of CPU that is reserved for use by all Pods in the namespace must not
exceed a specified limit.</li></ul><p>When you add a LimitRange:</p><p>If any Pod in that namespace that includes a container does not specify its own CPU limit,
the control plane applies the default CPU limit to that container, and the Pod can be
allowed to run in a namespace that is restricted by a CPU ResourceQuota.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace default-cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure Minimum and Maximum Memory Constraints for a Namespace</h1><div class="lead">Define a range of valid memory resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</div><p>This page shows how to set minimum and maximum values for memory used by containers
running in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.
You specify minimum and maximum memory values in a
<a href="/docs/reference/kubernetes-api/policy-resources/limit-range-v1/">LimitRange</a>
object. If a Pod does not meet the constraints imposed by the LimitRange,
it cannot be created in the namespace.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 1 GiB of memory available for Pods.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace constraints-mem-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's an example manifest for a LimitRange:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints.yaml"><code>admin/resource/memory-constraints.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-constraints.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-constraints-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LimitRange<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mem-min-max-demo-lr<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>  </span>- <span>max</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>1Gi<span>
</span></span></span><span><span><span>    </span><span>min</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>500Mi<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Container<span>
</span></span></span></code></pre></div></div></div><p>Create the LimitRange:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>View detailed information about the LimitRange:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get limitrange mem-min-max-demo-lr --namespace<span>=</span>constraints-mem-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows the minimum and maximum memory constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.</p><pre tabindex="0"><code>  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
</code></pre><p>Now whenever you define a Pod within the constraints-mem-example namespace, Kubernetes
performs these steps:</p><ul><li><p>If any container in that Pod does not specify its own memory request and limit,
the control plane assigns the default memory request and limit to that container.</p></li><li><p>Verify that every container in that Pod requests at least 500 MiB of memory.</p></li><li><p>Verify that every container in that Pod requests no more than 1024 MiB (1 GiB)
of memory.</p></li></ul><p>Here's a manifest for a Pod that has one container. Within the Pod spec, the sole
container specifies a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the
minimum and maximum memory constraints imposed by the LimitRange.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod.yaml"><code>admin/resource/memory-constraints-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-constraints-pod.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-constraints-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-mem-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-mem-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"800Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"600Mi"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>Verify that the Pod is running and that its container is healthy:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod constraints-mem-demo --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod constraints-mem-demo --output<span>=</span>yaml --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>The output shows that the container within that Pod has a memory request of 600 MiB and
a memory limit of 800 MiB. These satisfy the constraints imposed by the LimitRange for
this namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>     </span><span>memory</span>:<span> </span>800Mi<span>
</span></span></span><span><span><span>  </span><span>requests</span>:<span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span>600Mi<span>
</span></span></span></code></pre></div><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod constraints-mem-demo --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><h2 id="attempt-to-create-a-pod-that-exceeds-the-maximum-memory-constraint">Attempt to create a Pod that exceeds the maximum memory constraint</h2><p>Here's a manifest for a Pod that has one container. The container specifies a
memory request of 800 MiB and a memory limit of 1.5 GiB.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod-2.yaml"><code>admin/resource/memory-constraints-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-constraints-pod-2.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-constraints-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-mem-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-mem-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"1.5Gi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"800Mi"</span><span>
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines a container that
requests more memory than is allowed:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-2.yaml":
pods "constraints-mem-demo-2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
</code></pre><h2 id="attempt-to-create-a-pod-that-does-not-meet-the-minimum-memory-request">Attempt to create a Pod that does not meet the minimum memory request</h2><p>Here's a manifest for a Pod that has one container. That container specifies a
memory request of 100 MiB and a memory limit of 800 MiB.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod-3.yaml"><code>admin/resource/memory-constraints-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-constraints-pod-3.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-constraints-pod-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-mem-demo-3<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-mem-demo-3-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"800Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines a container
that requests less memory than the enforced minimum:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-3.yaml":
pods "constraints-mem-demo-3" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
</code></pre><h2 id="create-a-pod-that-does-not-specify-any-memory-request-or-limit">Create a Pod that does not specify any memory request or limit</h2><p>Here's a manifest for a Pod that has one container. The container does not
specify a memory request, and it does not specify a memory limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod-4.yaml"><code>admin/resource/memory-constraints-pod-4.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/memory-constraints-pod-4.yaml to clipboard"></div><div class="includecode" id="admin-resource-memory-constraints-pod-4-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-mem-demo-4<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-mem-demo-4-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod constraints-mem-demo-4 --namespace<span>=</span>constraints-mem-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that the Pod's only container has a memory request of 1 GiB and a memory limit of 1 GiB.
How did that container get those values?</p><pre tabindex="0"><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><p>Because your Pod did not define any memory request and limit for that container, the cluster
applied a
<a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">default memory request and limit</a>
from the LimitRange.</p><p>This means that the definition of that Pod shows those values. You can check it using
<code>kubectl describe</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Look for the "Requests:" section of the output</span>
</span></span><span><span>kubectl describe pod constraints-mem-demo-4 --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><p>At this point, your Pod might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 GiB of memory. If each of your Nodes has only
1 GiB of memory, then there is not enough allocatable memory on any Node to accommodate a memory
request of 1 GiB. If you happen to be using Nodes with 2 GiB of memory, then you probably have
enough space to accommodate the 1 GiB request.</p><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod constraints-mem-demo-4 --namespace<span>=</span>constraints-mem-example
</span></span></code></pre></div><h2 id="enforcement-of-minimum-and-maximum-memory-constraints">Enforcement of minimum and maximum memory constraints</h2><p>The maximum and minimum memory constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.</p><h2 id="motivation-for-minimum-and-maximum-memory-constraints">Motivation for minimum and maximum memory constraints</h2><p>As a cluster administrator, you might want to impose restrictions on the amount of memory that Pods can use.
For example:</p><ul><li><p>Each Node in a cluster has 2 GiB of memory. You do not want to accept any Pod that requests
more than 2 GiB of memory, because no Node in the cluster can support the request.</p></li><li><p>A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 8 GiB of memory, but
you want development workloads to be limited to 512 MiB. You create separate namespaces
for production and development, and you apply memory constraints to each namespace.</p></li></ul><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace constraints-mem-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure Minimum and Maximum CPU Constraints for a Namespace</h1><div class="lead">Define a range of valid CPU resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</div><p>This page shows how to set minimum and maximum values for the CPU resources used by containers
and Pods in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>. You specify minimum
and maximum CPU values in a
<a href="/docs/reference/kubernetes-api/policy-resources/limit-range-v1/">LimitRange</a>
object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created
in the namespace.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 1.0 CPU available for Pods.
See <a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>
to learn what Kubernetes means by &#8220;1 CPU&#8221;.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace constraints-cpu-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's a manifest for an example <a class="glossary-tooltip" title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." href="/docs/concepts/policy/limit-range/" target="_blank">LimitRange</a>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints.yaml"><code>admin/resource/cpu-constraints.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-constraints.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-constraints-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LimitRange<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu-min-max-demo-lr<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>  </span>- <span>max</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"800m"</span><span>
</span></span></span><span><span><span>    </span><span>min</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"200m"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Container<span>
</span></span></span></code></pre></div></div></div><p>Create the LimitRange:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>View detailed information about the LimitRange:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get limitrange cpu-min-max-demo-lr --output<span>=</span>yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows the minimum and maximum CPU constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>limits</span>:<span>
</span></span></span><span><span><span></span>- <span>default</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>800m<span>
</span></span></span><span><span><span>  </span><span>defaultRequest</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>800m<span>
</span></span></span><span><span><span>  </span><span>max</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>800m<span>
</span></span></span><span><span><span>  </span><span>min</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>200m<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>Container<span>
</span></span></span></code></pre></div><p>Now whenever you create a Pod in the constraints-cpu-example namespace (or some other client
of the Kubernetes API creates an equivalent Pod), Kubernetes performs these steps:</p><ul><li><p>If any container in that Pod does not specify its own CPU request and limit, the control plane
assigns the default CPU request and limit to that container.</p></li><li><p>Verify that every container in that Pod specifies a CPU request that is greater than or equal to 200 millicpu.</p></li><li><p>Verify that every container in that Pod specifies a CPU limit that is less than or equal to 800 millicpu.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When creating a <code>LimitRange</code> object, you can specify limits on huge-pages
or GPUs as well. However, when both <code>default</code> and <code>defaultRequest</code> are specified
on these resources, the two values must be the same.</div><p>Here's a manifest for a Pod that has one container. The container manifest
specifies a CPU request of 500 millicpu and a CPU limit of 800 millicpu. These satisfy the
minimum and maximum CPU constraints imposed by the LimitRange for this namespace.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod.yaml"><code>admin/resource/cpu-constraints-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-constraints-pod.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-constraints-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-cpu-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-cpu-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"800m"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>Verify that the Pod is running and that its container is healthy:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod constraints-cpu-demo --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod constraints-cpu-demo --output<span>=</span>yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod's only container has a CPU request of 500 millicpu and CPU limit
of 800 millicpu. These satisfy the constraints imposed by the LimitRange.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>800m<span>
</span></span></span><span><span><span>  </span><span>requests</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span></code></pre></div><h2 id="delete-the-pod">Delete the Pod</h2><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod constraints-cpu-demo --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><h2 id="attempt-to-create-a-pod-that-exceeds-the-maximum-cpu-constraint">Attempt to create a Pod that exceeds the maximum CPU constraint</h2><p>Here's a manifest for a Pod that has one container. The container specifies a
CPU request of 500 millicpu and a cpu limit of 1.5 cpu.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod-2.yaml"><code>admin/resource/cpu-constraints-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-constraints-pod-2.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-constraints-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-cpu-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-cpu-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"1.5"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines an unacceptable container.
That container is not acceptable because it specifies a CPU limit that is too large:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-2.yaml":
pods "constraints-cpu-demo-2" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m.
</code></pre><h2 id="attempt-to-create-a-pod-that-does-not-meet-the-minimum-cpu-request">Attempt to create a Pod that does not meet the minimum CPU request</h2><p>Here's a manifest for a Pod that has one container. The container specifies a
CPU request of 100 millicpu and a CPU limit of 800 millicpu.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod-3.yaml"><code>admin/resource/cpu-constraints-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-constraints-pod-3.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-constraints-pod-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-cpu-demo-3<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-cpu-demo-3-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"800m"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"100m"</span><span>
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines an unacceptable container.
That container is not acceptable because it specifies a CPU request that is lower than the
enforced minimum:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-3.yaml":
pods "constraints-cpu-demo-3" is forbidden: minimum cpu usage per Container is 200m, but request is 100m.
</code></pre><h2 id="create-a-pod-that-does-not-specify-any-cpu-request-or-limit">Create a Pod that does not specify any CPU request or limit</h2><p>Here's a manifest for a Pod that has one container. The container does not
specify a CPU request, nor does it specify a CPU limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod-4.yaml"><code>admin/resource/cpu-constraints-pod-4.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/cpu-constraints-pod-4.yaml to clipboard"></div><div class="includecode" id="admin-resource-cpu-constraints-pod-4-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>constraints-cpu-demo-4<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>constraints-cpu-demo-4-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>vish/stress<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace<span>=</span>constraints-cpu-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><pre tabindex="0"><code>kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml
</code></pre><p>The output shows that the Pod's single container has a CPU request of 800 millicpu and a
CPU limit of 800 millicpu.
How did that container get those values?</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>800m<span>
</span></span></span><span><span><span>  </span><span>requests</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>800m<span>
</span></span></span></code></pre></div><p>Because that container did not specify its own CPU request and limit, the control plane
applied the
<a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">default CPU request and limit</a>
from the LimitRange for this namespace.</p><p>At this point, your Pod may or may not be running. Recall that a prerequisite for
this task is that your Nodes must have at least 1 CPU available for use. If each of your Nodes has only 1 CPU,
then there might not be enough allocatable CPU on any Node to accommodate a request of 800 millicpu.
If you happen to be using Nodes with 2 CPU, then you probably have enough CPU to accommodate the 800 millicpu request.</p><p>Delete your Pod:</p><pre tabindex="0"><code>kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example
</code></pre><h2 id="enforcement-of-minimum-and-maximum-cpu-constraints">Enforcement of minimum and maximum CPU constraints</h2><p>The maximum and minimum CPU constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.</p><h2 id="motivation-for-minimum-and-maximum-cpu-constraints">Motivation for minimum and maximum CPU constraints</h2><p>As a cluster administrator, you might want to impose restrictions on the CPU resources that Pods can use.
For example:</p><ul><li><p>Each Node in a cluster has 2 CPU. You do not want to accept any Pod that requests
more than 2 CPU, because no Node in the cluster can support the request.</p></li><li><p>A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 3 CPU, but you want development workloads to be limited
to 1 CPU. You create separate namespaces for production and development, and you apply CPU constraints to
each namespace.</p></li></ul><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace constraints-cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure Memory and CPU Quotas for a Namespace</h1><div class="lead">Define overall memory and CPU resource limits for a namespace.</div><p>This page shows how to set quotas for the total amount memory and CPU that
can be used by all Pods running in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.
You specify quotas in a
<a href="/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/">ResourceQuota</a>
object.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 1 GiB of memory.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace quota-mem-cpu-example
</span></span></code></pre></div><h2 id="create-a-resourcequota">Create a ResourceQuota</h2><p>Here is a manifest for an example ResourceQuota:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-mem-cpu.yaml"><code>admin/resource/quota-mem-cpu.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-mem-cpu.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-mem-cpu-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mem-cpu-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>requests.cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>    </span><span>requests.memory</span>:<span> </span>1Gi<span>
</span></span></span><span><span><span>    </span><span>limits.cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>    </span><span>limits.memory</span>:<span> </span>2Gi<span>
</span></span></span></code></pre></div></div></div><p>Create the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace<span>=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>View detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourcequota mem-cpu-demo --namespace<span>=</span>quota-mem-cpu-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:</p><ul><li>For every Pod in the namespace, each container must have a memory request, memory limit, cpu request, and cpu limit.</li><li>The memory request total for all Pods in that namespace must not exceed 1 GiB.</li><li>The memory limit total for all Pods in that namespace must not exceed 2 GiB.</li><li>The CPU request total for all Pods in that namespace must not exceed 1 cpu.</li><li>The CPU limit total for all Pods in that namespace must not exceed 2 cpu.</li></ul><p>See <a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>
to learn what Kubernetes means by &#8220;1 CPU&#8221;.</p><h2 id="create-a-pod">Create a Pod</h2><p>Here is a manifest for an example Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-mem-cpu-pod.yaml"><code>admin/resource/quota-mem-cpu-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-mem-cpu-pod.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-mem-cpu-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>quota-mem-cpu-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>quota-mem-cpu-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"800Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"800m"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"600Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"400m"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace<span>=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>Verify that the Pod is running and that its (only) container is healthy:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod quota-mem-cpu-demo --namespace<span>=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>Once again, view detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourcequota mem-cpu-demo --namespace<span>=</span>quota-mem-cpu-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows the quota along with how much of the quota has been used.
You can see that the memory and CPU requests and limits for your Pod do not
exceed the quota.</p><pre tabindex="0"><code>status:
  hard:
    limits.cpu: "2"
    limits.memory: 2Gi
    requests.cpu: "1"
    requests.memory: 1Gi
  used:
    limits.cpu: 800m
    limits.memory: 800Mi
    requests.cpu: 400m
    requests.memory: 600Mi
</code></pre><p>If you have the <code>jq</code> tool, you can also query (using <a href="/docs/reference/kubectl/jsonpath/">JSONPath</a>)
for just the <code>used</code> values, <strong>and</strong> pretty-print that that of the output. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourcequota mem-cpu-demo --namespace<span>=</span>quota-mem-cpu-example -o <span>jsonpath</span><span>=</span><span>'{ .status.used }'</span> | jq .
</span></span></code></pre></div><h2 id="attempt-to-create-a-second-pod">Attempt to create a second Pod</h2><p>Here is a manifest for a second Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-mem-cpu-pod-2.yaml"><code>admin/resource/quota-mem-cpu-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-mem-cpu-pod-2.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-mem-cpu-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>quota-mem-cpu-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>quota-mem-cpu-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"1Gi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"800m"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"700Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"400m"</span><span>
</span></span></span></code></pre></div></div></div><p>In the manifest, you can see that the Pod has a memory request of 700 MiB.
Notice that the sum of the used memory request and this new memory
request exceeds the memory request quota: 600 MiB + 700 MiB &gt; 1 GiB.</p><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace<span>=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>The second Pod does not get created. The output shows that creating the second Pod
would cause the memory request total to exceed the memory request quota.</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/quota-mem-cpu-pod-2.yaml":
pods "quota-mem-cpu-demo-2" is forbidden: exceeded quota: mem-cpu-demo,
requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi
</code></pre><h2 id="discussion">Discussion</h2><p>As you have seen in this exercise, you can use a ResourceQuota to restrict
the memory request total for all Pods running in a namespace.
You can also restrict the totals for memory limit, cpu request, and cpu limit.</p><p>Instead of managing total resource use within a namespace, you might want to restrict
individual Pods, or the containers in those Pods. To achieve that kind of limiting, use a
<a href="/docs/concepts/policy/limit-range/">LimitRange</a>.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace quota-mem-cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure a Pod Quota for a Namespace</h1><div class="lead">Restrict how many Pods you can create within a namespace.</div><p>This page shows how to set a quota for the total number of Pods that can run
in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">Namespace</a>. You specify quotas in a
<a href="/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/">ResourceQuota</a>
object.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace quota-pod-example
</span></span></code></pre></div><h2 id="create-a-resourcequota">Create a ResourceQuota</h2><p>Here is an example manifest for a ResourceQuota:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-pod.yaml"><code>admin/resource/quota-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-pod.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"2"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace<span>=</span>quota-pod-example
</span></span></code></pre></div><p>View detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourcequota pod-demo --namespace<span>=</span>quota-pod-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that the namespace has a quota of two Pods, and that currently there are
no Pods; that is, none of the quota is used.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>  </span><span>used</span>:<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"0"</span><span>
</span></span></span></code></pre></div><p>Here is an example manifest for a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-pod-deployment.yaml"><code>admin/resource/quota-pod-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-pod-deployment.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-pod-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-quota-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>purpose</span>:<span> </span>quota-demo<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>purpose</span>:<span> </span>quota-demo<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>pod-quota-demo<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div></div></div><p>In that manifest, <code>replicas: 3</code> tells Kubernetes to attempt to create three new Pods, all
running the same application.</p><p>Create the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace<span>=</span>quota-pod-example
</span></span></code></pre></div><p>View detailed information about the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment pod-quota-demo --namespace<span>=</span>quota-pod-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that even though the Deployment specifies three replicas, only two
Pods were created because of the quota you defined earlier:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>availableReplicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>lastUpdateTime</span>:<span> </span>2021-04-02T20:57:05Z<span>
</span></span></span><span><span><span>    </span><span>message: 'unable to create pods</span>:<span> </span>pods "pod-quota-demo-1650323038-" is forbidden:<span>
</span></span></span><span><span><span>      </span><span>exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited</span>:<span> </span>pods=2'<span>
</span></span></span></code></pre></div><h3 id="choice-of-resource">Choice of resource</h3><p>In this task you have defined a ResourceQuota that limited the total number of Pods, but
you could also limit the total number of other kinds of object. For example, you
might decide to limit how many <a class="glossary-tooltip" title="A repeating task (a Job) that runs on a regular schedule." href="/docs/concepts/workloads/controllers/cron-jobs/" target="_blank">CronJobs</a>
that can live in a single namespace.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace quota-pod-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Install a Network Policy Provider</h1><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/">Use Antrea for NetworkPolicy</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/">Use Calico for NetworkPolicy</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">Use Cilium for NetworkPolicy</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">Use Kube-router for NetworkPolicy</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">Romana for NetworkPolicy</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">Weave Net for NetworkPolicy</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Use Antrea for NetworkPolicy</h1><p>This page shows how to install and use Antrea CNI plugin on Kubernetes.
For background on Project Antrea, read the <a href="https://antrea.io/docs/">Introduction to Antrea</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster. Follow the
<a href="/docs/reference/setup-tools/kubeadm/">kubeadm getting started guide</a> to bootstrap one.</p><h2 id="deploying-antrea-with-kubeadm">Deploying Antrea with kubeadm</h2><p>Follow <a href="https://github.com/vmware-tanzu/antrea/blob/main/docs/getting-started.md">Getting Started</a> guide to deploy Antrea for kubeadm.</p><h2 id="what-s-next">What's next</h2><p>Once your cluster is running, you can follow the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p></div></div><div><div class="td-content"><h1>Use Calico for NetworkPolicy</h1><p>This page shows a couple of quick ways to create a Calico cluster on Kubernetes.</p><h2 id="before-you-begin">Before you begin</h2><p>Decide whether you want to deploy a <a href="#creating-a-calico-cluster-with-google-kubernetes-engine-gke">cloud</a> or <a href="#creating-a-local-calico-cluster-with-kubeadm">local</a> cluster.</p><h2 id="creating-a-calico-cluster-with-google-kubernetes-engine-gke">Creating a Calico cluster with Google Kubernetes Engine (GKE)</h2><p><strong>Prerequisite</strong>: <a href="https://cloud.google.com/sdk/docs/quickstarts">gcloud</a>.</p><ol><li><p>To launch a GKE cluster with Calico, include the <code>--enable-network-policy</code> flag.</p><p><strong>Syntax</strong></p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>gcloud container clusters create <span>[</span>CLUSTER_NAME<span>]</span> --enable-network-policy
</span></span></code></pre></div><p><strong>Example</strong></p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>gcloud container clusters create my-calico-cluster --enable-network-policy
</span></span></code></pre></div></li><li><p>To verify the deployment, use the following command.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The Calico pods begin with <code>calico</code>. Check to make sure each one has a status of <code>Running</code>.</p></li></ol><h2 id="creating-a-local-calico-cluster-with-kubeadm">Creating a local Calico cluster with kubeadm</h2><p>To get a local single-host Calico cluster in fifteen minutes using kubeadm, refer to the
<a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/">Calico Quickstart</a>.</p><h2 id="what-s-next">What's next</h2><p>Once your cluster is running, you can follow the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p></div></div><div><div class="td-content"><h1>Use Cilium for NetworkPolicy</h1><p>This page shows how to use Cilium for NetworkPolicy.</p><p>For background on Cilium, read the <a href="https://docs.cilium.io/en/stable/overview/intro">Introduction to Cilium</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="deploying-cilium-on-minikube-for-basic-testing">Deploying Cilium on Minikube for Basic Testing</h2><p>To get familiar with Cilium easily you can follow the
<a href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/">Cilium Kubernetes Getting Started Guide</a>
to perform a basic DaemonSet installation of Cilium in minikube.</p><p>To start minikube, minimal version required is &gt;= v1.5.2, run the with the
following arguments:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube version
</span></span></code></pre></div><pre tabindex="0"><code>minikube version: v1.5.2
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube start --network-plugin<span>=</span>cni
</span></span></code></pre></div><p>For minikube you can install Cilium using its CLI tool. To do so, first download the latest
version of the CLI with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -LO https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz
</span></span></code></pre></div><p>Then extract the downloaded file to your <code>/usr/local/bin</code> directory with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
</span></span><span><span>rm cilium-linux-amd64.tar.gz
</span></span></code></pre></div><p>After running the above commands, you can now install Cilium with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cilium install
</span></span></code></pre></div><p>Cilium will then automatically detect the cluster configuration and create and
install the appropriate components for a successful installation.
The components are:</p><ul><li>Certificate Authority (CA) in Secret <code>cilium-ca</code> and certificates for Hubble (Cilium's observability layer).</li><li>Service accounts.</li><li>Cluster roles.</li><li>ConfigMap.</li><li>Agent DaemonSet and an Operator Deployment.</li></ul><p>After the installation, you can view the overall status of the Cilium deployment with the <code>cilium status</code> command.
See the expected output of the <code>status</code> command
<a href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#validate-the-installation">here</a>.</p><p>The remainder of the Getting Started Guide explains how to enforce both L3/L4
(i.e., IP address + port) security policies, as well as L7 (e.g., HTTP) security
policies using an example application.</p><h2 id="deploying-cilium-for-production-use">Deploying Cilium for Production Use</h2><p>For detailed instructions around deploying Cilium for production, see:
<a href="https://docs.cilium.io/en/stable/network/kubernetes/concepts/">Cilium Kubernetes Installation Guide</a>
This documentation includes detailed requirements, instructions and example
production DaemonSet files.</p><h2 id="understanding-cilium-components">Understanding Cilium components</h2><p>Deploying a cluster with Cilium adds Pods to the <code>kube-system</code> namespace. To see
this list of Pods run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --namespace<span>=</span>kube-system -l k8s-app<span>=</span>cilium
</span></span></code></pre></div><p>You'll see a list of Pods similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME           READY   STATUS    RESTARTS   AGE
</span></span></span><span><span><span>cilium-kkdhz   1/1     Running   0          3m23s
</span></span></span><span><span><span>...
</span></span></span></code></pre></div><p>A <code>cilium</code> Pod runs on each node in your cluster and enforces network policy
on the traffic to/from Pods on that node using Linux BPF.</p><h2 id="what-s-next">What's next</h2><p>Once your cluster is running, you can follow the
<a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
to try out Kubernetes NetworkPolicy with Cilium.
Have fun, and if you have questions, contact us using the
<a href="https://cilium.herokuapp.com/">Cilium Slack Channel</a>.</p></div></div><div><div class="td-content"><h1>Use Kube-router for NetworkPolicy</h1><p>This page shows how to use <a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a> for NetworkPolicy.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster running. If you do not already have a cluster, you can create one by using any of the cluster installers like Kops, Bootkube, Kubeadm etc.</p><h2 id="installing-kube-router-addon">Installing Kube-router addon</h2><p>The Kube-router Addon comes with a Network Policy Controller that watches Kubernetes API server for any NetworkPolicy and pods updated and configures iptables rules and ipsets to allow or block traffic as directed by the policies. Please follow the <a href="https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers">trying Kube-router with cluster installers</a> guide to install Kube-router addon.</p><h2 id="what-s-next">What's next</h2><p>Once you have installed the Kube-router addon, you can follow the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p></div></div><div><div class="td-content"><h1>Romana for NetworkPolicy</h1><p>This page shows how to use Romana for NetworkPolicy.</p><h2 id="before-you-begin">Before you begin</h2><p>Complete steps 1, 2, and 3 of the <a href="/docs/reference/setup-tools/kubeadm/">kubeadm getting started guide</a>.</p><h2 id="installing-romana-with-kubeadm">Installing Romana with kubeadm</h2><p>Follow the <a href="https://github.com/romana/romana/tree/master/containerize">containerized installation guide</a> for kubeadm.</p><h2 id="applying-network-policies">Applying network policies</h2><p>To apply network policies use one of the following:</p><ul><li><a href="https://github.com/romana/romana/wiki/Romana-policies">Romana network policies</a>.<ul><li><a href="https://github.com/romana/core/blob/master/doc/policy.md">Example of Romana network policy</a>.</li></ul></li><li>The NetworkPolicy API.</li></ul><h2 id="what-s-next">What's next</h2><p>Once you have installed Romana, you can follow the
<a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
to try out Kubernetes NetworkPolicy.</p></div></div><div><div class="td-content"><h1>Weave Net for NetworkPolicy</h1><p>This page shows how to use Weave Net for NetworkPolicy.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster. Follow the
<a href="/docs/reference/setup-tools/kubeadm/">kubeadm getting started guide</a> to bootstrap one.</p><h2 id="install-the-weave-net-addon">Install the Weave Net addon</h2><p>Follow the <a href="https://github.com/weaveworks/weave/blob/master/site/kubernetes/kube-addon.md#-installation">Integrating Kubernetes via the Addon</a> guide.</p><p>The Weave Net addon for Kubernetes comes with a
<a href="https://github.com/weaveworks/weave/blob/master/site/kubernetes/kube-addon.md#network-policy">Network Policy Controller</a>
that automatically monitors Kubernetes for any NetworkPolicy annotations on all
namespaces and configures <code>iptables</code> rules to allow or block traffic as directed by the policies.</p><h2 id="test-the-installation">Test the installation</h2><p>Verify that the weave works.</p><p>Enter the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -n kube-system -o wide
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
weave-net-1t1qg                         2/2       Running   0          9d        192.168.2.10    worknode3
weave-net-231d7                         2/2       Running   1          7d        10.2.0.17       worknodegpu
weave-net-7nmwt                         2/2       Running   3          9d        192.168.2.131   masternode
weave-net-pmw8w                         2/2       Running   0          9d        192.168.2.216   worknode2
</code></pre><p>Each Node has a weave Pod, and all Pods are <code>Running</code> and <code>2/2 READY</code>. (<code>2/2</code> means that each Pod has <code>weave</code> and <code>weave-npc</code>.)</p><h2 id="what-s-next">What's next</h2><p>Once you have installed the Weave Net addon, you can follow the
<a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
to try out Kubernetes NetworkPolicy. If you have any question, contact us at
<a href="https://github.com/weaveworks/weave#getting-help">#weave-community on Slack or Weave User Group</a>.</p></div></div><div><div class="td-content"><h1>Access Clusters Using the Kubernetes API</h1><p>This page shows how to access clusters using the Kubernetes API.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="accessing-the-kubernetes-api">Accessing the Kubernetes API</h2><h3 id="accessing-for-the-first-time-with-kubectl">Accessing for the first time with kubectl</h3><p>When accessing the Kubernetes API for the first time, use the
Kubernetes command-line tool, <code>kubectl</code>.</p><p>To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a <a href="/docs/setup/">Getting started guide</a>,
or someone else set up the cluster and provided you with credentials and a location.</p><p>Check the location and credentials that kubectl knows about with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view
</span></span></code></pre></div><p>Many of the <a href="https://github.com/kubernetes/examples/tree/master/">examples</a> provide an introduction to using
kubectl. Complete documentation is found in the <a href="/docs/reference/kubectl/">kubectl manual</a>.</p><h3 id="directly-accessing-the-rest-api">Directly accessing the REST API</h3><p>kubectl handles locating and authenticating to the API server. If you want to directly access the REST API with an http client like
<code>curl</code> or <code>wget</code>, or a browser, there are multiple ways you can locate and authenticate against the API server:</p><ol><li>Run kubectl in proxy mode (recommended). This method is recommended, since it uses
the stored API server location and verifies the identity of the API server using a
self-signed certificate. No man-in-the-middle (MITM) attack is possible using this method.</li><li>Alternatively, you can provide the location and credentials directly to the http client.
This works with client code that is confused by proxies. To protect against man in the
middle attacks, you'll need to import a root cert into your browser.</li></ol><p>Using the Go or Python client libraries provides accessing kubectl in proxy mode.</p><h4 id="using-kubectl-proxy">Using kubectl proxy</h4><p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the API server and authenticating.</p><p>Run it like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span> &amp;
</span></span></code></pre></div><p>See <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl proxy</a> for more details.</p><p>Then you can explore the API with curl, wget, or a browser, like so:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://localhost:8080/api/
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"versions"</span>: [
</span></span><span><span>    <span>"v1"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"serverAddressByClientCIDRs"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"clientCIDR"</span>: <span>"0.0.0.0/0"</span>,
</span></span><span><span>      <span>"serverAddress"</span>: <span>"10.0.1.149:443"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><h4 id="without-kubectl-proxy">Without kubectl proxy</h4><p>It is possible to avoid using kubectl proxy by passing an authentication token
directly to the API server, like this:</p><p>Using <code>grep/cut</code> approach:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Check all possible clusters, as your .KUBECONFIG may have multiple contexts:</span>
</span></span><span><span>kubectl config view -o <span>jsonpath</span><span>=</span><span>'{"Cluster name\tServer\n"}{range .clusters[*]}{.name}{"\t"}{.cluster.server}{"\n"}{end}'</span>
</span></span><span><span>
</span></span><span><span><span># Select name of cluster you want to interact with from above output:</span>
</span></span><span><span><span>export</span> <span>CLUSTER_NAME</span><span>=</span><span>"some_server_name"</span>
</span></span><span><span>
</span></span><span><span><span># Point to the API server referring the cluster name</span>
</span></span><span><span><span>APISERVER</span><span>=</span><span>$(</span>kubectl config view -o <span>jsonpath</span><span>=</span><span>"{.clusters[?(@.name==\"</span><span>$CLUSTER_NAME</span><span>\")].cluster.server}"</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># Create a secret to hold a token for the default service account</span>
</span></span><span><span>kubectl apply -f - <span>&lt;&lt;EOF
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Secret
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: default-token
</span></span></span><span><span><span>  annotations:
</span></span></span><span><span><span>    kubernetes.io/service-account.name: default
</span></span></span><span><span><span>type: kubernetes.io/service-account-token
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Wait for the token controller to populate the secret with a token:</span>
</span></span><span><span><span>while</span> ! kubectl describe secret default-token | grep -E <span>'^token'</span> &gt;/dev/null; <span>do</span>
</span></span><span><span>  <span>echo</span> <span>"waiting for token..."</span> &gt;&amp;<span>2</span>
</span></span><span><span>  sleep <span>1</span>
</span></span><span><span><span>done</span>
</span></span><span><span>
</span></span><span><span><span># Get the token value</span>
</span></span><span><span><span>TOKEN</span><span>=</span><span>$(</span>kubectl get secret default-token -o <span>jsonpath</span><span>=</span><span>'{.data.token}'</span> | base64 --decode<span>)</span>
</span></span><span><span>
</span></span><span><span><span># Explore the API with TOKEN</span>
</span></span><span><span>curl -X GET <span>$APISERVER</span>/api --header <span>"Authorization: Bearer </span><span>$TOKEN</span><span>"</span> --insecure
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"APIVersions"</span>,
</span></span><span><span>  <span>"versions"</span>: [
</span></span><span><span>    <span>"v1"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"serverAddressByClientCIDRs"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"clientCIDR"</span>: <span>"0.0.0.0/0"</span>,
</span></span><span><span>      <span>"serverAddress"</span>: <span>"10.0.1.149:443"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>The above example uses the <code>--insecure</code> flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
<code>~/.kube</code> directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.</p><p>On some clusters, the API server does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. <a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a>
describes how you can configure this as a cluster administrator.</p><h3 id="programmatic-access-to-the-api">Programmatic access to the API</h3><p>Kubernetes officially supports client libraries for <a href="#go-client">Go</a>, <a href="#python-client">Python</a>,
<a href="#java-client">Java</a>, <a href="#dotnet-client">dotnet</a>, <a href="#javascript-client">JavaScript</a>, and
<a href="#haskell-client">Haskell</a>. There are other client libraries that are provided and maintained by
their authors, not the Kubernetes team. See <a href="/docs/reference/using-api/client-libraries/">client libraries</a>
for accessing the API from other languages and how they authenticate.</p><h4 id="go-client">Go client</h4><ul><li>To get the library, run the following command: <code>go get k8s.io/client-go@kubernetes-&lt;kubernetes-version-number&gt;</code>
See <a href="https://github.com/kubernetes/client-go/releases">https://github.com/kubernetes/client-go/releases</a>
to see which versions are supported.</li><li>Write an application atop of the client-go clients.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>client-go</code> defines its own API objects, so if needed, import API definitions from client-go rather than
from the main repository. For example, <code>import "k8s.io/client-go/kubernetes"</code> is correct.</div><p>The Go client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">example</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-golang"><span><span><span>package</span> main
</span></span><span><span>
</span></span><span><span><span>import</span> (
</span></span><span><span>  <span>"context"</span>
</span></span><span><span>  <span>"fmt"</span>
</span></span><span><span>  <span>"k8s.io/apimachinery/pkg/apis/meta/v1"</span>
</span></span><span><span>  <span>"k8s.io/client-go/kubernetes"</span>
</span></span><span><span>  <span>"k8s.io/client-go/tools/clientcmd"</span>
</span></span><span><span>)
</span></span><span><span>
</span></span><span><span><span>func</span> <span>main</span>() {
</span></span><span><span>  <span>// uses the current context in kubeconfig
</span></span></span><span><span><span></span>  <span>// path-to-kubeconfig -- for example, /root/.kube/config
</span></span></span><span><span><span></span>  config, _ <span>:=</span> clientcmd.<span>BuildConfigFromFlags</span>(<span>""</span>, <span>"&lt;path-to-kubeconfig&gt;"</span>)
</span></span><span><span>  <span>// creates the clientset
</span></span></span><span><span><span></span>  clientset, _ <span>:=</span> kubernetes.<span>NewForConfig</span>(config)
</span></span><span><span>  <span>// access the API to list pods
</span></span></span><span><span><span></span>  pods, _ <span>:=</span> clientset.<span>CoreV1</span>().<span>Pods</span>(<span>""</span>).<span>List</span>(context.<span>TODO</span>(), v1.ListOptions{})
</span></span><span><span>  fmt.<span>Printf</span>(<span>"There are %d pods in the cluster\n"</span>, <span>len</span>(pods.Items))
</span></span><span><span>}
</span></span></code></pre></div><p>If the application is deployed as a Pod in the cluster, see
<a href="/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod">Accessing the API from within a Pod</a>.</p><h4 id="python-client">Python client</h4><p>To use <a href="https://github.com/kubernetes-client/python">Python client</a>, run the following command:
<code>pip install kubernetes</code>. See <a href="https://github.com/kubernetes-client/python">Python Client Library page</a>
for more installation options.</p><p>The Python client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py">example</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-python"><span><span><span>from</span> <span>kubernetes</span> <span>import</span> client, config
</span></span><span><span>
</span></span><span><span>config<span>.</span>load_kube_config()
</span></span><span><span>
</span></span><span><span>v1<span>=</span>client<span>.</span>CoreV1Api()
</span></span><span><span><span>print</span>(<span>"Listing pods with their IPs:"</span>)
</span></span><span><span>ret <span>=</span> v1<span>.</span>list_pod_for_all_namespaces(watch<span>=</span><span>False</span>)
</span></span><span><span><span>for</span> i <span>in</span> ret<span>.</span>items:
</span></span><span><span>    <span>print</span>(<span>"</span><span>%s</span><span>\t</span><span>%s</span><span>\t</span><span>%s</span><span>"</span> <span>%</span> (i<span>.</span>status<span>.</span>pod_ip, i<span>.</span>metadata<span>.</span>namespace, i<span>.</span>metadata<span>.</span>name))
</span></span></code></pre></div><h4 id="java-client">Java client</h4><p>To install the <a href="https://github.com/kubernetes-client/java">Java Client</a>, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Clone java library</span>
</span></span><span><span>git clone --recursive https://github.com/kubernetes-client/java
</span></span><span><span>
</span></span><span><span><span># Installing project artifacts, POM etc:</span>
</span></span><span><span><span>cd</span> java
</span></span><span><span>mvn install
</span></span></code></pre></div><p>See <a href="https://github.com/kubernetes-client/java/releases">https://github.com/kubernetes-client/java/releases</a>
to see which versions are supported.</p><p>The Java client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java">example</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-java"><span><span><span>package</span><span> </span><span>io.kubernetes.client.examples</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.ApiClient</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.ApiException</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.Configuration</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.apis.CoreV1Api</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.models.V1Pod</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.models.V1PodList</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.util.ClientBuilder</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>io.kubernetes.client.util.KubeConfig</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>java.io.FileReader</span>;<span>
</span></span></span><span><span><span></span><span>import</span><span> </span><span>java.io.IOException</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>/**
</span></span></span><span><span><span> * A simple example of how to use the Java API from an application outside a kubernetes cluster
</span></span></span><span><span><span> *
</span></span></span><span><span><span> * &lt;p&gt;Easiest way to run this: mvn exec:java
</span></span></span><span><span><span> * -Dexec.mainClass="io.kubernetes.client.examples.KubeConfigFileClientExample"
</span></span></span><span><span><span> *
</span></span></span><span><span><span> */</span><span>
</span></span></span><span><span><span></span><span>public</span><span> </span><span>class</span> <span>KubeConfigFileClientExample</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>public</span><span> </span><span>static</span><span> </span><span>void</span><span> </span><span>main</span>(String<span>[]</span><span> </span>args)<span> </span><span>throws</span><span> </span>IOException,<span> </span>ApiException<span> </span>{<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// file path to your KubeConfig</span><span>
</span></span></span><span><span><span>    </span>String<span> </span>kubeConfigPath<span> </span><span>=</span><span> </span><span>"~/.kube/config"</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// loading the out-of-cluster config, a kubeconfig from file-system</span><span>
</span></span></span><span><span><span>    </span>ApiClient<span> </span>client<span> </span><span>=</span><span>
</span></span></span><span><span><span>        </span>ClientBuilder.<span>kubeconfig</span>(KubeConfig.<span>loadKubeConfig</span>(<span>new</span><span> </span>FileReader(kubeConfigPath))).<span>build</span>();<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// set the global default api-client to the in-cluster one from above</span><span>
</span></span></span><span><span><span>    </span>Configuration.<span>setDefaultApiClient</span>(client);<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// the CoreV1Api loads default api-client from global configuration.</span><span>
</span></span></span><span><span><span>    </span>CoreV1Api<span> </span>api<span> </span><span>=</span><span> </span><span>new</span><span> </span>CoreV1Api();<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// invokes the CoreV1Api client</span><span>
</span></span></span><span><span><span>    </span>V1PodList<span> </span>list<span> </span><span>=</span><span> </span>api.<span>listPodForAllNamespaces</span>(<span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>,<span> </span><span>null</span>);<span>
</span></span></span><span><span><span>    </span>System.<span>out</span>.<span>println</span>(<span>"Listing all pods: "</span>);<span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span>(V1Pod<span> </span>item<span> </span>:<span> </span>list.<span>getItems</span>())<span> </span>{<span>
</span></span></span><span><span><span>      </span>System.<span>out</span>.<span>println</span>(item.<span>getMetadata</span>().<span>getName</span>());<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><h4 id="dotnet-client">dotnet client</h4><p>To use <a href="https://github.com/kubernetes-client/csharp">dotnet client</a>,
run the following command: <code>dotnet add package KubernetesClient --version 1.6.1</code>.
See <a href="https://github.com/kubernetes-client/csharp">dotnet Client Library page</a>
for more installation options. See
<a href="https://github.com/kubernetes-client/csharp/releases">https://github.com/kubernetes-client/csharp/releases</a>
to see which versions are supported.</p><p>The dotnet client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs">example</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-csharp"><span><span><span>using</span> <span>System</span>;
</span></span><span><span><span>using</span> <span>k8s</span>;
</span></span><span><span>
</span></span><span><span><span>namespace</span> <span>simple</span>
</span></span><span><span>{
</span></span><span><span>    <span>internal</span> <span>class</span> <span>PodList</span>
</span></span><span><span>    {
</span></span><span><span>        <span>private</span> <span>static</span> <span>void</span> Main(<span>string</span>[] args)
</span></span><span><span>        {
</span></span><span><span>            <span>var</span> config = KubernetesClientConfiguration.BuildDefaultConfig();
</span></span><span><span>            IKubernetes client = <span>new</span> Kubernetes(config);
</span></span><span><span>            Console.WriteLine(<span>"Starting Request!"</span>);
</span></span><span><span>
</span></span><span><span>            <span>var</span> list = client.ListNamespacedPod(<span>"default"</span>);
</span></span><span><span>            <span>foreach</span> (<span>var</span> item <span>in</span> list.Items)
</span></span><span><span>            {
</span></span><span><span>                Console.WriteLine(item.Metadata.Name);
</span></span><span><span>            }
</span></span><span><span>            <span>if</span> (list.Items.Count == <span>0</span>)
</span></span><span><span>            {
</span></span><span><span>                Console.WriteLine(<span>"Empty!"</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><h4 id="javascript-client">JavaScript client</h4><p>To install <a href="https://github.com/kubernetes-client/javascript">JavaScript client</a>,
run the following command: <code>npm install @kubernetes/client-node</code>. See
<a href="https://github.com/kubernetes-client/javascript/releases">https://github.com/kubernetes-client/javascript/releases</a>
to see which versions are supported.</p><p>The JavaScript client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/javascript/blob/master/examples/example.js">example</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-javascript"><span><span><span>const</span> k8s <span>=</span> require(<span>'@kubernetes/client-node'</span>);
</span></span><span><span>
</span></span><span><span><span>const</span> kc <span>=</span> <span>new</span> k8s.KubeConfig();
</span></span><span><span>kc.loadFromDefault();
</span></span><span><span>
</span></span><span><span><span>const</span> k8sApi <span>=</span> kc.makeApiClient(k8s.CoreV1Api);
</span></span><span><span>
</span></span><span><span>k8sApi.listNamespacedPod({ namespace<span>:</span> <span>'default'</span> }).then((res) =&gt; {
</span></span><span><span>    console.log(res);
</span></span><span><span>});
</span></span></code></pre></div><h4 id="haskell-client">Haskell client</h4><p>See <a href="https://github.com/kubernetes-client/haskell/releases">https://github.com/kubernetes-client/haskell/releases</a>
to see which versions are supported.</p><p>The <a href="https://github.com/kubernetes-client/haskell">Haskell client</a> can use the same
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs">example</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-haskell"><span><span><span>exampleWithKubeConfig</span> <span>::</span> <span>IO</span> <span>()</span>
</span></span><span><span><span>exampleWithKubeConfig</span> <span>=</span> <span>do</span>
</span></span><span><span>    oidcCache <span>&lt;-</span> atomically <span>$</span> newTVar <span>$</span> <span>Map</span><span>.</span>fromList <span>[]</span>
</span></span><span><span>    (mgr, kcfg) <span>&lt;-</span> mkKubeClientConfig oidcCache <span>$</span> <span>KubeConfigFile</span> <span>"/path/to/kubeconfig"</span>
</span></span><span><span>    dispatchMime
</span></span><span><span>            mgr
</span></span><span><span>            kcfg
</span></span><span><span>            (<span>CoreV1</span><span>.</span>listPodForAllNamespaces (<span>Accept</span> <span>MimeJSON</span>))
</span></span><span><span>        <span>&gt;&gt;=</span> print
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/run-application/access-api-from-pod/">Accessing the Kubernetes API from a Pod</a></li></ul></div></div><div><div class="td-content"><h1>Advertise Extended Resources for a Node</h1><p>This page shows how to specify extended resources for a Node.
Extended resources allow cluster administrators to advertise node-level
resources that would otherwise be unknown to Kubernetes.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="get-the-names-of-your-nodes">Get the names of your Nodes</h2><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes
</span></span></code></pre></div><p>Choose one of your Nodes to use for this exercise.</p><h2 id="advertise-a-new-extended-resource-on-one-of-your-nodes">Advertise a new extended resource on one of your Nodes</h2><p>To advertise a new extended resource on a Node, send an HTTP PATCH request to
the Kubernetes API server. For example, suppose one of your Nodes has four dongles
attached. Here's an example of a PATCH request that advertises four dongle resources
for your Node.</p><pre tabindex="0"><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "add",
    "path": "/status/capacity/example.com~1dongle",
    "value": "4"
  }
]
</code></pre><p>Note that Kubernetes does not need to know what a dongle is or what a dongle is for.
The preceding PATCH request tells Kubernetes that your Node has four things that
you call dongles.</p><p>Start a proxy, so that you can easily send requests to the Kubernetes API server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy
</span></span></code></pre></div><p>In another command window, send the HTTP PATCH request.
Replace <code>&lt;your-node-name&gt;</code> with the name of your Node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --header <span>"Content-Type: application/json-patch+json"</span> <span>\
</span></span></span><span><span><span></span>  --request PATCH <span>\
</span></span></span><span><span><span></span>  --data <span>'[{"op": "add", "path": "/status/capacity/example.com~1dongle", "value": "4"}]'</span> <span>\
</span></span></span><span><span><span></span>  http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the preceding request, <code>~1</code> is the encoding for the character / in
the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href="https://tools.ietf.org/html/rfc6901">IETF RFC 6901</a>, section 3.</div><p>The output shows that the Node has a capacity of 4 dongles:</p><pre tabindex="0"><code>"capacity": {
  "cpu": "2",
  "memory": "2049008Ki",
  "example.com/dongle": "4",
</code></pre><p>Describe your Node:</p><pre tabindex="0"><code>kubectl describe node &lt;your-node-name&gt;
</code></pre><p>Once again, the output shows the dongle resource:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>Capacity</span>:<span>
</span></span></span><span><span><span>  </span><span>cpu</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>memory</span>:<span> </span>2049008Ki<span>
</span></span></span><span><span><span>  </span><span>example.com/dongle</span>:<span> </span><span>4</span><span>
</span></span></span></code></pre></div><p>Now, application developers can create Pods that request a certain
number of dongles. See
<a href="/docs/tasks/configure-pod-container/extended-resource/">Assign Extended Resources to a Container</a>.</p><h2 id="discussion">Discussion</h2><p>Extended resources are similar to memory and CPU resources. For example,
just as a Node has a certain amount of memory and CPU to be shared by all components
running on the Node, it can have a certain number of dongles to be shared
by all components running on the Node. And just as application developers
can create Pods that request a certain amount of memory and CPU, they can
create Pods that request a certain number of dongles.</p><p>Extended resources are opaque to Kubernetes; Kubernetes does not
know anything about what they are. Kubernetes knows only that a Node
has a certain number of them. Extended resources must be advertised in integer
amounts. For example, a Node can advertise four dongles, but not 4.5 dongles.</p><h3 id="storage-example">Storage example</h3><p>Suppose a Node has 800 GiB of a special kind of disk storage. You could
create a name for the special storage, say example.com/special-storage.
Then you could advertise it in chunks of a certain size, say 100 GiB. In that case,
your Node would advertise that it has eight resources of type
example.com/special-storage.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>Capacity</span>:<span>
</span></span></span><span><span><span> </span>...<span>
</span></span></span><span><span><span> </span><span>example.com/special-storage</span>:<span> </span><span>8</span><span>
</span></span></span></code></pre></div><p>If you want to allow arbitrary requests for special storage, you
could advertise special storage in chunks of size 1 byte. In that case, you would advertise
800Gi resources of type example.com/special-storage.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>Capacity</span>:<span>
</span></span></span><span><span><span> </span>...<span>
</span></span></span><span><span><span> </span><span>example.com/special-storage</span>:<span>  </span>800Gi<span>
</span></span></span></code></pre></div><p>Then a Container could request any number of bytes of special storage, up to 800Gi.</p><h2 id="clean-up">Clean up</h2><p>Here is a PATCH request that removes the dongle advertisement from a Node.</p><pre tabindex="0"><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "remove",
    "path": "/status/capacity/example.com~1dongle",
  }
]
</code></pre><p>Start a proxy, so that you can easily send requests to the Kubernetes API server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy
</span></span></code></pre></div><p>In another command window, send the HTTP PATCH request.
Replace <code>&lt;your-node-name&gt;</code> with the name of your Node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --header <span>"Content-Type: application/json-patch+json"</span> <span>\
</span></span></span><span><span><span></span>  --request PATCH <span>\
</span></span></span><span><span><span></span>  --data <span>'[{"op": "remove", "path": "/status/capacity/example.com~1dongle"}]'</span> <span>\
</span></span></span><span><span><span></span>  http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</span></span></code></pre></div><p>Verify that the dongle advertisement has been removed:</p><pre tabindex="0"><code>kubectl describe node &lt;your-node-name&gt; | grep dongle
</code></pre><p>(you should not see any output)</p><h2 id="what-s-next">What's next</h2><h3 id="for-application-developers">For application developers</h3><ul><li><a href="/docs/tasks/configure-pod-container/extended-resource/">Assign Extended Resources to a Container</a></li><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></li><li><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></li><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul></div></div><div><div class="td-content"><h1>Autoscale the DNS Service in a Cluster</h1><p>This page shows how to enable and configure autoscaling of the DNS service in
your Kubernetes cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p></li><li><p>This guide assumes your nodes use the AMD64 or Intel 64 CPU architecture.</p></li><li><p>Make sure <a href="/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS</a> is enabled.</p></li></ul><h2 id="determining-whether-dns-horizontal-autoscaling-is-already-enabled">Determine whether DNS horizontal autoscaling is already enabled</h2><p>List the <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployments</a>
in your cluster in the kube-system <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The output is similar to this:</p><pre><code>NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
...
kube-dns-autoscaler    1/1     1            1           ...
...
</code></pre><p>If you see "kube-dns-autoscaler" in the output, DNS horizontal autoscaling is
already enabled, and you can skip to
<a href="#tuning-autoscaling-parameters">Tuning autoscaling parameters</a>.</p><h2 id="find-scaling-target">Get the name of your DNS Deployment</h2><p>List the DNS deployments in your cluster in the kube-system namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment -l k8s-app<span>=</span>kube-dns --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The output is similar to this:</p><pre><code>NAME      READY   UP-TO-DATE   AVAILABLE   AGE
...
coredns   2/2     2            2           ...
...
</code></pre><p>If you don't see a Deployment for DNS services, you can also look for it by name:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>and look for a deployment named <code>coredns</code> or <code>kube-dns</code>.</p><p>Your scale target is</p><pre><code>Deployment/&lt;your-deployment-name&gt;
</code></pre><p>where <code>&lt;your-deployment-name&gt;</code> is the name of your DNS Deployment. For example, if
the name of your Deployment for DNS is coredns, your scale target is Deployment/coredns.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label
<code>k8s-app=kube-dns</code> so that it can work in clusters that originally used
kube-dns.</div><h2 id="enablng-dns-horizontal-autoscaling">Enable DNS horizontal autoscaling</h2><p>In this section, you create a new Deployment. The Pods in the Deployment run a
container based on the <code>cluster-proportional-autoscaler-amd64</code> image.</p><p>Create a file named <code>dns-horizontal-autoscaler.yaml</code> with this content:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/dns/dns-horizontal-autoscaler.yaml"><code>admin/dns/dns-horizontal-autoscaler.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/dns/dns-horizontal-autoscaler.yaml to clipboard"></div><div class="includecode" id="admin-dns-dns-horizontal-autoscaler-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:kube-dns-autoscaler<span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span> </span>[<span>""</span>]<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span> </span>[<span>"nodes"</span>]<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span> </span>[<span>"list"</span>,<span> </span><span>"watch"</span>]<span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span> </span>[<span>""</span>]<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span> </span>[<span>"replicationcontrollers/scale"</span>]<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span> </span>[<span>"get"</span>,<span> </span><span>"update"</span>]<span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span> </span>[<span>"apps"</span>]<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span> </span>[<span>"deployments/scale"</span>,<span> </span><span>"replicasets/scale"</span>]<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span> </span>[<span>"get"</span>,<span> </span><span>"update"</span>]<span>
</span></span></span><span><span><span></span><span># Remove the configmaps rule once below issue is fixed:</span><span>
</span></span></span><span><span><span></span><span># kubernetes-incubator/cluster-proportional-autoscaler#16</span><span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span> </span>[<span>""</span>]<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span> </span>[<span>"configmaps"</span>]<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span> </span>[<span>"get"</span>,<span> </span><span>"create"</span>]<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRoleBinding<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:kube-dns-autoscaler<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span>  </span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:kube-dns-autoscaler<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>k8s-app</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>k8s-app</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>priorityClassName</span>:<span> </span>system-cluster-critical<span>
</span></span></span><span><span><span>      </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>        </span><span>seccompProfile</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>RuntimeDefault<span>
</span></span></span><span><span><span>        </span><span>supplementalGroups</span>:<span> </span>[<span> </span><span>65534</span><span> </span>]<span>
</span></span></span><span><span><span>        </span><span>fsGroup</span>:<span> </span><span>65534</span><span>
</span></span></span><span><span><span>      </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>kubernetes.io/os</span>:<span> </span>linux<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>autoscaler<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/cpa/cluster-proportional-autoscaler:1.8.4<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>            </span><span>requests</span>:<span>
</span></span></span><span><span><span>                </span><span>cpu</span>:<span> </span><span>"20m"</span><span>
</span></span></span><span><span><span>                </span><span>memory</span>:<span> </span><span>"10Mi"</span><span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>          </span>- /cluster-proportional-autoscaler<span>
</span></span></span><span><span><span>          </span>- --namespace=kube-system<span>
</span></span></span><span><span><span>          </span>- --configmap=kube-dns-autoscaler<span>
</span></span></span><span><span><span>          </span><span># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span><span>
</span></span></span><span><span><span>          </span>- --target=&lt;SCALE_TARGET&gt;<span>
</span></span></span><span><span><span>          </span><span># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span><span>
</span></span></span><span><span><span>          </span><span># If using small nodes, "nodesPerReplica" should dominate.</span><span>
</span></span></span><span><span><span>          </span>- --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true,"includeUnschedulableNodes":true}}<span>
</span></span></span><span><span><span>          </span>- --logtostderr=true<span>
</span></span></span><span><span><span>          </span>- --v=2<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span><span>"CriticalAddonsOnly"</span><span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span><span>"Exists"</span><span>
</span></span></span><span><span><span>      </span><span>serviceAccountName</span>:<span> </span>kube-dns-autoscaler<span>
</span></span></span></code></pre></div></div></div><p>In the file, replace <code>&lt;SCALE_TARGET&gt;</code> with your scale target.</p><p>Go to the directory that contains your configuration file, and enter this
command to create the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f dns-horizontal-autoscaler.yaml
</span></span></code></pre></div><p>The output of a successful command is:</p><pre><code>deployment.apps/kube-dns-autoscaler created
</code></pre><p>DNS horizontal autoscaling is now enabled.</p><h2 id="tuning-autoscaling-parameters">Tune DNS autoscaling parameters</h2><p>Verify that the kube-dns-autoscaler <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a> exists:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get configmap --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The output is similar to this:</p><pre><code>NAME                  DATA      AGE
...
kube-dns-autoscaler   1         ...
...
</code></pre><p>Modify the data in the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit configmap kube-dns-autoscaler --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>Look for this line:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>linear</span>:<span> </span><span>'{"coresPerReplica":256,"min":1,"nodesPerReplica":16}'</span><span>
</span></span></span></code></pre></div><p>Modify the fields according to your needs. The "min" field indicates the
minimal number of DNS backends. The actual number of backends is
calculated using this equation:</p><pre><code>replicas = max( ceil( cores &#215; 1/coresPerReplica ) , ceil( nodes &#215; 1/nodesPerReplica ) )
</code></pre><p>Note that the values of both <code>coresPerReplica</code> and <code>nodesPerReplica</code> are
floats.</p><p>The idea is that when a cluster is using nodes that have many cores,
<code>coresPerReplica</code> dominates. When a cluster is using nodes that have fewer
cores, <code>nodesPerReplica</code> dominates.</p><p>There are other supported scaling patterns. For details, see
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler</a>.</p><h2 id="disable-dns-horizontal-autoscaling">Disable DNS horizontal autoscaling</h2><p>There are a few options for tuning DNS horizontal autoscaling. Which option to
use depends on different conditions.</p><h3 id="option-1-scale-down-the-kube-dns-autoscaler-deployment-to-0-replicas">Option 1: Scale down the kube-dns-autoscaler deployment to 0 replicas</h3><p>This option works for all situations. Enter this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale deployment --replicas<span>=</span><span>0</span> kube-dns-autoscaler --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The output is:</p><pre><code>deployment.apps/kube-dns-autoscaler scaled
</code></pre><p>Verify that the replica count is zero:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The output displays 0 in the DESIRED and CURRENT columns:</p><pre><code>NAME                                  DESIRED   CURRENT   READY   AGE
...
kube-dns-autoscaler-6b59789fc8        0         0         0       ...
...
</code></pre><h3 id="option-2-delete-the-kube-dns-autoscaler-deployment">Option 2: Delete the kube-dns-autoscaler deployment</h3><p>This option works if kube-dns-autoscaler is under your own control, which means
no one will re-create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment kube-dns-autoscaler --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>The output is:</p><pre><code>deployment.apps "kube-dns-autoscaler" deleted
</code></pre><h3 id="option-3-delete-the-kube-dns-autoscaler-manifest-file-from-the-master-node">Option 3: Delete the kube-dns-autoscaler manifest file from the master node</h3><p>This option works if kube-dns-autoscaler is under control of the (deprecated)
<a href="https://git.k8s.io/kubernetes/cluster/addons/README.md">Addon Manager</a>,
and you have write access to the master node.</p><p>Sign in to the master node and delete the corresponding manifest file.
The common path for this kube-dns-autoscaler is:</p><pre><code>/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre><p>After the manifest file is deleted, the Addon Manager will delete the
kube-dns-autoscaler Deployment.</p><h2 id="understanding-how-dns-horizontal-autoscaling-works">Understanding how DNS horizontal autoscaling works</h2><ul><li><p>The cluster-proportional-autoscaler application is deployed separately from
the DNS service.</p></li><li><p>An autoscaler Pod runs a client that polls the Kubernetes API server for the
number of nodes and cores in the cluster.</p></li><li><p>A desired replica count is calculated and applied to the DNS backends based on
the current schedulable nodes and cores and the given scaling parameters.</p></li><li><p>The scaling parameters and data points are provided via a ConfigMap to the
autoscaler, and it refreshes its parameters table every poll interval to be up
to date with the latest desired scaling parameters.</p></li><li><p>Changes to the scaling parameters are allowed without rebuilding or restarting
the autoscaler Pod.</p></li><li><p>The autoscaler provides a controller interface to support two control
patterns: <em>linear</em> and <em>ladder</em>.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">Guaranteed Scheduling For Critical Add-On Pods</a>.</li><li>Learn more about the
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">implementation of cluster-proportional-autoscaler</a>.</li></ul></div></div><div><div class="td-content"><h1>Change the Access Mode of a PersistentVolume to ReadWriteOncePod</h1><p>This page shows how to change the access mode on an existing PersistentVolume to
use <code>ReadWriteOncePod</code>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.22.<p>To check the version, enter <code>kubectl version</code>.</p></p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>ReadWriteOncePod</code> access mode graduated to stable in the Kubernetes v1.29
release. If you are running a version of Kubernetes older than v1.29, you might
need to enable a feature gate. Check the documentation for your version of
Kubernetes.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>ReadWriteOncePod</code> access mode is only supported for
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> volumes.
To use this volume access mode you will need to update the following
<a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html">CSI sidecars</a>
to these versions or greater:</p><ul><li><a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+</a></li></ul></div><h2 id="why-should-i-use-readwriteoncepod">Why should I use <code>ReadWriteOncePod</code>?</h2><p>Prior to Kubernetes v1.22, the <code>ReadWriteOnce</code> access mode was commonly used to
restrict PersistentVolume access for workloads that required single-writer
access to storage. However, this access mode had a limitation: it restricted
volume access to a single <em>node</em>, allowing multiple pods on the same node to
read from and write to the same volume simultaneously. This could pose a risk
for applications that demand strict single-writer access for data safety.</p><p>If ensuring single-writer access is critical for your workloads, consider
migrating your volumes to <code>ReadWriteOncePod</code>.</p><h2 id="migrating-existing-persistentvolumes">Migrating existing PersistentVolumes</h2><p>If you have existing PersistentVolumes, they can be migrated to use
<code>ReadWriteOncePod</code>. Only migrations from <code>ReadWriteOnce</code> to <code>ReadWriteOncePod</code>
are supported.</p><p>In this example, there is already a <code>ReadWriteOnce</code> "cat-pictures-pvc"
PersistentVolumeClaim that is bound to a "cat-pictures-pv" PersistentVolume,
and a "cat-pictures-writer" Deployment that uses this PersistentVolumeClaim.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If your storage plugin supports
<a href="/docs/concepts/storage/dynamic-provisioning/">Dynamic provisioning</a>,
the "cat-picutres-pv" will be created for you, but its name may differ. To get
your PersistentVolume's name run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pvc cat-pictures-pvc -o <span>jsonpath</span><span>=</span><span>'{.spec.volumeName}'</span>
</span></span></code></pre></div></div><p>And you can view the PVC before you make changes. Either view the manifest
locally, or run <code>kubectl get pvc &lt;name-of-pvc&gt; -o yaml</code>. The output is similar
to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># cat-pictures-pvc.yaml</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cat-pictures-pvc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>  </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>1Gi<span>
</span></span></span></code></pre></div><p>Here's an example Deployment that relies on that PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># cat-pictures-writer-deployment.yaml</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cat-pictures-writer<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>cat-pictures-writer<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>cat-pictures-writer<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>cat-pictures<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/mnt<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>cat-pictures<span>
</span></span></span><span><span><span>        </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>          </span><span>claimName</span>:<span> </span>cat-pictures-pvc<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div><p>As a first step, you need to edit your PersistentVolume's
<code>spec.persistentVolumeReclaimPolicy</code> and set it to <code>Retain</code>. This ensures your
PersistentVolume will not be deleted when you delete the corresponding
PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pv cat-pictures-pv -p <span>'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'</span>
</span></span></code></pre></div><p>Next you need to stop any workloads that are using the PersistentVolumeClaim
bound to the PersistentVolume you want to migrate, and then delete the
PersistentVolumeClaim. Avoid making any other changes to the
PersistentVolumeClaim, such as volume resizes, until after the migration is
complete.</p><p>Once that is done, you need to clear your PersistentVolume's <code>spec.claimRef.uid</code>
to ensure PersistentVolumeClaims can bind to it upon recreation:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale --replicas<span>=</span><span>0</span> deployment cat-pictures-writer
</span></span><span><span>kubectl delete pvc cat-pictures-pvc
</span></span><span><span>kubectl patch pv cat-pictures-pv -p <span>'{"spec":{"claimRef":{"uid":""}}}'</span>
</span></span></code></pre></div><p>After that, replace the PersistentVolume's list of valid access modes to be
(only) <code>ReadWriteOncePod</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pv cat-pictures-pv -p <span>'{"spec":{"accessModes":["ReadWriteOncePod"]}}'</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>ReadWriteOncePod</code> access mode cannot be combined with other access modes.
Make sure <code>ReadWriteOncePod</code> is the only access mode on the PersistentVolume
when updating, otherwise the request will fail.</div><p>Next you need to modify your PersistentVolumeClaim to set <code>ReadWriteOncePod</code> as
the only access mode. You should also set the PersistentVolumeClaim's
<code>spec.volumeName</code> to the name of your PersistentVolume to ensure it binds to
this specific PersistentVolume.</p><p>Once this is done, you can recreate your PersistentVolumeClaim and start up your
workloads:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to:</span>
</span></span><span><span><span># - Set ReadWriteOncePod as the only access mode</span>
</span></span><span><span><span># - Set spec.volumeName to "cat-pictures-pv"</span>
</span></span><span><span>
</span></span><span><span>kubectl apply -f cat-pictures-pvc.yaml
</span></span><span><span>kubectl apply -f cat-pictures-writer-deployment.yaml
</span></span></code></pre></div><p>Lastly you may edit your PersistentVolume's <code>spec.persistentVolumeReclaimPolicy</code>
and set to it back to <code>Delete</code> if you previously changed it.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pv cat-pictures-pv -p <span>'{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}'</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">Configuring a Pod to Use a PersistentVolume for Storage</a></li></ul></div></div><div><div class="td-content"><h1>Change the default StorageClass</h1><p>This page shows how to change the default Storage Class that is used to
provision volumes for PersistentVolumeClaims that have no special requirements.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="why-change-the-default-storage-class">Why change the default storage class?</h2><p>Depending on the installation method, your Kubernetes cluster may be deployed with
an existing StorageClass that is marked as default. This default StorageClass
is then used to dynamically provision storage for PersistentVolumeClaims
that do not require any specific storage class. See
<a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim documentation</a>
for details.</p><p>The pre-installed default StorageClass may not fit well with your expected workload;
for example, it might provision storage that is too expensive. If this is the case,
you can either change the default StorageClass or disable it completely to avoid
dynamic provisioning of storage.</p><p>Deleting the default StorageClass may not work, as it may be re-created
automatically by the addon manager running in your cluster. Please consult the docs for your installation
for details about addon manager and how to disable individual addons.</p><h2 id="changing-the-default-storageclass">Changing the default StorageClass</h2><ol><li><p>List the StorageClasses in your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get storageclass
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>NAME                 PROVISIONER               AGE
</span></span><span><span>standard <span>(</span>default<span>)</span>   kubernetes.io/gce-pd      1d
</span></span><span><span>gold                 kubernetes.io/gce-pd      1d
</span></span></code></pre></div><p>The default StorageClass is marked by <code>(default)</code>.</p></li><li><p>Mark the default StorageClass as non-default:</p><p>The default StorageClass has an annotation
<code>storageclass.kubernetes.io/is-default-class</code> set to <code>true</code>. Any other value
or absence of the annotation is interpreted as <code>false</code>.</p><p>To mark a StorageClass as non-default, you need to change its value to <code>false</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl patch storageclass standard -p <span>'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'</span>
</span></span></code></pre></div><p>where <code>standard</code> is the name of your chosen StorageClass.</p></li><li><p>Mark a StorageClass as default:</p><p>Similar to the previous step, you need to add/set the annotation
<code>storageclass.kubernetes.io/is-default-class=true</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl patch storageclass gold -p <span>'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span>
</span></span></code></pre></div><p>Please note you can have multiple <code>StorageClass</code> marked as default. If more
than one <code>StorageClass</code> is marked as default, a <code>PersistentVolumeClaim</code> without
an explicitly defined <code>storageClassName</code> will be created using the most recently
created default <code>StorageClass</code>.
When a <code>PersistentVolumeClaim</code> is created with a specified <code>volumeName</code>, it remains
in a pending state if the static volume's <code>storageClassName</code> does not match the
<code>StorageClass</code> on the <code>PersistentVolumeClaim</code>.</p></li><li><p>Verify that your chosen StorageClass is default:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get storageclass
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>NAME             PROVISIONER               AGE
</span></span><span><span>standard         kubernetes.io/gce-pd      1d
</span></span><span><span>gold <span>(</span>default<span>)</span>   kubernetes.io/gce-pd      1d
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li></ul></div></div><div><div class="td-content"><h1>Switching from Polling to CRI Event-based Updates to Container Status</h1><div class="feature-state-notice feature-alpha" title="Feature Gate: EventedPLEG"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [alpha]</code> (enabled by default: false)</div><p>This page shows how to migrate nodes to use event based updates for container status. The event-based
implementation reduces node resource consumption by the kubelet, compared to the legacy approach
that relies on polling.
You may know this feature as <em>evented Pod lifecycle event generator (PLEG)</em>. That's the name used
internally within the Kubernetes project for a key implementation detail.</p><p>The polling based approach is referred to as <em>generic PLEG</em>.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to run a version of Kubernetes that provides this feature.
Kubernetes v1.27 includes beta support for event-based container
status updates. The feature is beta but is <em>disabled</em> by default
because it requires support from the container runtime.</li><li>Your Kubernetes server must be at or later than version 1.26.<p>To check the version, enter <code>kubectl version</code>.</p>If you are running a different version of Kubernetes, check the documentation for that release.</li><li>The container runtime in use must support container lifecycle events.
The kubelet automatically switches back to the legacy generic PLEG
mechanism if the container runtime does not announce support for
container lifecycle events, even if you have this feature gate enabled.</li></ul><h2 id="why-switch-to-evented-pleg">Why switch to Evented PLEG?</h2><ul><li>The <em>Generic PLEG</em> incurs non-negligible overhead due to frequent polling of container statuses.</li><li>This overhead is exacerbated by Kubelet's parallelized polling of container states, thus limiting
its scalability and causing poor performance and reliability problems.</li><li>The goal of <em>Evented PLEG</em> is to reduce unnecessary work during inactivity
by replacing periodic polling.</li></ul><h2 id="switching-to-evented-pleg">Switching to Evented PLEG</h2><ol><li><p>Start the Kubelet with the <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
<code>EventedPLEG</code> enabled. You can manage the kubelet feature gates editing the kubelet
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">config file</a> and restarting the kubelet service.
You need to do this on each node where you are using this feature.</p></li><li><p>Make sure the node is <a href="/docs/tasks/administer-cluster/safely-drain-node/">drained</a> before proceeding.</p></li><li><p>Start the container runtime with the container event generation enabled.</p><ul class="nav nav-tabs" id="tab-with-code"><li class="nav-item"><a class="nav-link active" href="#tab-with-code-0">Containerd</a></li><li class="nav-item"><a class="nav-link" href="#tab-with-code-1">CRI-O</a></li></ul><div class="tab-content" id="tab-with-code"><div id="tab-with-code-0" class="tab-pane show active"><p><p>Version 1.7+</p></p></div><div id="tab-with-code-1" class="tab-pane"><p><p>Version 1.26+</p><p>Check if the CRI-O is already configured to emit CRI events by verifying the configuration,</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crio config | grep enable_pod_events
</span></span></code></pre></div><p>If it is enabled, the output should be similar to the following:</p><pre tabindex="0"><code class="language-none">enable_pod_events = true
</code></pre><p>To enable it, start the CRI-O daemon with the flag <code>--enable-pod-events=true</code> or
use a dropin config with the following lines:</p><div class="highlight"><pre tabindex="0"><code class="language-toml"><span><span>[crio.runtime]
</span></span><span><span>enable_pod_events<span>:</span> <span>true</span>
</span></span></code></pre></div></p></div></div>Your Kubernetes server must be at or later than version 1.26.<p>To check the version, enter <code>kubectl version</code>.</p></li><li><p>Verify that the kubelet is using event-based container stage change monitoring.
To check, look for the term <code>EventedPLEG</code> in the kubelet logs.</p><p>The output should be similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>I0314 11:10:13.909915 1105457 feature_gate.go:249] feature gates: &amp;{map[EventedPLEG:true]}
</span></span></span></code></pre></div><p>If you have set <code>--v</code> to 4 and above, you might see more entries that indicate
that the kubelet is using event-based container state monitoring.</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>I0314 11:12:42.009542 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=3b2c6172-b112-447a-ba96-94e7022912dc
</span></span></span><span><span><span>I0314 11:12:44.623326 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=b3fba5ea-a8c5-4b76-8f43-481e17e8ec40
</span></span></span><span><span><span>I0314 11:12:44.714564 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=b3fba5ea-a8c5-4b76-8f43-481e17e8ec40
</span></span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the design in the Kubernetes Enhancement Proposal (KEP):
<a href="https://github.com/kubernetes/enhancements/blob/5b258a990adabc2ffdc9d84581ea6ed696f7ce6c/keps/sig-node/3386-kubelet-evented-pleg/README.md">Kubelet Evented PLEG for Better Performance</a>.</li></ul></div></div><div><div class="td-content"><h1>Change the Reclaim Policy of a PersistentVolume</h1><p>This page shows how to change the reclaim policy of a Kubernetes
PersistentVolume.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="why-change-reclaim-policy-of-a-persistentvolume">Why change reclaim policy of a PersistentVolume</h2><p>PersistentVolumes can have various reclaim policies, including "Retain",
"Recycle", and "Delete". For dynamically provisioned PersistentVolumes,
the default reclaim policy is "Delete". This means that a dynamically provisioned
volume is automatically deleted when a user deletes the corresponding
PersistentVolumeClaim. This automatic behavior might be inappropriate if the volume
contains precious data. In that case, it is more appropriate to use the "Retain"
policy. With the "Retain" policy, if a user deletes a PersistentVolumeClaim,
the corresponding PersistentVolume will not be deleted. Instead, it is moved to the
Released phase, where all of its data can be manually recovered.</p><h2 id="changing-the-reclaim-policy-of-a-persistentvolume">Changing the reclaim policy of a PersistentVolume</h2><ol><li><p>List the PersistentVolumes in your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pv
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM             STORAGECLASS     REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1    manual                     10s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2    manual                     6s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim3    manual                     3s
</code></pre><p>This list also includes the name of the claims that are bound to each volume
for easier identification of dynamically provisioned volumes.</p></li><li><p>Choose one of your PersistentVolumes and change its reclaim policy:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pv &lt;your-pv-name&gt; -p <span>'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'</span>
</span></span></code></pre></div><p>where <code>&lt;your-pv-name&gt;</code> is the name of your chosen PersistentVolume.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>On Windows, you must <em>double</em> quote any JSONPath template that contains spaces (not single
quote as shown above for bash). This in turn means that you must use a single quote or escaped
double quote around any literals in the template. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>kubectl patch pv &lt;your-pv-name&gt; -p <span>"{\"</span>spec\<span>":{\"</span>persistentVolumeReclaimPolicy\<span>":\"</span>Retain\<span>"}}"</span>
</span></span></code></pre></div></div></li><li><p>Verify that your chosen PersistentVolume has the right policy:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pv
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM             STORAGECLASS     REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1    manual                     40s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2    manual                     36s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Retain          Bound     default/claim3    manual                     33s
</code></pre><p>In the preceding output, you can see that the volume bound to claim
<code>default/claim3</code> has reclaim policy <code>Retain</code>. It will not be automatically
deleted when a user deletes claim <code>default/claim3</code>.</p></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a>.</li></ul><h3 id="reference">References</h3><ul><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/">PersistentVolume</a><ul><li>Pay attention to the <code>.spec.persistentVolumeReclaimPolicy</code>
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec">field</a>
of PersistentVolume.</li></ul></li><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/">PersistentVolumeClaim</a></li></ul></div></div><div><div class="td-content"><h1>Cloud Controller Manager Administration</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.11 [beta]</code></div><p>Since cloud providers develop and release at a different pace compared to the
Kubernetes project, abstracting the provider-specific code to the
<code><a class="glossary-tooltip" title="Control plane component that integrates Kubernetes with third-party cloud providers." href="/docs/concepts/architecture/cloud-controller/" target="_blank">cloud-controller-manager</a></code>
binary allows cloud vendors to evolve independently from the core Kubernetes code.</p><p>The <code>cloud-controller-manager</code> can be linked to any cloud provider that satisfies
<a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go">cloudprovider.Interface</a>.
For backwards compatibility, the
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">cloud-controller-manager</a>
provided in the core Kubernetes project uses the same cloud libraries as <code>kube-controller-manager</code>.
Cloud providers already supported in Kubernetes core are expected to use the in-tree
cloud-controller-manager to transition out of Kubernetes core.</p><h2 id="administration">Administration</h2><h3 id="requirements">Requirements</h3><p>Every cloud has their own set of requirements for running their own cloud provider
integration, it should not be too different from the requirements when running
<code>kube-controller-manager</code>. As a general rule of thumb you'll need:</p><ul><li>cloud authentication/authorization: your cloud may require a token or IAM rules
to allow access to their APIs</li><li>kubernetes authentication/authorization: cloud-controller-manager may need RBAC
rules set to speak to the kubernetes apiserver</li><li>high availability: like kube-controller-manager, you may want a high available
setup for cloud controller manager using leader election (on by default).</li></ul><h3 id="running-cloud-controller-manager">Running cloud-controller-manager</h3><p>Successfully running cloud-controller-manager requires some changes to your cluster configuration.</p><ul><li><code>kubelet</code>, <code>kube-apiserver</code>, and <code>kube-controller-manager</code> must be set according to the
user's usage of external CCM. If the user has an external CCM (not the internal cloud
controller loops in the Kubernetes Controller Manager), then <code>--cloud-provider=external</code>
must be specified. Otherwise, it should not be specified.</li></ul><p>Keep in mind that setting up your cluster to use cloud controller manager will
change your cluster behaviour in a few ways:</p><ul><li>Components that specify <code>--cloud-provider=external</code> will add a taint
<code>node.cloudprovider.kubernetes.io/uninitialized</code> with an effect <code>NoSchedule</code>
during initialization. This marks the node as needing a second initialization
from an external controller before it can be scheduled work. Note that in the
event that cloud controller manager is not available, new nodes in the cluster
will be left unschedulable. The taint is important since the scheduler may
require cloud specific information about nodes such as their region or type
(high cpu, gpu, high memory, spot instance, etc).</li><li>cloud information about nodes in the cluster will no longer be retrieved using
local metadata, but instead all API calls to retrieve node information will go
through cloud controller manager. This may mean you can restrict access to your
cloud API on the kubelets for better security. For larger clusters you may want
to consider if cloud controller manager will hit rate limits since it is now
responsible for almost all API calls to your cloud from within the cluster.</li></ul><p>The cloud controller manager can implement:</p><ul><li>Node controller - responsible for updating kubernetes nodes using cloud APIs
and deleting kubernetes nodes that were deleted on your cloud.</li><li>Service controller - responsible for loadbalancers on your cloud against
services of type LoadBalancer.</li><li>Route controller - responsible for setting up network routes on your cloud</li><li>any other features you would like to implement if you are running an out-of-tree provider.</li></ul><h2 id="examples">Examples</h2><p>If you are using a cloud that is currently supported in Kubernetes core and would
like to adopt cloud controller manager, see the
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">cloud controller manager in kubernetes core</a>.</p><p>For cloud controller managers not in Kubernetes core, you can find the respective
projects in repositories maintained by cloud vendors or by SIGs.</p><p>For providers already in Kubernetes core, you can run the in-tree cloud controller
manager as a DaemonSet in your cluster, use the following as a guideline:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/cloud/ccm-example.yaml"><code>admin/cloud/ccm-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/cloud/ccm-example.yaml to clipboard"></div><div class="includecode" id="admin-cloud-ccm-example-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># This is an example of how to set up cloud-controller-manager as a Daemonset in your cluster.</span><span>
</span></span></span><span><span><span></span><span># It assumes that your masters can run pods and has the role node-role.kubernetes.io/master</span><span>
</span></span></span><span><span><span></span><span># Note that this Daemonset will not work straight out of the box for your cloud, this is</span><span>
</span></span></span><span><span><span></span><span># meant to be a guideline.</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRoleBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:cloud-controller-manager<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cluster-admin<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span></span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>k8s-app</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>k8s-app</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>serviceAccountName</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>        </span><span># for in-tree providers we use registry.k8s.io/cloud-controller-manager</span><span>
</span></span></span><span><span><span>        </span><span># this can be replaced with any other image for out-of-tree providers</span><span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/cloud-controller-manager:v1.8.0<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- /usr/local/bin/cloud-controller-manager<span>
</span></span></span><span><span><span>        </span>- --cloud-provider=[YOUR_CLOUD_PROVIDER] <span> </span><span># Add your own cloud provider here!</span><span>
</span></span></span><span><span><span>        </span>- --leader-elect=true<span>
</span></span></span><span><span><span>        </span>- --use-service-account-credentials<span>
</span></span></span><span><span><span>        </span><span># these flags will vary for every cloud provider</span><span>
</span></span></span><span><span><span>        </span>- --allocate-node-cidrs=true<span>
</span></span></span><span><span><span>        </span>- --configure-cloud-routes=true<span>
</span></span></span><span><span><span>        </span>- --cluster-cidr=172.17.0.0/16<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span><span># this is required so CCM can bootstrap itself</span><span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node.cloudprovider.kubernetes.io/uninitialized<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span><span># these tolerations are to have the daemonset runnable on control plane nodes</span><span>
</span></span></span><span><span><span>      </span><span># remove them if your control plane nodes should not run pods</span><span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/control-plane<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/master<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span><span># this is to restrict CCM to only run on master nodes</span><span>
</span></span></span><span><span><span>      </span><span># the node selector may vary depending on your cluster setup</span><span>
</span></span></span><span><span><span>      </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>node-role.kubernetes.io/master</span>:<span> </span><span>""</span><span>
</span></span></span></code></pre></div></div></div><h2 id="limitations">Limitations</h2><p>Running cloud controller manager comes with a few possible limitations. Although
these limitations are being addressed in upcoming releases, it's important that
you are aware of these limitations for production workloads.</p><h3 id="support-for-volumes">Support for Volumes</h3><p>Cloud controller manager does not implement any of the volume controllers found
in <code>kube-controller-manager</code> as the volume integrations also require coordination
with kubelets. As we evolve CSI (container storage interface) and add stronger
support for flex volume plugins, necessary support will be added to cloud
controller manager so that clouds can fully integrate with volumes. Learn more
about out-of-tree CSI volume plugins <a href="https://github.com/kubernetes/features/issues/178">here</a>.</p><h3 id="scalability">Scalability</h3><p>The cloud-controller-manager queries your cloud provider's APIs to retrieve
information for all nodes. For very large clusters, consider possible
bottlenecks such as resource requirements and API rate limiting.</p><h3 id="chicken-and-egg">Chicken and Egg</h3><p>The goal of the cloud controller manager project is to decouple development
of cloud features from the core Kubernetes project. Unfortunately, many aspects
of the Kubernetes project has assumptions that cloud provider features are tightly
integrated into the project. As a result, adopting this new architecture can create
several situations where a request is being made for information from a cloud provider,
but the cloud controller manager may not be able to return that information without
the original request being complete.</p><p>A good example of this is the TLS bootstrapping feature in the Kubelet.
TLS bootstrapping assumes that the Kubelet has the ability to ask the cloud provider
(or a local metadata service) for all its address types (private, public, etc)
but cloud controller manager cannot set a node's address types without being
initialized in the first place which requires that the kubelet has TLS certificates
to communicate with the apiserver.</p><p>As this initiative evolves, changes will be made to address these issues in upcoming releases.</p><h2 id="what-s-next">What's next</h2><p>To build and develop your own cloud controller manager, read
<a href="/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a>.</p></div></div><div><div class="td-content"><h1>Configure a kubelet image credential provider</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Starting from Kubernetes v1.20, the kubelet can dynamically retrieve credentials for a container image registry
using exec plugins. The kubelet and the exec plugin communicate through stdio (stdin, stdout, and stderr) using
Kubernetes versioned APIs. These plugins allow the kubelet to request credentials for a container registry dynamically
as opposed to storing static credentials on disk. For example, the plugin may talk to a local metadata server to retrieve
short-lived credentials for an image that is being pulled by the kubelet.</p><p>You may be interested in using this capability if any of the below are true:</p><ul><li>API calls to a cloud provider service are required to retrieve authentication information for a registry.</li><li>Credentials have short expiration times and requesting new credentials frequently is required.</li><li>Storing registry credentials on disk or in imagePullSecrets is not acceptable.</li></ul><p>This guide demonstrates how to configure the kubelet's image credential provider plugin mechanism.</p><h2 id="service-account-token-for-image-pulls">Service Account Token for Image Pulls</h2><div class="feature-state-notice feature-beta" title="Feature Gate: KubeletServiceAccountTokenForCredentialProviders"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>Starting from Kubernetes v1.33,
the kubelet can be configured to send a service account token
bound to the pod for which the image pull is being performed
to the credential provider plugin.</p><p>This allows the plugin to exchange the token for credentials
to access the image registry.</p><p>To enable this feature,
the <code>KubeletServiceAccountTokenForCredentialProviders</code> feature gate
must be enabled on the kubelet,
and the <code>tokenAttributes</code> field must be set
in the <code>CredentialProviderConfig</code> file for the plugin.</p><p>The <code>tokenAttributes</code> field contains information
about the service account token that will be passed to the plugin,
including the intended audience for the token
and whether the plugin requires the pod to have a service account.</p><p>Using service account token credentials can enable the following use-cases:</p><ul><li>Avoid needing a kubelet/node-based identity to pull images from a registry.</li><li>Allow workloads to pull images based on their own runtime identity
without long-lived/persisted secrets.</li></ul><h2 id="before-you-begin">Before you begin</h2><ul><li>You need a Kubernetes cluster with nodes that support kubelet credential
provider plugins. This support is available in Kubernetes 1.34;
Kubernetes v1.24 and v1.25 included this as a beta feature, enabled by default.</li><li>If you are configuring a credential provider plugin
that requires the service account token,
you need a Kubernetes cluster with nodes running Kubernetes v1.33 or later
and the <code>KubeletServiceAccountTokenForCredentialProviders</code> feature gate
enabled on the kubelet.</li><li>A working implementation of a credential provider exec plugin. You can build your own plugin or use one provided by cloud providers.</li></ul>Your Kubernetes server must be at or later than version v1.26.<p>To check the version, enter <code>kubectl version</code>.</p><h2 id="installing-plugins-on-nodes">Installing Plugins on Nodes</h2><p>A credential provider plugin is an executable binary that will be run by the kubelet. Ensure that the plugin binary exists on
every node in your cluster and stored in a known directory. The directory will be required later when configuring kubelet flags.</p><h2 id="configuring-the-kubelet">Configuring the Kubelet</h2><p>In order to use this feature, the kubelet expects two flags to be set:</p><ul><li><code>--image-credential-provider-config</code> - the path to the credential provider plugin config file.</li><li><code>--image-credential-provider-bin-dir</code> - the path to the directory where credential provider plugin binaries are located.</li></ul><h3 id="configure-a-kubelet-credential-provider">Configure a kubelet credential provider</h3><p>The configuration file passed into <code>--image-credential-provider-config</code> is read by the kubelet to determine which exec plugins
should be invoked for which container images. Here's an example configuration file you may end up using if you are using the
<a href="https://github.com/kubernetes/cloud-provider-aws/tree/master/cmd/ecr-credential-provider">ECR-based plugin</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CredentialProviderConfig<span>
</span></span></span><span><span><span></span><span># providers is a list of credential provider helper plugins that will be enabled by the kubelet.</span><span>
</span></span></span><span><span><span></span><span># Multiple providers may match against a single image, in which case credentials</span><span>
</span></span></span><span><span><span></span><span># from all providers will be returned to the kubelet. If multiple providers are called</span><span>
</span></span></span><span><span><span></span><span># for a single image, the results are combined. If providers return overlapping</span><span>
</span></span></span><span><span><span></span><span># auth keys, the value from the provider earlier in this list is used.</span><span>
</span></span></span><span><span><span></span><span>providers</span>:<span>
</span></span></span><span><span><span>  </span><span># name is the required name of the credential provider. It must match the name of the</span><span>
</span></span></span><span><span><span>  </span><span># provider executable as seen by the kubelet. The executable must be in the kubelet's</span><span>
</span></span></span><span><span><span>  </span><span># bin directory (set by the --image-credential-provider-bin-dir flag).</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>ecr-credential-provider<span>
</span></span></span><span><span><span>    </span><span># matchImages is a required list of strings used to match against images in order to</span><span>
</span></span></span><span><span><span>    </span><span># determine if this provider should be invoked. If one of the strings matches the</span><span>
</span></span></span><span><span><span>    </span><span># requested image from the kubelet, the plugin will be invoked and given a chance</span><span>
</span></span></span><span><span><span>    </span><span># to provide credentials. Images are expected to contain the registry domain</span><span>
</span></span></span><span><span><span>    </span><span># and URL path.</span><span>
</span></span></span><span><span><span>    </span><span>#</span><span>
</span></span></span><span><span><span>    </span><span># Each entry in matchImages is a pattern which can optionally contain a port and a path.</span><span>
</span></span></span><span><span><span>    </span><span># Globs can be used in the domain, but not in the port or the path. Globs are supported</span><span>
</span></span></span><span><span><span>    </span><span># as subdomains like '*.k8s.io' or 'k8s.*.io', and top-level-domains such as 'k8s.*'.</span><span>
</span></span></span><span><span><span>    </span><span># Matching partial subdomains like 'app*.k8s.io' is also supported. Each glob can only match</span><span>
</span></span></span><span><span><span>    </span><span># a single subdomain segment, so `*.io` does **not** match `*.k8s.io`.</span><span>
</span></span></span><span><span><span>    </span><span>#</span><span>
</span></span></span><span><span><span>    </span><span># A match exists between an image and a matchImage when all of the below are true:</span><span>
</span></span></span><span><span><span>    </span><span># - Both contain the same number of domain parts and each part matches.</span><span>
</span></span></span><span><span><span>    </span><span># - The URL path of an matchImages must be a prefix of the target image URL path.</span><span>
</span></span></span><span><span><span>    </span><span># - If the matchImages contains a port, then the port must match in the image as well.</span><span>
</span></span></span><span><span><span>    </span><span>#</span><span>
</span></span></span><span><span><span>    </span><span># Example values of matchImages:</span><span>
</span></span></span><span><span><span>    </span><span># - 123456789.dkr.ecr.us-east-1.amazonaws.com</span><span>
</span></span></span><span><span><span>    </span><span># - *.azurecr.io</span><span>
</span></span></span><span><span><span>    </span><span># - gcr.io</span><span>
</span></span></span><span><span><span>    </span><span># - *.*.registry.io</span><span>
</span></span></span><span><span><span>    </span><span># - registry.io:8080/path</span><span>
</span></span></span><span><span><span>    </span><span>matchImages</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"*.dkr.ecr.*.amazonaws.com"</span><span>
</span></span></span><span><span><span>      </span>- <span>"*.dkr.ecr.*.amazonaws.com.cn"</span><span>
</span></span></span><span><span><span>      </span>- <span>"*.dkr.ecr-fips.*.amazonaws.com"</span><span>
</span></span></span><span><span><span>      </span>- <span>"*.dkr.ecr.us-iso-east-1.c2s.ic.gov"</span><span>
</span></span></span><span><span><span>      </span>- <span>"*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov"</span><span>
</span></span></span><span><span><span>    </span><span># defaultCacheDuration is the default duration the plugin will cache credentials in-memory</span><span>
</span></span></span><span><span><span>    </span><span># if a cache duration is not provided in the plugin response. This field is required.</span><span>
</span></span></span><span><span><span>    </span><span>defaultCacheDuration</span>:<span> </span><span>"12h"</span><span>
</span></span></span><span><span><span>    </span><span># Required input version of the exec CredentialProviderRequest. The returned CredentialProviderResponse</span><span>
</span></span></span><span><span><span>    </span><span># MUST use the same encoding version as the input. Current supported values are:</span><span>
</span></span></span><span><span><span>    </span><span># - credentialprovider.kubelet.k8s.io/v1</span><span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>credentialprovider.kubelet.k8s.io/v1<span>
</span></span></span><span><span><span>    </span><span># Arguments to pass to the command when executing it.</span><span>
</span></span></span><span><span><span>    </span><span># +optional</span><span>
</span></span></span><span><span><span>    </span><span># args:</span><span>
</span></span></span><span><span><span>    </span><span>#   - --example-argument</span><span>
</span></span></span><span><span><span>    </span><span># Env defines additional environment variables to expose to the process. These</span><span>
</span></span></span><span><span><span>    </span><span># are unioned with the host's environment, as well as variables client-go uses</span><span>
</span></span></span><span><span><span>    </span><span># to pass argument to the plugin.</span><span>
</span></span></span><span><span><span>    </span><span># +optional</span><span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>AWS_PROFILE<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>example_profile<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span># tokenAttributes is the configuration for the service account token that will be passed to the plugin.</span><span>
</span></span></span><span><span><span>    </span><span># The credential provider opts in to using service account tokens for image pull by setting this field.</span><span>
</span></span></span><span><span><span>    </span><span># if this field is set without the `KubeletServiceAccountTokenForCredentialProviders` feature gate enabled, </span><span>
</span></span></span><span><span><span>    </span><span># kubelet will fail to start with invalid configuration error.</span><span>
</span></span></span><span><span><span>    </span><span># +optional</span><span>
</span></span></span><span><span><span>    </span><span>tokenAttributes</span>:<span>
</span></span></span><span><span><span>      </span><span># serviceAccountTokenAudience is the intended audience for the projected service account token.</span><span>
</span></span></span><span><span><span>      </span><span># +required</span><span>
</span></span></span><span><span><span>      </span><span>serviceAccountTokenAudience</span>:<span> </span><span>"&lt;audience for the token&gt;"</span><span>
</span></span></span><span><span><span>      </span><span># cacheType indicates the type of cache key use for caching the credentials returned by the plugin</span><span>
</span></span></span><span><span><span>      </span><span># when the service account token is used.</span><span>
</span></span></span><span><span><span>      </span><span># The most conservative option is to set this to "Token", which means the kubelet will cache</span><span>
</span></span></span><span><span><span>      </span><span># returned credentials on a per-token basis. This should be set if the returned credential's</span><span>
</span></span></span><span><span><span>      </span><span># lifetime is limited to the service account token's lifetime.</span><span>
</span></span></span><span><span><span>      </span><span># If the plugin's credential retrieval logic depends only on the service account and not on</span><span>
</span></span></span><span><span><span>      </span><span># pod-specific claims, then the plugin can set this to "ServiceAccount". In this case, the</span><span>
</span></span></span><span><span><span>      </span><span># kubelet will cache returned credentials on a per-serviceaccount basis. Use this when the</span><span>
</span></span></span><span><span><span>      </span><span># returned credential is valid for all pods using the same service account.</span><span>
</span></span></span><span><span><span>      </span><span># +required</span><span>
</span></span></span><span><span><span>      </span><span>cacheType</span>:<span> </span><span>"&lt;Token or ServiceAccount&gt;"</span><span>
</span></span></span><span><span><span>      </span><span># requireServiceAccount indicates whether the plugin requires the pod to have a service account.</span><span>
</span></span></span><span><span><span>      </span><span># If set to true, kubelet will only invoke the plugin if the pod has a service account.</span><span>
</span></span></span><span><span><span>      </span><span># If set to false, kubelet will invoke the plugin even if the pod does not have a service account</span><span>
</span></span></span><span><span><span>      </span><span># and will not include a token in the CredentialProviderRequest. This is useful for plugins</span><span>
</span></span></span><span><span><span>      </span><span># that are used to pull images for pods without service accounts (e.g., static pods).</span><span>
</span></span></span><span><span><span>      </span><span># +required</span><span>
</span></span></span><span><span><span>      </span><span>requireServiceAccount</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span># requiredServiceAccountAnnotationKeys is the list of annotation keys that the plugin is interested in</span><span>
</span></span></span><span><span><span>      </span><span># and that are required to be present in the service account.</span><span>
</span></span></span><span><span><span>      </span><span># The keys defined in this list will be extracted from the corresponding service account and passed</span><span>
</span></span></span><span><span><span>      </span><span># to the plugin as part of the CredentialProviderRequest. If any of the keys defined in this list</span><span>
</span></span></span><span><span><span>      </span><span># are not present in the service account, kubelet will not invoke the plugin and will return an error.</span><span>
</span></span></span><span><span><span>      </span><span># This field is optional and may be empty. Plugins may use this field to extract additional information</span><span>
</span></span></span><span><span><span>      </span><span># required to fetch credentials or allow workloads to opt in to using service account tokens for image pull.</span><span>
</span></span></span><span><span><span>      </span><span># If non-empty, requireServiceAccount must be set to true.</span><span>
</span></span></span><span><span><span>      </span><span># The keys defined in this list must be unique and not overlap with the keys defined in the</span><span>
</span></span></span><span><span><span>      </span><span># optionalServiceAccountAnnotationKeys list.</span><span>
</span></span></span><span><span><span>      </span><span># +optional</span><span>
</span></span></span><span><span><span>      </span><span>requiredServiceAccountAnnotationKeys</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"example.com/required-annotation-key-1"</span><span>
</span></span></span><span><span><span>      </span>- <span>"example.com/required-annotation-key-2"</span><span>
</span></span></span><span><span><span>      </span><span># optionalServiceAccountAnnotationKeys is the list of annotation keys that the plugin is interested in</span><span>
</span></span></span><span><span><span>      </span><span># and that are optional to be present in the service account.</span><span>
</span></span></span><span><span><span>      </span><span># The keys defined in this list will be extracted from the corresponding service account and passed</span><span>
</span></span></span><span><span><span>      </span><span># to the plugin as part of the CredentialProviderRequest. The plugin is responsible for validating the</span><span>
</span></span></span><span><span><span>      </span><span># existence of annotations and their values. This field is optional and may be empty.</span><span>
</span></span></span><span><span><span>      </span><span># Plugins may use this field to extract additional information required to fetch credentials.</span><span>
</span></span></span><span><span><span>      </span><span># The keys defined in this list must be unique and not overlap with the keys defined in the</span><span>
</span></span></span><span><span><span>      </span><span># requiredServiceAccountAnnotationKeys list.</span><span>
</span></span></span><span><span><span>      </span><span># +optional</span><span>
</span></span></span><span><span><span>      </span><span>optionalServiceAccountAnnotationKeys</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"example.com/optional-annotation-key-1"</span><span>
</span></span></span><span><span><span>      </span>- <span>"example.com/optional-annotation-key-2"</span><span>
</span></span></span></code></pre></div><p>The <code>providers</code> field is a list of enabled plugins used by the kubelet. Each entry has a few required fields:</p><ul><li><code>name</code>: the name of the plugin which MUST match the name of the executable binary that exists
in the directory passed into <code>--image-credential-provider-bin-dir</code>.</li><li><code>matchImages</code>: a list of strings used to match against images in order to determine
if this provider should be invoked. More on this below.</li><li><code>defaultCacheDuration</code>: the default duration the kubelet will cache credentials in-memory
if a cache duration was not specified by the plugin.</li><li><code>apiVersion</code>: the API version that the kubelet and the exec plugin will use when communicating.</li></ul><p>Each credential provider can also be given optional args and environment variables as well.
Consult the plugin implementors to determine what set of arguments and environment variables are required for a given plugin.</p><p>If you are using the KubeletServiceAccountTokenForCredentialProviders feature gate
and configuring the plugin to use the service account token
by setting the tokenAttributes field,
the following fields are required:</p><ul><li><code>serviceAccountTokenAudience</code>:
the intended audience for the projected service account token.
This cannot be the empty string.</li><li><code>cacheType</code>:
the type of cache key used for caching the credentials returned by the plugin
when the service account token is used.
The most conservative option is to set this to <code>Token</code>,
which means the kubelet will cache returned credentials
on a per-token basis.
This should be set if the returned credential's lifetime
is limited to the service account token's lifetime.
If the plugin's credential retrieval logic depends only on the service account
and not on pod-specific claims,
then the plugin can set this to <code>ServiceAccount</code>.
In this case, the kubelet will cache returned credentials
on a per-service account basis.
Use this when the returned credential is valid for all pods using the same service account.</li><li><code>requireServiceAccount</code>:
whether the plugin requires the pod to have a service account.<ul><li>If set to <code>true</code>, kubelet will only invoke the plugin
if the pod has a service account.</li><li>If set to <code>false</code>, kubelet will invoke the plugin
even if the pod does not have a service account
and will not include a token in the <code>CredentialProviderRequest</code>.</li></ul></li></ul><p>This is useful for plugins that are used
to pull images for pods without service accounts
(e.g., static pods).</p><h4 id="configure-image-matching">Configure image matching</h4><p>The <code>matchImages</code> field for each credential provider is used by the kubelet to determine whether a plugin should be invoked
for a given image that a Pod is using. Each entry in <code>matchImages</code> is an image pattern which can optionally contain a port and a path.
Globs can be used in the domain, but not in the port or the path. Globs are supported as subdomains like <code>*.k8s.io</code> or <code>k8s.*.io</code>,
and top-level domains such as <code>k8s.*</code>. Matching partial subdomains like <code>app*.k8s.io</code> is also supported. Each glob can only match
a single subdomain segment, so <code>*.io</code> does NOT match <code>*.k8s.io</code>.</p><p>A match exists between an image name and a <code>matchImage</code> entry when all of the below are true:</p><ul><li>Both contain the same number of domain parts and each part matches.</li><li>The URL path of match image must be a prefix of the target image URL path.</li><li>If the matchImages contains a port, then the port must match in the image as well.</li></ul><p>Some example values of <code>matchImages</code> patterns are:</p><ul><li><code>123456789.dkr.ecr.us-east-1.amazonaws.com</code></li><li><code>*.azurecr.io</code></li><li><code>gcr.io</code></li><li><code>*.*.registry.io</code></li><li><code>foo.registry.io:8080/path</code></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read the details about <code>CredentialProviderConfig</code> in the
<a href="/docs/reference/config-api/kubelet-config.v1/">kubelet configuration API (v1) reference</a>.</li><li>Read the <a href="/docs/reference/config-api/kubelet-credentialprovider.v1/">kubelet credential provider API reference (v1)</a>.</li></ul></div></div><div><div class="td-content"><h1>Configure Quotas for API Objects</h1><p>This page shows how to configure quotas for API objects, including
PersistentVolumeClaims and Services. A quota restricts the number of
objects, of a particular type, that can be created in a namespace.
You specify quotas in a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#resourcequota-v1-core">ResourceQuota</a>
object.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace quota-object-example
</span></span></code></pre></div><h2 id="create-a-resourcequota">Create a ResourceQuota</h2><p>Here is the configuration file for a ResourceQuota object:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-objects.yaml"><code>admin/resource/quota-objects.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-objects.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-objects-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>object-quota-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>persistentvolumeclaims</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>    </span><span>services.loadbalancers</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>    </span><span>services.nodeports</span>:<span> </span><span>"0"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace<span>=</span>quota-object-example
</span></span></code></pre></div><p>View detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourcequota object-quota-demo --namespace<span>=</span>quota-object-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that in the quota-object-example namespace, there can be at most
one PersistentVolumeClaim, at most two Services of type LoadBalancer, and no Services
of type NodePort.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>persistentvolumeclaims</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>    </span><span>services.loadbalancers</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>    </span><span>services.nodeports</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>  </span><span>used</span>:<span>
</span></span></span><span><span><span>    </span><span>persistentvolumeclaims</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>    </span><span>services.loadbalancers</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>    </span><span>services.nodeports</span>:<span> </span><span>"0"</span><span>
</span></span></span></code></pre></div><h2 id="create-a-persistentvolumeclaim">Create a PersistentVolumeClaim</h2><p>Here is the configuration file for a PersistentVolumeClaim object:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-objects-pvc.yaml"><code>admin/resource/quota-objects-pvc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-objects-pvc.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-objects-pvc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pvc-quota-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>manual<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>3Gi<span>
</span></span></span></code></pre></div></div></div><p>Create the PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace<span>=</span>quota-object-example
</span></span></code></pre></div><p>Verify that the PersistentVolumeClaim was created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get persistentvolumeclaims --namespace<span>=</span>quota-object-example
</span></span></code></pre></div><p>The output shows that the PersistentVolumeClaim exists and has status Pending:</p><pre tabindex="0"><code>NAME             STATUS
pvc-quota-demo   Pending
</code></pre><h2 id="attempt-to-create-a-second-persistentvolumeclaim">Attempt to create a second PersistentVolumeClaim</h2><p>Here is the configuration file for a second PersistentVolumeClaim:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-objects-pvc-2.yaml"><code>admin/resource/quota-objects-pvc-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/resource/quota-objects-pvc-2.yaml to clipboard"></div><div class="includecode" id="admin-resource-quota-objects-pvc-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pvc-quota-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>manual<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>4Gi<span>
</span></span></span></code></pre></div></div></div><p>Attempt to create the second PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace<span>=</span>quota-object-example
</span></span></code></pre></div><p>The output shows that the second PersistentVolumeClaim was not created,
because it would have exceeded the quota for the namespace.</p><pre tabindex="0"><code>persistentvolumeclaims "pvc-quota-demo-2" is forbidden:
exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1,
used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1
</code></pre><h2 id="notes">Notes</h2><p>These are the strings used to identify API resources that can be constrained
by quotas:</p><table><tr><th>String</th><th>API Object</th></tr><tr><td>"pods"</td><td>Pod</td></tr><tr><td>"services"</td><td>Service</td></tr><tr><td>"replicationcontrollers"</td><td>ReplicationController</td></tr><tr><td>"resourcequotas"</td><td>ResourceQuota</td></tr><tr><td>"secrets"</td><td>Secret</td></tr><tr><td>"configmaps"</td><td>ConfigMap</td></tr><tr><td>"persistentvolumeclaims"</td><td>PersistentVolumeClaim</td></tr><tr><td>"services.nodeports"</td><td>Service of type NodePort</td></tr><tr><td>"services.loadbalancers"</td><td>Service of type LoadBalancer</td></tr></table><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace quota-object-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div></div><div><div class="td-content"><h1>Control CPU Management Policies on the Node</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Kubernetes keeps many aspects of how pods execute on nodes abstracted
from the user. This is by design. &#160;However, some workloads require
stronger guarantees in terms of latency and/or performance in order to operate
acceptably. The kubelet provides methods to enable more complex workload
placement policies while keeping the abstraction free from explicit placement
directives.</p><p>For detailed information on resource management, please refer to the
<a href="/docs/concepts/configuration/manage-resources-containers/">Resource Management for Pods and Containers</a>
documentation.</p><p>For detailed information on how the kubelet implements resource management, please refer to the
<a href="/docs/concepts/policy/node-resource-managers/">Node ResourceManagers</a> documentation.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.26.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>If you are running an older version of Kubernetes, please look at the documentation for the version you are actually running.</p><h2 id="configuring-cpu-management-policies">Configuring CPU management policies</h2><p>By default, the kubelet uses <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS quota</a>
to enforce pod CPU limits. &#160;When the node runs many CPU-bound pods,
the workload can move to different CPU cores depending on
whether the pod is throttled and which CPU cores are available at
scheduling time. Many workloads are not sensitive to this migration and thus
work fine without any intervention.</p><p>However, in workloads where CPU cache affinity and scheduling latency
significantly affect workload performance, the kubelet allows alternative CPU
management policies to determine some placement preferences on the node.</p><h2 id="windows-support">Windows Support</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>CPU Manager support can be enabled on Windows by using the <code>WindowsCPUAndMemoryAffinity</code> feature gate
and it requires support in the container runtime.
Once the feature gate is enabled, follow the steps below to configure the <a href="#configuration">CPU manager policy</a>.</p><h2 id="configuration">Configuration</h2><p>The CPU Manager policy is set with the <code>--cpu-manager-policy</code> kubelet
flag or the <code>cpuManagerPolicy</code> field in <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration</a>.
There are two supported policies:</p><ul><li><a href="#none-policy"><code>none</code></a>: the default policy.</li><li><a href="#static-policy"><code>static</code></a>: allows pods with certain resource characteristics to be
granted increased CPU affinity and exclusivity on the node.</li></ul><p>The CPU manager periodically writes resource updates through the CRI in
order to reconcile in-memory CPU assignments with cgroupfs. The reconcile
frequency is set through a new Kubelet configuration value
<code>--cpu-manager-reconcile-period</code>. If not specified, it defaults to the same
duration as <code>--node-status-update-frequency</code>.</p><p>The behavior of the static policy can be fine-tuned using the <code>--cpu-manager-policy-options</code> flag.
The flag takes a comma-separated list of <code>key=value</code> policy options.
If you disable the <code>CPUManagerPolicyOptions</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
then you cannot fine-tune CPU manager policies. In that case, the CPU manager
operates only using its default settings.</p><p>In addition to the top-level <code>CPUManagerPolicyOptions</code> feature gate, the policy options are split
into two groups: alpha quality (hidden by default) and beta quality (visible by default).
The groups are guarded respectively by the <code>CPUManagerPolicyAlphaOptions</code>
and <code>CPUManagerPolicyBetaOptions</code> feature gates. Diverging from the Kubernetes standard, these
feature gates guard groups of options, because it would have been too cumbersome to add a feature
gate for each individual option.</p><h2 id="changing-the-cpu-manager-policy">Changing the CPU Manager Policy</h2><p>Since the CPU manager policy can only be applied when kubelet spawns new pods, simply changing from
"none" to "static" won't apply to existing pods. So in order to properly change the CPU manager
policy on a node, perform the following steps:</p><ol><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Drain</a> the node.</li><li>Stop kubelet.</li><li>Remove the old CPU manager state file. The path to this file is
<code>/var/lib/kubelet/cpu_manager_state</code> by default. This clears the state maintained by the
CPUManager so that the cpu-sets set up by the new policy won&#8217;t conflict with it.</li><li>Edit the kubelet configuration to change the CPU manager policy to the desired value.</li><li>Start kubelet.</li></ol><p>Repeat this process for every node that needs its CPU manager policy changed. Skipping this
process will result in kubelet crashlooping with the following error:</p><pre tabindex="0"><code>could not restore state from checkpoint: configured policy "static" differs from state checkpoint policy "none", please drain this node and delete the CPU manager checkpoint file "/var/lib/kubelet/cpu_manager_state" before restarting Kubelet
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>if the set of online CPUs changes on the node, the node must be drained and CPU manager manually reset by deleting the
state file <code>cpu_manager_state</code> in the kubelet root directory.</div><h3 id="none-policy-configuration"><code>none</code> policy configuration</h3><p>This policy has no extra configuration items.</p><h3 id="static-policy-configuration"><code>static</code> policy configuration</h3><p>This policy manages a shared pool of CPUs that initially contains all CPUs in the
node. The amount of exclusively allocatable CPUs is equal to the total
number of CPUs in the node minus any CPU reservations by the kubelet <code>--kube-reserved</code> or
<code>--system-reserved</code> options. From 1.17, the CPU reservation list can be specified
explicitly by kubelet <code>--reserved-cpus</code> option. The explicit CPU list specified by
<code>--reserved-cpus</code> takes precedence over the CPU reservation specified by
<code>--kube-reserved</code> and <code>--system-reserved</code>. CPUs reserved by these options are taken, in
integer quantity, from the initial shared pool in ascending order by physical
core ID. &#160;This shared pool is the set of CPUs on which any containers in
<code>BestEffort</code> and <code>Burstable</code> pods run. Containers in <code>Guaranteed</code> pods with fractional
CPU <code>requests</code> also run on CPUs in the shared pool. Only containers that are
both part of a <code>Guaranteed</code> pod and have integer CPU <code>requests</code> are assigned
exclusive CPUs.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet requires a CPU reservation greater than zero be made
using either <code>--kube-reserved</code> and/or <code>--system-reserved</code> or <code>--reserved-cpus</code> when
the static policy is enabled. This is because zero CPU reservation would allow the shared
pool to become empty.</div><h3 id="cpu-policy-static--options">Static policy options</h3><p>You can toggle groups of options on and off based upon their maturity level
using the following feature gates:</p><ul><li><code>CPUManagerPolicyBetaOptions</code> default enabled. Disable to hide beta-level options.</li><li><code>CPUManagerPolicyAlphaOptions</code> default disabled. Enable to show alpha-level options.
You will still have to enable each option using the <code>CPUManagerPolicyOptions</code> kubelet option.</li></ul><p>The following policy options exist for the static <code>CPUManager</code> policy:</p><ul><li><code>full-pcpus-only</code> (GA, visible by default) (1.33 or higher)</li><li><code>distribute-cpus-across-numa</code> (beta, visible by default) (1.33 or higher)</li><li><code>align-by-socket</code> (alpha, hidden by default) (1.25 or higher)</li><li><code>distribute-cpus-across-cores</code> (alpha, hidden by default) (1.31 or higher)</li><li><code>strict-cpu-reservation</code> (beta, visible by default) (1.32 or higher)</li><li><code>prefer-align-cpus-by-uncorecache</code> (beta, visible by default) (1.34 or higher)</li></ul><p>The <code>full-pcpus-only</code> option can be enabled by adding <code>full-pcpus-only=true</code> to
the CPUManager policy options.
Likewise, the <code>distribute-cpus-across-numa</code> option can be enabled by adding
<code>distribute-cpus-across-numa=true</code> to the CPUManager policy options.
When both are set, they are "additive" in the sense that CPUs will be
distributed across NUMA nodes in chunks of full-pcpus rather than individual
cores.
The <code>align-by-socket</code> policy option can be enabled by adding <code>align-by-socket=true</code>
to the <code>CPUManager</code> policy options. It is also additive to the <code>full-pcpus-only</code>
and <code>distribute-cpus-across-numa</code> policy options.</p><p>The <code>distribute-cpus-across-cores</code> option can be enabled by adding
<code>distribute-cpus-across-cores=true</code> to the <code>CPUManager</code> policy options.
It cannot be used with <code>full-pcpus-only</code> or <code>distribute-cpus-across-numa</code> policy
options together at this moment.</p><p>The <code>strict-cpu-reservation</code> option can be enabled by adding <code>strict-cpu-reservation=true</code> to
the CPUManager policy options followed by removing the <code>/var/lib/kubelet/cpu_manager_state</code> file and restart kubelet.</p><p>The <code>prefer-align-cpus-by-uncorecache</code> option can be enabled by adding the
<code>prefer-align-cpus-by-uncorecache</code> to the <code>CPUManager</code> policy options. If
incompatible options are used, the kubelet will fail to start with the error
explained in the logs.</p><p>For mode detail about the behavior of the individual options you can configure, please refer to the
<a href="/docs/concepts/policy/node-resource-managers/">Node ResourceManagers</a> documentation.</p></div></div><div><div class="td-content"><h1>Control Topology Management Policies on a node</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>An increasing number of systems leverage a combination of CPUs and hardware accelerators to
support latency-critical execution and high-throughput parallel computation. These include
workloads in fields such as telecommunications, scientific computing, machine learning, financial
services and data analytics. Such hybrid systems comprise a high performance environment.</p><p>In order to extract the best performance, optimizations related to CPU isolation, memory and
device locality are required. However, in Kubernetes, these optimizations are handled by a
disjoint set of components.</p><p><em>Topology Manager</em> is a kubelet component that aims to coordinate the set of components that are
responsible for these optimizations.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.18.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="how-topology-manager-works">How topology manager works</h2><p>Prior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make
resource allocation decisions independently of each other. This can result in undesirable
allocations on multiple-socketed systems, and performance/latency sensitive applications will suffer
due to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and
devices being allocated from different NUMA Nodes, thus incurring additional latency.</p><p>The Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet
components can make topology aligned resource allocation choices.</p><p>The Topology Manager provides an interface for components, called <em>Hint Providers</em>, to send and
receive topology information. The Topology Manager has a set of node level policies which are
explained below.</p><p>The Topology Manager receives topology information from the <em>Hint Providers</em> as a bitmask denoting
NUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform
a set of operations on the hints provided and converge on the hint determined by the policy to
give the optimal result. If an undesirable hint is stored, the preferred field for the hint will be
set to false. In the current policies preferred is the narrowest preferred mask.
The selected hint is stored as part of the Topology Manager. Depending on the policy configured,
the pod can be accepted or rejected from the node based on the selected hint.
The hint is then stored in the Topology Manager for use by the <em>Hint Providers</em> when making the
resource allocation decisions.</p><p>The flow can be seen in the following diagram.</p><p><img alt="topology_manager_flow" src="/images/docs/topology-manager-flow.png"></p><h2 id="windows-support">Windows Support</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>The Topology Manager support can be enabled on Windows by using the <code>WindowsCPUAndMemoryAffinity</code> feature gate and
it requires support in the container runtime.</p><h2 id="topology-manager-scopes-and-policies">Topology manager scopes and policies</h2><p>The Topology Manager currently:</p><ul><li>aligns Pods of all QoS classes.</li><li>aligns the requested resources that Hint Provider provides topology hints for.</li></ul><p>If these conditions are met, the Topology Manager will align the requested resources.</p><p>In order to customize how this alignment is carried out, the Topology Manager provides two
distinct options: <code>scope</code> and <code>policy</code>.</p><p>The <code>scope</code> defines the granularity at which you would like resource alignment to be performed,
for example, at the <code>pod</code> or <code>container</code> level. And the <code>policy</code> defines the actual policy used to
carry out the alignment, for example, <code>best-effort</code>, <code>restricted</code>, and <code>single-numa-node</code>.
Details on the various <code>scopes</code> and <code>policies</code> available today can be found below.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be
enabled and proper CPU Manager policy should be configured on a Node.
See <a href="/docs/tasks/administer-cluster/cpu-management-policies/">Control CPU Management Policies on the Node</a>.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory
Manager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to
<a href="/docs/tasks/administer-cluster/memory-manager/">Memory Manager</a> documentation.</div><h2 id="topology-manager-scopes">Topology manager scopes</h2><p>The Topology Manager can deal with the alignment of resources in a couple of distinct scopes:</p><ul><li><code>container</code> (default)</li><li><code>pod</code></li></ul><p>Either option can be selected at a time of the kubelet startup, by setting the
<code>topologyManagerScope</code> in the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><h3 id="container-scope"><code>container</code> scope</h3><p>The <code>container</code> scope is used by default. You can also explicitly set the
<code>topologyManagerScope</code> to <code>container</code> in the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><p>Within this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,
for each container (in a pod) a separate alignment is computed. In other words, there is no notion
of grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,
the Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.</p><p>The notion of grouping the containers was endorsed and implemented on purpose in the following
scope, for example the <code>pod</code> scope.</p><h3 id="pod-scope"><code>pod</code> scope</h3><p>To select the <code>pod</code> scope, set <code>topologyManagerScope</code> in the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a> to <code>pod</code>.</p><p>This scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the
Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)
to either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the
alignments produced by the Topology Manager on different occasions:</p><ul><li>all containers can be and are allocated to a single NUMA node;</li><li>all containers can be and are allocated to a shared set of NUMA nodes.</li></ul><p>The total amount of particular resource demanded for the entire pod is calculated according to
<a href="/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers">effective requests/limits</a>
formula, and thus, this total value is equal to the maximum of:</p><ul><li>the sum of all app container requests,</li><li>the maximum of init container requests,</li></ul><p>for a resource.</p><p>Using the <code>pod</code> scope in tandem with <code>single-numa-node</code> Topology Manager policy is specifically
valuable for workloads that are latency sensitive or for high-throughput applications that perform
IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA
node; hence, the inter-NUMA communication overhead can be eliminated for that pod.</p><p>In the case of <code>single-numa-node</code> policy, a pod is accepted only if a suitable set of NUMA nodes
is present among possible allocations. Reconsider the example above:</p><ul><li>a set containing only a single NUMA node - it leads to pod being admitted,</li><li>whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one
NUMA node, two or more NUMA nodes are required to satisfy the allocation).</li></ul><p>To recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology
Manager policy, which either leads to the rejection or admission of the pod.</p><h2 id="topology-manager-policies">Topology manager policies</h2><p>The Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,
<code>--topology-manager-policy</code>. There are four supported policies:</p><ul><li><code>none</code> (default)</li><li><code>best-effort</code></li><li><code>restricted</code></li><li><code>single-numa-node</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If the Topology Manager is configured with the <strong>pod</strong> scope, the container, which is considered by
the policy, is reflecting requirements of the entire pod, and thus each container from the pod
will result with <strong>the same</strong> topology alignment decision.</div><h3 id="policy-none"><code>none</code> policy</h3><p>This is the default policy and does not perform any topology alignment.</p><h3 id="policy-best-effort"><code>best-effort</code> policy</h3><p>For each container in a Pod, the kubelet, with <code>best-effort</code> topology management policy, calls
each Hint Provider to discover their resource availability. Using this information, the Topology
Manager stores the preferred NUMA Node affinity for that container. If the affinity is not
preferred, the Topology Manager will store this and admit the pod to the node anyway.</p><p>The <em>Hint Providers</em> can then use this information when making the
resource allocation decision.</p><h3 id="policy-restricted"><code>restricted</code> policy</h3><p>For each container in a Pod, the kubelet, with <code>restricted</code> topology management policy, calls each
Hint Provider to discover their resource availability. Using this information, the Topology
Manager stores the preferred NUMA Node affinity for that container. If the affinity is not
preferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a
<code>Terminated</code> state with a pod admission failure.</p><p>Once the pod is in a <code>Terminated</code> state, the Kubernetes scheduler will <strong>not</strong> attempt to
reschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of
the pod. An external control loop could be also implemented to trigger a redeployment of pods that
have the <code>Topology Affinity</code> error.</p><p>If the pod is admitted, the <em>Hint Providers</em> can then use this information when making the
resource allocation decision.</p><h3 id="policy-single-numa-node"><code>single-numa-node</code> policy</h3><p>For each container in a Pod, the kubelet, with <code>single-numa-node</code> topology management policy,
calls each Hint Provider to discover their resource availability. Using this information, the
Topology Manager determines if a single NUMA Node affinity is possible. If it is, Topology
Manager will store this and the <em>Hint Providers</em> can then use this information when making the
resource allocation decision. If, however, this is not possible then the Topology Manager will
reject the pod from the node. This will result in a pod in a <code>Terminated</code> state with a pod
admission failure.</p><p>Once the pod is in a <code>Terminated</code> state, the Kubernetes scheduler will <strong>not</strong> attempt to
reschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of
the Pod. An external control loop could be also implemented to trigger a redeployment of pods
that have the <code>Topology Affinity</code> error.</p><h2 id="topology-manager-policy-options">Topology manager policy options</h2><p>Support for the Topology Manager policy options requires <code>TopologyManagerPolicyOptions</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> to be enabled
(it is enabled by default).</p><p>You can toggle groups of options on and off based upon their maturity level using the following feature gates:</p><ul><li><code>TopologyManagerPolicyBetaOptions</code> default enabled. Enable to show beta-level options.</li><li><code>TopologyManagerPolicyAlphaOptions</code> default disabled. Enable to show alpha-level options.</li></ul><p>You will still have to enable each option using the <code>TopologyManagerPolicyOptions</code> kubelet option.</p><h3 id="policy-option-prefer-closest-numa-nodes"><code>prefer-closest-numa-nodes</code></h3><p>The <code>prefer-closest-numa-nodes</code> option is GA since Kubernetes 1.32. In Kubernetes 1.34
this policy option is visible by default provided that the <code>TopologyManagerPolicyOptions</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled.</p><p>The Topology Manager is not aware by default of NUMA distances, and does not take them into account when making
Pod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,
and can cause significant performance degradation in latency-critical execution and high-throughput applications
if the Topology Manager decides to align resources on non-adjacent NUMA nodes.</p><p>If you specify the <code>prefer-closest-numa-nodes</code> policy option, the <code>best-effort</code> and <code>restricted</code>
policies favor sets of NUMA nodes with shorter distance between them when making admission decisions.</p><p>You can enable this option by adding <code>prefer-closest-numa-nodes=true</code> to the Topology Manager policy options.</p><p>By default (without this option), the Topology Manager aligns resources on either a single NUMA node or,
in the case where more than one NUMA node is required, using the minimum number of NUMA nodes.</p><h3 id="policy-option-max-allowable-numa-nodes"><code>max-allowable-numa-nodes</code> (beta)</h3><p>The <code>max-allowable-numa-nodes</code> option is beta since Kubernetes 1.31. In Kubernetes 1.34,
this policy option is visible by default provided that the <code>TopologyManagerPolicyOptions</code> and
<code>TopologyManagerPolicyBetaOptions</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>
are enabled.</p><p>The time to admit a pod is tied to the number of NUMA nodes on the physical machine.
By default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where
more than 8 NUMA nodes are detected.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you select the <code>max-allowable-numa-nodes</code> policy option, nodes with more than 8 NUMA nodes can
be allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact
of using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that
lack of data, using this policy option with Kubernetes 1.34 is <strong>not</strong> recommended and is
at your own risk.</div><p>You can enable this option by adding <code>max-allowable-numa-nodes=true</code> to the Topology Manager policy options.</p><p>Setting a value of <code>max-allowable-numa-nodes</code> does not (in and of itself) affect the
latency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.
Future, potential improvements to Kubernetes may improve Pod admission performance and the high
latency that happens as the number of NUMA nodes increases.</p><h2 id="pod-interactions-with-topology-manager-policies">Pod interactions with topology manager policies</h2><p>Consider the containers in the following Pod manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because no resource <code>requests</code> or <code>limits</code> are specified.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span></code></pre></div><p>This pod runs in the <code>Burstable</code> QoS class because requests are less than limits.</p><p>If the selected policy is anything other than <code>none</code>, the Topology Manager would consider these Pod
specifications. The Topology Manager would consult the Hint Providers to get topology hints.
In the case of the <code>static</code>, the CPU Manager policy would return default topology hint, because
these Pods do not explicitly request CPU resources.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>This pod with integer CPU request runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal
to <code>limits</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"300m"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"300m"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>This pod with sharing CPU request runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal
to <code>limits</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/deviceA</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/deviceB</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/deviceA</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/deviceB</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because there are no CPU and memory requests.</p><p>The Topology Manager would consider the above pods. The Topology Manager would consult the Hint
Providers, which are CPU and Device Manager to get topology hints for the pods.</p><p>In the case of the <code>Guaranteed</code> pod with integer CPU request, the <code>static</code> CPU Manager policy
would return topology hints relating to the exclusive CPU and the Device Manager would send back
hints for the requested device.</p><p>In the case of the <code>Guaranteed</code> pod with sharing CPU request, the <code>static</code> CPU Manager policy
would return default topology hint as there is no exclusive CPU request and the Device Manager
would send back hints for the requested device.</p><p>In the above two cases of the <code>Guaranteed</code> pod, the <code>none</code> CPU Manager policy would return default
topology hint.</p><p>In the case of the <code>BestEffort</code> pod, the <code>static</code> CPU Manager policy would send back the default
topology hint as there is no CPU request and the Device Manager would send back the hints for each
of the requested devices.</p><p>Using this information the Topology Manager calculates the optimal hint for the pod and stores
this information, which will be used by the Hint Providers when they are making their resource
assignments.</p><h2 id="known-limitations">Known limitations</h2><ol><li><p>The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes,
there will be a state explosion when trying to enumerate the possible NUMA affinities and
generating their hints. See <a href="#policy-option-max-allowable-numa-nodes"><code>max-allowable-numa-nodes</code></a>
(beta) for more options.</p></li><li><p>The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail
on the node due to the Topology Manager.</p></li></ol></div></div><div><div class="td-content"><h1>Customizing DNS Service</h1><p>This page explains how to configure your DNS
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod(s)</a> and customize the
DNS resolution process in your cluster.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your cluster must be running the CoreDNS add-on.</p><p>Your Kubernetes server must be at or later than version v1.12.</p><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="introduction">Introduction</h2><p>DNS is a built-in Kubernetes service launched automatically
using the <em>addon manager</em> <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/addon-manager/README.md">cluster add-on</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The CoreDNS Service is named <code>kube-dns</code> in the <code>metadata.name</code> field.<br>The intent is to ensure greater interoperability with workloads that relied on
the legacy <code>kube-dns</code> Service name to resolve addresses internal to the cluster.
Using a Service named <code>kube-dns</code> abstracts away the implementation detail of
which DNS provider is running behind that common name.</div><p>If you are running CoreDNS as a Deployment, it will typically be exposed as
a Kubernetes Service with a static IP address.
The kubelet passes DNS resolver information to each container with the
<code>--cluster-dns=&lt;dns-service-ip&gt;</code> flag.</p><p>DNS names also need domains. You configure the local domain in the kubelet
with the flag <code>--cluster-domain=&lt;default-local-domain&gt;</code>.</p><p>The DNS server supports forward lookups (A and AAAA records), port lookups (SRV records),
reverse IP address lookups (PTR records), and more. For more information, see
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a>.</p><p>If a Pod's <code>dnsPolicy</code> is set to <code>default</code>, it inherits the name resolution
configuration from the node that the Pod runs on. The Pod's DNS resolution
should behave the same as the node.
But see <a href="/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues">Known issues</a>.</p><p>If you don't want this, or if you want a different DNS config for pods, you can
use the kubelet's <code>--resolv-conf</code> flag. Set this flag to "" to prevent Pods from
inheriting DNS. Set it to a valid file path to specify a file other than
<code>/etc/resolv.conf</code> for DNS inheritance.</p><h2 id="coredns">CoreDNS</h2><p>CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS,
complying with the <a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS specifications</a>.</p><h3 id="coredns-configmap-options">CoreDNS ConfigMap options</h3><p>CoreDNS is a DNS server that is modular and pluggable, with plugins adding new functionalities.
The CoreDNS server can be configured by maintaining a <a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile</a>,
which is the CoreDNS configuration file. As a cluster administrator, you can modify the
<a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a> for the CoreDNS Corefile to
change how DNS service discovery behaves for that cluster.</p><p>In Kubernetes, CoreDNS is installed with the following default Corefile configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>coredns<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>Corefile</span>:<span> </span>|<span>
</span></span></span><span><span><span>    .:53 {
</span></span></span><span><span><span>        errors
</span></span></span><span><span><span>        health {
</span></span></span><span><span><span>            lameduck 5s
</span></span></span><span><span><span>        }
</span></span></span><span><span><span>        ready
</span></span></span><span><span><span>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span><span><span>            pods insecure
</span></span></span><span><span><span>            fallthrough in-addr.arpa ip6.arpa
</span></span></span><span><span><span>            ttl 30
</span></span></span><span><span><span>        }
</span></span></span><span><span><span>        prometheus :9153
</span></span></span><span><span><span>        forward . /etc/resolv.conf
</span></span></span><span><span><span>        cache 30
</span></span></span><span><span><span>        loop
</span></span></span><span><span><span>        reload
</span></span></span><span><span><span>        loadbalance
</span></span></span><span><span><span>    }</span><span>    
</span></span></span></code></pre></div><p>The Corefile configuration includes the following <a href="https://coredns.io/plugins/">plugins</a> of CoreDNS:</p><ul><li><a href="https://coredns.io/plugins/errors/">errors</a>: Errors are logged to stdout.</li><li><a href="https://coredns.io/plugins/health/">health</a>: Health of CoreDNS is reported to
<code>http://localhost:8080/health</code>. In this extended syntax <code>lameduck</code> will make the process
unhealthy then wait for 5 seconds before the process is shut down.</li><li><a href="https://coredns.io/plugins/ready/">ready</a>: An HTTP endpoint on port 8181 will return 200 OK,
when all plugins that are able to signal readiness have done so.</li><li><a href="https://coredns.io/plugins/kubernetes/">kubernetes</a>: CoreDNS will reply to DNS queries
based on IP of the Services and Pods. You can find <a href="https://coredns.io/plugins/kubernetes/">more details</a>
about this plugin on the CoreDNS website.<ul><li><code>ttl</code> allows you to set a custom TTL for responses. The default is 5 seconds.
The minimum TTL allowed is 0 seconds, and the maximum is capped at 3600 seconds.
Setting TTL to 0 will prevent records from being cached.</li><li>The <code>pods insecure</code> option is provided for backward compatibility with <code>kube-dns</code>.</li><li>You can use the <code>pods verified</code> option, which returns an A record only if there exists a pod
in the same namespace with a matching IP.</li><li>The <code>pods disabled</code> option can be used if you don't use pod records.</li></ul></li><li><a href="https://coredns.io/plugins/metrics/">prometheus</a>: Metrics of CoreDNS are available at
<code>http://localhost:9153/metrics</code> in the <a href="https://prometheus.io/">Prometheus</a> format
(also known as OpenMetrics).</li><li><a href="https://coredns.io/plugins/forward/">forward</a>: Any queries that are not within the Kubernetes
cluster domain are forwarded to predefined resolvers (/etc/resolv.conf).</li><li><a href="https://coredns.io/plugins/cache/">cache</a>: This enables a frontend cache.</li><li><a href="https://coredns.io/plugins/loop/">loop</a>: Detects simple forwarding loops and
halts the CoreDNS process if a loop is found.</li><li><a href="https://coredns.io/plugins/reload">reload</a>: Allows automatic reload of a changed Corefile.
After you edit the ConfigMap configuration, allow two minutes for your changes to take effect.</li><li><a href="https://coredns.io/plugins/loadbalance">loadbalance</a>: This is a round-robin DNS loadbalancer
that randomizes the order of A, AAAA, and MX records in the answer.</li></ul><p>You can modify the default CoreDNS behavior by modifying the ConfigMap.</p><h3 id="configuration-of-stub-domain-and-upstream-nameserver-using-coredns">Configuration of Stub-domain and upstream nameserver using CoreDNS</h3><p>CoreDNS has the ability to configure stub-domains and upstream nameservers
using the <a href="https://coredns.io/plugins/forward/">forward plugin</a>.</p><h4 id="example">Example</h4><p>If a cluster operator has a <a href="https://www.consul.io/">Consul</a> domain server located at "10.150.0.1",
and all Consul names have the suffix ".consul.local". To configure it in CoreDNS,
the cluster administrator creates the following stanza in the CoreDNS ConfigMap.</p><pre tabindex="0"><code>consul.local:53 {
    errors
    cache 30
    forward . 10.150.0.1
}
</code></pre><p>To explicitly force all non-cluster DNS lookups to go through a specific nameserver at 172.16.0.1,
point the <code>forward</code> to the nameserver instead of <code>/etc/resolv.conf</code></p><pre tabindex="0"><code>forward .  172.16.0.1
</code></pre><p>The final ConfigMap along with the default <code>Corefile</code> configuration looks like:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>coredns<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>Corefile</span>:<span> </span>|<span>
</span></span></span><span><span><span>    .:53 {
</span></span></span><span><span><span>        errors
</span></span></span><span><span><span>        health
</span></span></span><span><span><span>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span><span><span>           pods insecure
</span></span></span><span><span><span>           fallthrough in-addr.arpa ip6.arpa
</span></span></span><span><span><span>        }
</span></span></span><span><span><span>        prometheus :9153
</span></span></span><span><span><span>        forward . 172.16.0.1
</span></span></span><span><span><span>        cache 30
</span></span></span><span><span><span>        loop
</span></span></span><span><span><span>        reload
</span></span></span><span><span><span>        loadbalance
</span></span></span><span><span><span>    }
</span></span></span><span><span><span>    consul.local:53 {
</span></span></span><span><span><span>        errors
</span></span></span><span><span><span>        cache 30
</span></span></span><span><span><span>        forward . 10.150.0.1
</span></span></span><span><span><span>    }</span><span>    
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>CoreDNS does not support FQDNs for stub-domains and nameservers (eg: "ns.foo.com").
During translation, all FQDN nameservers will be omitted from the CoreDNS config.</div><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/docs/tasks/administer-cluster/dns-debugging-resolution/">Debugging DNS Resolution</a></li></ul></div></div><div><div class="td-content"><h1>Debugging DNS Resolution</h1><p>This page provides hints on diagnosing DNS problems.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><br>Your cluster must be configured to use the CoreDNS
<a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." href="/docs/concepts/cluster-administration/addons/" target="_blank">addon</a> or its precursor,
kube-dns.</p><p>Your Kubernetes server must be at or later than version v1.6.</p><p>To check the version, enter <code>kubectl version</code>.</p><h3 id="create-a-simple-pod-to-use-as-a-test-environment">Create a simple Pod to use as a test environment</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/dns/dnsutils.yaml"><code>admin/dns/dnsutils.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/dns/dnsutils.yaml to clipboard"></div><div class="includecode" id="admin-dns-dnsutils-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dnsutils<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>dnsutils<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/e2e-test-images/agnhost:2.39<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Always<span>
</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This example creates a pod in the <code>default</code> namespace. DNS name resolution for
services depends on the namespace of the pod. For more information, review
<a href="/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">DNS for Services and Pods</a>.</div><p>Use that manifest to create a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
</span></span></code></pre></div><pre tabindex="0"><code>pod/dnsutils created
</code></pre><p>&#8230;and verify its status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods dnsutils
</span></span></code></pre></div><pre tabindex="0"><code>NAME       READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          &lt;some-time&gt;
</code></pre><p>Once that Pod is running, you can exec <code>nslookup</code> in that environment.
If you see something like the following, DNS is working correctly.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
</code></pre><p>If the <code>nslookup</code> command fails, check the following:</p><h3 id="check-the-local-dns-configuration-first">Check the local DNS configuration first</h3><p>Take a look inside the resolv.conf file.
(See <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a> and
<a href="#known-issues">Known issues</a> below for more information)</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -ti dnsutils -- cat /etc/resolv.conf
</span></span></code></pre></div><p>Verify that the search path and name server are set up like the following
(note that search path may vary for different cloud providers):</p><pre tabindex="0"><code>search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
</code></pre><p>Errors such as the following indicate a problem with the CoreDNS (or kube-dns)
add-on or with associated Services:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
</code></pre><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
</code></pre><h3 id="check-if-the-dns-pod-is-running">Check if the DNS pod is running</h3><p>Use the <code>kubectl get pods</code> command to verify that the DNS pod is running.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --namespace<span>=</span>kube-system -l k8s-app<span>=</span>kube-dns
</span></span></code></pre></div><pre tabindex="0"><code>NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The value for label <code>k8s-app</code> is <code>kube-dns</code> for both CoreDNS and kube-dns deployments.</div><p>If you see that no CoreDNS Pod is running or that the Pod has failed/completed,
the DNS add-on may not be deployed by default in your current environment and you
will have to deploy it manually.</p><h3 id="check-for-errors-in-the-dns-pod">Check for errors in the DNS pod</h3><p>Use the <code>kubectl logs</code> command to see logs for the DNS containers.</p><p>For CoreDNS:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs --namespace<span>=</span>kube-system -l k8s-app<span>=</span>kube-dns
</span></span></code></pre></div><p>Here is an example of a healthy CoreDNS log:</p><pre tabindex="0"><code>.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
</code></pre><p>See if there are any suspicious or unexpected messages in the logs.</p><h3 id="is-dns-service-up">Is DNS service up?</h3><p>Verify that the DNS service is up by using the <code>kubectl get service</code> command.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc --namespace<span>=</span>kube-system
</span></span></code></pre></div><pre tabindex="0"><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h
...
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The service name is <code>kube-dns</code> for both CoreDNS and kube-dns deployments.</div><p>If you have created the Service or in the case it should be created by default
but it does not appear, see
<a href="/docs/tasks/debug/debug-application/debug-service/">debugging Services</a> for
more information.</p><h3 id="are-dns-endpoints-exposed">Are DNS endpoints exposed?</h3><p>You can verify that DNS endpoints are exposed by using the <code>kubectl get endpointslice</code>
command.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get endpointslice -l k8s.io/service-name<span>=</span>kube-dns --namespace<span>=</span>kube-system
</span></span></code></pre></div><pre tabindex="0"><code>NAME             ADDRESSTYPE   PORTS   ENDPOINTS                  AGE
kube-dns-zxoja   IPv4          53      10.180.3.17,10.180.3.17    1h
</code></pre><p>If you do not see the endpoints, see the endpoints section in the
<a href="/docs/tasks/debug/debug-application/debug-service/">debugging Services</a> documentation.</p><p>For additional Kubernetes DNS examples, see the
<a href="https://github.com/kubernetes/examples/tree/master/staging/cluster-dns">cluster-dns examples</a>
in the Kubernetes GitHub repository.</p><h3 id="are-dns-queries-being-received-processed">Are DNS queries being received/processed?</h3><p>You can verify if queries are being received by CoreDNS by adding the <code>log</code> plugin to the CoreDNS configuration (aka Corefile).
The CoreDNS Corefile is held in a <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a> named <code>coredns</code>. To edit it, use the command:</p><pre tabindex="0"><code>kubectl -n kube-system edit configmap coredns
</code></pre><p>Then add <code>log</code> in the Corefile section per the example below:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>coredns<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>Corefile</span>:<span> </span>|<span>
</span></span></span><span><span><span>    .:53 {
</span></span></span><span><span><span>        log
</span></span></span><span><span><span>        errors
</span></span></span><span><span><span>        health
</span></span></span><span><span><span>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span><span><span>          pods insecure
</span></span></span><span><span><span>          upstream
</span></span></span><span><span><span>          fallthrough in-addr.arpa ip6.arpa
</span></span></span><span><span><span>        }
</span></span></span><span><span><span>        prometheus :9153
</span></span></span><span><span><span>        forward . /etc/resolv.conf
</span></span></span><span><span><span>        cache 30
</span></span></span><span><span><span>        loop
</span></span></span><span><span><span>        reload
</span></span></span><span><span><span>        loadbalance
</span></span></span><span><span><span>    }</span><span>    
</span></span></span></code></pre></div><p>After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.</p><p>Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.</p><p>Here is an example of a query in the log:</p><pre tabindex="0"><code>.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 "A IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd,ra 106 0.000066649s
</code></pre><h3 id="does-coredns-have-sufficient-permissions">Does CoreDNS have sufficient permissions?</h3><p>CoreDNS must be able to list <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">service</a> and <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." href="/docs/concepts/services-networking/endpoint-slices/" target="_blank">endpointslice</a> related resources to properly resolve service names.</p><p>Sample error message:</p><pre tabindex="0"><code>2022-03-18T07:12:15.699431183Z [INFO] 10.96.144.227:52299 - 3686 "A IN serverproxy.contoso.net.cluster.local. udp 52 false 512" SERVFAIL qr,aa,rd 145 0.000091221s
</code></pre><p>First, get the current ClusterRole of <code>system:coredns</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe clusterrole system:coredns -n kube-system
</span></span></code></pre></div><p>Expected output:</p><pre tabindex="0"><code>PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  endpoints                        []                 []              [list watch]
  namespaces                       []                 []              [list watch]
  pods                             []                 []              [list watch]
  services                         []                 []              [list watch]
  endpointslices.discovery.k8s.io  []                 []              [list watch]
</code></pre><p>If any permissions are missing, edit the ClusterRole to add them:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit clusterrole system:coredns -n kube-system
</span></span></code></pre></div><p>Example insertion of EndpointSlices permissions:</p><pre tabindex="0"><code>...
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
...
</code></pre><h3 id="are-you-in-the-right-namespace-for-the-service">Are you in the right namespace for the service?</h3><p>DNS queries that don't specify a namespace are limited to the pod's
namespace.</p><p>If the namespace of the pod and service differ, the DNS query must include
the namespace of the service.</p><p>This query is limited to the pod's namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;
</span></span></code></pre></div><p>This query specifies the namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;.&lt;namespace&gt;
</span></span></code></pre></div><p>To learn more about name resolution, see
<a href="/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">DNS for Services and Pods</a>.</p><h2 id="known-issues">Known issues</h2><p>Some Linux distributions (e.g. Ubuntu) use a local DNS resolver by default (systemd-resolved).
Systemd-resolved moves and replaces <code>/etc/resolv.conf</code> with a stub file that can cause a fatal forwarding
loop when resolving names in upstream servers. This can be fixed manually by using kubelet's <code>--resolv-conf</code> flag
to point to the correct <code>resolv.conf</code> (With <code>systemd-resolved</code>, this is <code>/run/systemd/resolve/resolv.conf</code>).
kubeadm automatically detects <code>systemd-resolved</code>, and adjusts the kubelet flags accordingly.</p><p>Kubernetes installs do not configure the nodes' <code>resolv.conf</code> files to use the
cluster DNS by default, because that process is inherently distribution-specific.
This should probably be implemented eventually.</p><p>Linux's libc (a.k.a. glibc) has a limit for the DNS <code>nameserver</code> records to 3 by
default and Kubernetes needs to consume 1 <code>nameserver</code> record. This means that
if a local installation already uses 3 <code>nameserver</code>s, some of those entries will
be lost. To work around this limit, the node can run <code>dnsmasq</code>, which will
provide more <code>nameserver</code> entries. You can also use kubelet's <code>--resolv-conf</code>
flag.</p><p>If you are using Alpine version 3.17 or earlier as your base image, DNS may not
work properly due to a design issue with Alpine.
Until musl version 1.24 didn't include TCP fallback to the DNS stub resolver meaning any DNS call above 512 bytes would fail.
Please upgrade your images to Alpine version 3.18 or above.</p><h2 id="what-s-next">What's next</h2><ul><li>See <a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscaling the DNS Service in a Cluster</a>.</li><li>Read <a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a></li></ul></div></div><div><div class="td-content"><h1>Declare Network Policy</h1><p>This document helps you get started using the Kubernetes <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy API</a> to declare network policies that govern how pods communicate with each other.</p><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.8.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>Make sure you've configured a network provider with network policy support. There are a number of network providers that support NetworkPolicy, including:</p><ul><li><a href="/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/">Antrea</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/">Calico</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">Cilium</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">Kube-router</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">Romana</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">Weave Net</a></li></ul><h2 id="create-an-nginx-deployment-and-expose-it-via-a-service">Create an <code>nginx</code> deployment and expose it via a service</h2><p>To see how Kubernetes network policy works, start off by creating an <code>nginx</code> Deployment.</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl create deployment nginx --image=nginx
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/nginx created
</code></pre><p>Expose the Deployment through a Service called <code>nginx</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl expose deployment nginx --port=80
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none">service/nginx exposed
</code></pre><p>The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named <code>nginx</code>. The <code>nginx</code> Pod and Deployment are found in the <code>default</code> namespace.</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl get svc,pod
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/kubernetes          10.100.0.1    &lt;none&gt;        443/TCP    46m
service/nginx               10.100.0.16   &lt;none&gt;        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
pod/nginx-701339712-e0qfq   1/1           Running       0          35s
</code></pre><h2 id="test-the-service-by-accessing-it-from-another-pod">Test the service by accessing it from another Pod</h2><p>You should be able to access the new <code>nginx</code> service from other Pods. To access the <code>nginx</code> Service from another Pod in the <code>default</code> namespace, start a busybox container:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl run busybox --rm -ti --image=busybox -- /bin/sh
</span></span></span></code></pre></div><p>In your shell, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>wget --spider --timeout<span>=</span><span>1</span> nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre><h2 id="limit-access-to-the-nginx-service">Limit access to the <code>nginx</code> service</h2><p>To limit the access to the <code>nginx</code> service so that only Pods with the label <code>access: true</code> can query it, create a NetworkPolicy object as follows:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/nginx-policy.yaml"><code>service/networking/nginx-policy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/nginx-policy.yaml to clipboard"></div><div class="includecode" id="service-networking-nginx-policy-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>access-nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>ingress</span>:<span>
</span></span></span><span><span><span>  </span>- <span>from</span>:<span>
</span></span></span><span><span><span>    </span>- <span>podSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>access</span>:<span> </span><span>"true"</span><span>
</span></span></span></code></pre></div></div></div><p>The name of a NetworkPolicy object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>NetworkPolicy includes a <code>podSelector</code> which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label <code>app=nginx</code>. The label was automatically added to the Pod in the <code>nginx</code> Deployment. An empty <code>podSelector</code> selects all pods in the namespace.</div><h2 id="assign-the-policy-to-the-service">Assign the policy to the service</h2><p>Use kubectl to create a NetworkPolicy from the above <code>nginx-policy.yaml</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none">networkpolicy.networking.k8s.io/access-nginx created
</code></pre><h2 id="test-access-to-the-service-when-access-label-is-not-defined">Test access to the service when access label is not defined</h2><p>When you attempt to access the <code>nginx</code> Service from a Pod without the correct labels, the request times out:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl run busybox --rm -ti --image=busybox -- /bin/sh
</span></span></span></code></pre></div><p>In your shell, run the command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>wget --spider --timeout<span>=</span><span>1</span> nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Connecting to nginx (10.100.0.16:80)
wget: download timed out
</code></pre><h2 id="define-access-label-and-test-again">Define access label and test again</h2><p>You can create a Pod with the correct labels to see that the request is allowed:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>kubectl run busybox --rm -ti --labels="access=true" --image=busybox -- /bin/sh
</span></span></span></code></pre></div><p>In your shell, run the command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>wget --spider --timeout<span>=</span><span>1</span> nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre></div></div><div><div class="td-content"><h1>Developing Cloud Controller Manager</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.11 [beta]</code></div><p><p>The cloud-controller-manager is a Kubernetes <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p><p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p><h2 id="background">Background</h2><p>Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the <code>cloud-controller-manager</code> binary allows cloud vendors to evolve independently from the core Kubernetes code.</p><p>The Kubernetes project provides skeleton cloud-controller-manager code with Go interfaces to allow you (or your cloud provider) to plug in your own implementations. This means that a cloud provider can implement a cloud-controller-manager by importing packages from Kubernetes core; each cloudprovider will register their own code by calling <code>cloudprovider.RegisterCloudProvider</code> to update a global variable of available cloud providers.</p><h2 id="developing">Developing</h2><h3 id="out-of-tree">Out of tree</h3><p>To build an out-of-tree cloud-controller-manager for your cloud:</p><ol><li>Create a go package with an implementation that satisfies <a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go">cloudprovider.Interface</a>.</li><li>Use <a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go"><code>main.go</code> in cloud-controller-manager</a> from Kubernetes core as a template for your <code>main.go</code>. As mentioned above, the only difference should be the cloud package that will be imported.</li><li>Import your cloud package in <code>main.go</code>, ensure your package has an <code>init</code> block to run <a href="https://github.com/kubernetes/cloud-provider/blob/master/plugins.go"><code>cloudprovider.RegisterCloudProvider</code></a>.</li></ol><p>Many cloud providers publish their controller manager code as open source. If you are creating
a new cloud-controller-manager from scratch, you could take an existing out-of-tree cloud
controller manager as your starting point.</p><h3 id="in-tree">In tree</h3><p>For in-tree cloud providers, you can run the in-tree cloud controller manager as a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a> in your cluster. See <a href="/docs/tasks/administer-cluster/running-cloud-controller/">Cloud Controller Manager Administration</a> for more details.</p></div></div><div><div class="td-content"><h1>Enable Or Disable A Kubernetes API</h1><p>This page shows how to enable or disable an API version from your cluster's
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>.</p><p>Specific API versions can be turned on or off by passing <code>--runtime-config=api/&lt;version&gt;</code> as a
command line argument to the API server. The values for this argument are a comma-separated
list of API versions. Later values override earlier values.</p><p>The <code>runtime-config</code> command line argument also supports 2 special keys:</p><ul><li><code>api/all</code>, representing all known APIs</li><li><code>api/legacy</code>, representing only legacy APIs. Legacy APIs are any APIs that have been
explicitly <a href="/docs/reference/using-api/deprecation-policy/">deprecated</a>.</li></ul><p>For example, to turn off all API versions except v1, pass <code>--runtime-config=api/all=false,api/v1=true</code>
to the <code>kube-apiserver</code>.</p><h2 id="what-s-next">What's next</h2><p>Read the <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">full documentation</a>
for the <code>kube-apiserver</code> component.</p></div></div><div><div class="td-content"><h1>Encrypting Confidential Data at Rest</h1><p>All of the APIs in Kubernetes that let you write persistent API resource data support
at-rest encryption. For example, you can enable at-rest encryption for
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secrets</a>.
This at-rest encryption is additional to any system-level encryption for the
etcd cluster or for the filesystem(s) on hosts where you are running the
kube-apiserver.</p><p>This page shows how to enable and configure encryption of API data at rest.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>This task covers encryption for resource data stored using the
<a class="glossary-tooltip" title="The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster." href="/docs/concepts/overview/kubernetes-api/" target="_blank">Kubernetes API</a>. For example, you can
encrypt Secret objects, including the key-value data they contain.</p><p>If you want to encrypt data in filesystems that are mounted into containers, you instead need
to either:</p><ul><li>use a storage integration that provides encrypted
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volumes</a></li><li>encrypt the data within your own application</li></ul></div><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>This task assumes that you are running the Kubernetes API server as a
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static pod</a> on each control
plane node.</p></li><li><p>Your cluster's control plane <strong>must</strong> use etcd v3.x (major version 3, any minor version).</p></li><li><p>To encrypt a custom resource, your cluster must be running Kubernetes v1.26 or newer.</p></li><li><p>To use a wildcard to match resources, your cluster must be running Kubernetes v1.27 or newer.</p></li></ul><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="determining-whether-encryption-at-rest-is-already-enabled">Determine whether encryption at rest is already enabled</h2><p>By default, the API server stores plain-text representations of resources into etcd, with
no at-rest encryption.</p><p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code>
that specifies a path to a configuration file. The contents of that file, if you specify one,
control how Kubernetes API data is encrypted in etcd.
If you are running the kube-apiserver without the <code>--encryption-provider-config</code> command line
argument, you do not have encryption at rest enabled. If you are running the kube-apiserver
with the <code>--encryption-provider-config</code> command line argument, and the file that it references
specifies the <code>identity</code> provider as the first encryption provider in the list, then you
do not have at-rest encryption enabled
(<strong>the default <code>identity</code> provider does not provide any confidentiality protection.</strong>)</p><p>If you are running the kube-apiserver
with the <code>--encryption-provider-config</code> command line argument, and the file that it references
specifies a provider other than <code>identity</code> as the first encryption provider in the list, then
you already have at-rest encryption enabled. However, that check does not tell you whether
a previous migration to encrypted storage has succeeded. If you are not sure, see
<a href="#ensure-all-secrets-are-encrypted">ensure all relevant data are encrypted</a>.</p><h2 id="understanding-the-encryption-at-rest-configuration">Understanding the encryption at rest configuration</h2><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># CAUTION: this is an example configuration.</span><span>
</span></span></span><span><span><span></span><span>#          Do not use this for your own cluster!</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>      </span>- configmaps<span>
</span></span></span><span><span><span>      </span>- pandas.awesome.bears.example<span> </span><span># a custom resource API</span><span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span><span># This configuration does not provide data confidentiality. The first</span><span>
</span></span></span><span><span><span>      </span><span># configured provider is specifying the "identity" mechanism, which</span><span>
</span></span></span><span><span><span>      </span><span># stores resources as plain text.</span><span>
</span></span></span><span><span><span>      </span><span>#</span><span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span> </span><span># plain text, in other words NO encryption</span><span>
</span></span></span><span><span><span>      </span>- <span>aesgcm</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key2<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>dGhpcyBpcyBwYXNzd29yZA==<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key2<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>dGhpcyBpcyBwYXNzd29yZA==<span>
</span></span></span><span><span><span>      </span>- <span>secretbox</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- events<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span> </span><span># do not encrypt Events even though *.* is specified below</span><span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>'*.apps'</span><span> </span><span># wildcard match requires Kubernetes 1.27 or later</span><span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>key2<span>
</span></span></span><span><span><span>            </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>'*.*'</span><span> </span><span># wildcard match requires Kubernetes 1.27 or later</span><span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>key3<span>
</span></span></span><span><span><span>            </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==</span></span></code></pre></div><p>Each <code>resources</code> array item is a separate config and contains a complete configuration. The
<code>resources.resources</code> field is an array of Kubernetes resource names (<code>resource</code> or <code>resource.group</code>)
that should be encrypted like Secrets, ConfigMaps, or other resources.</p><p>If custom resources are added to <code>EncryptionConfiguration</code> and the cluster version is 1.26 or newer,
any newly created custom resources mentioned in the <code>EncryptionConfiguration</code> will be encrypted.
Any custom resources that existed in etcd prior to that version and configuration will be unencrypted
until they are next written to storage. This is the same behavior as built-in resources.
See the <a href="#ensure-all-secrets-are-encrypted">Ensure all secrets are encrypted</a> section.</p><p>The <code>providers</code> array is an ordered list of the possible encryption providers to use for the APIs that you listed.
Each provider supports multiple keys - the keys are tried in order for decryption, and if the provider
is the first provider, the first key is used for encryption.</p><p>Only one provider type may be specified per entry (<code>identity</code> or <code>aescbc</code> may be provided,
but not both in the same item).
The first provider in the list is used to encrypt resources written into the storage. When reading
resources from storage, each provider that matches the stored data attempts in order to decrypt the
data. If no provider can read the stored data due to a mismatch in format or secret key, an error
is returned which prevents clients from accessing that resource.</p><p><code>EncryptionConfiguration</code> supports the use of wildcards to specify the resources that should be encrypted.
Use '<code>*.&lt;group&gt;</code>' to encrypt all resources within a group (for eg '<code>*.apps</code>' in above example) or '<code>*.*</code>'
to encrypt all resources. '<code>*.</code>' can be used to encrypt all resource in the core group. '<code>*.*</code>' will
encrypt all resources, even custom resources that are added after API server start.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Use of wildcards that overlap within the same resource list or across multiple entries are not allowed
since part of the configuration would be ineffective. The <code>resources</code> list's processing order and precedence
are determined by the order it's listed in the configuration.</div><p>If you have a wildcard covering resources and want to opt out of at-rest encryption for a particular kind
of resource, you achieve that by adding a separate <code>resources</code> array item with the name of the resource that
you want to exempt, followed by a <code>providers</code> array item where you specify the <code>identity</code> provider. You add
this item to the list so that it appears earlier than the configuration where you do specify encryption
(a provider that is not <code>identity</code>).</p><p>For example, if '<code>*.*</code>' is enabled and you want to opt out of encryption for Events and ConfigMaps, add a
new <strong>earlier</strong> item to the <code>resources</code>, followed by the providers array item with <code>identity</code> as the
provider. The more specific entry must come before the wildcard entry.</p><p>The new item would look similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- configmaps.<span> </span><span># specifically from the core API group,</span><span>
</span></span></span><span><span><span>                    </span><span># because of trailing "."</span><span>
</span></span></span><span><span><span>      </span>- events<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span># and then other entries in resources</span><span>
</span></span></span></code></pre></div><p>Ensure that the exemption is listed <em>before</em> the wildcard '<code>*.*</code>' item in the resources array
to give it precedence.</p><p>For more detailed information about the <code>EncryptionConfiguration</code> struct, please refer to the
<a href="/docs/reference/config-api/apiserver-config.v1/">encryption configuration API</a>.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>If any resource is not readable via the encryption configuration (because keys were changed),
and you cannot restore a working configuration, your only recourse is to delete that entry from
the underlying etcd directly.</p><p>Any calls to the Kubernetes API that attempt to read that resource will fail until it is deleted
or a valid decryption key is provided.</p></div><h3 id="providers">Available providers</h3><p>Before you configure encryption-at-rest for data in your cluster's Kubernetes API, you
need to select which provider(s) you will use.</p><p>The following table describes each available provider.</p><table class="complex-layout"><caption>Providers for Kubernetes encryption at rest</caption><thead><tr><th>Name</th><th>Encryption</th><th>Strength</th><th>Speed</th><th>Key length</th></tr></thead><tbody id="encryption-providers-identity"><tr><th rowspan="2" scope="row"><tt>identity</tt></th><td><strong>None</strong></td><td>N/A</td><td>N/A</td><td>N/A</td></tr><tr><td colspan="4">Resources written as-is without encryption. When set as the first provider, the resource will be decrypted as new values are written. Existing encrypted resources are <strong>not</strong> automatically overwritten with the plaintext data.
The <tt>identity</tt> provider is the default if you do not specify otherwise.</td></tr></tbody><tbody id="encryption-providers-that-encrypt"><tr><th rowspan="2" scope="row"><tt>aescbc</tt></th><td>AES-CBC with <a href="https://datatracker.ietf.org/doc/html/rfc2315">PKCS#7</a> padding</td><td>Weak</td><td>Fast</td><td>16, 24, or 32-byte</td></tr><tr><td colspan="4">Not recommended due to CBC's vulnerability to padding oracle attacks. Key material accessible from control plane host.</td></tr><tr><th rowspan="2" scope="row"><tt>aesgcm</tt></th><td>AES-GCM with random nonce</td><td>Must be rotated every 200,000 writes</td><td>Fastest</td><td>16, 24, or 32-byte</td></tr><tr><td colspan="4">Not recommended for use except when an automated key rotation scheme is implemented. Key material accessible from control plane host.</td></tr><tr><th rowspan="2" scope="row"><tt>kms</tt> v1 <em>(deprecated since Kubernetes v1.28)</em></th><td>Uses envelope encryption scheme with DEK per resource.</td><td>Strongest</td><td>Slow (<em>compared to <tt>kms</tt> version 2</em>)</td><td>32-bytes</td></tr><tr><td colspan="4">Data is encrypted by data encryption keys (DEKs) using AES-GCM;
DEKs are encrypted by key encryption keys (KEKs) according to
configuration in Key Management Service (KMS).
Simple key rotation, with a new DEK generated for each encryption, and
KEK rotation controlled by the user.<br>Read how to <a href="/docs/tasks/administer-cluster/kms-provider#configuring-the-kms-provider-kms-v1">configure the KMS V1 provider</a>.</td></tr><tr><th rowspan="2" scope="row"><tt>kms</tt> v2</th><td>Uses envelope encryption scheme with DEK per API server.</td><td>Strongest</td><td>Fast</td><td>32-bytes</td></tr><tr><td colspan="4">Data is encrypted by data encryption keys (DEKs) using AES-GCM; DEKs
are encrypted by key encryption keys (KEKs) according to configuration
in Key Management Service (KMS).
Kubernetes generates a new DEK per encryption from a secret seed.
The seed is rotated whenever the KEK is rotated.<br>A good choice if using a third party tool for key management.
Available as stable from Kubernetes v1.29.<br>Read how to <a href="/docs/tasks/administer-cluster/kms-provider#configuring-the-kms-provider-kms-v2">configure the KMS V2 provider</a>.</td></tr><tr><th rowspan="2" scope="row"><tt>secretbox</tt></th><td>XSalsa20 and Poly1305</td><td>Strong</td><td>Faster</td><td>32-byte</td></tr><tr><td colspan="4">Uses relatively new encryption technologies that may not be considered acceptable in environments that require high levels of review. Key material accessible from control plane host.</td></tr></tbody></table><p>The <code>identity</code> provider is the default if you do not specify otherwise. <strong>The <code>identity</code> provider does not
encrypt stored data and provides <em>no</em> additional confidentiality protection.</strong></p><h3 id="key-storage">Key storage</h3><h4 id="local-key-storage">Local key storage</h4><p>Encrypting secret data with a locally managed key protects against an etcd compromise, but it fails to
protect against a host compromise. Since the encryption keys are stored on the host in the
EncryptionConfiguration YAML file, a skilled attacker can access that file and extract the encryption
keys.</p><h4 id="kms-key-storage">Managed (KMS) key storage</h4><p>The KMS provider uses <em>envelope encryption</em>: Kubernetes encrypts resources using a data key, and then
encrypts that data key using the managed encryption service. Kubernetes generates a unique data key for
each resource. The API server stores an encrypted version of the data key in etcd alongside the ciphertext;
when reading the resource, the API server calls the managed encryption service and provides both the
ciphertext and the (encrypted) data key.
Within the managed encryption service, the provider use a <em>key encryption key</em> to decipher the data key,
deciphers the data key, and finally recovers the plain text. Communication between the control plane
and the KMS requires in-transit protection, such as TLS.</p><p>Using envelope encryption creates dependence on the key encryption key, which is not stored in Kubernetes.
In the KMS case, an attacker who intends to get unauthorised access to the plaintext
values would need to compromise etcd <strong>and</strong> the third-party KMS provider.</p><h3 id="protection-for-encryption-keys">Protection for encryption keys</h3><p>You should take appropriate measures to protect the confidential information that allows decryption,
whether that is a local encryption key, or an authentication token that allows the API server to
call KMS.</p><p>Even when you rely on a provider to manage the use and lifecycle of the main encryption key (or keys), you are still responsible
for making sure that access controls and other security measures for the managed encryption service are
appropriate for your security needs.</p><h2 id="encrypting-your-data">Encrypt your data</h2><h3 id="generate-key-no-kms">Generate the encryption key</h3><p>The following steps assume that you are not using KMS, and therefore the steps also
assume that you need to generate an encryption key. If you already have an encryption key,
skip to <a href="#write-an-encryption-configuration-file">Write an encryption configuration file</a>.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>Storing the raw encryption key in the EncryptionConfig only moderately improves your security posture,
compared to no encryption.</p><p>For additional secrecy, consider using the <code>kms</code> provider as this relies on keys held outside your
Kubernetes cluster. Implementations of <code>kms</code> can work with hardware security modules or with
encryption services managed by your cloud provider.</p><p>To learn about setting
up encryption at rest using KMS, see
<a href="/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a>.
The KMS provider plugin that you use may also come with additional specific documentation.</p></div><p>Start by generating a new encryption key, and then encode it using base64:</p><ul class="nav nav-tabs" id="generate-encryption-key"><li class="nav-item"><a class="nav-link active" href="#generate-encryption-key-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#generate-encryption-key-1">macOS</a></li><li class="nav-item"><a class="nav-link" href="#generate-encryption-key-2">Windows</a></li></ul><div class="tab-content" id="generate-encryption-key"><div id="generate-encryption-key-0" class="tab-pane show active"><p><p>Generate a 32-byte random key and base64 encode it. You can use this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>head -c <span>32</span> /dev/urandom | base64
</span></span></code></pre></div><p>You can use <code>/dev/hwrng</code> instead of <code>/dev/urandom</code> if you want to
use your PC's built-in hardware entropy source. Not all Linux
devices provide a hardware random generator.</p></p></div><div id="generate-encryption-key-1" class="tab-pane"><p><p>Generate a 32-byte random key and base64 encode it. You can use this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>head -c <span>32</span> /dev/urandom | base64
</span></span></code></pre></div></p></div><div id="generate-encryption-key-2" class="tab-pane"><p><p>Generate a 32-byte random key and base64 encode it. You can use this command:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span># Do not run this in a session where you have set a random number</span>
</span></span><span><span><span># generator seed.</span>
</span></span><span><span>[<span>Convert</span>]::ToBase64String((<span>1</span>.<span>.32</span>|%{[<span>byte</span>](<span>Get-Random</span> -Max <span>256</span>)}))
</span></span></code></pre></div></p></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Keep the encryption key confidential, including while you generate it and
ideally even after you are no longer actively using it.</div><h3 id="replicate-the-encryption-key">Replicate the encryption key</h3><p>Using a secure mechanism for file transfer, make a copy of that encryption key
available to every other control plane host.</p><p>At a minimum, use encryption in transit - for example, secure shell (SSH). For more
security, use asymmetric encryption between hosts, or change the approach you are using
so that you're relying on KMS encryption.</p><h3 id="write-an-encryption-configuration-file">Write an encryption configuration file</h3><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>The encryption configuration file may contain keys that can decrypt content in etcd.
If the configuration file contains any key material, you must properly
restrict permissions on all your control plane hosts so only the user
who runs the kube-apiserver can read this configuration.</div><p>Create a new encryption configuration file. The contents should be similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>      </span>- configmaps<span>
</span></span></span><span><span><span>      </span>- pandas.awesome.bears.example<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span># See the following text for more details about the secret value</span><span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>&lt;BASE 64 ENCODED SECRET&gt;<span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span> </span><span># this fallback allows reading unencrypted secrets;</span><span>
</span></span></span><span><span><span>                     </span><span># for example, during initial migration</span><span>
</span></span></span></code></pre></div><p>To create a new encryption key (that does not use KMS), see
<a href="#generate-key-no-kms">Generate the encryption key</a>.</p><h3 id="use-the-new-encryption-configuration-file">Use the new encryption configuration file</h3><p>You will need to mount the new encryption config file to the <code>kube-apiserver</code> static pod. Here is an example on how to do that:</p><ol><li><p>Save the new encryption config file to <code>/etc/kubernetes/enc/enc.yaml</code> on the control-plane node.</p></li><li><p>Edit the manifest for the <code>kube-apiserver</code> static pod: <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> so that it is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># This is a fragment of a manifest for a static Pod.</span><span>
</span></span></span><span><span><span></span><span># Check whether this is correct for your cluster and for your API server.</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint</span>:<span> </span><span>10.20.30.40</span>:<span>443</span><span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>kube-apiserver<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>control-plane<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kube-apiserver<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>command</span>:<span>
</span></span></span><span><span><span>    </span>- kube-apiserver<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span>- --encryption-provider-config=/etc/kubernetes/enc/enc.yaml <span> </span><span># add this line</span><span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>enc                          <span> </span><span># add this line</span><span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/etc/kubernetes/enc     <span> </span><span># add this line</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>                      </span><span># add this line</span><span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>enc                            <span> </span><span># add this line</span><span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>                             </span><span># add this line</span><span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/etc/kubernetes/enc          <span> </span><span># add this line</span><span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>DirectoryOrCreate            <span> </span><span># add this line</span><span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div></li><li><p>Restart your API server.</p></li></ol><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Your config file contains keys that can decrypt the contents in etcd, so you must properly restrict
permissions on your control-plane nodes so only the user who runs the <code>kube-apiserver</code> can read it.</div><p>You now have encryption in place for <strong>one</strong> control plane host. A typical
Kubernetes cluster has multiple control plane hosts, so there is more to do.</p><h3 id="api-server-config-update-more">Reconfigure other control plane hosts</h3><p>If you have multiple API servers in your cluster, you should deploy the
changes in turn to each API server.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>For cluster configurations with two or more control plane nodes, the encryption configuration
should be identical across each control plane node.</p><p>If there is a difference in the encryption provider configuration between control plane
nodes, this difference may mean that the kube-apiserver can't decrypt data.</p></div><p>When you are planning to update the encryption configuration of your cluster, plan this
so that the API servers in your control plane can always decrypt the stored data
(even part way through rolling out the change).</p><p>Make sure that you use the <strong>same</strong> encryption configuration on each
control plane host.</p><h3 id="verifying-that-data-is-encrypted">Verify that newly written data is encrypted</h3><p>Data is encrypted when written to etcd. After restarting your <code>kube-apiserver</code>, any newly
created or updated Secret (or other resource kinds configured in <code>EncryptionConfiguration</code>)
should be encrypted when stored.</p><p>To check this, you can use the <code>etcdctl</code> command line
program to retrieve the contents of your secret data.</p><p>This example shows how to check this for encrypting the Secret API.</p><ol><li><p>Create a new Secret called <code>secret1</code> in the <code>default</code> namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic secret1 -n default --from-literal<span>=</span><span>mykey</span><span>=</span>mydata
</span></span></code></pre></div></li><li><p>Using the <code>etcdctl</code> command line tool, read that Secret out of etcd:</p><pre tabindex="0"><code>ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C
</code></pre><p>where <code>[...]</code> must be the additional arguments for connecting to the etcd server.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl <span>\
</span></span></span><span><span><span></span>   --cacert<span>=</span>/etc/kubernetes/pki/etcd/ca.crt   <span>\
</span></span></span><span><span><span></span>   --cert<span>=</span>/etc/kubernetes/pki/etcd/server.crt <span>\
</span></span></span><span><span><span></span>   --key<span>=</span>/etc/kubernetes/pki/etcd/server.key  <span>\
</span></span></span><span><span><span></span>   get /registry/secrets/default/secret1 | hexdump -C
</span></span></code></pre></div><p>The output is similar to this (abbreviated):</p><div class="highlight"><pre tabindex="0"><code class="language-hexdump"><span><span><span>00000000</span>  <span>2f</span> <span>72</span> <span>65</span> <span>67</span> <span>69</span> <span>73</span> <span>74</span> <span>72</span>  <span>79</span> <span>2f</span> <span>73</span> <span>65</span> <span>63</span> <span>72</span> <span>65</span> <span>74</span>  |<span>/registry/secret</span>|
</span></span><span><span><span>00000010</span>  <span>73</span> <span>2f</span> <span>64</span> <span>65</span> <span>66</span> <span>61</span> <span>75</span> <span>6c</span>  <span>74</span> <span>2f</span> <span>73</span> <span>65</span> <span>63</span> <span>72</span> <span>65</span> <span>74</span>  |<span>s/default/secret</span>|
</span></span><span><span><span>00000020</span>  <span>31</span> <span>0a</span> <span>6b</span> <span>38</span> <span>73</span> <span>3a</span> <span>65</span> <span>6e</span>  <span>63</span> <span>3a</span> <span>61</span> <span>65</span> <span>73</span> <span>63</span> <span>62</span> <span>63</span>  |<span>1.k8s:enc:aescbc</span>|
</span></span><span><span><span>00000030</span>  <span>3a</span> <span>76</span> <span>31</span> <span>3a</span> <span>6b</span> <span>65</span> <span>79</span> <span>31</span>  <span>3a</span> <span>c7</span> <span>6c</span> <span>e7</span> <span>d3</span> <span>09</span> <span>bc</span> <span>06</span>  |<span>:v1:key1:.l.....</span>|
</span></span><span><span><span>00000040</span>  <span>25</span> <span>51</span> <span>91</span> <span>e4</span> <span>e0</span> <span>6c</span> <span>e5</span> <span>b1</span>  <span>4d</span> <span>7a</span> <span>8b</span> <span>3d</span> <span>b9</span> <span>c2</span> <span>7c</span> <span>6e</span>  |<span>%Q...l..Mz.=..|n</span>|
</span></span><span><span><span>00000050</span>  <span>b4</span> <span>79</span> <span>df</span> <span>05</span> <span>28</span> <span>ae</span> <span>0d</span> <span>8e</span>  <span>5f</span> <span>35</span> <span>13</span> <span>2c</span> <span>c0</span> <span>18</span> <span>99</span> <span>3e</span>  |<span>.y..(..._5.,...&gt;</span>|
</span></span><span><span><span>[...]</span>
</span></span><span><span><span>00000110</span>  <span>23</span> <span>3a</span> <span>0d</span> <span>fc</span> <span>28</span> <span>ca</span> <span>48</span> <span>2d</span>  <span>6b</span> <span>2d</span> <span>46</span> <span>cc</span> <span>72</span> <span>0b</span> <span>70</span> <span>4c</span>  |<span>#:..(.H-k-F.r.pL</span>|
</span></span><span><span><span>00000120</span>  <span>a5</span> <span>fc</span> <span>35</span> <span>43</span> <span>12</span> <span>4e</span> <span>60</span> <span>ef</span>  <span>bf</span> <span>6f</span> <span>fe</span> <span>cf</span> <span>df</span> <span>0b</span> <span>ad</span> <span>1f</span>  |<span>..5C.N`..o......</span>|
</span></span><span><span><span>00000130</span>  <span>82</span> <span>c4</span> <span>88</span> <span>53</span> <span>02</span> <span>da</span> <span>3e</span> <span>66</span>  <span>ff</span> <span>0a</span>                    |<span>...S..&gt;f..</span>|
</span></span><span><span><span>0000013a</span>
</span></span></code></pre></div></li><li><p>Verify the stored Secret is prefixed with <code>k8s:enc:aescbc:v1:</code> which indicates
the <code>aescbc</code> provider has encrypted the resulting data. Confirm that the key name shown in <code>etcd</code>
matches the key name specified in the <code>EncryptionConfiguration</code> mentioned above. In this example,
you can see that the encryption key named <code>key1</code> is used in <code>etcd</code> and in <code>EncryptionConfiguration</code>.</p></li><li><p>Verify the Secret is correctly decrypted when retrieved via the API:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret secret1 -n default -o yaml
</span></span></code></pre></div><p>The output should contain <code>mykey: bXlkYXRh</code>, with contents of <code>mydata</code> encoded using base64;
read
<a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret">decoding a Secret</a>
to learn how to completely decode the Secret.</p></li></ol><h3 id="ensure-all-secrets-are-encrypted">Ensure all relevant data are encrypted</h3><p>It's often not enough to make sure that new objects get encrypted: you also want that
encryption to apply to the objects that are already stored.</p><p>For this example, you have configured your cluster so that Secrets are encrypted on write.
Performing a replace operation for each Secret will encrypt that content at rest,
where the objects are unchanged.</p><p>You can make this change across all Secrets in your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this as an administrator that can read and write all Secrets</span>
</span></span><span><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><p>The command above reads all Secrets and then updates them with the same data, in order to
apply server side encryption.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If an error occurs due to a conflicting write, retry the command.
It is safe to run that command more than once.</p><p>For larger clusters, you may wish to subdivide the Secrets by namespace,
or script an update.</p></div><h2 id="cleanup-all-secrets-encrypted">Prevent plain text retrieval</h2><p>If you want to make sure that the only access to a particular API kind is done using
encryption, you can remove the API server's ability to read that API's backing data
as plaintext.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>Making this change prevents the API server from retrieving resources that are marked
as encrypted at rest, but are actually stored in the clear.</p><p>When you have configured encryption at rest for an API (for example: the API kind
<code>Secret</code>, representing <code>secrets</code> resources in the core API group), you <strong>must</strong> ensure
that all those resources in this cluster really are encrypted at rest. Check this before
you carry on with the next steps.</p></div><p>Once all Secrets in your cluster are encrypted, you can remove the <code>identity</code>
part of the encryption configuration. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>&lt;BASE 64 ENCODED SECRET&gt;<span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span> </span><span># REMOVE THIS LINE</span></span></span></code></pre></div><p>&#8230;and then restart each API server in turn. This change prevents the API server
from accessing a plain-text Secret, even by accident.</p><h2 id="rotating-a-decryption-key">Rotate a decryption key</h2><p>Changing an encryption key for Kubernetes without incurring downtime requires a multi-step operation,
especially in the presence of a highly-available deployment where multiple <code>kube-apiserver</code> processes
are running.</p><ol><li>Generate a new key and add it as the second key entry for the current provider on all
control plane nodes.</li><li>Restart <strong>all</strong> <code>kube-apiserver</code> processes, to ensure each server can decrypt
any data that are encrypted with the new key.</li><li>Make a secure backup of the new encryption key. If you lose all copies of this key you would
need to delete all the resources were encrypted under the lost key, and workloads may not
operate as expected during the time that at-rest encryption is broken.</li><li>Make the new key the first entry in the <code>keys</code> array so that it is used for encryption-at-rest
for new writes</li><li>Restart all <code>kube-apiserver</code> processes to ensure each control plane host now encrypts using the new key</li><li>As a privileged user, run <code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code>
to encrypt all existing Secrets with the new key</li><li>After you have updated all existing Secrets to use the new key and have made a secure backup of the
new key, remove the old decryption key from the configuration.</li></ol><h2 id="decrypting-all-data">Decrypt all data</h2><p>This example shows how to stop encrypting the Secret API at rest. If you are encrypting
other API kinds, adjust the steps to match.</p><p>To disable encryption at rest, place the <code>identity</code> provider as the first
entry in your encryption configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>      </span><span># list any other resources here that you previously were</span><span>
</span></span></span><span><span><span>      </span><span># encrypting at rest</span><span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span> </span><span># add this line</span><span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>&lt;BASE 64 ENCODED SECRET&gt;<span> </span><span># keep this in place</span><span>
</span></span></span><span><span><span>                                               </span><span># make sure it comes after "identity"</span><span>
</span></span></span></code></pre></div><p>Then run the following command to force decryption of all Secrets:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><p>Once you have replaced all existing encrypted resources with backing data that
don't use encryption, you can remove the encryption settings from the
<code>kube-apiserver</code>.</p><h2 id="configure-automatic-reloading">Configure automatic reloading</h2><p>You can configure automatic reloading of encryption provider configuration.
That setting determines whether the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a> should
load the file you specify for <code>--encryption-provider-config</code> only once at
startup, or automatically whenever you change that file. Enabling this option
allows you to change the keys for encryption at rest without restarting the
API server.</p><p>To allow automatic reloading, configure the API server to run with:
<code>--encryption-provider-config-automatic-reload=true</code>.
When enabled, file changes are polled every minute to observe the modifications.
The <code>apiserver_encryption_config_controller_automatic_reload_last_timestamp_seconds</code>
metric identifies when the new config becomes effective. This allows
encryption keys to be rotated without restarting the API server.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/tasks/administer-cluster/decrypt-data/">decrypting data that are already stored at rest</a></li><li>Learn more about the <a href="/docs/reference/config-api/apiserver-config.v1/">EncryptionConfiguration configuration API (v1)</a>.</li></ul></div></div><div><div class="td-content"><h1>Decrypt Confidential Data that is Already Encrypted at Rest</h1><p>All of the APIs in Kubernetes that let you write persistent API resource data support
at-rest encryption. For example, you can enable at-rest encryption for
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secrets</a>.
This at-rest encryption is additional to any system-level encryption for the
etcd cluster or for the filesystem(s) on hosts where you are running the
kube-apiserver.</p><p>This page shows how to switch from encryption of API data at rest, so that API data
are stored unencrypted. You might want to do this to improve performance; usually,
though, if it was a good idea to encrypt some data, it's also a good idea to leave them
encrypted.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>This task covers encryption for resource data stored using the
<a class="glossary-tooltip" title="The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster." href="/docs/concepts/overview/kubernetes-api/" target="_blank">Kubernetes API</a>. For example, you can
encrypt Secret objects, including the key-value data they contain.</p><p>If you wanted to manage encryption for data in filesystems that are mounted into containers, you instead
need to either:</p><ul><li>use a storage integration that provides encrypted
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volumes</a></li><li>encrypt the data within your own application</li></ul></div><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>This task assumes that you are running the Kubernetes API server as a
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static pod</a> on each control
plane node.</p></li><li><p>Your cluster's control plane <strong>must</strong> use etcd v3.x (major version 3, any minor version).</p></li><li><p>To encrypt a custom resource, your cluster must be running Kubernetes v1.26 or newer.</p></li><li><p>You should have some API data that are already encrypted.</p></li></ul><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="determine-whether-encryption-at-rest-is-already-enabled">Determine whether encryption at rest is already enabled</h2><p>By default, the API server uses an <code>identity</code> provider that stores plain-text representations
of resources.
<strong>The default <code>identity</code> provider does not provide any confidentiality protection.</strong></p><p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code>
that specifies a path to a configuration file. The contents of that file, if you specify one,
control how Kubernetes API data is encrypted in etcd.
If it is not specified, you do not have encryption at rest enabled.</p><p>The format of that configuration file is YAML, representing a configuration API kind named
<a href="/docs/reference/config-api/apiserver-config.v1/"><code>EncryptionConfiguration</code></a>.
You can see an example configuration
in <a href="/docs/tasks/administer-cluster/encrypt-data/#understanding-the-encryption-at-rest-configuration">Encryption at rest configuration</a>.</p><p>If <code>--encryption-provider-config</code> is set, check which resources (such as <code>secrets</code>) are
configured for encryption, and what provider is used.
Make sure that the preferred provider for that resource type is <strong>not</strong> <code>identity</code>; you
only set <code>identity</code> (<em>no encryption</em>) as default when you want to disable encryption at
rest.
Verify that the first-listed provider for a resource is something <strong>other</strong> than <code>identity</code>,
which means that any new information written to resources of that type will be encrypted as
configured. If you do see <code>identity</code> as the first-listed provider for any resource, this
means that those resources are being written out to etcd without encryption.</p><h2 id="decrypting-all-data">Decrypt all data</h2><p>This example shows how to stop encrypting the Secret API at rest. If you are encrypting
other API kinds, adjust the steps to match.</p><h3 id="locate-the-encryption-configuration-file">Locate the encryption configuration file</h3><p>First, find the API server configuration files. On each control plane node, static Pod manifest
for the kube-apiserver specifies a command line argument, <code>--encryption-provider-config</code>.
You are likely to find that this file is mounted into the static Pod using a
<a href="/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code></a> volume mount. Once you locate the volume
you can find the file on the node filesystem and inspect it.</p><h3 id="configure-the-api-server-to-decrypt-objects">Configure the API server to decrypt objects</h3><p>To disable encryption at rest, place the <code>identity</code> provider as the first
entry in your encryption configuration file.</p><p>For example, if your existing EncryptionConfiguration file reads:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span><span># Do not use this (invalid) example key for encryption</span><span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>2KfZgdiq2K0g2YrYpyDYs9mF2LPZhQ==<span>
</span></span></span></code></pre></div><p>then change it to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>identity</span>:<span> </span>{}<span> </span><span># add this line</span><span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>2KfZgdiq2K0g2YrYpyDYs9mF2LPZhQ==<span>
</span></span></span></code></pre></div><p>and restart the kube-apiserver Pod on this node.</p><h3 id="api-server-config-update-more-1">Reconfigure other control plane hosts</h3><p>If you have multiple API servers in your cluster, you should deploy the changes in turn to each API server.</p><p>Make sure that you use the same encryption configuration on each control plane host.</p><h3 id="force-decryption">Force decryption</h3><p>Then run the following command to force decryption of all Secrets:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If you are decrypting a different kind of object, change "secrets" to match.</span>
</span></span><span><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><p>Once you have replaced <strong>all</strong> existing encrypted resources with backing data that
don't use encryption, you can remove the encryption settings from the
<code>kube-apiserver</code>.</p><p>The command line options to remove are:</p><ul><li><code>--encryption-provider-config</code></li><li><code>--encryption-provider-config-automatic-reload</code></li></ul><p>Restart the kube-apiserver Pod again to apply the new configuration.</p><h3 id="api-server-config-update-more-2">Reconfigure other control plane hosts</h3><p>If you have multiple API servers in your cluster, you should again deploy the changes in turn to each API server.</p><p>Make sure that you use the same encryption configuration on each control plane host.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the <a href="/docs/reference/config-api/apiserver-config.v1/">EncryptionConfiguration configuration API (v1)</a>.</li></ul></div></div><div><div class="td-content"><h1>Guaranteed Scheduling For Critical Add-On Pods</h1><p>Kubernetes core components such as the API server, scheduler, and controller-manager run on a control plane node. However, add-ons must run on a regular cluster node.
Some of these add-ons are critical to a fully functional cluster, such as metrics-server, DNS, and UI.
A cluster may stop working properly if a critical add-on is evicted (either manually or as a side effect of another operation like upgrade)
and becomes pending (for example when the cluster is highly utilized and either there are other pending pods that schedule into the space
vacated by the evicted critical add-on pod or the amount of resources available on the node changed for some other reason).</p><p>Note that marking a pod as critical is not meant to prevent evictions entirely; it only prevents the pod from becoming permanently unavailable.
A static pod marked as critical can't be evicted. However, non-static pods marked as critical are always rescheduled.</p><h3 id="marking-pod-as-critical">Marking pod as critical</h3><p>To mark a Pod as critical, set priorityClassName for that Pod to <code>system-cluster-critical</code> or <code>system-node-critical</code>. <code>system-node-critical</code> is the highest available priority, even higher than <code>system-cluster-critical</code>.</p></div></div><div><div class="td-content"><h1>IP Masquerade Agent User Guide</h1><p>This page shows how to configure and enable the <code>ip-masq-agent</code>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="ip-masquerade-agent-user-guide">IP Masquerade Agent User Guide</h2><p>The <code>ip-masq-agent</code> configures iptables rules to hide a pod's IP address behind the cluster
node's IP address. This is typically done when sending traffic to destinations outside the
cluster's pod <a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR</a> range.</p><h3 id="key-terms">Key Terms</h3><ul><li><strong>NAT (Network Address Translation)</strong>:
Is a method of remapping one IP address to another by modifying either the source and/or
destination address information in the IP header. Typically performed by a device doing IP routing.</li><li><strong>Masquerading</strong>:
A form of NAT that is typically used to perform a many to one address translation, where
multiple source IP addresses are masked behind a single address, which is typically the
device doing the IP routing. In Kubernetes this is the Node's IP address.</li><li><strong>CIDR (Classless Inter-Domain Routing)</strong>:
Based on the variable-length subnet masking, allows specifying arbitrary-length prefixes.
CIDR introduced a new method of representation for IP addresses, now commonly known as
<strong>CIDR notation</strong>, in which an address or routing prefix is written with a suffix indicating
the number of bits of the prefix, such as 192.168.2.0/24.</li><li><strong>Link Local</strong>:
A link-local address is a network address that is valid only for communications within the
network segment or the broadcast domain that the host is connected to. Link-local addresses
for IPv4 are defined in the address block 169.254.0.0/16 in CIDR notation.</li></ul><p>The ip-masq-agent configures iptables rules to handle masquerading node/pod IP addresses when
sending traffic to destinations outside the cluster node's IP and the Cluster IP range. This
essentially hides pod IP addresses behind the cluster node's IP address. In some environments,
traffic to "external" addresses must come from a known machine address. For example, in Google
Cloud, any traffic to the internet must come from a VM's IP. When containers are used, as in
Google Kubernetes Engine, the Pod IP will be rejected for egress. To avoid this, we must hide
the Pod IP behind the VM's own IP address - generally known as "masquerade". By default, the
agent is configured to treat the three private IP ranges specified by
<a href="https://tools.ietf.org/html/rfc1918">RFC 1918</a> as non-masquerade
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR</a>.
These ranges are <code>10.0.0.0/8</code>, <code>172.16.0.0/12</code>, and <code>192.168.0.0/16</code>.
The agent will also treat link-local (169.254.0.0/16) as a non-masquerade CIDR by default.
The agent is configured to reload its configuration from the location
<em>/etc/config/ip-masq-agent</em> every 60 seconds, which is also configurable.</p><p><img alt="masq/non-masq example" src="/images/docs/ip-masq.png"></p><p>The agent configuration file must be written in YAML or JSON syntax, and may contain three
optional keys:</p><ul><li><code>nonMasqueradeCIDRs</code>: A list of strings in
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR</a> notation that specify
the non-masquerade ranges.</li><li><code>masqLinkLocal</code>: A Boolean (true/false) which indicates whether to masquerade traffic to the
link local prefix <code>169.254.0.0/16</code>. False by default.</li><li><code>resyncInterval</code>: A time interval at which the agent attempts to reload config from disk.
For example: '30s', where 's' means seconds, 'ms' means milliseconds.</li></ul><p>Traffic to 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16 ranges will NOT be masqueraded. Any
other traffic (assumed to be internet) will be masqueraded. An example of a local destination
from a pod could be its Node's IP address as well as another node's address or one of the IP
addresses in Cluster's IP range. Any other traffic will be masqueraded by default. The
below entries show the default set of rules that are applied by the ip-masq-agent:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>iptables -t nat -L IP-MASQ-AGENT
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><p>By default, in GCE/Google Kubernetes Engine, if network policy is enabled or
you are using a cluster CIDR not in the 10.0.0.0/8 range, the <code>ip-masq-agent</code>
will run in your cluster. If you are running in another environment,
you can add the <code>ip-masq-agent</code> <a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>
to your cluster.</p><h2 id="create-an-ip-masq-agent">Create an ip-masq-agent</h2><p>To create an ip-masq-agent, run the following kubectl command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</span></span></code></pre></div><p>You must also apply the appropriate node label to any nodes in your cluster that you want the
agent to run on.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready<span>=</span><span>true</span>
</span></span></code></pre></div><p>More information can be found in the ip-masq-agent documentation <a href="https://github.com/kubernetes-sigs/ip-masq-agent">here</a>.</p><p>In most cases, the default set of rules should be sufficient; however, if this is not the case
for your cluster, you can create and apply a
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> to customize the IP
ranges that are affected. For example, to allow
only 10.0.0.0/8 to be considered by the ip-masq-agent, you can create the following
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> in a file called
"config".</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>It is important that the file is called config since, by default, that will be used as the key
for lookup by the <code>ip-masq-agent</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>nonMasqueradeCIDRs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>10.0.0.0</span>/8<span>
</span></span></span><span><span><span></span><span>resyncInterval</span>:<span> </span>60s<span>
</span></span></span></code></pre></div></div><p>Run the following command to add the configmap to your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap ip-masq-agent --from-file<span>=</span>config --namespace<span>=</span>kube-system
</span></span></code></pre></div><p>This will update a file located at <code>/etc/config/ip-masq-agent</code> which is periodically checked
every <code>resyncInterval</code> and applied to the cluster node.
After the resync interval has expired, you should see the iptables rules reflect your changes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>iptables -t nat -L IP-MASQ-AGENT
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><p>By default, the link local range (169.254.0.0/16) is also handled by the ip-masq agent, which
sets up the appropriate iptables rules. To have the ip-masq-agent ignore link local, you can
set <code>masqLinkLocal</code> to true in the ConfigMap.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>nonMasqueradeCIDRs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>10.0.0.0</span>/8<span>
</span></span></span><span><span><span></span><span>resyncInterval</span>:<span> </span>60s<span>
</span></span></span><span><span><span></span><span>masqLinkLocal</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div></div></div><div><div class="td-content"><h1>Limit Storage Consumption</h1><p>This example demonstrates how to limit the amount of storage consumed in a namespace.</p><p>The following resources are used in the demonstration: <a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a>,
<a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">LimitRange</a>,
and <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a>.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></li></ul><h2 id="scenario-limiting-storage-consumption">Scenario: Limiting Storage Consumption</h2><p>The cluster-admin is operating a cluster on behalf of a user population and the admin wants to control
how much storage a single namespace can consume in order to control cost.</p><p>The admin would like to limit:</p><ol><li>The number of persistent volume claims in a namespace</li><li>The amount of storage each claim can request</li><li>The amount of cumulative storage the namespace can have</li></ol><h2 id="limitrange-to-limit-requests-for-storage">LimitRange to limit requests for storage</h2><p>Adding a <code>LimitRange</code> to a namespace enforces storage request sizes to a minimum and maximum. Storage is requested
via <code>PersistentVolumeClaim</code>. The admission controller that enforces limit ranges will reject any PVC that is above or below
the values set by the admin.</p><p>In this example, a PVC requesting 10Gi of storage would be rejected because it exceeds the 2Gi max.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LimitRange<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>storagelimits<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span>    </span><span>max</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>2Gi<span>
</span></span></span><span><span><span>    </span><span>min</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>1Gi<span>
</span></span></span></code></pre></div><p>Minimum storage requests are used when the underlying storage provider requires certain minimums. For example,
AWS EBS volumes have a 1Gi minimum requirement.</p><h2 id="resourcequota-to-limit-pvc-count-and-cumulative-storage-capacity">ResourceQuota to limit PVC count and cumulative storage capacity</h2><p>Admins can limit the number of PVCs in a namespace as well as the cumulative capacity of those PVCs. New PVCs that exceed
either maximum value will be rejected.</p><p>In this example, a 6th PVC in the namespace would be rejected because it exceeds the maximum count of 5. Alternatively,
a 5Gi maximum quota when combined with the 2Gi max limit above, cannot have 3 PVCs where each has 2Gi. That would be 6Gi requested
for a namespace capped at 5Gi.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>storagequota<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>persistentvolumeclaims</span>:<span> </span><span>"5"</span><span>
</span></span></span><span><span><span>    </span><span>requests.storage</span>:<span> </span><span>"5Gi"</span><span>
</span></span></span></code></pre></div><h2 id="summary">Summary</h2><p>A limit range can put a ceiling on how much storage is requested while a resource quota can effectively cap the storage
consumed by a namespace through claim counts and cumulative storage capacity. The allows a cluster-admin to plan their
cluster's storage budget without risk of any one project going over their allotment.</p></div></div><div><div class="td-content"><h1>Migrate Replicated Control Plane To Use Cloud Controller Manager</h1><p><p>The cloud-controller-manager is a Kubernetes <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p><p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p><h2 id="background">Background</h2><p>As part of the <a href="/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/">cloud provider extraction effort</a>,
all cloud specific controllers must be moved out of the <code>kube-controller-manager</code>.
All existing clusters that run cloud controllers in the <code>kube-controller-manager</code>
must migrate to instead run the controllers in a cloud provider specific
<code>cloud-controller-manager</code>.</p><p>Leader Migration provides a mechanism in which HA clusters can safely migrate "cloud
specific" controllers between the <code>kube-controller-manager</code> and the
<code>cloud-controller-manager</code> via a shared resource lock between the two components
while upgrading the replicated control plane. For a single-node control plane, or if
unavailability of controller managers can be tolerated during the upgrade, Leader
Migration is not needed and this guide can be ignored.</p><p>Leader Migration can be enabled by setting <code>--enable-leader-migration</code> on
<code>kube-controller-manager</code> or <code>cloud-controller-manager</code>. Leader Migration only
applies during the upgrade and can be safely disabled or left enabled after the
upgrade is complete.</p><p>This guide walks you through the manual process of upgrading the control plane from
<code>kube-controller-manager</code> with built-in cloud provider to running both
<code>kube-controller-manager</code> and <code>cloud-controller-manager</code>. If you use a tool to deploy
and manage the cluster, please refer to the documentation of the tool and the cloud
provider for specific instructions of the migration.</p><h2 id="before-you-begin">Before you begin</h2><p>It is assumed that the control plane is running Kubernetes version N and to be
upgraded to version N + 1. Although it is possible to migrate within the same
version, ideally the migration should be performed as part of an upgrade so that
changes of configuration can be aligned to each release. The exact versions of N and
N + 1 depend on each cloud provider. For example, if a cloud provider builds a
<code>cloud-controller-manager</code> to work with Kubernetes 1.24, then N can be 1.23 and N + 1
can be 1.24.</p><p>The control plane nodes should run <code>kube-controller-manager</code> with Leader Election
enabled, which is the default. As of version N, an in-tree cloud provider must be set
with <code>--cloud-provider</code> flag and <code>cloud-controller-manager</code> should not yet be
deployed.</p><p>The out-of-tree cloud provider must have built a <code>cloud-controller-manager</code> with
Leader Migration implementation. If the cloud provider imports
<code>k8s.io/cloud-provider</code> and <code>k8s.io/controller-manager</code> of version v0.21.0 or later,
Leader Migration will be available. However, for version before v0.22.0, Leader
Migration is alpha and requires feature gate <code>ControllerManagerLeaderMigration</code> to be
enabled in <code>cloud-controller-manager</code>.</p><p>This guide assumes that kubelet of each control plane node starts
<code>kube-controller-manager</code> and <code>cloud-controller-manager</code> as static pods defined by
their manifests. If the components run in a different setting, please adjust the
steps accordingly.</p><p>For authorization, this guide assumes that the cluster uses RBAC. If another
authorization mode grants permissions to <code>kube-controller-manager</code> and
<code>cloud-controller-manager</code> components, please grant the needed access in a way that
matches the mode.</p><h3 id="grant-access-to-migration-lease">Grant access to Migration Lease</h3><p>The default permissions of the controller manager allow only accesses to their main
Lease. In order for the migration to work, accesses to another Lease are required.</p><p>You can grant <code>kube-controller-manager</code> full access to the leases API by modifying
the <code>system::leader-locking-kube-controller-manager</code> role. This task guide assumes
that the name of the migration lease is <code>cloud-provider-extraction-migration</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch -n kube-system role <span>'system::leader-locking-kube-controller-manager'</span> -p <span>'{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}'</span> --type<span>=</span>merge<span>`</span>
</span></span></code></pre></div><p>Do the same to the <code>system::leader-locking-cloud-controller-manager</code> role.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch -n kube-system role <span>'system::leader-locking-cloud-controller-manager'</span> -p <span>'{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}'</span> --type<span>=</span>merge<span>`</span>
</span></span></code></pre></div><h3 id="initial-leader-migration-configuration">Initial Leader Migration configuration</h3><p>Leader Migration optionally takes a configuration file representing the state of
controller-to-manager assignment. At this moment, with in-tree cloud provider,
<code>kube-controller-manager</code> runs <code>route</code>, <code>service</code>, and <code>cloud-node-lifecycle</code>. The
following example configuration shows the assignment.</p><p>Leader Migration can be enabled without a configuration. Please see
<a href="#default-configuration">Default Configuration</a> for details.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>LeaderMigrationConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>controllermanager.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>leaderName</span>:<span> </span>cloud-provider-extraction-migration<span>
</span></span></span><span><span><span></span><span>resourceLock</span>:<span> </span>leases<span>
</span></span></span><span><span><span></span><span>controllerLeaders</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>route<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>kube-controller-manager<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>service<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>kube-controller-manager<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cloud-node-lifecycle<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>kube-controller-manager<span>
</span></span></span></code></pre></div><p>Alternatively, because the controllers can run under either controller managers,
setting <code>component</code> to <code>*</code> for both sides makes the configuration file consistent
between both parties of the migration.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># wildcard version</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LeaderMigrationConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>controllermanager.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>leaderName</span>:<span> </span>cloud-provider-extraction-migration<span>
</span></span></span><span><span><span></span><span>resourceLock</span>:<span> </span>leases<span>
</span></span></span><span><span><span></span><span>controllerLeaders</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>route<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>*<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>service<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>*<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cloud-node-lifecycle<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>*<span>
</span></span></span></code></pre></div><p>On each control plane node, save the content to <code>/etc/leadermigration.conf</code>, and
update the manifest of <code>kube-controller-manager</code> so that the file is mounted inside
the container at the same location. Also, update the same manifest to add the
following arguments:</p><ul><li><code>--enable-leader-migration</code> to enable Leader Migration on the controller manager</li><li><code>--leader-migration-config=/etc/leadermigration.conf</code> to set configuration file</li></ul><p>Restart <code>kube-controller-manager</code> on each node. At this moment,
<code>kube-controller-manager</code> has leader migration enabled and is ready for the
migration.</p><h3 id="deploy-cloud-controller-manager">Deploy Cloud Controller Manager</h3><p>In version N + 1, the desired state of controller-to-manager assignment can be
represented by a new configuration file, shown as follows. Please note <code>component</code>
field of each <code>controllerLeaders</code> changing from <code>kube-controller-manager</code> to
<code>cloud-controller-manager</code>. Alternatively, use the wildcard version mentioned above,
which has the same effect.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>LeaderMigrationConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>controllermanager.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>leaderName</span>:<span> </span>cloud-provider-extraction-migration<span>
</span></span></span><span><span><span></span><span>resourceLock</span>:<span> </span>leases<span>
</span></span></span><span><span><span></span><span>controllerLeaders</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>route<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>service<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cloud-node-lifecycle<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>cloud-controller-manager<span>
</span></span></span></code></pre></div><p>When creating control plane nodes of version N + 1, the content should be deployed to
<code>/etc/leadermigration.conf</code>. The manifest of <code>cloud-controller-manager</code> should be
updated to mount the configuration file in the same manner as
<code>kube-controller-manager</code> of version N. Similarly, add <code>--enable-leader-migration</code>
and <code>--leader-migration-config=/etc/leadermigration.conf</code> to the arguments of
<code>cloud-controller-manager</code>.</p><p>Create a new control plane node of version N + 1 with the updated
<code>cloud-controller-manager</code> manifest, and with the <code>--cloud-provider</code> flag set to
<code>external</code> for <code>kube-controller-manager</code>. <code>kube-controller-manager</code> of version N + 1
MUST NOT have Leader Migration enabled because, with an external cloud provider, it
does not run the migrated controllers anymore, and thus it is not involved in the
migration.</p><p>Please refer to <a href="/docs/tasks/administer-cluster/running-cloud-controller/">Cloud Controller Manager Administration</a>
for more detail on how to deploy <code>cloud-controller-manager</code>.</p><h3 id="upgrade-control-plane">Upgrade Control Plane</h3><p>The control plane now contains nodes of both version N and N + 1. The nodes of
version N run <code>kube-controller-manager</code> only, and these of version N + 1 run both
<code>kube-controller-manager</code> and <code>cloud-controller-manager</code>. The migrated controllers,
as specified in the configuration, are running under either <code>kube-controller-manager</code>
of version N or <code>cloud-controller-manager</code> of version N + 1 depending on which
controller manager holds the migration lease. No controller will ever be running
under both controller managers at any time.</p><p>In a rolling manner, create a new control plane node of version N + 1 and bring down
one of version N until the control plane contains only nodes of version N + 1.
If a rollback from version N + 1 to N is required, add nodes of version N with Leader
Migration enabled for <code>kube-controller-manager</code> back to the control plane, replacing
one of version N + 1 each time until there are only nodes of version N.</p><h3 id="disable-leader-migration">(Optional) Disable Leader Migration</h3><p>Now that the control plane has been upgraded to run both <code>kube-controller-manager</code>
and <code>cloud-controller-manager</code> of version N + 1, Leader Migration has finished its
job and can be safely disabled to save one Lease resource. It is safe to re-enable
Leader Migration for the rollback in the future.</p><p>In a rolling manager, update manifest of <code>cloud-controller-manager</code> to unset both
<code>--enable-leader-migration</code> and <code>--leader-migration-config=</code> flag, also remove the
mount of <code>/etc/leadermigration.conf</code>, and finally remove <code>/etc/leadermigration.conf</code>.
To re-enable Leader Migration, recreate the configuration file and add its mount and
the flags that enable Leader Migration back to <code>cloud-controller-manager</code>.</p><h3 id="default-configuration">Default Configuration</h3><p>Starting Kubernetes 1.22, Leader Migration provides a default configuration suitable
for the default controller-to-manager assignment.
The default configuration can be enabled by setting <code>--enable-leader-migration</code> but
without <code>--leader-migration-config=</code>.</p><p>For <code>kube-controller-manager</code> and <code>cloud-controller-manager</code>, if there are no flags
that enable any in-tree cloud provider or change ownership of controllers, the
default configuration can be used to avoid manual creation of the configuration file.</p><h3 id="node-ipam-controller-migration">Special case: migrating the Node IPAM controller</h3><p>If your cloud provider provides an implementation of Node IPAM controller, you should
switch to the implementation in <code>cloud-controller-manager</code>. Disable Node IPAM
controller in <code>kube-controller-manager</code> of version N + 1 by adding
<code>--controllers=*,-nodeipam</code> to its flags. Then add <code>nodeipam</code> to the list of migrated
controllers.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># wildcard version, with nodeipam</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LeaderMigrationConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>controllermanager.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>leaderName</span>:<span> </span>cloud-provider-extraction-migration<span>
</span></span></span><span><span><span></span><span>resourceLock</span>:<span> </span>leases<span>
</span></span></span><span><span><span></span><span>controllerLeaders</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>route<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>*<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>service<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>*<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cloud-node-lifecycle<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>*<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nodeipam<span>
</span></span></span><span><span><span></span>- <span>  </span><span>component</span>:<span> </span>*<span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration">Controller Manager Leader Migration</a>
enhancement proposal.</li></ul></div></div><div><div class="td-content"><h1>Operating etcd clusters for Kubernetes</h1><p><p>etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p></p><p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a
<a href="/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">back up</a> plan
for the data.</p><p>You can find in-depth information about etcd in the official <a href="https://etcd.io/docs/">documentation</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>Before you follow steps in this page to deploy, manage, back up or restore etcd,
you need to understand the typical expectations for operating an etcd cluster.
Refer to the <a href="https://etcd.io/docs/">etcd documentation</a> for more context.</p><p>Key details include:</p><ul><li><p>The minimum recommended etcd versions to run in production are <code>3.4.22+</code> and <code>3.5.6+</code>.</p></li><li><p>etcd is a leader-based distributed system. Ensure that the leader
periodically send heartbeats on time to all followers to keep the cluster
stable.</p></li><li><p>You should run etcd as a cluster with an odd number of members.</p></li><li><p>Aim to ensure that no resource starvation occurs.</p><p>Performance and stability of the cluster is sensitive to network and disk
I/O. Any resource starvation can lead to heartbeat timeout, causing instability
of the cluster. An unstable etcd indicates that no leader is elected. Under
such circumstances, a cluster cannot make any changes to its current state,
which implies no new pods can be scheduled.</p></li></ul><h3 id="resource-requirements-for-etcd">Resource requirements for etcd</h3><p>Operating etcd with limited resources is suitable only for testing purposes.
For deploying in production, advanced hardware configuration is required.
Before deploying etcd in production, see
<a href="https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations">resource requirement reference</a>.</p><p>Keeping etcd clusters stable is critical to the stability of Kubernetes
clusters. Therefore, run etcd clusters on dedicated machines or isolated
environments for <a href="https://etcd.io/docs/current/op-guide/hardware/">guaranteed resource requirements</a>.</p><h3 id="tools">Tools</h3><p>Depending on which specific outcome you're working on, you will need the <code>etcdctl</code> tool or the
<code>etcdutl</code> tool (you may need both).</p><h2 id="understanding-etcdctl-and-etcdutl">Understanding etcdctl and etcdutl</h2><p><code>etcdctl</code> and <code>etcdutl</code> are command-line tools used to interact with etcd clusters, but they serve different purposes:</p><ul><li><p><code>etcdctl</code>: This is the primary command-line client for interacting with etcd over a
network. It is used for day-to-day operations such as managing keys and values,
administering the cluster, checking health, and more.</p></li><li><p><code>etcdutl</code>: This is an administration utility designed to operate directly on etcd data
files, including migrating data between etcd versions, defragmenting the database,
restoring snapshots, and validating data consistency. For network operations, <code>etcdctl</code>
should be used.</p></li></ul><p>For more information on <code>etcdutl</code>, you can refer to the <a href="https://etcd.io/docs/v3.5/op-guide/recovery/">etcd recovery documentation</a>.</p><h2 id="starting-etcd-clusters">Starting etcd clusters</h2><p>This section covers starting a single-node and multi-node etcd cluster.</p><p>This guide assumes that <code>etcd</code> is already installed.</p><h3 id="single-node-etcd-cluster">Single-node etcd cluster</h3><p>Use a single-node etcd cluster only for testing purposes.</p><ol><li><p>Run the following:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>etcd --listen-client-urls<span>=</span>http://<span>$PRIVATE_IP</span>:2379 <span>\
</span></span></span><span><span><span></span>   --advertise-client-urls<span>=</span>http://<span>$PRIVATE_IP</span>:2379
</span></span></code></pre></div></li><li><p>Start the Kubernetes API server with the flag
<code>--etcd-servers=$PRIVATE_IP:2379</code>.</p><p>Make sure <code>PRIVATE_IP</code> is set to your etcd client IP.</p></li></ol><h3 id="multi-node-etcd-cluster">Multi-node etcd cluster</h3><p>For durability and high availability, run etcd as a multi-node cluster in
production and back it up periodically. A five-member cluster is recommended
in production. For more information, see
<a href="https://etcd.io/docs/current/faq/#what-is-failure-tolerance">FAQ documentation</a>.</p><p>As you're using Kubernetes, you have the option to run etcd as a container inside
one or more Pods. The <code>kubeadm</code> tool sets up etcd
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static pods</a> by default, or
you can deploy a
<a href="/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">separate cluster</a>
and instruct kubeadm to use that etcd cluster as the control plane's backing store.</p><p>You configure an etcd cluster either by static member information or by dynamic
discovery. For more information on clustering, see
<a href="https://etcd.io/docs/current/op-guide/clustering/">etcd clustering documentation</a>.</p><p>For an example, consider a five-member etcd cluster running with the following
client URLs: <code>http://$IP1:2379</code>, <code>http://$IP2:2379</code>, <code>http://$IP3:2379</code>,
<code>http://$IP4:2379</code>, and <code>http://$IP5:2379</code>. To start a Kubernetes API server:</p><ol><li><p>Run the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>etcd --listen-client-urls<span>=</span>http://<span>$IP1</span>:2379,http://<span>$IP2</span>:2379,http://<span>$IP3</span>:2379,http://<span>$IP4</span>:2379,http://<span>$IP5</span>:2379 --advertise-client-urls<span>=</span>http://<span>$IP1</span>:2379,http://<span>$IP2</span>:2379,http://<span>$IP3</span>:2379,http://<span>$IP4</span>:2379,http://<span>$IP5</span>:2379
</span></span></code></pre></div></li><li><p>Start the Kubernetes API servers with the flag
<code>--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379</code>.</p><p>Make sure the <code>IP&lt;n&gt;</code> variables are set to your client IP addresses.</p></li></ol><h3 id="multi-node-etcd-cluster-with-load-balancer">Multi-node etcd cluster with load balancer</h3><p>To run a load balancing etcd cluster:</p><ol><li>Set up an etcd cluster.</li><li>Configure a load balancer in front of the etcd cluster.
For example, let the address of the load balancer be <code>$LB</code>.</li><li>Start Kubernetes API Servers with the flag <code>--etcd-servers=$LB:2379</code>.</li></ol><h2 id="securing-etcd-clusters">Securing etcd clusters</h2><p>Access to etcd is equivalent to root permission in the cluster so ideally only
the API server should have access to it. Considering the sensitivity of the
data, it is recommended to grant permission to only those nodes that require
access to etcd clusters.</p><p>To secure etcd, either set up firewall rules or use the security features
provided by etcd. etcd security features depend on x509 Public Key
Infrastructure (PKI). To begin, establish secure communication channels by
generating a key and certificate pair. For example, use key pairs <code>peer.key</code>
and <code>peer.cert</code> for securing communication between etcd members, and
<code>client.key</code> and <code>client.cert</code> for securing communication between etcd and its
clients. See the <a href="https://github.com/coreos/etcd/tree/master/hack/tls-setup">example scripts</a>
provided by the etcd project to generate key pairs and CA files for client
authentication.</p><h3 id="securing-communication">Securing communication</h3><p>To configure etcd with secure peer communication, specify flags
<code>--peer-key-file=peer.key</code> and <code>--peer-cert-file=peer.cert</code>, and use HTTPS as
the URL schema.</p><p>Similarly, to configure etcd with secure client communication, specify flags
<code>--key=k8sclient.key</code> and <code>--cert=k8sclient.cert</code>, and use HTTPS as
the URL schema. Here is an example on a client command that uses secure
communication:</p><pre tabindex="0"><code>ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
</code></pre><h3 id="limiting-access-of-etcd-clusters">Limiting access of etcd clusters</h3><p>After configuring secure communication, restrict the access of the etcd cluster to
only the Kubernetes API servers using TLS authentication.</p><p>For example, consider key pairs <code>k8sclient.key</code> and <code>k8sclient.cert</code> that are
trusted by the CA <code>etcd.ca</code>. When etcd is configured with <code>--client-cert-auth</code>
along with TLS, it verifies the certificates from clients by using system CAs
or the CA passed in by <code>--trusted-ca-file</code> flag. Specifying flags
<code>--client-cert-auth=true</code> and <code>--trusted-ca-file=etcd.ca</code> will restrict the
access to clients with the certificate <code>k8sclient.cert</code>.</p><p>Once etcd is configured correctly, only clients with valid certificates can
access it. To give Kubernetes API servers the access, configure them with the
flags <code>--etcd-certfile=k8sclient.cert</code>, <code>--etcd-keyfile=k8sclient.key</code> and
<code>--etcd-cafile=ca.cert</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>etcd authentication is not planned for Kubernetes.</div><h2 id="replacing-a-failed-etcd-member">Replacing a failed etcd member</h2><p>etcd cluster achieves high availability by tolerating minor member failures.
However, to improve the overall health of the cluster, replace failed members
immediately. When multiple members fail, replace them one by one. Replacing a
failed member involves two steps: removing the failed member and adding a new
member.</p><p>Though etcd keeps unique member IDs internally, it is recommended to use a
unique name for each member to avoid human errors. For example, consider a
three-member etcd cluster. Let the URLs be, <code>member1=http://10.0.0.1</code>,
<code>member2=http://10.0.0.2</code>, and <code>member3=http://10.0.0.3</code>. When <code>member1</code> fails,
replace it with <code>member4=http://10.0.0.4</code>.</p><ol><li><p>Get the member ID of the failed <code>member1</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>etcdctl --endpoints<span>=</span>http://10.0.0.2,http://10.0.0.3 member list
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
</span></span></span><span><span><span>91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
</span></span></span><span><span><span>fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</span></span></span></code></pre></div></li><li><p>Do either of the following:</p><ol><li>If each Kubernetes API server is configured to communicate with all etcd
members, remove the failed member from the <code>--etcd-servers</code> flag, then
restart each Kubernetes API server.</li><li>If each Kubernetes API server communicates with a single etcd member,
then stop the Kubernetes API server that communicates with the failed
etcd.</li></ol></li><li><p>Stop the etcd server on the broken node. It is possible that other
clients besides the Kubernetes API server are causing traffic to etcd
and it is desirable to stop all traffic to prevent writes to the data
directory.</p></li><li><p>Remove the failed member:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>etcdctl member remove 8211f1d0f64f3269
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Removed member 8211f1d0f64f3269 from cluster
</span></span></span></code></pre></div></li><li><p>Add the new member:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>etcdctl member add member4 --peer-urls<span>=</span>http://10.0.0.4:2380
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</span></span></span></code></pre></div></li><li><p>Start the newly added member on a machine with the IP <code>10.0.0.4</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>ETCD_NAME</span><span>=</span><span>"member4"</span>
</span></span><span><span><span>export</span> <span>ETCD_INITIAL_CLUSTER</span><span>=</span><span>"member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"</span>
</span></span><span><span><span>export</span> <span>ETCD_INITIAL_CLUSTER_STATE</span><span>=</span>existing
</span></span><span><span>etcd <span>[</span>flags<span>]</span>
</span></span></code></pre></div></li><li><p>Do either of the following:</p><ol><li>If each Kubernetes API server is configured to communicate with all etcd
members, add the newly added member to the <code>--etcd-servers</code> flag, then
restart each Kubernetes API server.</li><li>If each Kubernetes API server communicates with a single etcd member,
start the Kubernetes API server that was stopped in step 2. Then
configure Kubernetes API server clients to again route requests to the
Kubernetes API server that was stopped. This can often be done by
configuring a load balancer.</li></ol></li></ol><p>For more information on cluster reconfiguration, see
<a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd reconfiguration documentation</a>.</p><h2 id="backing-up-an-etcd-cluster">Backing up an etcd cluster</h2><p>All Kubernetes objects are stored in etcd. Periodically backing up the etcd
cluster data is important to recover Kubernetes clusters under disaster
scenarios, such as losing all control plane nodes. The snapshot file contains
all the Kubernetes state and critical information. In order to keep the
sensitive Kubernetes data safe, encrypt the snapshot files.</p><p>Backing up an etcd cluster can be accomplished in two ways: etcd built-in
snapshot and volume snapshot.</p><h3 id="built-in-snapshot">Built-in snapshot</h3><p>etcd supports built-in snapshot. A snapshot may either be created from a live
member with the <code>etcdctl snapshot save</code> command or by copying the
<code>member/snap/db</code> file from an etcd
<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">data directory</a>
that is not currently used by an etcd process. Creating the snapshot will
not affect the performance of the member.</p><p>Below is an example for creating a snapshot of the keyspace served by
<code>$ENDPOINT</code> to the file <code>snapshot.db</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl --endpoints <span>$ENDPOINT</span> snapshot save snapshot.db
</span></span></code></pre></div><p>Verify the snapshot:</p><ul class="nav nav-tabs" id="etcd-verify-snapshot"><li class="nav-item"><a class="nav-link active" href="#etcd-verify-snapshot-0">Use etcdutl</a></li><li class="nav-item"><a class="nav-link" href="#etcd-verify-snapshot-1">Use etcdctl (Deprecated)</a></li></ul><div class="tab-content" id="etcd-verify-snapshot"><div id="etcd-verify-snapshot-0" class="tab-pane show active"><p><p>The below example depicts the usage of the <code>etcdutl</code> tool for verifying a snapshot:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>etcdutl --write-out<span>=</span>table snapshot status snapshot.db 
</span></span></code></pre></div><p>This should generate an output resembling the example provided below:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>+----------+----------+------------+------------+
</span></span></span><span><span><span>|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span></span><span><span><span>+----------+----------+------------+------------+
</span></span></span><span><span><span>| fe01cf57 |       10 |          7 | 2.1 MB     |
</span></span></span><span><span><span>+----------+----------+------------+------------+
</span></span></span></code></pre></div></p></div><div id="etcd-verify-snapshot-1" class="tab-pane"><p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The usage of <code>etcdctl snapshot status</code> has been <strong>deprecated</strong> since etcd v3.5.x and is slated for removal from etcd v3.6.
It is recommended to utilize <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a> instead.</div><p>The below example depicts the usage of the <code>etcdctl</code> tool for verifying a snapshot:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>ETCDCTL_API</span><span>=</span><span>3</span>
</span></span><span><span>etcdctl --write-out<span>=</span>table snapshot status snapshot.db
</span></span></code></pre></div><p>This should generate an output resembling the example provided below:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Deprecated: Use `etcdutl snapshot status` instead.
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>+----------+----------+------------+------------+
</span></span></span><span><span><span>|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span></span><span><span><span>+----------+----------+------------+------------+
</span></span></span><span><span><span>| fe01cf57 |       10 |          7 | 2.1 MB     |
</span></span></span><span><span><span>+----------+----------+------------+------------+
</span></span></span></code></pre></div></p></div></div><h3 id="volume-snapshot">Volume snapshot</h3><p>If etcd is running on a storage volume that supports backup, such as Amazon
Elastic Block Store, back up etcd data by creating a snapshot of the storage
volume.</p><h3 id="snapshot-using-etcdctl-options">Snapshot using etcdctl options</h3><p>We can also create the snapshot using various options given by etcdctl. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl -h 
</span></span></code></pre></div><p>will list various options available from etcdctl. For example, you can create a snapshot by specifying
the endpoint, certificates and key as shown below:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl --endpoints<span>=</span>https://127.0.0.1:2379 <span>\
</span></span></span><span><span><span></span>  --cacert<span>=</span>&lt;trusted-ca-file&gt; --cert<span>=</span>&lt;cert-file&gt; --key<span>=</span>&lt;key-file&gt; <span>\
</span></span></span><span><span><span></span>  snapshot save &lt;backup-file-location&gt;
</span></span></code></pre></div><p>where <code>trusted-ca-file</code>, <code>cert-file</code> and <code>key-file</code> can be obtained from the description of the etcd Pod.</p><h2 id="scaling-out-etcd-clusters">Scaling out etcd clusters</h2><p>Scaling out etcd clusters increases availability by trading off performance.
Scaling does not increase cluster performance nor capability. A general rule
is not to scale out or in etcd clusters. Do not configure any auto scaling
groups for etcd clusters. It is strongly recommended to always run a static
five-member etcd cluster for production Kubernetes clusters at any officially
supported scale.</p><p>A reasonable scaling is to upgrade a three-member cluster to a five-member
one, when more reliability is desired. See
<a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd reconfiguration documentation</a>
for information on how to add members into an existing cluster.</p><h2 id="restoring-an-etcd-cluster">Restoring an etcd cluster</h2><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>If any API servers are running in your cluster, you should not attempt to
restore instances of etcd. Instead, follow these steps to restore etcd:</p><ul><li>stop <em>all</em> API server instances</li><li>restore state in all etcd instances</li><li>restart all API server instances</li></ul><p>The Kubernetes project also recommends restarting Kubernetes components (<code>kube-scheduler</code>,
<code>kube-controller-manager</code>, <code>kubelet</code>) to ensure that they don't rely on some
stale data. In practice the restore takes a bit of time. During the
restoration, critical components will lose leader lock and restart themselves.</p></div><p>etcd supports restoring from snapshots that are taken from an etcd process of
the <a href="https://semver.org/">major.minor</a> version. Restoring a version from a
different patch version of etcd is also supported. A restore operation is
employed to recover the data of a failed cluster.</p><p>Before starting the restore operation, a snapshot file must be present. It can
either be a snapshot file from a previous backup operation, or from a remaining
<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">data directory</a>.</p><ul class="nav nav-tabs" id="etcd-restore"><li class="nav-item"><a class="nav-link active" href="#etcd-restore-0">Use etcdutl</a></li><li class="nav-item"><a class="nav-link" href="#etcd-restore-1">Use etcdctl (Deprecated)</a></li></ul><div class="tab-content" id="etcd-restore"><div id="etcd-restore-0" class="tab-pane show active"><p><p>When restoring the cluster using <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a>,
use the <code>--data-dir</code> option to specify to which folder the cluster should be restored:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>etcdutl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</span></span></code></pre></div><p>where <code>&lt;data-dir-location&gt;</code> is a directory that will be created during the restore process.</p></p></div><div id="etcd-restore-1" class="tab-pane"><p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The usage of <code>etcdctl</code> for restoring has been <strong>deprecated</strong> since etcd v3.5.x and is slated for removal from etcd v3.6.
It is recommended to utilize <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a> instead.</div><p>The below example depicts the usage of the <code>etcdctl</code> tool for the restore operation:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>ETCDCTL_API</span><span>=</span><span>3</span>
</span></span><span><span>etcdctl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</span></span></code></pre></div><p>If <code>&lt;data-dir-location&gt;</code> is the same folder as before, delete it and stop the etcd process before restoring the cluster.
Otherwise, change etcd configuration and restart the etcd process after restoration to have it use the new data directory:
first change <code>/etc/kubernetes/manifests/etcd.yaml</code>'s <code>volumes.hostPath.path</code> for <code>name: etcd-data</code> to <code>&lt;data-dir-location&gt;</code>,
then execute <code>kubectl -n kube-system delete pod &lt;name-of-etcd-pod&gt;</code> or <code>systemctl restart kubelet.service</code> (or both).</p></p></div></div><p>For more information and examples on restoring a cluster from a snapshot file, see
<a href="https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster">etcd disaster recovery documentation</a>.</p><p>If the access URLs of the restored cluster are changed from the previous
cluster, the Kubernetes API server must be reconfigured accordingly. In this
case, restart Kubernetes API servers with the flag
<code>--etcd-servers=$NEW_ETCD_CLUSTER</code> instead of the flag
<code>--etcd-servers=$OLD_ETCD_CLUSTER</code>. Replace <code>$NEW_ETCD_CLUSTER</code> and
<code>$OLD_ETCD_CLUSTER</code> with the respective IP addresses. If a load balancer is
used in front of an etcd cluster, you might need to update the load balancer
instead.</p><p>If the majority of etcd members have permanently failed, the etcd cluster is
considered failed. In this scenario, Kubernetes cannot make any changes to its
current state. Although the scheduled pods might continue to run, no new pods
can be scheduled. In such cases, recover the etcd cluster and potentially
reconfigure Kubernetes API servers to fix the issue.</p><h2 id="upgrading-etcd-clusters">Upgrading etcd clusters</h2><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Before you start an upgrade, back up your etcd cluster first.</div><p>For details on etcd upgrade, refer to the <a href="https://etcd.io/docs/latest/upgrades/">etcd upgrades</a> documentation.</p><h2 id="maintaining-etcd-clusters">Maintaining etcd clusters</h2><p>For more details on etcd maintenance, please refer to the <a href="https://etcd.io/docs/latest/op-guide/maintenance/">etcd maintenance</a> documentation.</p><h3 id="cluster-defragmentation">Cluster defragmentation</h3><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>Defragmentation is an expensive operation, so it should be executed as infrequently
as possible. On the other hand, it's also necessary to make sure any etcd member
will not exceed the storage quota. The Kubernetes project recommends that when
you perform defragmentation, you use a tool such as <a href="https://github.com/ahrtr/etcd-defrag">etcd-defrag</a>.</p><p>You can also run the defragmentation tool as a Kubernetes CronJob, to make sure that
defragmentation happens regularly. See <a href="https://github.com/ahrtr/etcd-defrag/blob/main/doc/etcd-defrag-cronjob.yaml"><code>etcd-defrag-cronjob.yaml</code></a>
for details.</p></div></div><div><div class="td-content"><h1>Reserve Compute Resources for System Daemons</h1><p>Kubernetes nodes can be scheduled to <code>Capacity</code>. Pods can consume all the
available capacity on a node by default. This is an issue because nodes
typically run quite a few system daemons that power the OS and Kubernetes
itself. Unless resources are set aside for these system daemons, pods and system
daemons compete for resources and lead to resource starvation issues on the
node.</p><p>The <code>kubelet</code> exposes a feature named 'Node Allocatable' that helps to reserve
compute resources for system daemons. Kubernetes recommends cluster
administrators to configure 'Node Allocatable' based on their workload density
on each node.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You can configure below kubelet <a href="/docs/reference/config-api/kubelet-config.v1beta1/">configuration settings</a>
using the <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><h2 id="node-allocatable">Node Allocatable</h2><p><img alt="node capacity" src="/images/docs/node-capacity.svg"></p><p>'Allocatable' on a Kubernetes node is defined as the amount of compute resources
that are available for pods. The scheduler does not over-subscribe
'Allocatable'. 'CPU', 'memory' and 'ephemeral-storage' are supported as of now.</p><p>Node Allocatable is exposed as part of <code>v1.Node</code> object in the API and as part
of <code>kubectl describe node</code> in the CLI.</p><p>Resources can be reserved for two categories of system daemons in the <code>kubelet</code>.</p><h3 id="enabling-qos-and-pod-level-cgroups">Enabling QoS and Pod level cgroups</h3><p>To properly enforce node allocatable constraints on the node, you must
enable the new cgroup hierarchy via the <code>cgroupsPerQOS</code> setting. This setting is
enabled by default. When enabled, the <code>kubelet</code> will parent all end-user pods
under a cgroup hierarchy managed by the <code>kubelet</code>.</p><h3 id="configuring-a-cgroup-driver">Configuring a cgroup driver</h3><p>The <code>kubelet</code> supports manipulation of the cgroup hierarchy on
the host using a cgroup driver. The driver is configured via the <code>cgroupDriver</code> setting.</p><p>The supported values are the following:</p><ul><li><code>cgroupfs</code> is the default driver that performs direct manipulation of the
cgroup filesystem on the host in order to manage cgroup sandboxes.</li><li><code>systemd</code> is an alternative driver that manages cgroup sandboxes using
transient slices for resources that are supported by that init system.</li></ul><p>Depending on the configuration of the associated container runtime,
operators may have to choose a particular cgroup driver to ensure
proper system behavior. For example, if operators use the <code>systemd</code>
cgroup driver provided by the <code>containerd</code> runtime, the <code>kubelet</code> must
be configured to use the <code>systemd</code> cgroup driver.</p><h3 id="kube-reserved">Kube Reserved</h3><ul><li><strong>KubeletConfiguration Setting</strong>: <code>kubeReserved: {}</code>. Example value <code>{cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}</code></li><li><strong>KubeletConfiguration Setting</strong>: <code>kubeReservedCgroup: ""</code></li></ul><p><code>kubeReserved</code> is meant to capture resource reservation for kubernetes system
daemons like the <code>kubelet</code>, <code>container runtime</code>, etc.
It is not meant to reserve resources for system daemons that are run as pods.
<code>kubeReserved</code> is typically a function of <code>pod density</code> on the nodes.</p><p>In addition to <code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be
specified to reserve the specified number of process IDs for
kubernetes system daemons.</p><p>To optionally enforce <code>kubeReserved</code> on kubernetes system daemons, specify the parent
control group for kube daemons as the value for <code>kubeReservedCgroup</code> setting,
and <a href="#enforcing-node-allocatable">add <code>kube-reserved</code> to <code>enforceNodeAllocatable</code></a>.</p><p>It is recommended that the kubernetes system daemons are placed under a top
level control group (<code>runtime.slice</code> on systemd machines for example). Each
system daemon should ideally run within its own child control group. Refer to
<a href="https://git.k8s.io/design-proposals-archive/node/node-allocatable.md#recommended-cgroups-setup">the design proposal</a>
for more details on recommended control group hierarchy.</p><p>Note that Kubelet <strong>does not</strong> create <code>kubeReservedCgroup</code> if it doesn't
exist. The kubelet will fail to start if an invalid cgroup is specified. With <code>systemd</code>
cgroup driver, you should follow a specific pattern for the name of the cgroup you
define: the name should be the value you set for <code>kubeReservedCgroup</code>,
with <code>.slice</code> appended.</p><h3 id="system-reserved">System Reserved</h3><ul><li><strong>KubeletConfiguration Setting</strong>: <code>systemReserved: {}</code>. Example value <code>{cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}</code></li><li><strong>KubeletConfiguration Setting</strong>: <code>systemReservedCgroup: ""</code></li></ul><p><code>systemReserved</code> is meant to capture resource reservation for OS system daemons
like <code>sshd</code>, <code>udev</code>, etc. <code>systemReserved</code> should reserve <code>memory</code> for the
<code>kernel</code> too since <code>kernel</code> memory is not accounted to pods in Kubernetes at this time.
Reserving resources for user login sessions is also recommended (<code>user.slice</code> in
systemd world).</p><p>In addition to <code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be
specified to reserve the specified number of process IDs for OS system
daemons.</p><p>To optionally enforce <code>systemReserved</code> on system daemons, specify the parent
control group for OS system daemons as the value for <code>systemReservedCgroup</code> setting,
and <a href="#enforcing-node-allocatable">add <code>system-reserved</code> to <code>enforceNodeAllocatable</code></a>.</p><p>It is recommended that the OS system daemons are placed under a top level
control group (<code>system.slice</code> on systemd machines for example).</p><p>Note that <code>kubelet</code> <strong>does not</strong> create <code>systemReservedCgroup</code> if it doesn't
exist. <code>kubelet</code> will fail if an invalid cgroup is specified. With <code>systemd</code>
cgroup driver, you should follow a specific pattern for the name of the cgroup you
define: the name should be the value you set for <code>systemReservedCgroup</code>,
with <code>.slice</code> appended.</p><h3 id="explicitly-reserved-cpu-list">Explicitly Reserved CPU List</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p><strong>KubeletConfiguration Setting</strong>: <code>reservedSystemCPUs:</code>. Example value <code>0-3</code></p><p><code>reservedSystemCPUs</code> is meant to define an explicit CPU set for OS system daemons and
kubernetes system daemons. <code>reservedSystemCPUs</code> is for systems that do not intend to
define separate top level cgroups for OS system daemons and kubernetes system daemons
with regard to cpuset resource.
If the Kubelet <strong>does not</strong> have <code>kubeReservedCgroup</code> and <code>systemReservedCgroup</code>,
the explicit cpuset provided by <code>reservedSystemCPUs</code> will take precedence over the CPUs
defined by <code>kubeReservedCgroup</code> and <code>systemReservedCgroup</code> options.</p><p>This option is specifically designed for Telco/NFV use cases where uncontrolled
interrupts/timers may impact the workload performance. you can use this option
to define the explicit cpuset for the system/kubernetes daemons as well as the
interrupts/timers, so the rest CPUs on the system can be used exclusively for
workloads, with less impact from uncontrolled interrupts/timers. To move the
system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset
defined by this option, other mechanism outside Kubernetes should be used.
For example: in Centos, you can do this using the tuned toolset.</p><h3 id="eviction-thresholds">Eviction Thresholds</h3><p><strong>KubeletConfiguration Setting</strong>: <code>evictionHard: {memory.available: "100Mi", nodefs.available: "10%", nodefs.inodesFree: "5%", imagefs.available: "15%"}</code>. Example value: <code>{memory.available: "&lt;500Mi"}</code></p><p>Memory pressure at the node level leads to System OOMs which affects the entire
node and all pods running on it. Nodes can go offline temporarily until memory
has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet
provides <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">out of resource</a>
management. Evictions are
supported for <code>memory</code> and <code>ephemeral-storage</code> only. By reserving some memory via
<code>evictionHard</code> setting, the <code>kubelet</code> attempts to evict pods whenever memory
availability on the node drops below the reserved value. Hypothetically, if
system daemons did not exist on a node, pods cannot use more than <code>capacity - eviction-hard</code>. For this reason, resources reserved for evictions are not
available for pods.</p><h3 id="enforcing-node-allocatable">Enforcing Node Allocatable</h3><p><strong>KubeletConfiguration setting</strong>: <code>enforceNodeAllocatable: [pods]</code>. Example value: <code>[pods,system-reserved,kube-reserved]</code></p><p>The scheduler treats 'Allocatable' as the available <code>capacity</code> for pods.</p><p><code>kubelet</code> enforce 'Allocatable' across pods by default. Enforcement is performed
by evicting pods whenever the overall usage across all pods exceeds
'Allocatable'. More details on eviction policy can be found
on the <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">node pressure eviction</a>
page. This enforcement is controlled by
specifying <code>pods</code> value to the KubeletConfiguration setting <code>enforceNodeAllocatable</code>.</p><p>Optionally, <code>kubelet</code> can be made to enforce <code>kubeReserved</code> and
<code>systemReserved</code> by specifying <code>kube-reserved</code> &amp; <code>system-reserved</code> values in
the same setting. Note that to enforce <code>kubeReserved</code> or <code>systemReserved</code>,
<code>kubeReservedCgroup</code> or <code>systemReservedCgroup</code> needs to be specified
respectively.</p><h2 id="general-guidelines">General Guidelines</h2><p>System daemons are expected to be treated similar to
<a href="/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed">Guaranteed pods</a>.
System daemons can burst within their bounding control groups and this behavior needs
to be managed as part of kubernetes deployments. For example, <code>kubelet</code> should
have its own control group and share <code>kubeReserved</code> resources with the
container runtime. However, Kubelet cannot burst and use up all available Node
resources if <code>kubeReserved</code> is enforced.</p><p>Be extra careful while enforcing <code>systemReserved</code> reservation since it can lead
to critical system services being CPU starved, OOM killed, or unable
to fork on the node. The
recommendation is to enforce <code>systemReserved</code> only if a user has profiled their
nodes exhaustively to come up with precise estimates and is confident in their
ability to recover if any process in that group is oom-killed.</p><ul><li>To begin with enforce 'Allocatable' on <code>pods</code>.</li><li>Once adequate monitoring and alerting is in place to track kube system
daemons, attempt to enforce <code>kubeReserved</code> based on usage heuristics.</li><li>If absolutely necessary, enforce <code>systemReserved</code> over time.</li></ul><p>The resource requirements of kube system daemons may grow over time as more and
more features are added. Over time, kubernetes project will attempt to bring
down utilization of node system daemons, but that is not a priority as of now.
So expect a drop in <code>Allocatable</code> capacity in future releases.</p><h2 id="example-scenario">Example Scenario</h2><p>Here is an example to illustrate Node Allocatable computation:</p><ul><li>Node has <code>32Gi</code> of <code>memory</code>, <code>16 CPUs</code> and <code>100Gi</code> of <code>Storage</code></li><li><code>kubeReserved</code> is set to <code>{cpu: 1000m, memory: 2Gi, ephemeral-storage: 1Gi}</code></li><li><code>systemReserved</code> is set to <code>{cpu: 500m, memory: 1Gi, ephemeral-storage: 1Gi}</code></li><li><code>evictionHard</code> is set to <code>{memory.available: "&lt;500Mi", nodefs.available: "&lt;10%"}</code></li></ul><p>Under this scenario, 'Allocatable' will be 14.5 CPUs, 28.5Gi of memory and
<code>88Gi</code> of local storage.
Scheduler ensures that the total memory <code>requests</code> across all pods on this node does
not exceed 28.5Gi and storage doesn't exceed 88Gi.
Kubelet evicts pods whenever the overall memory usage across pods exceeds 28.5Gi,
or if overall disk usage exceeds 88Gi. If all processes on the node consume as
much CPU as they can, pods together cannot consume more than 14.5 CPUs.</p><p>If <code>kubeReserved</code> and/or <code>systemReserved</code> is not enforced and system daemons
exceed their reservation, <code>kubelet</code> evicts pods whenever the overall node memory
usage is higher than 31.5Gi or <code>storage</code> is greater than 90Gi.</p></div></div><div><div class="td-content"><h1>Running Kubernetes Node Components as a Non-root User</h1><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [alpha]</code></div><p>This document describes how to run Kubernetes Node components such as kubelet, CRI, OCI, and CNI
without root privileges, by using a <a class="glossary-tooltip" title="A Linux kernel feature to emulate superuser privilege for unprivileged users." href="https://man7.org/linux/man-pages/man7/user_namespaces.7.html" target="_blank">user namespace</a>.</p><p>This technique is also known as <em>rootless mode</em>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>This document describes how to run Kubernetes Node components (and hence pods) as a non-root user.</p><p>If you are just looking for how to run a pod as a non-root user, see <a href="/docs/tasks/configure-pod-container/security-context/">SecurityContext</a>.</p></div><h2 id="before-you-begin">Before you begin</h2><p>Your Kubernetes server must be at or later than version 1.22.</p><p>To check the version, enter <code>kubectl version</code>.</p><ul><li><a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">Enable Cgroup v2</a></li><li><a href="https://rootlesscontaine.rs/getting-started/common/login/">Enable systemd with user session</a></li><li><a href="https://rootlesscontaine.rs/getting-started/common/sysctl/">Configure several sysctl values, depending on host Linux distribution</a></li><li><a href="https://rootlesscontaine.rs/getting-started/common/subuid/">Ensure that your unprivileged user is listed in <code>/etc/subuid</code> and <code>/etc/subgid</code></a></li><li>Enable the <code>KubeletInUserNamespace</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a></li></ul><h2 id="running-kubernetes-inside-rootless-docker-podman">Running Kubernetes inside Rootless Docker/Podman</h2><h3 id="kind">kind</h3><p><a href="https://kind.sigs.k8s.io/">kind</a> supports running Kubernetes inside Rootless Docker or Rootless Podman.</p><p>See <a href="https://kind.sigs.k8s.io/docs/user/rootless/">Running kind with Rootless Docker</a>.</p><h3 id="minikube">minikube</h3><p><a href="https://minikube.sigs.k8s.io/">minikube</a> also supports running Kubernetes inside Rootless Docker or Rootless Podman.</p><p>See the Minikube documentation:</p><ul><li><a href="https://minikube.sigs.k8s.io/docs/drivers/docker/">Rootless Docker</a></li><li><a href="https://minikube.sigs.k8s.io/docs/drivers/podman/">Rootless Podman</a></li></ul><h2 id="running-kubernetes-inside-unprivileged-containers">Running Kubernetes inside Unprivileged Containers</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h3 id="sysbox">sysbox</h3><p><a href="https://github.com/nestybox/sysbox">Sysbox</a> is an open-source container runtime
(similar to "runc") that supports running system-level workloads such as Docker
and Kubernetes inside unprivileged containers isolated with the Linux user
namespace.</p><p>See <a href="https://github.com/nestybox/sysbox/blob/master/docs/quickstart/kind.md">Sysbox Quick Start Guide: Kubernetes-in-Docker</a> for more info.</p><p>Sysbox supports running Kubernetes inside unprivileged containers without
requiring Cgroup v2 and without the <code>KubeletInUserNamespace</code> feature gate. It
does this by exposing specially crafted <code>/proc</code> and <code>/sys</code> filesystems inside
the container plus several other advanced OS virtualization techniques.</p><h2 id="running-rootless-kubernetes-directly-on-a-host">Running Rootless Kubernetes directly on a host</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h3 id="k3s">K3s</h3><p><a href="https://k3s.io/">K3s</a> experimentally supports rootless mode.</p><p>See <a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">Running K3s with Rootless mode</a> for the usage.</p><h3 id="usernetes">Usernetes</h3><p><a href="https://github.com/rootless-containers/usernetes">Usernetes</a> is a reference distribution of Kubernetes that can be installed under <code>$HOME</code> directory without the root privilege.</p><p>Usernetes supports both containerd and CRI-O as CRI runtimes.
Usernetes supports multi-node clusters using Flannel (VXLAN).</p><p>See <a href="https://github.com/rootless-containers/usernetes">the Usernetes repo</a> for the usage.</p><h2 id="userns-the-hard-way">Manually deploy a node that runs the kubelet in a user namespace</h2><p>This section provides hints for running Kubernetes in a user namespace manually.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This section is intended to be read by developers of Kubernetes distributions, not by end users.</div><h3 id="creating-a-user-namespace">Creating a user namespace</h3><p>The first step is to create a <a class="glossary-tooltip" title="A Linux kernel feature to emulate superuser privilege for unprivileged users." href="https://man7.org/linux/man-pages/man7/user_namespaces.7.html" target="_blank">user namespace</a>.</p><p>If you are trying to run Kubernetes in a user-namespaced container such as
Rootless Docker/Podman or LXC/LXD, you are all set, and you can go to the next subsection.</p><p>Otherwise you have to create a user namespace by yourself, by calling <code>unshare(2)</code> with <code>CLONE_NEWUSER</code>.</p><p>A user namespace can be also unshared by using command line tools such as:</p><ul><li><a href="https://man7.org/linux/man-pages/man1/unshare.1.html"><code>unshare(1)</code></a></li><li><a href="https://github.com/rootless-containers/rootlesskit">RootlessKit</a></li><li><a href="https://github.com/giuseppe/become-root">become-root</a></li></ul><p>After unsharing the user namespace, you will also have to unshare other namespaces such as mount namespace.</p><p>You do <em>not</em> need to call <code>chroot()</code> nor <code>pivot_root()</code> after unsharing the mount namespace,
however, you have to mount writable filesystems on several directories <em>in</em> the namespace.</p><p>At least, the following directories need to be writable <em>in</em> the namespace (not <em>outside</em> the namespace):</p><ul><li><code>/etc</code></li><li><code>/run</code></li><li><code>/var/logs</code></li><li><code>/var/lib/kubelet</code></li><li><code>/var/lib/cni</code></li><li><code>/var/lib/containerd</code> (for containerd)</li><li><code>/var/lib/containers</code> (for CRI-O)</li></ul><h3 id="creating-a-delegated-cgroup-tree">Creating a delegated cgroup tree</h3><p>In addition to the user namespace, you also need to have a writable cgroup tree with cgroup v2.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes support for running Node components in user namespaces requires cgroup v2.
Cgroup v1 is not supported.</div><p>If you are trying to run Kubernetes in Rootless Docker/Podman or LXC/LXD on a systemd-based host, you are all set.</p><p>Otherwise you have to create a systemd unit with <code>Delegate=yes</code> property to delegate a cgroup tree with writable permission.</p><p>On your node, systemd must already be configured to allow delegation; for more details, see
<a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">cgroup v2</a> in the Rootless
Containers documentation.</p><h3 id="configuring-network">Configuring network</h3><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The network namespace of the Node components has to have a non-loopback interface, which can be for example configured with
<a href="https://github.com/rootless-containers/slirp4netns">slirp4netns</a>,
<a href="https://github.com/moby/vpnkit">VPNKit</a>, or
<a href="https://www.man7.org/linux/man-pages/man1/lxc-user-nic.1.html">lxc-user-nic(1)</a>.</p><p>The network namespaces of the Pods can be configured with regular CNI plugins.
For multi-node networking, Flannel (VXLAN, 8472/UDP) is known to work.</p><p>Ports such as the kubelet port (10250/TCP) and <code>NodePort</code> service ports have to be exposed from the Node network namespace to
the host with an external port forwarder, such as RootlessKit, slirp4netns, or
<a href="https://linux.die.net/man/1/socat">socat(1)</a>.</p><p>You can use the port forwarder from K3s.
See <a href="https://rancher.com/docs/k3s/latest/en/advanced/#known-issues-with-rootless-mode">Running K3s in Rootless Mode</a>
for more details.
The implementation can be found in <a href="https://github.com/k3s-io/k3s/blob/v1.22.3+k3s1/pkg/rootlessports/controller.go">the <code>pkg/rootlessports</code> package</a> of k3s.</p><h3 id="configuring-cri">Configuring CRI</h3><p>The kubelet relies on a container runtime. You should deploy a container runtime such as
containerd or CRI-O and ensure that it is running within the user namespace before the kubelet starts.</p><ul class="nav nav-tabs" id="cri"><li class="nav-item"><a class="nav-link active" href="#cri-0">containerd</a></li><li class="nav-item"><a class="nav-link" href="#cri-1">CRI-O</a></li></ul><div class="tab-content" id="cri"><div id="cri-0" class="tab-pane show active"><p><p>Running CRI plugin of containerd in a user namespace is supported since containerd 1.4.</p><p>Running containerd within a user namespace requires the following configurations.</p><div class="highlight"><pre tabindex="0"><code class="language-toml"><span><span>version = <span>2</span>
</span></span><span><span>
</span></span><span><span>[plugins.<span>"io.containerd.grpc.v1.cri"</span>]
</span></span><span><span><span># Disable AppArmor</span>
</span></span><span><span>  disable_apparmor = <span>true</span>
</span></span><span><span><span># Ignore an error during setting oom_score_adj</span>
</span></span><span><span>  restrict_oom_score_adj = <span>true</span>
</span></span><span><span><span># Disable hugetlb cgroup v2 controller (because systemd does not support delegating hugetlb controller)</span>
</span></span><span><span>  disable_hugetlb_controller = <span>true</span>
</span></span><span><span>
</span></span><span><span>[plugins.<span>"io.containerd.grpc.v1.cri"</span>.containerd]
</span></span><span><span><span># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
</span></span><span><span>  snapshotter = <span>"fuse-overlayfs"</span>
</span></span><span><span>
</span></span><span><span>[plugins.<span>"io.containerd.grpc.v1.cri"</span>.containerd.runtimes.runc.options]
</span></span><span><span><span># We use cgroupfs that is delegated by systemd, so we do not use SystemdCgroup driver</span>
</span></span><span><span><span># (unless you run another systemd in the namespace)</span>
</span></span><span><span>  SystemdCgroup = <span>false</span>
</span></span></code></pre></div><p>The default path of the configuration file is <code>/etc/containerd/config.toml</code>.
The path can be specified with <code>containerd -c /path/to/containerd/config.toml</code>.</p></p></div><div id="cri-1" class="tab-pane"><p><p>Running CRI-O in a user namespace is supported since CRI-O 1.22.</p><p>CRI-O requires an environment variable <code>_CRIO_ROOTLESS=1</code> to be set.</p><p>The following configurations are also recommended:</p><div class="highlight"><pre tabindex="0"><code class="language-toml"><span><span>[crio]
</span></span><span><span>  storage_driver = <span>"overlay"</span>
</span></span><span><span><span># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
</span></span><span><span>  storage_option = [<span>"overlay.mount_program=/usr/local/bin/fuse-overlayfs"</span>]
</span></span><span><span>
</span></span><span><span>[crio.runtime]
</span></span><span><span><span># We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver</span>
</span></span><span><span><span># (unless you run another systemd in the namespace)</span>
</span></span><span><span>  cgroup_manager = <span>"cgroupfs"</span>
</span></span></code></pre></div><p>The default path of the configuration file is <code>/etc/crio/crio.conf</code>.
The path can be specified with <code>crio --config /path/to/crio/crio.conf</code>.</p></p></div></div><h3 id="configuring-kubelet">Configuring kubelet</h3><p>Running kubelet in a user namespace requires the following configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>featureGates</span>:<span>
</span></span></span><span><span><span>  </span><span>KubeletInUserNamespace</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span></span><span># We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver</span><span>
</span></span></span><span><span><span></span><span># (unless you run another systemd in the namespace)</span><span>
</span></span></span><span><span><span></span><span>cgroupDriver</span>:<span> </span><span>"cgroupfs"</span><span>
</span></span></span></code></pre></div><p>When the <code>KubeletInUserNamespace</code> feature gate is enabled, the kubelet ignores errors
that may happen during setting the following sysctl values on the node.</p><ul><li><code>vm.overcommit_memory</code></li><li><code>vm.panic_on_oom</code></li><li><code>kernel.panic</code></li><li><code>kernel.panic_on_oops</code></li><li><code>kernel.keys.root_maxkeys</code></li><li><code>kernel.keys.root_maxbytes</code>.</li></ul><p>Within a user namespace, the kubelet also ignores any error raised from trying to open <code>/dev/kmsg</code>.
This feature gate also allows kube-proxy to ignore an error during setting <code>RLIMIT_NOFILE</code>.</p><p>The <code>KubeletInUserNamespace</code> feature gate was introduced in Kubernetes v1.22 with "alpha" status.</p><p>Running kubelet in a user namespace without using this feature gate is also possible
by mounting a specially crafted proc filesystem (as done by <a href="https://github.com/nestybox/sysbox">Sysbox</a>), but not officially supported.</p><h3 id="configuring-kube-proxy">Configuring kube-proxy</h3><p>Running kube-proxy in a user namespace requires the following configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubeproxy.config.k8s.io/v1alpha1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeProxyConfiguration<span>
</span></span></span><span><span><span></span><span>mode</span>:<span> </span><span>"iptables"</span><span> </span><span># or "userspace"</span><span>
</span></span></span><span><span><span></span><span>conntrack</span>:<span>
</span></span></span><span><span><span></span><span># Skip setting sysctl value "net.netfilter.nf_conntrack_max"</span><span>
</span></span></span><span><span><span>  </span><span>maxPerCore</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span></span><span># Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"</span><span>
</span></span></span><span><span><span>  </span><span>tcpEstablishedTimeout</span>:<span> </span>0s<span>
</span></span></span><span><span><span></span><span># Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"</span><span>
</span></span></span><span><span><span>  </span><span>tcpCloseWaitTimeout</span>:<span> </span>0s<span>
</span></span></span></code></pre></div><h2 id="caveats">Caveats</h2><ul><li><p>Most of "non-local" volume drivers such as <code>nfs</code> and <code>iscsi</code> do not work.
Local volumes like <code>local</code>, <code>hostPath</code>, <code>emptyDir</code>, <code>configMap</code>, <code>secret</code>, and <code>downwardAPI</code> are known to work.</p></li><li><p>Some CNI plugins may not work. Flannel (VXLAN) is known to work.</p></li></ul><p>For more on this, see the <a href="https://rootlesscontaine.rs/caveats/">Caveats and Future work</a> page
on the rootlesscontaine.rs website.</p><h2 id="see-also">See Also</h2><ul><li><a href="https://rootlesscontaine.rs/">rootlesscontaine.rs</a></li><li><a href="https://www.slideshare.net/AkihiroSuda/kubecon-na-2020-containerd-rootless-containers-2020">Rootless Containers 2020 (KubeCon NA 2020)</a></li><li><a href="https://kind.sigs.k8s.io/docs/user/rootless/">Running kind with Rootless Docker</a></li><li><a href="https://github.com/rootless-containers/usernetes">Usernetes</a></li><li><a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">Running K3s with rootless mode</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2033-kubelet-in-userns-aka-rootless">KEP-2033: Kubelet-in-UserNS (aka Rootless mode)</a></li></ul></div></div><div><div class="td-content"><h1>Safely Drain a Node</h1><p>This page shows how to safely drain a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a>,
optionally respecting the PodDisruptionBudget you have defined.</p><h2 id="before-you-begin">Before you begin</h2><p>This task assumes that you have met the following prerequisites:</p><ol><li>You do not require your applications to be highly available during the
node drain, or</li><li>You have read about the <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> concept,
and have <a href="/docs/tasks/run-application/configure-pdb/">configured PodDisruptionBudgets</a> for
applications that need them.</li></ol><h2 id="configure-poddisruptionbudget">(Optional) Configure a disruption budget</h2><p>To ensure that your workloads remain available during maintenance, you can
configure a <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>.</p><p>If availability is important for any applications that run or could run on the node(s)
that you are draining, <a href="/docs/tasks/run-application/configure-pdb/">configure a PodDisruptionBudgets</a>
first and then continue following this guide.</p><p>It is recommended to set <code>AlwaysAllow</code> <a href="/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a>
to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.
The default behavior is to wait for the application pods to become <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>
before the drain can proceed.</p><h2 id="use-kubectl-drain-to-remove-a-node-from-service">Use <code>kubectl drain</code> to remove a node from service</h2><p>You can use <code>kubectl drain</code> to safely evict all of your pods from a
node before you perform maintenance on the node (e.g. kernel upgrade,
hardware maintenance, etc.). Safe evictions allow the pod's containers
to <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">gracefully terminate</a>
and will respect the PodDisruptionBudgets you have specified.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>By default <code>kubectl drain</code> ignores certain system pods on the node
that cannot be killed; see
the <a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a>
documentation for more details.</div><p>When <code>kubectl drain</code> returns successfully, that indicates that all of
the pods (except the ones excluded as described in the previous paragraph)
have been safely evicted (respecting the desired graceful termination period,
and respecting the PodDisruptionBudget you have defined). It is then safe to
bring down the node by powering down its physical machine or, if running on a
cloud platform, deleting its virtual machine.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If any new Pods tolerate the <code>node.kubernetes.io/unschedulable</code> taint, then those Pods
might be scheduled to the node you have drained. Avoid tolerating that taint other than
for DaemonSets.</p><p>If you or another API user directly set the <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#nodename"><code>nodeName</code></a>
field for a Pod (bypassing the scheduler), then the Pod is bound to the specified node
and will run there, even though you have drained that node and marked it unschedulable.</p></div><p>First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster with</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes
</span></span></code></pre></div><p>Next, tell Kubernetes to drain the node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl drain --ignore-daemonsets &lt;node name&gt;
</span></span></code></pre></div><p>If there are pods managed by a DaemonSet, you will need to specify
<code>--ignore-daemonsets</code> with <code>kubectl</code> to successfully drain the node. The <code>kubectl drain</code> subcommand on its own does not actually drain
a node of its DaemonSet pods:
the DaemonSet controller (part of the control plane) immediately replaces missing Pods with
new equivalent Pods. The DaemonSet controller also creates Pods that ignore unschedulable
taints, which allows the new Pods to launch onto a node that you are draining.</p><p>Once it returns (without giving an error), you can power down the node
(or equivalently, if on a cloud platform, delete the virtual machine backing the node).
If you leave the node in the cluster during the maintenance operation, you need to run</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl uncordon &lt;node name&gt;
</span></span></code></pre></div><p>afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.</p><h2 id="draining-multiple-nodes-in-parallel">Draining multiple nodes in parallel</h2><p>The <code>kubectl drain</code> command should only be issued to a single node at a
time. However, you can run multiple <code>kubectl drain</code> commands for
different nodes in parallel, in different terminals or in the
background. Multiple drain commands running concurrently will still
respect the PodDisruptionBudget you specify.</p><p>For example, if you have a StatefulSet with three replicas and have
set a PodDisruptionBudget for that set specifying <code>minAvailable: 2</code>,
<code>kubectl drain</code> only evicts a pod from the StatefulSet if all three
replicas pods are <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>;
if then you issue multiple drain commands in parallel,
Kubernetes respects the PodDisruptionBudget and ensures that
only 1 (calculated as <code>replicas - minAvailable</code>) Pod is unavailable
at any given time. Any drains that would cause the number of <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>
replicas to fall below the specified budget are blocked.</p><h2 id="eviction-api">The Eviction API</h2><p>If you prefer not to use <a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a> (such as
to avoid calling to an external command, or to get finer control over the pod
eviction process), you can also programmatically cause evictions using the
eviction API.</p><p>For more information, see <a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated eviction</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow steps to protect your application by <a href="/docs/tasks/run-application/configure-pdb/">configuring a Pod Disruption Budget</a>.</li></ul></div></div><div><div class="td-content"><h1>Securing a Cluster</h1><p>This document covers topics related to protecting a cluster from accidental or malicious access
and provides recommendations on overall security.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></li></ul><h2 id="controlling-access-to-the-kubernetes-api">Controlling access to the Kubernetes API</h2><p>As Kubernetes is entirely API-driven, controlling and limiting who can access the cluster and what actions
they are allowed to perform is the first line of defense.</p><h3 id="use-transport-layer-security-tls-for-all-api-traffic">Use Transport Layer Security (TLS) for all API traffic</h3><p>Kubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the
majority of installation methods will allow the necessary certificates to be created and distributed to
the cluster components. Note that some components and installation methods may enable local ports over
HTTP and administrators should familiarize themselves with the settings of each component to identify
potentially unsecured traffic.</p><h3 id="api-authentication">API Authentication</h3><p>Choose an authentication mechanism for the API servers to use that matches the common access patterns
when you install a cluster. For instance, small, single-user clusters may wish to use a simple certificate
or static Bearer token approach. Larger clusters may wish to integrate an existing OIDC or LDAP server that
allow users to be subdivided into groups.</p><p>All API clients must be authenticated, even those that are part of the infrastructure like nodes,
proxies, the scheduler, and volume plugins. These clients are typically <a href="/docs/reference/access-authn-authz/service-accounts-admin/">service accounts</a> or use x509 client certificates, and they are created automatically at cluster startup or are setup as part of the cluster installation.</p><p>Consult the <a href="/docs/reference/access-authn-authz/authentication/">authentication reference document</a> for more information.</p><h3 id="api-authorization">API Authorization</h3><p>Once authenticated, every API call is also expected to pass an authorization check. Kubernetes ships
an integrated <a href="/docs/reference/access-authn-authz/rbac/">Role-Based Access Control (RBAC)</a> component that matches an incoming user or group to a
set of permissions bundled into roles. These permissions combine verbs (get, create, delete) with
resources (pods, services, nodes) and can be namespace-scoped or cluster-scoped. A set of out-of-the-box
roles are provided that offer reasonable default separation of responsibility depending on what
actions a client might want to perform. It is recommended that you use the
<a href="/docs/reference/access-authn-authz/node/">Node</a> and
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> authorizers together, in combination with the
<a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction</a> admission plugin.</p><p>As with authentication, simple and broad roles may be appropriate for smaller clusters, but as
more users interact with the cluster, it may become necessary to separate teams into separate
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespaces</a> with more limited roles.</p><p>With authorization, it is important to understand how updates on one object may cause actions in
other places. For instance, a user may not be able to create pods directly, but allowing them to
create a deployment, which creates pods on their behalf, will let them create those pods
indirectly. Likewise, deleting a node from the API will result in the pods scheduled to that node
being terminated and recreated on other nodes. The out-of-the box roles represent a balance
between flexibility and common use cases, but more limited roles should be carefully reviewed
to prevent accidental escalation. You can make roles specific to your use case if the out-of-box ones don't meet your needs.</p><p>Consult the <a href="/docs/reference/access-authn-authz/authorization/">authorization reference section</a> for more information.</p><h2 id="controlling-access-to-the-kubelet">Controlling access to the Kubelet</h2><p>Kubelets expose HTTPS endpoints which grant powerful control over the node and containers.
By default Kubelets allow unauthenticated access to this API.</p><p>Production clusters should enable Kubelet authentication and authorization.</p><p>Consult the <a href="/docs/reference/access-authn-authz/kubelet-authn-authz/">Kubelet authentication/authorization reference</a>
for more information.</p><h2 id="controlling-the-capabilities-of-a-workload-or-user-at-runtime">Controlling the capabilities of a workload or user at runtime</h2><p>Authorization in Kubernetes is intentionally high level, focused on coarse actions on resources.
More powerful controls exist as <strong>policies</strong> to limit by use case how those objects act on the
cluster, themselves, and other resources.</p><h3 id="limiting-resource-usage-on-a-cluster">Limiting resource usage on a cluster</h3><p><a href="/docs/concepts/policy/resource-quotas/">Resource quota</a> limits the number or capacity of
resources granted to a namespace. This is most often used to limit the amount of CPU, memory,
or persistent disk a namespace can allocate, but can also control how many pods, services, or
volumes exist in each namespace.</p><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Limit ranges</a> restrict the maximum or minimum size of some of the
resources above, to prevent users from requesting unreasonably high or low values for commonly
reserved resources like memory, or to provide default limits when none are specified.</p><h3 id="controlling-what-privileges-containers-run-with">Controlling what privileges containers run with</h3><p>A pod definition contains a <a href="/docs/tasks/configure-pod-container/security-context/">security context</a>
that allows it to request access to run as a specific Linux user on a node (like root),
access to run privileged or access the host network, and other controls that would otherwise
allow it to run unfettered on a hosting node.</p><p>You can configure <a href="/docs/concepts/security/pod-security-admission/">Pod security admission</a>
to enforce use of a particular <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standard</a>
in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>, or to detect breaches.</p><p>Generally, most application workloads need limited access to host resources so they can
successfully run as a root process (uid 0) without access to host information. However,
considering the privileges associated with the root user, you should write application
containers to run as a non-root user. Similarly, administrators who wish to prevent
client applications from escaping their containers should apply the <strong>Baseline</strong>
or <strong>Restricted</strong> Pod Security Standard.</p><h3 id="preventing-containers-from-loading-unwanted-kernel-modules">Preventing containers from loading unwanted kernel modules</h3><p>The Linux kernel automatically loads kernel modules from disk if needed in certain
circumstances, such as when a piece of hardware is attached or a filesystem is mounted. Of
particular relevance to Kubernetes, even unprivileged processes can cause certain
network-protocol-related kernel modules to be loaded, just by creating a socket of the
appropriate type. This may allow an attacker to exploit a security hole in a kernel module
that the administrator assumed was not in use.</p><p>To prevent specific modules from being automatically loaded, you can uninstall them from
the node, or add rules to block them. On most Linux distributions, you can do that by
creating a file such as <code>/etc/modprobe.d/kubernetes-blacklist.conf</code> with contents like:</p><pre tabindex="0"><code># DCCP is unlikely to be needed, has had multiple serious
# vulnerabilities, and is not well-maintained.
blacklist dccp

# SCTP is not used in most Kubernetes clusters, and has also had
# vulnerabilities in the past.
blacklist sctp
</code></pre><p>To block module loading more generically, you can use a Linux Security Module (such as
SELinux) to completely deny the <code>module_request</code> permission to containers, preventing the
kernel from loading modules for containers under any circumstances. (Pods would still be
able to use modules that had been loaded manually, or modules that were loaded by the
kernel on behalf of some more-privileged process.)</p><h3 id="restricting-network-access">Restricting network access</h3><p>The <a href="/docs/tasks/administer-cluster/declare-network-policy/">network policies</a> for a namespace
allows application authors to restrict which pods in other namespaces may access pods and ports
within their namespaces. Many of the supported <a href="/docs/concepts/cluster-administration/networking/">Kubernetes networking providers</a>
now respect network policy.</p><p>Quota and limit ranges can also be used to control whether users may request node ports or
load-balanced services, which on many clusters can control whether those users applications
are visible outside of the cluster.</p><p>Additional protections may be available that control network rules on a per-plugin or per-
environment basis, such as per-node firewalls, physically separating cluster nodes to
prevent cross talk, or advanced networking policy.</p><h3 id="restricting-cloud-metadata-api-access">Restricting cloud metadata API access</h3><p>Cloud platforms (AWS, Azure, GCE, etc.) often expose metadata services locally to instances.
By default these APIs are accessible by pods running on an instance and can contain cloud
credentials for that node, or provisioning data such as kubelet credentials. These credentials
can be used to escalate within the cluster or to other cloud services under the same account.</p><p>When running Kubernetes on a cloud platform, limit permissions given to instance credentials, use
<a href="/docs/tasks/administer-cluster/declare-network-policy/">network policies</a> to restrict pod access
to the metadata API, and avoid using provisioning data to deliver secrets.</p><h3 id="controlling-which-nodes-pods-may-access">Controlling which nodes pods may access</h3><p>By default, there are no restrictions on which nodes may run a pod. Kubernetes offers a
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">rich set of policies for controlling placement of pods onto nodes</a>
and the <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taint-based pod placement and eviction</a>
that are available to end users. For many clusters use of these policies to separate workloads
can be a convention that authors adopt or enforce via tooling.</p><p>As an administrator, a beta admission plugin <code>PodNodeSelector</code> can be used to force pods
within a namespace to default or require a specific node selector, and if end users cannot
alter namespaces, this can strongly limit the placement of all of the pods in a specific workload.</p><h2 id="protecting-cluster-components-from-compromise">Protecting cluster components from compromise</h2><p>This section describes some common patterns for protecting clusters from compromise.</p><h3 id="restrict-access-to-etcd">Restrict access to etcd</h3><p>Write access to the etcd backend for the API is equivalent to gaining root on the entire cluster,
and read access can be used to escalate fairly quickly. Administrators should always use strong
credentials from the API servers to their etcd server, such as mutual auth via TLS client certificates,
and it is often recommended to isolate the etcd servers behind a firewall that only the API servers
may access.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Allowing other components within the cluster to access the master etcd instance with
read or write access to the full keyspace is equivalent to granting cluster-admin access. Using
separate etcd instances for non-master components or using etcd ACLs to restrict read and write
access to a subset of the keyspace is strongly recommended.</div><h3 id="enable-audit-logging">Enable audit logging</h3><p>The <a href="/docs/tasks/debug/debug-cluster/audit/">audit logger</a> is a beta feature that records actions taken by the
API for later analysis in the event of a compromise. It is recommended to enable audit logging
and archive the audit file on a secure server.</p><h3 id="restrict-access-to-alpha-or-beta-features">Restrict access to alpha or beta features</h3><p>Alpha and beta Kubernetes features are in active development and may have limitations or bugs
that result in security vulnerabilities. Always assess the value an alpha or beta feature may
provide against the possible risk to your security posture. When in doubt, disable features you
do not use.</p><h3 id="rotate-infrastructure-credentials-frequently">Rotate infrastructure credentials frequently</h3><p>The shorter the lifetime of a secret or credential the harder it is for an attacker to make
use of that credential. Set short lifetimes on certificates and automate their rotation. Use
an authentication provider that can control how long issued tokens are available and use short
lifetimes where possible. If you use service-account tokens in external integrations, plan to
rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap
token used for setting up nodes should be revoked or its authorization removed.</p><h3 id="review-third-party-integrations-before-enabling-them">Review third party integrations before enabling them</h3><p>Many third party integrations to Kubernetes may alter the security profile of your cluster. When
enabling an integration, always review the permissions that an extension requests before granting
it access. For example, many security integrations may request access to view all secrets on
your cluster which is effectively making that component a cluster admin. When in doubt,
restrict the integration to functioning in a single namespace if possible.</p><p>Components that create pods may also be unexpectedly powerful if they can do so inside namespaces
like the <code>kube-system</code> namespace, because those pods can gain access to service account secrets
or run with elevated permissions if those service accounts are granted access to permissive
<a href="/docs/concepts/security/pod-security-policy/">PodSecurityPolicies</a>.</p><p>If you use <a href="/docs/concepts/security/pod-security-admission/">Pod Security admission</a> and allow
any component to create Pods within a namespace that permits privileged Pods, those Pods may
be able to escape their containers and use this widened access to elevate their privileges.</p><p>You should not allow untrusted components to create Pods in any system namespace (those with
names that start with <code>kube-</code>) nor in any namespace where that access grant allows the possibility
of privilege escalation.</p><h3 id="encrypt-secrets-at-rest">Encrypt secrets at rest</h3><p>In general, the etcd database will contain any information accessible via the Kubernetes API
and may grant an attacker significant visibility into the state of your cluster. Always encrypt
your backups using a well reviewed backup and encryption solution, and consider using full disk
encryption where possible.</p><p>Kubernetes supports optional <a href="/docs/tasks/administer-cluster/encrypt-data/">encryption at rest</a> for information in the Kubernetes API.
This lets you ensure that when Kubernetes stores data for objects (for example, <code>Secret</code> or
<code>ConfigMap</code> objects), the API server writes an encrypted representation of the object.
That encryption means that even someone who has access to etcd backup data is unable
to view the content of those objects.
In Kubernetes 1.34 you can also encrypt custom resources;
encryption-at-rest for extension APIs defined in CustomResourceDefinitions was added to
Kubernetes as part of the v1.26 release.</p><h3 id="receiving-alerts-for-security-updates-and-reporting-vulnerabilities">Receiving alerts for security updates and reporting vulnerabilities</h3><p>Join the <a href="https://groups.google.com/forum/#!forum/kubernetes-announce">kubernetes-announce</a>
group for emails about security announcements. See the
<a href="/docs/reference/issues-security/security/">security reporting</a>
page for more on how to report vulnerabilities.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/security/security-checklist/">Security Checklist</a> for additional information on Kubernetes security guidance.</li><li><a href="/docs/reference/node/seccomp/">Seccomp Node Reference</a></li></ul></div></div><div><div class="td-content"><h1>Set Kubelet Parameters Via A Configuration File</h1><h2 id="before-you-begin">Before you begin</h2><p>Some steps in this page use the <code>jq</code> tool. If you don't have <code>jq</code>, you can
install it via your operating system's software sources, or fetch it from
<a href="https://jqlang.github.io/jq/">https://jqlang.github.io/jq/</a>.</p><p>Some steps also involve installing <code>curl</code>, which can be installed via your
operating system's software sources.</p><p>A subset of the kubelet's configuration parameters may be
set via an on-disk config file, as a substitute for command-line flags.</p><p>Providing parameters via a config file is the recommended approach because
it simplifies node deployment and configuration management.</p><h2 id="create-the-config-file">Create the config file</h2><p>The subset of the kubelet's configuration that can be configured via a file
is defined by the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
struct.</p><p>The configuration file must be a JSON or YAML representation of the parameters
in this struct. Make sure the kubelet has read permissions on the file.</p><p>Here is an example of what this file might look like:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>address</span>:<span> </span><span>"192.168.0.8"</span><span>
</span></span></span><span><span><span></span><span>port</span>:<span> </span><span>20250</span><span>
</span></span></span><span><span><span></span><span>serializeImagePulls</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span></span><span>evictionHard</span>:<span>
</span></span></span><span><span><span>    </span><span>memory.available</span>:<span>  </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>    </span><span>nodefs.available</span>:<span>  </span><span>"10%"</span><span>
</span></span></span><span><span><span>    </span><span>nodefs.inodesFree</span>:<span> </span><span>"5%"</span><span>
</span></span></span><span><span><span>    </span><span>imagefs.available</span>:<span> </span><span>"15%"</span><span>
</span></span></span><span><span><span>    </span><span>imagefs.inodesFree</span>:<span> </span><span>"5%"</span><span>
</span></span></span></code></pre></div><p>In this example, the kubelet is configured with the following settings:</p><ol><li><p><code>address</code>: The kubelet will serve on IP address <code>192.168.0.8</code>.</p></li><li><p><code>port</code>: The kubelet will serve on port <code>20250</code>.</p></li><li><p><code>serializeImagePulls</code>: Image pulls will be done in parallel.</p></li><li><p><code>evictionHard</code>: The kubelet will evict Pods under one of the following conditions:</p><ul><li>When the node's available memory drops below 100MiB.</li><li>When the node's main filesystem's available space is less than 10%.</li><li>When the image filesystem's available space is less than 15%.</li><li>When more than 95% of the node's main filesystem's inodes are in use.</li></ul></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the example, by changing the default value of only one parameter for
evictionHard, the default values of other parameters will not be inherited and
will be set to zero. In order to provide custom values, you should provide all
the threshold values respectively.
Alternatively, you can set the MergeDefaultEvictionSettings to true in the kubelet
configuration file, if any parameter is changed then the other parameters will inherit
their default values instead of 0.</div><p>The <code>imagefs</code> is an optional filesystem that container runtimes use to store container
images and container writable layers.</p><h2 id="start-a-kubelet-process-configured-via-the-config-file">Start a kubelet process configured via the config file</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you use kubeadm to initialize your cluster, use the kubelet-config while creating your cluster with <code>kubeadm init</code>.
See <a href="/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">configuring kubelet using kubeadm</a> for details.</div><p>Start the kubelet with the <code>--config</code> flag set to the path of the kubelet's config file.
The kubelet will then load its config from this file.</p><p>Note that command line flags which target the same value as a config file will override that value.
This helps ensure backwards compatibility with the command-line API.</p><p>Note that relative file paths in the kubelet config file are resolved relative to the
location of the kubelet config file, whereas relative paths in command line flags are resolved
relative to the kubelet's current working directory.</p><p>Note that some default values differ between command-line flags and the kubelet config file.
If <code>--config</code> is provided and the values are not specified via the command line, the
defaults for the <code>KubeletConfiguration</code> version apply.
In the above example, this version is <code>kubelet.config.k8s.io/v1beta1</code>.</p><h2 id="kubelet-conf-d">Drop-in directory for kubelet configuration files</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code></div><p>You can specify a drop-in configuration directory for the kubelet. By default, the kubelet does not look
for drop-in configuration files anywhere - you must specify a path.
For example: <code>--config-dir=/etc/kubernetes/kubelet.conf.d</code></p><p>For Kubernetes v1.28 to v1.29, you can only specify <code>--config-dir</code> if you also set
the environment variable <code>KUBELET_CONFIG_DROPIN_DIR_ALPHA</code> for the kubelet process (the value
of that variable does not matter).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The suffix of a valid kubelet drop-in configuration file <strong>must</strong> be <code>.conf</code>. For instance: <code>99-kubelet-address.conf</code></div><p>The kubelet processes files in its config drop-in directory by sorting the <strong>entire file name</strong> alphanumerically.
For instance, <code>00-kubelet.conf</code> is processed first, and then overridden with a file named <code>01-kubelet.conf</code>.</p><p>These files may contain partial configurations but should not be invalid and must include type metadata, specifically <code>apiVersion</code> and <code>kind</code>.
Validation is only performed on the final resulting configuration structure stored internally in the kubelet.
This offers flexibility in managing and merging kubelet configurations from different sources while preventing undesirable configurations.
However, it is important to note that behavior varies based on the data type of the configuration fields.</p><p>Different data types in the kubelet configuration structure merge differently. See the
<a href="/docs/reference/node/kubelet-config-directory-merging/">reference document</a>
for more information.</p><h3 id="kubelet-configuration-merging-order">Kubelet configuration merging order</h3><p>On startup, the kubelet merges configuration from:</p><ul><li>Feature gates specified over the command line (lowest precedence).</li><li>The kubelet configuration.</li><li>Drop-in configuration files, according to sort order.</li><li>Command line arguments excluding feature gates (highest precedence).</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The config drop-in dir mechanism for the kubelet is similar but different from how the <code>kubeadm</code> tool allows you to patch configuration.
The <code>kubeadm</code> tool uses a specific <a href="/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches">patching strategy</a>
for its configuration, whereas the only patch strategy for kubelet configuration drop-in files is <code>replace</code>.
The kubelet determines the order of merges based on sorting the <strong>suffixes</strong> alphanumerically,
and replaces every field present in a higher priority file.</div><h2 id="viewing-the-kubelet-configuration">Viewing the kubelet configuration</h2><p>Since the configuration could now be spread over multiple files with this feature, if someone wants to inspect the final actuated configuration,
they can follow these steps to inspect the kubelet configuration:</p><ol><li><p>Start a proxy server using <a href="/docs/reference/kubectl/generated/kubectl_proxy/"><code>kubectl proxy</code></a> in your terminal.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl proxy
</span></span></code></pre></div><p>Which gives output like:</p><pre tabindex="0"><code class="language-none">Starting to serve on 127.0.0.1:8001
</code></pre></li><li><p>Open another terminal window and use <code>curl</code> to fetch the kubelet configuration.
Replace <code>&lt;node-name&gt;</code> with the actual name of your node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>curl -X GET http://127.0.0.1:8001/api/v1/nodes/&lt;node-name&gt;/proxy/configz | jq .
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kubeletconfig"</span>: {
</span></span><span><span>    <span>"enableServer"</span>: <span>true</span>,
</span></span><span><span>    <span>"staticPodPath"</span>: <span>"/var/run/kubernetes/static-pods"</span>,
</span></span><span><span>    <span>"syncFrequency"</span>: <span>"1m0s"</span>,
</span></span><span><span>    <span>"fileCheckFrequency"</span>: <span>"20s"</span>,
</span></span><span><span>    <span>"httpCheckFrequency"</span>: <span>"20s"</span>,
</span></span><span><span>    <span>"address"</span>: <span>"192.168.1.16"</span>,
</span></span><span><span>    <span>"port"</span>: <span>10250</span>,
</span></span><span><span>    <span>"readOnlyPort"</span>: <span>10255</span>,
</span></span><span><span>    <span>"tlsCertFile"</span>: <span>"/var/lib/kubelet/pki/kubelet.crt"</span>,
</span></span><span><span>    <span>"tlsPrivateKeyFile"</span>: <span>"/var/lib/kubelet/pki/kubelet.key"</span>,
</span></span><span><span>    <span>"rotateCertificates"</span>: <span>true</span>,
</span></span><span><span>    <span>"authentication"</span>: {
</span></span><span><span>      <span>"x509"</span>: {
</span></span><span><span>        <span>"clientCAFile"</span>: <span>"/var/run/kubernetes/client-ca.crt"</span>
</span></span><span><span>      },
</span></span><span><span>      <span>"webhook"</span>: {
</span></span><span><span>        <span>"enabled"</span>: <span>true</span>,
</span></span><span><span>        <span>"cacheTTL"</span>: <span>"2m0s"</span>
</span></span><span><span>      },
</span></span><span><span>      <span>"anonymous"</span>: {
</span></span><span><span>        <span>"enabled"</span>: <span>true</span>
</span></span><span><span>      }
</span></span><span><span>    },
</span></span><span><span>    <span>"authorization"</span>: {
</span></span><span><span>      <span>"mode"</span>: <span>"AlwaysAllow"</span>,
</span></span><span><span>      <span>"webhook"</span>: {
</span></span><span><span>        <span>"cacheAuthorizedTTL"</span>: <span>"5m0s"</span>,
</span></span><span><span>        <span>"cacheUnauthorizedTTL"</span>: <span>"30s"</span>
</span></span><span><span>      }
</span></span><span><span>    },
</span></span><span><span>    <span>"registryPullQPS"</span>: <span>5</span>,
</span></span><span><span>    <span>"registryBurst"</span>: <span>10</span>,
</span></span><span><span>    <span>"eventRecordQPS"</span>: <span>50</span>,
</span></span><span><span>    <span>"eventBurst"</span>: <span>100</span>,
</span></span><span><span>    <span>"enableDebuggingHandlers"</span>: <span>true</span>,
</span></span><span><span>    <span>"healthzPort"</span>: <span>10248</span>,
</span></span><span><span>    <span>"healthzBindAddress"</span>: <span>"127.0.0.1"</span>,
</span></span><span><span>    <span>"oomScoreAdj"</span>: <span>-999</span>,
</span></span><span><span>    <span>"clusterDomain"</span>: <span>"cluster.local"</span>,
</span></span><span><span>    <span>"clusterDNS"</span>: [
</span></span><span><span>      <span>"10.0.0.10"</span>
</span></span><span><span>    ],
</span></span><span><span>    <span>"streamingConnectionIdleTimeout"</span>: <span>"4h0m0s"</span>,
</span></span><span><span>    <span>"nodeStatusUpdateFrequency"</span>: <span>"10s"</span>,
</span></span><span><span>    <span>"nodeStatusReportFrequency"</span>: <span>"5m0s"</span>,
</span></span><span><span>    <span>"nodeLeaseDurationSeconds"</span>: <span>40</span>,
</span></span><span><span>    <span>"imageMinimumGCAge"</span>: <span>"2m0s"</span>,
</span></span><span><span>    <span>"imageMaximumGCAge"</span>: <span>"0s"</span>,
</span></span><span><span>    <span>"imageGCHighThresholdPercent"</span>: <span>85</span>,
</span></span><span><span>    <span>"imageGCLowThresholdPercent"</span>: <span>80</span>,
</span></span><span><span>    <span>"volumeStatsAggPeriod"</span>: <span>"1m0s"</span>,
</span></span><span><span>    <span>"cgroupsPerQOS"</span>: <span>true</span>,
</span></span><span><span>    <span>"cgroupDriver"</span>: <span>"systemd"</span>,
</span></span><span><span>    <span>"cpuManagerPolicy"</span>: <span>"none"</span>,
</span></span><span><span>    <span>"cpuManagerReconcilePeriod"</span>: <span>"10s"</span>,
</span></span><span><span>    <span>"memoryManagerPolicy"</span>: <span>"None"</span>,
</span></span><span><span>    <span>"topologyManagerPolicy"</span>: <span>"none"</span>,
</span></span><span><span>    <span>"topologyManagerScope"</span>: <span>"container"</span>,
</span></span><span><span>    <span>"runtimeRequestTimeout"</span>: <span>"2m0s"</span>,
</span></span><span><span>    <span>"hairpinMode"</span>: <span>"promiscuous-bridge"</span>,
</span></span><span><span>    <span>"maxPods"</span>: <span>110</span>,
</span></span><span><span>    <span>"podPidsLimit"</span>: <span>-1</span>,
</span></span><span><span>    <span>"resolvConf"</span>: <span>"/run/systemd/resolve/resolv.conf"</span>,
</span></span><span><span>    <span>"cpuCFSQuota"</span>: <span>true</span>,
</span></span><span><span>    <span>"cpuCFSQuotaPeriod"</span>: <span>"100ms"</span>,
</span></span><span><span>    <span>"nodeStatusMaxImages"</span>: <span>50</span>,
</span></span><span><span>    <span>"maxOpenFiles"</span>: <span>1000000</span>,
</span></span><span><span>    <span>"contentType"</span>: <span>"application/vnd.kubernetes.protobuf"</span>,
</span></span><span><span>    <span>"kubeAPIQPS"</span>: <span>50</span>,
</span></span><span><span>    <span>"kubeAPIBurst"</span>: <span>100</span>,
</span></span><span><span>    <span>"serializeImagePulls"</span>: <span>true</span>,
</span></span><span><span>    <span>"evictionHard"</span>: {
</span></span><span><span>      <span>"imagefs.available"</span>: <span>"15%"</span>,
</span></span><span><span>      <span>"memory.available"</span>: <span>"100Mi"</span>,
</span></span><span><span>      <span>"nodefs.available"</span>: <span>"10%"</span>,
</span></span><span><span>      <span>"nodefs.inodesFree"</span>: <span>"5%"</span>,
</span></span><span><span>      <span>"imagefs.inodesFree"</span>: <span>"5%"</span>
</span></span><span><span>    },
</span></span><span><span>    <span>"evictionPressureTransitionPeriod"</span>: <span>"1m0s"</span>,
</span></span><span><span>    <span>"mergeDefaultEvictionSettings"</span>: <span>false</span>,
</span></span><span><span>    <span>"enableControllerAttachDetach"</span>: <span>true</span>,
</span></span><span><span>    <span>"makeIPTablesUtilChains"</span>: <span>true</span>,
</span></span><span><span>    <span>"iptablesMasqueradeBit"</span>: <span>14</span>,
</span></span><span><span>    <span>"iptablesDropBit"</span>: <span>15</span>,
</span></span><span><span>    <span>"featureGates"</span>: {
</span></span><span><span>      <span>"AllAlpha"</span>: <span>false</span>
</span></span><span><span>    },
</span></span><span><span>    <span>"failSwapOn"</span>: <span>false</span>,
</span></span><span><span>    <span>"memorySwap"</span>: {},
</span></span><span><span>    <span>"containerLogMaxSize"</span>: <span>"10Mi"</span>,
</span></span><span><span>    <span>"containerLogMaxFiles"</span>: <span>5</span>,
</span></span><span><span>    <span>"configMapAndSecretChangeDetectionStrategy"</span>: <span>"Watch"</span>,
</span></span><span><span>    <span>"enforceNodeAllocatable"</span>: [
</span></span><span><span>      <span>"pods"</span>
</span></span><span><span>    ],
</span></span><span><span>    <span>"volumePluginDir"</span>: <span>"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"</span>,
</span></span><span><span>    <span>"logging"</span>: {
</span></span><span><span>      <span>"format"</span>: <span>"text"</span>,
</span></span><span><span>      <span>"flushFrequency"</span>: <span>"5s"</span>,
</span></span><span><span>      <span>"verbosity"</span>: <span>3</span>,
</span></span><span><span>      <span>"options"</span>: {
</span></span><span><span>        <span>"json"</span>: {
</span></span><span><span>          <span>"infoBufferSize"</span>: <span>"0"</span>
</span></span><span><span>        }
</span></span><span><span>      }
</span></span><span><span>    },
</span></span><span><span>    <span>"enableSystemLogHandler"</span>: <span>true</span>,
</span></span><span><span>    <span>"enableSystemLogQuery"</span>: <span>false</span>,
</span></span><span><span>    <span>"shutdownGracePeriod"</span>: <span>"0s"</span>,
</span></span><span><span>    <span>"shutdownGracePeriodCriticalPods"</span>: <span>"0s"</span>,
</span></span><span><span>    <span>"enableProfilingHandler"</span>: <span>true</span>,
</span></span><span><span>    <span>"enableDebugFlagsHandler"</span>: <span>true</span>,
</span></span><span><span>    <span>"seccompDefault"</span>: <span>false</span>,
</span></span><span><span>    <span>"memoryThrottlingFactor"</span>: <span>0.9</span>,
</span></span><span><span>    <span>"registerNode"</span>: <span>true</span>,
</span></span><span><span>    <span>"localStorageCapacityIsolation"</span>: <span>true</span>,
</span></span><span><span>    <span>"containerRuntimeEndpoint"</span>: <span>"unix:///var/run/crio/crio.sock"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about kubelet configuration by checking the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
reference.</li><li>Learn more about kubelet configuration merging in the
<a href="/docs/reference/node/kubelet-config-directory-merging/">reference document</a>.</li></ul></div></div><div><div class="td-content"><h1>Share a Cluster with Namespaces</h1><p>This page shows how to view, work in, and delete <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespaces</a>.
The page also shows how to use Kubernetes namespaces to subdivide your cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Have an <a href="/docs/setup/">existing Kubernetes cluster</a>.</li><li>You have a basic understanding of Kubernetes <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>,
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a>, and
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployments</a>.</li></ul><h2 id="viewing-namespaces">Viewing namespaces</h2><p>List the current namespaces in a cluster using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get namespaces
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME              STATUS   AGE
</span></span></span><span><span><span>default           Active   11d
</span></span></span><span><span><span>kube-node-lease   Active   11d
</span></span></span><span><span><span>kube-public       Active   11d
</span></span></span><span><span><span>kube-system       Active   11d
</span></span></span></code></pre></div><p>Kubernetes starts with four initial namespaces:</p><ul><li><code>default</code> The default namespace for objects with no other namespace</li><li><code>kube-node-lease</code> This namespace holds <a href="/docs/concepts/architecture/leases/">Lease</a> objects associated with each node. Node leases allow the kubelet to send <a href="/docs/concepts/architecture/nodes/#heartbeats">heartbeats</a> so that the control plane can detect node failure.</li><li><code>kube-public</code> This namespace is created automatically and is readable by all users
(including those not authenticated). This namespace is mostly reserved for cluster usage,
in case that some resources should be visible and readable publicly throughout the whole cluster.
The public aspect of this namespace is only a convention, not a requirement.</li><li><code>kube-system</code> The namespace for objects created by the Kubernetes system</li></ul><p>You can also get the summary of a specific namespace using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get namespaces &lt;name&gt;
</span></span></code></pre></div><p>Or you can get detailed information with:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe namespaces &lt;name&gt;
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Name:           default
</span></span></span><span><span><span>Labels:         &lt;none&gt;
</span></span></span><span><span><span>Annotations:    &lt;none&gt;
</span></span></span><span><span><span>Status:         Active
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>No resource quota.
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>Resource Limits
</span></span></span><span><span><span> Type       Resource    Min Max Default
</span></span></span><span><span><span> ----               --------    --- --- ---
</span></span></span><span><span><span> Container          cpu         -   -   100m
</span></span></span></code></pre></div><p>Note that these details show both resource quota (if present) as well as resource limit ranges.</p><p>Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators
to define <em>Hard</em> resource usage limits that a Namespace may consume.</p><p>A limit range defines min/max constraints on the amount of resources a single entity can consume in
a Namespace.</p><p>See <a href="https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md">Admission control: Limit Range</a></p><p>A namespace can be in one of two phases:</p><ul><li><code>Active</code> the namespace is in use</li><li><code>Terminating</code> the namespace is being deleted, and can not be used for new objects</li></ul><p>For more details, see <a href="/docs/reference/kubernetes-api/cluster-resources/namespace-v1/">Namespace</a>
in the API reference.</p><h2 id="creating-a-new-namespace">Creating a new namespace</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Avoid creating namespace with prefix <code>kube-</code>, since it is reserved for Kubernetes system namespaces.</div><p>Create a new YAML file called <code>my-namespace.yaml</code> with the contents:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Namespace<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>&lt;insert-namespace-name-here&gt;<span>
</span></span></span></code></pre></div><p>Then run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f ./my-namespace.yaml
</span></span></code></pre></div><p>Alternatively, you can create namespace using below command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace &lt;insert-namespace-name-here&gt;
</span></span></code></pre></div><p>The name of your namespace must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>There's an optional field <code>finalizers</code>, which allows observables to purge resources whenever the
namespace is deleted. Keep in mind that if you specify a nonexistent finalizer, the namespace will
be created but will get stuck in the <code>Terminating</code> state if the user tries to delete it.</p><p>More information on <code>finalizers</code> can be found in the namespace
<a href="https://git.k8s.io/design-proposals-archive/architecture/namespaces.md#finalizers">design doc</a>.</p><h2 id="deleting-a-namespace">Deleting a namespace</h2><p>Delete a namespace with</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespaces &lt;insert-some-namespace-name&gt;
</span></span></code></pre></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>This deletes <em>everything</em> under the namespace!</div><p>This delete is asynchronous, so for a time you will see the namespace in the <code>Terminating</code> state.</p><h2 id="subdividing-your-cluster-using-kubernetes-namespaces">Subdividing your cluster using Kubernetes namespaces</h2><p>By default, a Kubernetes cluster will instantiate a default namespace when provisioning the
cluster to hold the default set of Pods, Services, and Deployments used by the cluster.</p><p>Assuming you have a fresh cluster, you can introspect the available namespaces by doing the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get namespaces
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME      STATUS    AGE
</span></span></span><span><span><span>default   Active    13m
</span></span></span></code></pre></div><h3 id="create-new-namespaces">Create new namespaces</h3><p>For this exercise, we will create two additional Kubernetes namespaces to hold our content.</p><p>In a scenario where an organization is using a shared Kubernetes cluster for development and
production use cases:</p><ul><li><p>The development team would like to maintain a space in the cluster where they can get a view on
the list of Pods, Services, and Deployments they use to build and run their application.
In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify
resources are relaxed to enable agile development.</p></li><li><p>The operations team would like to maintain a space in the cluster where they can enforce strict
procedures on who can or cannot manipulate the set of Pods, Services, and Deployments that run
the production site.</p></li></ul><p>One pattern this organization could follow is to partition the Kubernetes cluster into two
namespaces: <code>development</code> and <code>production</code>. Let's create two new namespaces to hold our work.</p><p>Create the <code>development</code> namespace using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</span></span></code></pre></div><p>And then let's create the <code>production</code> namespace using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</span></span></code></pre></div><p>To be sure things are right, list all of the namespaces in our cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get namespaces --show-labels
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME          STATUS    AGE       LABELS
</span></span></span><span><span><span>default       Active    32m       &lt;none&gt;
</span></span></span><span><span><span>development   Active    29s       name=development
</span></span></span><span><span><span>production    Active    23s       name=production
</span></span></span></code></pre></div><h3 id="create-pods-in-each-namespace">Create pods in each namespace</h3><p>A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.
Users interacting with one namespace do not see the content in another namespace.
To demonstrate this, let's spin up a simple Deployment and Pods in the <code>development</code> namespace.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment snowflake <span>\
</span></span></span><span><span><span></span>  --image<span>=</span>registry.k8s.io/serve_hostname <span>\
</span></span></span><span><span><span></span>  -n<span>=</span>development --replicas<span>=</span><span>2</span>
</span></span></code></pre></div><p>We have created a deployment whose replica size is 2 that is running the pod called <code>snowflake</code>
with a basic container that serves the hostname.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment -n<span>=</span>development
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
</span></span></span><span><span><span>snowflake    2/2     2            2           2m
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>snowflake -n<span>=</span>development
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME                         READY     STATUS    RESTARTS   AGE
</span></span></span><span><span><span>snowflake-3968820950-9dgr8   1/1       Running   0          2m
</span></span></span><span><span><span>snowflake-3968820950-vgc4n   1/1       Running   0          2m
</span></span></span></code></pre></div><p>And this is great, developers are able to do what they want, and they do not have to worry about
affecting content in the <code>production</code> namespace.</p><p>Let's switch to the <code>production</code> namespace and show how resources in one namespace are hidden from
the other. The <code>production</code> namespace should be empty, and the following commands should return nothing.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment -n<span>=</span>production
</span></span><span><span>kubectl get pods -n<span>=</span>production
</span></span></code></pre></div><p>Production likes to run cattle, so let's create some cattle pods.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment cattle --image<span>=</span>registry.k8s.io/serve_hostname -n<span>=</span>production
</span></span><span><span>kubectl scale deployment cattle --replicas<span>=</span><span>5</span> -n<span>=</span>production
</span></span><span><span>
</span></span><span><span>kubectl get deployment -n<span>=</span>production
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
</span></span></span><span><span><span>cattle       5/5     5            5           10s
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>cattle -n<span>=</span>production
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME                      READY     STATUS    RESTARTS   AGE
</span></span></span><span><span><span>cattle-2263376956-41xy6   1/1       Running   0          34s
</span></span></span><span><span><span>cattle-2263376956-kw466   1/1       Running   0          34s
</span></span></span><span><span><span>cattle-2263376956-n4v97   1/1       Running   0          34s
</span></span></span><span><span><span>cattle-2263376956-p5p3i   1/1       Running   0          34s
</span></span></span><span><span><span>cattle-2263376956-sxpth   1/1       Running   0          34s
</span></span></span></code></pre></div><p>At this point, it should be clear that the resources users create in one namespace are hidden from
the other namespace.</p><p>As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.</p><h2 id="understanding-the-motivation-for-using-namespaces">Understanding the motivation for using namespaces</h2><p>A single cluster should be able to satisfy the needs of multiple users or groups of users
(henceforth in this document a <em>user community</em>).</p><p>Kubernetes <em>namespaces</em> help different projects, teams, or customers to share a Kubernetes cluster.</p><p>It does this by providing the following:</p><ol><li>A scope for <a href="/docs/concepts/overview/working-with-objects/names/">names</a>.</li><li>A mechanism to attach authorization and policy to a subsection of the cluster.</li></ol><p>Use of multiple namespaces is optional.</p><p>Each user community wants to be able to work in isolation from other communities.
Each user community has its own:</p><ol><li>resources (pods, services, replication controllers, etc.)</li><li>policies (who can or cannot perform actions in their community)</li><li>constraints (this community is allowed this much quota, etc.)</li></ol><p>A cluster operator may create a Namespace for each unique user community.</p><p>The Namespace provides a unique scope for:</p><ol><li>named resources (to avoid basic naming collisions)</li><li>delegated management authority to trusted users</li><li>ability to limit community resource consumption</li></ol><p>Use cases include:</p><ol><li>As a cluster operator, I want to support multiple user communities on a single cluster.</li><li>As a cluster operator, I want to delegate authority to partitions of the cluster to trusted
users in those communities.</li><li>As a cluster operator, I want to limit the amount of resources each community can consume in
order to limit the impact to other communities using the cluster.</li><li>As a cluster user, I want to interact with resources that are pertinent to my user community in
isolation of what other user communities are doing on the cluster.</li></ol><h2 id="understanding-namespaces-and-dns">Understanding namespaces and DNS</h2><p>When you create a <a href="/docs/concepts/services-networking/service/">Service</a>, it creates a corresponding
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS entry</a>.
This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>, which means
that if a container uses <code>&lt;service-name&gt;</code> it will resolve to the service which
is local to a namespace. This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production. If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference">setting the namespace preference</a>.</li><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request">setting the namespace for a request</a></li><li>See <a href="https://git.k8s.io/design-proposals-archive/architecture/namespaces.md">namespaces design</a>.</li></ul></div></div><div><div class="td-content"><h1>Upgrade A Cluster</h1><p>This page provides an overview of the steps you should follow to upgrade a
Kubernetes cluster.</p><p>The Kubernetes project recommends upgrading to the latest patch releases promptly, and
to ensure that you are running a supported minor release of Kubernetes.
Following this recommendation helps you to stay secure.</p><p>The way that you upgrade a cluster depends on how you initially deployed it
and on any subsequent changes.</p><p>At a high level, the steps you perform are:</p><ul><li>Upgrade the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a></li><li>Upgrade the nodes in your cluster</li><li>Upgrade clients such as <a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a></li><li>Adjust manifests and other resources based on the API changes that accompany the
new Kubernetes version</li></ul><h2 id="before-you-begin">Before you begin</h2><p>You must have an existing cluster. This page is about upgrading from Kubernetes
1.33 to Kubernetes 1.34. If your cluster
is not currently running Kubernetes 1.33 then please check
the documentation for the version of Kubernetes that you plan to upgrade to.</p><h2 id="upgrade-approaches">Upgrade approaches</h2><h3 id="upgrade-kubeadm">kubeadm</h3><p>If your cluster was deployed using the <code>kubeadm</code> tool, refer to
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a>
for detailed information on how to upgrade the cluster.</p><p>Once you have upgraded the cluster, remember to
<a href="/docs/tasks/tools/">install the latest version of <code>kubectl</code></a>.</p><h3 id="manual-deployments">Manual deployments</h3><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>These steps do not account for third-party extensions such as network and storage
plugins.</div><p>You should manually update the control plane following this sequence:</p><ul><li>etcd (all instances)</li><li>kube-apiserver (all control plane hosts)</li><li>kube-controller-manager</li><li>kube-scheduler</li><li>cloud controller manager, if you use one</li></ul><p>At this point you should
<a href="/docs/tasks/tools/">install the latest version of <code>kubectl</code></a>.</p><p>For each node in your cluster, <a href="/docs/tasks/administer-cluster/safely-drain-node/">drain</a>
that node and then either replace it with a new node that uses the 1.34
kubelet, or upgrade the kubelet on that node and bring the node back into service.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Draining nodes before upgrading kubelet ensures that pods are re-admitted and containers are
re-created, which may be necessary to resolve some security issues or other important bugs.</div><h3 id="upgrade-other">Other deployments</h3><p>Refer to the documentation for your cluster deployment tool to learn the recommended set
up steps for maintenance.</p><h2 id="post-upgrade-tasks">Post-upgrade tasks</h2><h3 id="switch-your-cluster-s-storage-api-version">Switch your cluster's storage API version</h3><p>The objects that are serialized into etcd for a cluster's internal
representation of the Kubernetes resources active in the cluster are
written using a particular version of the API.</p><p>When the supported API changes, these objects may need to be rewritten
in the newer API. Failure to do this will eventually result in resources
that are no longer decodable or usable by the Kubernetes API server.</p><p>For each affected object, fetch it using the latest supported API and then
write it back also using the latest supported API.</p><h3 id="update-manifests">Update manifests</h3><p>Upgrading to a new Kubernetes version can provide new APIs.</p><p>You can use <code>kubectl convert</code> command to convert manifests between different API versions.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl convert -f pod.yaml --output-version v1
</span></span></code></pre></div><p>The <code>kubectl</code> tool replaces the contents of <code>pod.yaml</code> with a manifest that sets <code>kind</code> to
Pod (unchanged), but with a revised <code>apiVersion</code>.</p><h3 id="device-plugins">Device Plugins</h3><p>If your cluster is running device plugins and the node needs to be upgraded to a Kubernetes
release with a newer device plugin API version, device plugins must be upgraded to support
both version before the node is upgraded in order to guarantee that device allocations
continue to complete successfully during the upgrade.</p><p>Refer to <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#api-compatibility">API compatibility</a> and <a href="/docs/reference/node/device-plugin-api-versions/">Kubelet Device Manager API Versions</a> for more details.</p></div></div><div><div class="td-content"><h1>Use Cascading Deletion in a Cluster</h1><p>This page shows you how to specify the type of
<a href="/docs/concepts/architecture/garbage-collection/#cascading-deletion">cascading deletion</a>
to use in your cluster during <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." href="/docs/concepts/architecture/garbage-collection/" target="_blank">garbage collection</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You also need to <a href="/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">create a sample Deployment</a>
to experiment with the different types of cascading deletion. You will need to
recreate the Deployment for each type.</p><h2 id="check-owner-references-on-your-pods">Check owner references on your pods</h2><p>Check that the <code>ownerReferences</code> field is present on your pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx --output<span>=</span>yaml
</span></span></code></pre></div><p>The output has an <code>ownerReferences</code> field similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span><span>ownerReferences</span>:<span>
</span></span></span><span><span><span>    </span>- <span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>      </span><span>blockOwnerDeletion</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>controller</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>kind</span>:<span> </span>ReplicaSet<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>nginx-deployment-6b474476c4<span>
</span></span></span><span><span><span>      </span><span>uid</span>:<span> </span>4fdcd81c-bd5d-41f7-97af-3a3b759af9a7<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span></code></pre></div><h2 id="use-foreground-cascading-deletion">Use foreground cascading deletion</h2><p>By default, Kubernetes uses <a href="/docs/concepts/architecture/garbage-collection/#background-deletion">background cascading deletion</a>
to delete dependents of an object. You can switch to foreground cascading deletion
using either <code>kubectl</code> or the Kubernetes API, depending on the Kubernetes
version your cluster runs.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>You can delete objects using foreground cascading deletion using <code>kubectl</code> or the
Kubernetes API.</p><p><strong>Using kubectl</strong></p><p>Run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment nginx-deployment --cascade<span>=</span>foreground
</span></span></code></pre></div><p><strong>Using the Kubernetes API</strong></p><ol><li><p>Start a local proxy session:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span>
</span></span></code></pre></div></li><li><p>Use <code>curl</code> to trigger deletion:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span>\
</span></span></span><span><span><span></span>    -d <span>'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}'</span> <span>\
</span></span></span><span><span><span></span>    -H <span>"Content-Type: application/json"</span>
</span></span></code></pre></div><p>The output contains a <code>foregroundDeletion</code> <a class="glossary-tooltip" title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." href="/docs/concepts/overview/working-with-objects/finalizers/" target="_blank">finalizer</a>
like this:</p><pre tabindex="0"><code>"kind": "Deployment",
"apiVersion": "apps/v1",
"metadata": {
    "name": "nginx-deployment",
    "namespace": "default",
    "uid": "d1ce1b02-cae8-4288-8a53-30e84d8fa505",
    "resourceVersion": "1363097",
    "creationTimestamp": "2021-07-08T20:24:37Z",
    "deletionTimestamp": "2021-07-08T20:27:39Z",
    "finalizers": [
      "foregroundDeletion"
    ]
    ...
</code></pre></li></ol><h2 id="use-background-cascading-deletion">Use background cascading deletion</h2><ol><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">Create a sample Deployment</a>.</li><li>Use either <code>kubectl</code> or the Kubernetes API to delete the Deployment,
depending on the Kubernetes version your cluster runs.<p>To check the version, enter <code>kubectl version</code>.</p></li></ol><p>You can delete objects using background cascading deletion using <code>kubectl</code>
or the Kubernetes API.</p><p>Kubernetes uses background cascading deletion by default, and does so
even if you run the following commands without the <code>--cascade</code> flag or the
<code>propagationPolicy</code> argument.</p><p><strong>Using kubectl</strong></p><p>Run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment nginx-deployment --cascade<span>=</span>background
</span></span></code></pre></div><p><strong>Using the Kubernetes API</strong></p><ol><li><p>Start a local proxy session:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span>
</span></span></code></pre></div></li><li><p>Use <code>curl</code> to trigger deletion:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span>\
</span></span></span><span><span><span></span>    -d <span>'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Background"}'</span> <span>\
</span></span></span><span><span><span></span>    -H <span>"Content-Type: application/json"</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>"kind": "Status",
"apiVersion": "v1",
...
"status": "Success",
"details": {
    "name": "nginx-deployment",
    "group": "apps",
    "kind": "deployments",
    "uid": "cc9eefb9-2d49-4445-b1c1-d261c9396456"
}
</code></pre></li></ol><h2 id="set-orphan-deletion-policy">Delete owner objects and orphan dependents</h2><p>By default, when you tell Kubernetes to delete an object, the
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> also deletes
dependent objects. You can make Kubernetes <em>orphan</em> these dependents using
<code>kubectl</code> or the Kubernetes API, depending on the Kubernetes version your
cluster runs.<p>To check the version, enter <code>kubectl version</code>.</p></p><p><strong>Using kubectl</strong></p><p>Run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment nginx-deployment --cascade<span>=</span>orphan
</span></span></code></pre></div><p><strong>Using the Kubernetes API</strong></p><ol><li><p>Start a local proxy session:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span>
</span></span></code></pre></div></li><li><p>Use <code>curl</code> to trigger deletion:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span>\
</span></span></span><span><span><span></span>    -d <span>'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}'</span> <span>\
</span></span></span><span><span><span></span>    -H <span>"Content-Type: application/json"</span>
</span></span></code></pre></div><p>The output contains <code>orphan</code> in the <code>finalizers</code> field, similar to this:</p><pre tabindex="0"><code>"kind": "Deployment",
"apiVersion": "apps/v1",
"namespace": "default",
"uid": "6f577034-42a0-479d-be21-78018c466f1f",
"creationTimestamp": "2021-07-09T16:46:37Z",
"deletionTimestamp": "2021-07-09T16:47:08Z",
"deletionGracePeriodSeconds": 0,
"finalizers": [
  "orphan"
],
...
</code></pre></li></ol><p>You can check that the Pods managed by the Deployment are still running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/overview/working-with-objects/owners-dependents/">owners and dependents</a> in Kubernetes.</li><li>Learn about Kubernetes <a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a>.</li><li>Learn about <a href="/docs/concepts/architecture/garbage-collection/">garbage collection</a>.</li></ul></div></div><div><div class="td-content"><h1>Using a KMS provider for data encryption</h1><p>This page shows how to configure a Key Management Service (KMS) provider and plugin to enable secret data encryption.
In Kubernetes 1.34 there are two versions of KMS at-rest encryption.
You should use KMS v2 if feasible because KMS v1 is deprecated (since Kubernetes v1.28) and disabled by default (since Kubernetes v1.29).
KMS v2 offers significantly better performance characteristics than KMS v1.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>This documentation is for the generally available implementation of KMS v2 (and for the
deprecated version 1 implementation).
If you are using any control plane components older than Kubernetes v1.29, please check
the equivalent page in the documentation for the version of Kubernetes that your cluster
is running. Earlier releases of Kubernetes had different behavior that may be relevant
for information security.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>The version of Kubernetes that you need depends on which KMS API version
you have selected. Kubernetes recommends using KMS v2.</p><ul><li>If you selected KMS API v1 to support clusters prior to version v1.27
or if you have a legacy KMS plugin that only supports KMS v1,
any supported Kubernetes version will work. This API is deprecated as of Kubernetes v1.28.
Kubernetes does not recommend the use of this API.</li></ul><p>To check the version, enter <code>kubectl version</code>.</p><h3 id="kms-v1">KMS v1</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [deprecated]</code></div><ul><li><p>Kubernetes version 1.10.0 or later is required</p></li><li><p>For version 1.29 and later, the v1 implementation of KMS is disabled by default.
To enable the feature, set <code>--feature-gates=KMSv1=true</code> to configure a KMS v1 provider.</p></li><li><p>Your cluster must use etcd v3 or later</p></li></ul><h3 id="kms-v2">KMS v2</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div><ul><li>Your cluster must use etcd v3 or later</li></ul><h2 id="kms-encryption-and-per-object-encryption-keys">KMS encryption and per-object encryption keys</h2><p>The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd.
The data is encrypted using a data encryption key (DEK).
The DEKs are encrypted with a key encryption key (KEK) that is stored and managed in a remote KMS.</p><p>If you use the (deprecated) v1 implementation of KMS, a new DEK is generated for each encryption.</p><p>With KMS v2, a new DEK is generated <strong>per encryption</strong>: the API server uses a
<em>key derivation function</em> to generate single use data encryption keys from a secret seed
combined with some random data.
The seed is rotated whenever the KEK is rotated
(see the <em>Understanding key_id and Key Rotation</em> section below for more details).</p><p>The KMS provider uses gRPC to communicate with a specific KMS plugin over a UNIX domain socket.
The KMS plugin, which is implemented as a gRPC server and deployed on the same host(s)
as the Kubernetes control plane, is responsible for all communication with the remote KMS.</p><h2 id="configuring-the-kms-provider">Configuring the KMS provider</h2><p>To configure a KMS provider on the API server, include a provider of type <code>kms</code> in the
<code>providers</code> array in the encryption configuration file and set the following properties:</p><h3 id="configuring-the-kms-provider-kms-v1">KMS v1</h3><ul><li><code>apiVersion</code>: API Version for KMS provider. Leave this value empty or set it to <code>v1</code>.</li><li><code>name</code>: Display name of the KMS plugin. Cannot be changed once set.</li><li><code>endpoint</code>: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.</li><li><code>cachesize</code>: Number of data encryption keys (DEKs) to be cached in the clear.
When cached, DEKs can be used without another call to the KMS;
whereas DEKs that are not cached require a call to the KMS to unwrap.</li><li><code>timeout</code>: How long should <code>kube-apiserver</code> wait for kms-plugin to respond before
returning an error (default is 3 seconds).</li></ul><h3 id="configuring-the-kms-provider-kms-v2">KMS v2</h3><ul><li><code>apiVersion</code>: API Version for KMS provider. Set this to <code>v2</code>.</li><li><code>name</code>: Display name of the KMS plugin. Cannot be changed once set.</li><li><code>endpoint</code>: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.</li><li><code>timeout</code>: How long should <code>kube-apiserver</code> wait for kms-plugin to respond before
returning an error (default is 3 seconds).</li></ul><p>KMS v2 does not support the <code>cachesize</code> property. All data encryption keys (DEKs) will be cached in
the clear once the server has unwrapped them via a call to the KMS. Once cached, DEKs can be used
to perform decryption indefinitely without making a call to the KMS.</p><p>See <a href="/docs/tasks/administer-cluster/encrypt-data/">Understanding the encryption at rest configuration</a>.</p><h2 id="implementing-a-kms-plugin">Implementing a KMS plugin</h2><p>To implement a KMS plugin, you can develop a new plugin gRPC server or enable a KMS plugin
already provided by your cloud provider.
You then integrate the plugin with the remote KMS and deploy it on the Kubernetes control plane.</p><h3 id="enabling-the-kms-supported-by-your-cloud-provider">Enabling the KMS supported by your cloud provider</h3><p>Refer to your cloud provider for instructions on enabling the cloud provider-specific KMS plugin.</p><h3 id="developing-a-kms-plugin-grpc-server">Developing a KMS plugin gRPC server</h3><p>You can develop a KMS plugin gRPC server using a stub file available for Go. For other languages,
you use a proto file to create a stub file that you can use to develop the gRPC server code.</p><h4 id="developing-a-kms-plugin-gRPC-server-kms-v1">KMS v1</h4><ul><li><p>Using Go: Use the functions and data structures in the stub file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v1beta1/api.pb.go">api.pb.go</a>
to develop the gRPC server code</p></li><li><p>Using languages other than Go: Use the protoc compiler with the proto file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v1beta1/api.proto">api.proto</a>
to generate a stub file for the specific language</p></li></ul><h4 id="developing-a-kms-plugin-gRPC-server-kms-v2">KMS v2</h4><ul><li><p>Using Go: A high level
<a href="https://github.com/kubernetes/kms/blob/release-1.34/pkg/service/interface.go">library</a>
is provided to make the process easier. Low level implementations
can use the functions and data structures in the stub file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v2/api.pb.go">api.pb.go</a>
to develop the gRPC server code</p></li><li><p>Using languages other than Go: Use the protoc compiler with the proto file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v2/api.proto">api.proto</a>
to generate a stub file for the specific language</p></li></ul><p>Then use the functions and data structures in the stub file to develop the server code.</p><h4 id="notes">Notes</h4><h5 id="developing-a-kms-plugin-gRPC-server-notes-kms-v1">KMS v1</h5><ul><li><p>kms plugin version: <code>v1beta1</code></p><p>In response to procedure call Version, a compatible KMS plugin should return <code>v1beta1</code> as <code>VersionResponse.version</code>.</p></li><li><p>message version: <code>v1beta1</code></p><p>All messages from KMS provider have the version field set to <code>v1beta1</code>.</p></li><li><p>protocol: UNIX domain socket (<code>unix</code>)</p><p>The plugin is implemented as a gRPC server that listens at UNIX domain socket. The plugin deployment should create a file on the file system to run the gRPC unix domain socket connection. The API server (gRPC client) is configured with the KMS provider (gRPC server) unix domain socket endpoint in order to communicate with it. An abstract Linux socket may be used by starting the endpoint with <code>/@</code>, i.e. <code>unix:///@foo</code>. Care must be taken when using this type of socket as they do not have concept of ACL (unlike traditional file based sockets). However, they are subject to Linux networking namespace, so will only be accessible to containers within the same pod unless host networking is used.</p></li></ul><h5 id="developing-a-kms-plugin-gRPC-server-notes-kms-v2">KMS v2</h5><ul><li><p>KMS plugin version: <code>v2</code></p><p>In response to the <code>Status</code> remote procedure call, a compatible KMS plugin should return its KMS compatibility
version as <code>StatusResponse.version</code>. That status response should also include
"ok" as <code>StatusResponse.healthz</code> and a <code>key_id</code> (remote KMS KEK ID) as <code>StatusResponse.key_id</code>.
The Kubernetes project recommends you make your plugin
compatible with the stable <code>v2</code> KMS API. Kubernetes 1.34 also supports the
<code>v2beta1</code> API for KMS; future Kubernetes releases are likely to continue supporting that beta version.</p><p>The API server polls the <code>Status</code> procedure call approximately every minute when everything is healthy,
and every 10 seconds when the plugin is not healthy. Plugins must take care to optimize this call as it will be
under constant load.</p></li><li><p>Encryption</p><p>The <code>EncryptRequest</code> procedure call provides the plaintext and a UID for logging purposes. The response must include
the ciphertext, the <code>key_id</code> for the KEK used, and, optionally, any metadata that the KMS plugin needs to aid in
future <code>DecryptRequest</code> calls (via the <code>annotations</code> field). The plugin must guarantee that any distinct plaintext
results in a distinct response <code>(ciphertext, key_id, annotations)</code>.</p><p>If the plugin returns a non-empty <code>annotations</code> map, all map keys must be fully qualified domain names such as
<code>example.com</code>. An example use case of <code>annotation</code> is <code>{"kms.example.io/remote-kms-auditid":"&lt;audit ID used by the remote KMS&gt;"}</code></p><p>The API server does not perform the <code>EncryptRequest</code> procedure call at a high rate. Plugin implementations should
still aim to keep each request's latency at under 100 milliseconds.</p></li><li><p>Decryption</p><p>The <code>DecryptRequest</code> procedure call provides the <code>(ciphertext, key_id, annotations)</code> from <code>EncryptRequest</code> and a UID
for logging purposes. As expected, it is the inverse of the <code>EncryptRequest</code> call. Plugins must verify that the
<code>key_id</code> is one that they understand - they must not attempt to decrypt data unless they are sure that it was
encrypted by them at an earlier time.</p><p>The API server may perform thousands of <code>DecryptRequest</code> procedure calls on startup to fill its watch cache. Thus
plugin implementations must perform these calls as quickly as possible, and should aim to keep each request's latency
at under 10 milliseconds.</p></li><li><p>Understanding <code>key_id</code> and Key Rotation</p><p>The <code>key_id</code> is the public, non-secret name of the remote KMS KEK that is currently in use. It may be logged
during regular operation of the API server, and thus must not contain any private data. Plugin implementations
are encouraged to use a hash to avoid leaking any data. The KMS v2 metrics take care to hash this value before
exposing it via the <code>/metrics</code> endpoint.</p><p>The API server considers the <code>key_id</code> returned from the <code>Status</code> procedure call to be authoritative. Thus, a change
to this value signals to the API server that the remote KEK has changed, and data encrypted with the old KEK should
be marked stale when a no-op write is performed (as described below). If an <code>EncryptRequest</code> procedure call returns a
<code>key_id</code> that is different from <code>Status</code>, the response is thrown away and the plugin is considered unhealthy. Thus
implementations must guarantee that the <code>key_id</code> returned from <code>Status</code> will be the same as the one returned by
<code>EncryptRequest</code>. Furthermore, plugins must ensure that the <code>key_id</code> is stable and does not flip-flop between values
(i.e. during a remote KEK rotation).</p><p>Plugins must not re-use <code>key_id</code>s, even in situations where a previously used remote KEK has been reinstated. For
example, if a plugin was using <code>key_id=A</code>, switched to <code>key_id=B</code>, and then went back to <code>key_id=A</code> - instead of
reporting <code>key_id=A</code> the plugin should report some derivative value such as <code>key_id=A_001</code> or use a new value such
as <code>key_id=C</code>.</p><p>Since the API server polls <code>Status</code> about every minute, <code>key_id</code> rotation is not immediate. Furthermore, the API
server will coast on the last valid state for about three minutes. Thus if a user wants to take a passive approach
to storage migration (i.e. by waiting), they must schedule a migration to occur at <code>3 + N + M</code> minutes after the
remote KEK has been rotated (<code>N</code> is how long it takes the plugin to observe the <code>key_id</code> change and <code>M</code> is the
desired buffer to allow config changes to be processed - a minimum <code>M</code> of five minutes is recommend). Note that no
API server restart is required to perform KEK rotation.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Because you don't control the number of writes performed with the DEK,
the Kubernetes project recommends rotating the KEK at least every 90 days.</div></li><li><p>protocol: UNIX domain socket (<code>unix</code>)</p><p>The plugin is implemented as a gRPC server that listens at UNIX domain socket.
The plugin deployment should create a file on the file system to run the gRPC unix domain socket connection.
The API server (gRPC client) is configured with the KMS provider (gRPC server) unix
domain socket endpoint in order to communicate with it.
An abstract Linux socket may be used by starting the endpoint with <code>/@</code>, i.e. <code>unix:///@foo</code>.
Care must be taken when using this type of socket as they do not have concept of ACL
(unlike traditional file based sockets).
However, they are subject to Linux networking namespace, so will only be accessible to
containers within the same pod unless host networking is used.</p></li></ul><h3 id="integrating-a-kms-plugin-with-the-remote-kms">Integrating a KMS plugin with the remote KMS</h3><p>The KMS plugin can communicate with the remote KMS using any protocol supported by the KMS.
All configuration data, including authentication credentials the KMS plugin uses to communicate with the remote KMS,
are stored and managed by the KMS plugin independently.
The KMS plugin can encode the ciphertext with additional metadata that may be required before sending it to the KMS
for decryption (KMS v2 makes this process easier by providing a dedicated <code>annotations</code> field).</p><h3 id="deploying-the-kms-plugin">Deploying the KMS plugin</h3><p>Ensure that the KMS plugin runs on the same host(s) as the Kubernetes API server(s).</p><h2 id="encrypting-your-data-with-the-kms-provider">Encrypting your data with the KMS provider</h2><p>To encrypt the data:</p><ol><li><p>Create a new <code>EncryptionConfiguration</code> file using the appropriate properties for the <code>kms</code> provider
to encrypt resources like Secrets and ConfigMaps. If you want to encrypt an extension API that is
defined in a CustomResourceDefinition, your cluster must be running Kubernetes v1.26 or newer.</p></li><li><p>Set the <code>--encryption-provider-config</code> flag on the kube-apiserver to point to the location of the configuration file.</p></li><li><p><code>--encryption-provider-config-automatic-reload</code> boolean argument
determines if the file set by <code>--encryption-provider-config</code> should be
<a href="/docs/tasks/administer-cluster/encrypt-data/#configure-automatic-reloading">automatically reloaded</a>
if the disk contents change.</p></li><li><p>Restart your API server.</p></li></ol><h3 id="encrypting-your-data-with-the-kms-provider-kms-v1">KMS v1</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>      </span>- configmaps<span>
</span></span></span><span><span><span>      </span>- pandas.awesome.bears.example<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>kms</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>myKmsPluginFoo<span>
</span></span></span><span><span><span>          </span><span>endpoint</span>:<span> </span>unix:///tmp/socketfile-foo.sock<span>
</span></span></span><span><span><span>          </span><span>cachesize</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>          </span><span>timeout</span>:<span> </span>3s<span>
</span></span></span><span><span><span>      </span>- <span>kms</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>myKmsPluginBar<span>
</span></span></span><span><span><span>          </span><span>endpoint</span>:<span> </span>unix:///tmp/socketfile-bar.sock<span>
</span></span></span><span><span><span>          </span><span>cachesize</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>          </span><span>timeout</span>:<span> </span>3s<span>
</span></span></span></code></pre></div><h3 id="encrypting-your-data-with-the-kms-provider-kms-v2">KMS v2</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>      </span>- configmaps<span>
</span></span></span><span><span><span>      </span>- pandas.awesome.bears.example<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>kms</span>:<span>
</span></span></span><span><span><span>          </span><span>apiVersion</span>:<span> </span>v2<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>myKmsPluginFoo<span>
</span></span></span><span><span><span>          </span><span>endpoint</span>:<span> </span>unix:///tmp/socketfile-foo.sock<span>
</span></span></span><span><span><span>          </span><span>timeout</span>:<span> </span>3s<span>
</span></span></span><span><span><span>      </span>- <span>kms</span>:<span>
</span></span></span><span><span><span>          </span><span>apiVersion</span>:<span> </span>v2<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>myKmsPluginBar<span>
</span></span></span><span><span><span>          </span><span>endpoint</span>:<span> </span>unix:///tmp/socketfile-bar.sock<span>
</span></span></span><span><span><span>          </span><span>timeout</span>:<span> </span>3s<span>
</span></span></span></code></pre></div><p>Setting <code>--encryption-provider-config-automatic-reload</code> to <code>true</code> collapses all health checks to a single health check endpoint. Individual health checks are only available when KMS v1 providers are in use and the encryption config is not auto-reloaded.</p><p>The following table summarizes the health check endpoints for each KMS version:</p><table><thead><tr><th>KMS configurations</th><th>Without Automatic Reload</th><th>With Automatic Reload</th></tr></thead><tbody><tr><td>KMS v1 only</td><td>Individual Healthchecks</td><td>Single Healthcheck</td></tr><tr><td>KMS v2 only</td><td>Single Healthcheck</td><td>Single Healthcheck</td></tr><tr><td>Both KMS v1 and v2</td><td>Individual Healthchecks</td><td>Single Healthcheck</td></tr><tr><td>No KMS</td><td>None</td><td>Single Healthcheck</td></tr></tbody></table><p><code>Single Healthcheck</code> means that the only health check endpoint is <code>/healthz/kms-providers</code>.</p><p><code>Individual Healthchecks</code> means that each KMS plugin has an associated health check endpoint based on its location in the encryption config: <code>/healthz/kms-provider-0</code>, <code>/healthz/kms-provider-1</code> etc.</p><p>These healthcheck endpoint paths are hard coded and generated/controlled by the server. The indices for individual healthchecks corresponds to the order in which the KMS encryption config is processed.</p><p>Until the steps defined in <a href="#ensuring-all-secrets-are-encrypted">Ensuring all secrets are encrypted</a> are performed, the <code>providers</code> list should end with the <code>identity: {}</code> provider to allow unencrypted data to be read. Once all resources are encrypted, the <code>identity</code> provider should be removed to prevent the API server from honoring unencrypted data.</p><p>For details about the <code>EncryptionConfiguration</code> format, please check the
<a href="/docs/reference/config-api/apiserver-config.v1/">API server encryption API reference</a>.</p><h2 id="verifying-that-the-data-is-encrypted">Verifying that the data is encrypted</h2><p>When encryption at rest is correctly configured, resources are encrypted on write.
After restarting your <code>kube-apiserver</code>, any newly created or updated Secret or other resource types
configured in <code>EncryptionConfiguration</code> should be encrypted when stored. To verify,
you can use the <code>etcdctl</code> command line program to retrieve the contents of your secret data.</p><ol><li><p>Create a new secret called <code>secret1</code> in the <code>default</code> namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic secret1 -n default --from-literal<span>=</span><span>mykey</span><span>=</span>mydata
</span></span></code></pre></div></li><li><p>Using the <code>etcdctl</code> command line, read that secret out of etcd:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl get /kubernetes.io/secrets/default/secret1 <span>[</span>...<span>]</span> | hexdump -C
</span></span></code></pre></div><p>where <code>[...]</code> contains the additional arguments for connecting to the etcd server.</p></li><li><p>Verify the stored secret is prefixed with <code>k8s:enc:kms:v1:</code> for KMS v1 or prefixed with <code>k8s:enc:kms:v2:</code> for KMS v2, which indicates that the <code>kms</code> provider has encrypted the resulting data.</p></li><li><p>Verify that the secret is correctly decrypted when retrieved via the API:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe secret secret1 -n default
</span></span></code></pre></div><p>The Secret should contain <code>mykey: mydata</code></p></li></ol><h2 id="ensuring-all-secrets-are-encrypted">Ensuring all secrets are encrypted</h2><p>When encryption at rest is correctly configured, resources are encrypted on write.
Thus we can perform an in-place no-op update to ensure that data is encrypted.</p><p>The following command reads all secrets and then updates them to apply server side encryption.
If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><h2 id="switching-from-a-local-encryption-provider-to-the-kms-provider">Switching from a local encryption provider to the KMS provider</h2><p>To switch from a local encryption provider to the <code>kms</code> provider and re-encrypt all of the secrets:</p><ol><li><p>Add the <code>kms</code> provider as the first entry in the configuration file as shown in the following example.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- secrets<span>
</span></span></span><span><span><span>    </span><span>providers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>kms</span>:<span>
</span></span></span><span><span><span>          </span><span>apiVersion</span>:<span> </span>v2<span>
</span></span></span><span><span><span>          </span><span>name </span>:<span> </span>myKmsPlugin<span>
</span></span></span><span><span><span>          </span><span>endpoint</span>:<span> </span>unix:///tmp/socketfile.sock<span>
</span></span></span><span><span><span>      </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>          </span><span>keys</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>              </span><span>secret</span>:<span> </span>&lt;BASE 64 ENCODED SECRET&gt;<span>
</span></span></span></code></pre></div></li><li><p>Restart all <code>kube-apiserver</code> processes.</p></li><li><p>Run the following command to force all secrets to be re-encrypted using the <code>kms</code> provider.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><a id="disabling-encryption-at-rest"><p>If you no longer want to use encryption for data persisted in the Kubernetes API, read
<a href="/docs/tasks/administer-cluster/decrypt-data/">decrypt data that are already stored at rest</a>.</p></a></div></div><div><div class="td-content"><h1>Using CoreDNS for Service Discovery</h1><p>This page describes the CoreDNS upgrade process and how to install CoreDNS instead of kube-dns.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.9.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="about-coredns">About CoreDNS</h2><p><a href="https://coredns.io">CoreDNS</a> is a flexible, extensible DNS server
that can serve as the Kubernetes cluster DNS.
Like Kubernetes, the CoreDNS project is hosted by the
<a class="glossary-tooltip" title="Cloud Native Computing Foundation" href="https://cncf.io/" target="_blank">CNCF</a>.</p><p>You can use CoreDNS instead of kube-dns in your cluster by replacing
kube-dns in an existing deployment, or by using tools like kubeadm
that will deploy and upgrade the cluster for you.</p><h2 id="installing-coredns">Installing CoreDNS</h2><p>For manual deployment or replacement of kube-dns, see the documentation at the
<a href="https://coredns.io/manual/installation/">CoreDNS website</a>.</p><h2 id="migrating-to-coredns">Migrating to CoreDNS</h2><h3 id="upgrading-an-existing-cluster-with-kubeadm">Upgrading an existing cluster with kubeadm</h3><p>In Kubernetes version 1.21, kubeadm removed its support for <code>kube-dns</code> as a DNS application.
For <code>kubeadm</code> v1.34, the only supported cluster DNS application
is CoreDNS.</p><p>You can move to CoreDNS when you use <code>kubeadm</code> to upgrade a cluster that is
using <code>kube-dns</code>. In this case, <code>kubeadm</code> generates the CoreDNS configuration
("Corefile") based upon the <code>kube-dns</code> ConfigMap, preserving configurations for
stub domains, and upstream name server.</p><h2 id="upgrading-coredns">Upgrading CoreDNS</h2><p>You can check the version of CoreDNS that kubeadm installs for each version of
Kubernetes in the page
<a href="https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md">CoreDNS version in Kubernetes</a>.</p><p>CoreDNS can be upgraded manually in case you want to only upgrade CoreDNS
or use your own custom image.
There is a helpful <a href="https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md">guideline and walkthrough</a>
available to ensure a smooth upgrade.
Make sure the existing CoreDNS configuration ("Corefile") is retained when
upgrading your cluster.</p><p>If you are upgrading your cluster using the <code>kubeadm</code> tool, <code>kubeadm</code>
can take care of retaining the existing CoreDNS configuration automatically.</p><h2 id="tuning-coredns">Tuning CoreDNS</h2><p>When resource utilisation is a concern, it may be useful to tune the
configuration of CoreDNS. For more details, check out the
<a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">documentation on scaling CoreDNS</a>.</p><h2 id="what-s-next">What's next</h2><p>You can configure <a href="https://coredns.io">CoreDNS</a> to support many more use cases than
kube-dns does by modifying the CoreDNS configuration ("Corefile").
For more information, see the <a href="https://coredns.io/plugins/kubernetes/">documentation</a>
for the <code>kubernetes</code> CoreDNS plugin, or read the
<a href="https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/">Custom DNS Entries for Kubernetes</a>.
in the CoreDNS blog.</p></div></div><div><div class="td-content"><h1>Using NodeLocal DNSCache in Kubernetes Clusters</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>This page provides an overview of NodeLocal DNSCache feature in Kubernetes.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="introduction">Introduction</h2><p>NodeLocal DNSCache improves Cluster DNS performance by running a DNS caching agent
on cluster nodes as a DaemonSet. In today's architecture, Pods in 'ClusterFirst' DNS mode
reach out to a kube-dns <code>serviceIP</code> for DNS queries. This is translated to a
kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy.
With this new architecture, Pods will reach out to the DNS caching agent
running on the same node, thereby avoiding iptables DNAT rules and connection tracking.
The local caching agent will query kube-dns service for cache misses of cluster
hostnames ("<code>cluster.local</code>" suffix by default).</p><h2 id="motivation">Motivation</h2><ul><li><p>With the current DNS architecture, it is possible that Pods with the highest DNS QPS
have to reach out to a different node, if there is no local kube-dns/CoreDNS instance.
Having a local cache will help improve the latency in such scenarios.</p></li><li><p>Skipping iptables DNAT and connection tracking will help reduce
<a href="https://github.com/kubernetes/kubernetes/issues/56903">conntrack races</a>
and avoid UDP DNS entries filling up conntrack table.</p></li><li><p>Connections from the local caching agent to kube-dns service can be upgraded to TCP.
TCP conntrack entries will be removed on connection close in contrast with
UDP entries that have to timeout
(<a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt">default</a>
<code>nf_conntrack_udp_timeout</code> is 30 seconds)</p></li><li><p>Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to
dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout).
Since the nodelocal cache listens for UDP DNS queries, applications don't need to be changed.</p></li><li><p>Metrics &amp; visibility into DNS requests at a node level.</p></li><li><p>Negative caching can be re-enabled, thereby reducing the number of queries for the kube-dns service.</p></li></ul><h2 id="architecture-diagram">Architecture Diagram</h2><p>This is the path followed by DNS Queries after NodeLocal DNSCache is enabled:</p><figure class="diagram-medium"><img src="/images/docs/nodelocaldns.svg" alt="NodeLocal DNSCache flow"><figcaption><h4>Nodelocal DNSCache flow</h4><p>This image shows how NodeLocal DNSCache handles DNS queries.</p></figcaption></figure><h2 id="configuration">Configuration</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The local listen IP address for NodeLocal DNSCache can be any address that
can be guaranteed to not collide with any existing IP in your cluster.
It's recommended to use an address with a local scope, for example,
from the 'link-local' range '169.254.0.0/16' for IPv4 or from the
'Unique Local Address' range in IPv6 'fd00::/8'.</div><p>This feature can be enabled using the following steps:</p><ul><li><p>Prepare a manifest similar to the sample
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"><code>nodelocaldns.yaml</code></a>
and save it as <code>nodelocaldns.yaml</code>.</p></li><li><p>If using IPv6, the CoreDNS configuration file needs to enclose all the IPv6 addresses
into square brackets if used in 'IP:Port' format.
If you are using the sample manifest from the previous point, this will require you to modify
<a href="https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70">the configuration line L70</a>
like this: "<code>health [__PILLAR__LOCAL__DNS__]:8080</code>"</p></li><li><p>Substitute the variables in the manifest with the right values:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>kubedns</span><span>=</span><span>`</span>kubectl get svc kube-dns -n kube-system -o <span>jsonpath</span><span>={</span>.spec.clusterIP<span>}</span><span>`</span>
</span></span><span><span><span>domain</span><span>=</span>&lt;cluster-domain&gt;
</span></span><span><span><span>localdns</span><span>=</span>&lt;node-local-address&gt;
</span></span></code></pre></div><p><code>&lt;cluster-domain&gt;</code> is "<code>cluster.local</code>" by default. <code>&lt;node-local-address&gt;</code> is the
local listen IP address chosen for NodeLocal DNSCache.</p><ul><li><p>If kube-proxy is running in IPTABLES mode:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sed -i <span>"s/__PILLAR__LOCAL__DNS__/</span><span>$localdns</span><span>/g; s/__PILLAR__DNS__DOMAIN__/</span><span>$domain</span><span>/g; s/__PILLAR__DNS__SERVER__/</span><span>$kubedns</span><span>/g"</span> nodelocaldns.yaml
</span></span></code></pre></div><p><code>__PILLAR__CLUSTER__DNS__</code> and <code>__PILLAR__UPSTREAM__SERVERS__</code> will be populated by
the <code>node-local-dns</code> pods.
In this mode, the <code>node-local-dns</code> pods listen on both the kube-dns service IP
as well as <code>&lt;node-local-address&gt;</code>, so pods can look up DNS records using either IP address.</p></li><li><p>If kube-proxy is running in IPVS mode:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>sed -i <span>"s/__PILLAR__LOCAL__DNS__/</span><span>$localdns</span><span>/g; s/__PILLAR__DNS__DOMAIN__/</span><span>$domain</span><span>/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/</span><span>$kubedns</span><span>/g"</span> nodelocaldns.yaml
</span></span></code></pre></div><p>In this mode, the <code>node-local-dns</code> pods listen only on <code>&lt;node-local-address&gt;</code>.
The <code>node-local-dns</code> interface cannot bind the kube-dns cluster IP since the
interface used for IPVS loadbalancing already uses this address.
<code>__PILLAR__UPSTREAM__SERVERS__</code> will be populated by the node-local-dns pods.</p></li></ul></li><li><p>Run <code>kubectl create -f nodelocaldns.yaml</code></p></li><li><p>If using kube-proxy in IPVS mode, <code>--cluster-dns</code> flag to kubelet needs to be modified
to use <code>&lt;node-local-address&gt;</code> that NodeLocal DNSCache is listening on.
Otherwise, there is no need to modify the value of the <code>--cluster-dns</code> flag,
since NodeLocal DNSCache listens on both the kube-dns service IP as well as
<code>&lt;node-local-address&gt;</code>.</p></li></ul><p>Once enabled, the <code>node-local-dns</code> Pods will run in the <code>kube-system</code> namespace
on each of the cluster nodes. This Pod runs <a href="https://github.com/coredns/coredns">CoreDNS</a>
in cache mode, so all CoreDNS metrics exposed by the different plugins will
be available on a per-node basis.</p><p>You can disable this feature by removing the DaemonSet, using <code>kubectl delete -f &lt;manifest&gt;</code>.
You should also revert any changes you made to the kubelet configuration.</p><h2 id="stubdomains-and-upstream-server-configuration">StubDomains and Upstream server Configuration</h2><p>StubDomains and upstream servers specified in the <code>kube-dns</code> ConfigMap in the <code>kube-system</code> namespace
are automatically picked up by <code>node-local-dns</code> pods. The ConfigMap contents need to follow the format
shown in <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/#example-1">the example</a>.
The <code>node-local-dns</code> ConfigMap can also be modified directly with the stubDomain configuration
in the Corefile format. Some cloud providers might not allow modifying <code>node-local-dns</code> ConfigMap directly.
In those cases, the <code>kube-dns</code> ConfigMap can be updated.</p><h2 id="setting-memory-limits">Setting memory limits</h2><p>The <code>node-local-dns</code> Pods use memory for storing cache entries and processing queries.
Since they do not watch Kubernetes objects, the cluster size or the number of Services / EndpointSlices do not directly affect memory usage. Memory usage is influenced by the DNS query pattern.
From <a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">CoreDNS docs</a>,</p><blockquote><p>The default cache size is 10000 entries, which uses about 30 MB when completely filled.</p></blockquote><p>This would be the memory usage for each server block (if the cache gets completely filled).
Memory usage can be reduced by specifying smaller cache sizes.</p><p>The number of concurrent queries is linked to the memory demand, because each extra
goroutine used for handling a query requires an amount of memory. You can set an upper limit
using the <code>max_concurrent</code> option in the forward plugin.</p><p>If a <code>node-local-dns</code> Pod attempts to use more memory than is available (because of total system
resources, or because of a configured
<a href="/docs/concepts/configuration/manage-resources-containers/">resource limit</a>), the operating system
may shut down that pod's container.
If this happens, the container that is terminated (&#8220;OOMKilled&#8221;) does not clean up the custom
packet filtering rules that it previously added during startup.
The <code>node-local-dns</code> container should get restarted (since managed as part of a DaemonSet), but this
will lead to a brief DNS downtime each time that the container fails: the packet filtering rules direct
DNS queries to a local Pod that is unhealthy.</p><p>You can determine a suitable memory limit by running node-local-dns pods without a limit and
measuring the peak usage. You can also set up and use a
<a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VerticalPodAutoscaler</a>
in <em>recommender mode</em>, and then check its recommendations.</p></div></div><div><div class="td-content"><h1>Using sysctls in a Kubernetes Cluster</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>This document describes how to configure and use kernel parameters within a
Kubernetes cluster using the <a class="glossary-tooltip" title="An interface for getting and setting Unix kernel parameters" href="/docs/tasks/administer-cluster/sysctl-cluster/" target="_blank">sysctl</a>
interface.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Starting from Kubernetes version 1.23, the kubelet supports the use of either <code>/</code> or <code>.</code>
as separators for sysctl names.
Starting from Kubernetes version 1.25, setting Sysctls for a Pod supports setting sysctls with slashes.
For example, you can represent the same sysctl name as <code>kernel.shm_rmid_forced</code> using a
period as the separator, or as <code>kernel/shm_rmid_forced</code> using a slash as a separator.
For more sysctl parameter conversion method details, please refer to
the page <a href="https://man7.org/linux/man-pages/man5/sysctl.d.5.html">sysctl.d(5)</a> from
the Linux man-pages project.</div><h2 id="before-you-begin">Before you begin</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>sysctl</code> is a Linux-specific command-line tool used to configure various kernel parameters
and it is not available on non-Linux operating systems.</div><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>For some steps, you also need to be able to reconfigure the command line
options for the kubelets running on your cluster.</p><h2 id="listing-all-sysctl-parameters">Listing all Sysctl Parameters</h2><p>In Linux, the sysctl interface allows an administrator to modify kernel
parameters at runtime. Parameters are available via the <code>/proc/sys/</code> virtual
process file system. The parameters cover various subsystems such as:</p><ul><li>kernel (common prefix: <code>kernel.</code>)</li><li>networking (common prefix: <code>net.</code>)</li><li>virtual memory (common prefix: <code>vm.</code>)</li><li>MDADM (common prefix: <code>dev.</code>)</li><li>More subsystems are described in <a href="https://www.kernel.org/doc/Documentation/sysctl/README">Kernel docs</a>.</li></ul><p>To get a list of all parameters, you can run</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo sysctl -a
</span></span></code></pre></div><h2 id="safe-and-unsafe-sysctls">Safe and Unsafe Sysctls</h2><p>Kubernetes classes sysctls as either <em>safe</em> or <em>unsafe</em>. In addition to proper
namespacing, a <em>safe</em> sysctl must be properly <em>isolated</em> between pods on the
same node. This means that setting a <em>safe</em> sysctl for one pod</p><ul><li>must not have any influence on any other pod on the node</li><li>must not allow to harm the node's health</li><li>must not allow to gain CPU or memory resources outside of the resource limits
of a pod.</li></ul><p>By far, most of the <em>namespaced</em> sysctls are not necessarily considered <em>safe</em>.
The following sysctls are supported in the <em>safe</em> set:</p><ul><li><code>kernel.shm_rmid_forced</code>;</li><li><code>net.ipv4.ip_local_port_range</code>;</li><li><code>net.ipv4.tcp_syncookies</code>;</li><li><code>net.ipv4.ping_group_range</code> (since Kubernetes 1.18);</li><li><code>net.ipv4.ip_unprivileged_port_start</code> (since Kubernetes 1.22);</li><li><code>net.ipv4.ip_local_reserved_ports</code> (since Kubernetes 1.27, needs kernel 3.16+);</li><li><code>net.ipv4.tcp_keepalive_time</code> (since Kubernetes 1.29, needs kernel 4.5+);</li><li><code>net.ipv4.tcp_fin_timeout</code> (since Kubernetes 1.29, needs kernel 4.6+);</li><li><code>net.ipv4.tcp_keepalive_intvl</code> (since Kubernetes 1.29, needs kernel 4.5+);</li><li><code>net.ipv4.tcp_keepalive_probes</code> (since Kubernetes 1.29, needs kernel 4.5+).</li><li><code>net.ipv4.tcp_rmem</code> (since Kubernetes 1.32, needs kernel 4.15+).</li><li><code>net.ipv4.tcp_wmem</code> (since Kubernetes 1.32, needs kernel 4.15+).</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>There are some exceptions to the set of safe sysctls:</p><ul><li>The <code>net.*</code> sysctls are not allowed with host networking enabled.</li><li>The <code>net.ipv4.tcp_syncookies</code> sysctl is not namespaced on Linux kernel version 4.5 or lower.</li></ul></div><p>This list will be extended in future Kubernetes versions when the kubelet
supports better isolation mechanisms.</p><h3 id="enabling-unsafe-sysctls">Enabling Unsafe Sysctls</h3><p>All <em>safe</em> sysctls are enabled by default.</p><p>All <em>unsafe</em> sysctls are disabled by default and must be allowed manually by the
cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be
scheduled, but will fail to launch.</p><p>With the warning above in mind, the cluster admin can allow certain <em>unsafe</em>
sysctls for very special situations such as high-performance or real-time
application tuning. <em>Unsafe</em> sysctls are enabled on a node-by-node basis with a
flag of the kubelet; for example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubelet --allowed-unsafe-sysctls <span>\
</span></span></span><span><span><span></span>  <span>'kernel.msg*,net.core.somaxconn'</span> ...
</span></span></code></pre></div><p>For <a class="glossary-tooltip" title="A tool for running Kubernetes locally." href="/docs/tasks/tools/#minikube" target="_blank">Minikube</a>, this can be done via the <code>extra-config</code> flag:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube start --extra-config<span>=</span><span>"kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn"</span>...
</span></span></code></pre></div><p>Only <em>namespaced</em> sysctls can be enabled this way.</p><h2 id="setting-sysctls-for-a-pod">Setting Sysctls for a Pod</h2><p>A number of sysctls are <em>namespaced</em> in today's Linux kernels. This means that
they can be set independently for each pod on a node. Only namespaced sysctls
are configurable via the pod securityContext within Kubernetes.</p><p>The following sysctls are known to be namespaced. This list could change
in future versions of the Linux kernel.</p><ul><li><code>kernel.shm*</code>,</li><li><code>kernel.msg*</code>,</li><li><code>kernel.sem</code>,</li><li><code>fs.mqueue.*</code>,</li><li>Those <code>net.*</code> that can be set in container networking namespace. However,
there are exceptions (e.g., <code>net.netfilter.nf_conntrack_max</code> and
<code>net.netfilter.nf_conntrack_expect_max</code> can be set in container networking
namespace but are unnamespaced before Linux 5.12.2).</li></ul><p>Sysctls with no namespace are called <em>node-level</em> sysctls. If you need to set
them, you must manually configure them on each node's operating system, or by
using a DaemonSet with privileged containers.</p><p>Use the pod securityContext to configure namespaced sysctls. The securityContext
applies to all containers in the same pod.</p><p>This example uses the pod securityContext to set a safe sysctl
<code>kernel.shm_rmid_forced</code> and two unsafe sysctls <code>net.core.somaxconn</code> and
<code>kernel.msgmax</code>. There is no distinction between <em>safe</em> and <em>unsafe</em> sysctls in
the specification.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Only modify sysctl parameters after you understand their effects, to avoid
destabilizing your operating system.</div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>sysctl-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>sysctls</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>kernel.shm_rmid_forced<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>net.core.somaxconn<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"1024"</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>kernel.msgmax<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"65536"</span><span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Due to their nature of being <em>unsafe</em>, the use of <em>unsafe</em> sysctls
is at-your-own-risk and can lead to severe problems like wrong behavior of
containers, resource shortage or complete breakage of a node.</div><p>It is good practice to consider nodes with special sysctl settings as
<em>tainted</em> within a cluster, and only schedule pods onto them which need those
sysctl settings. It is suggested to use the Kubernetes <a href="/docs/reference/generated/kubectl/kubectl-commands/#taint"><em>taints and toleration</em>
feature</a> to implement this.</p><p>A pod with the <em>unsafe</em> sysctls will fail to launch on any node which has not
enabled those two <em>unsafe</em> sysctls explicitly. As with <em>node-level</em> sysctls it
is recommended to use
<a href="/docs/reference/generated/kubectl/kubectl-commands/#taint"><em>taints and toleration</em> feature</a> or
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taints on nodes</a>
to schedule those pods onto the right nodes.</p></div></div><div><div class="td-content"><h1>Utilizing the NUMA-aware Memory Manager</h1><div class="feature-state-notice feature-stable" title="Feature Gate: MemoryManager"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The Kubernetes <em>Memory Manager</em> enables the feature of guaranteed memory (and hugepages)
allocation for pods in the <code>Guaranteed</code> <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">QoS class</a>.</p><p>The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.
The Memory Manager feeds the central manager (<em>Topology Manager</em>) with these affinity hints.
Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.</p><p>Moreover, the Memory Manager ensures that the memory which a pod requests
is allocated from a minimum number of NUMA nodes.</p><p>The Memory Manager is only pertinent to Linux based hosts.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.32.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>To align memory resources with other requested resources in a Pod spec:</p><ul><li>the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node.
See <a href="/docs/tasks/administer-cluster/cpu-management-policies/">control CPU Management Policies</a>;</li><li>the Topology Manager should be enabled and proper Topology Manager policy should be configured on a Node.
See <a href="/docs/tasks/administer-cluster/topology-manager/">control Topology Management Policies</a>.</li></ul><p>Starting from v1.22, the Memory Manager is enabled by default through <code>MemoryManager</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>Preceding v1.22, the <code>kubelet</code> must be started with the following flag:</p><p><code>--feature-gates=MemoryManager=true</code></p><p>in order to enable the Memory Manager feature.</p><h2 id="how-does-the-memory-manager-operate">How does the Memory Manager Operate?</h2><p>The Memory Manager currently offers the guaranteed memory (and hugepages) allocation
for Pods in Guaranteed QoS class.
To immediately put the Memory Manager into operation follow the guidelines in the section
<a href="#memory-manager-configuration">Memory Manager configuration</a>, and subsequently,
prepare and deploy a <code>Guaranteed</code> pod as illustrated in the section
<a href="#placing-a-pod-in-the-guaranteed-qos-class">Placing a Pod in the Guaranteed QoS class</a>.</p><p>The Memory Manager is a Hint Provider, and it provides topology hints for
the Topology Manager which then aligns the requested resources according to these topology hints.
On Linux, it also enforces <code>cgroups</code> (i.e. <code>cpuset.mems</code>) for pods.
The complete flow diagram concerning pod admission and deployment process is illustrated in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview">Memory Manager KEP: Design Overview</a> and below:</p><p><img alt="Memory Manager in the pod admission and deployment process" src="/images/docs/memory-manager-diagram.svg"></p><p>During this process, the Memory Manager updates its internal counters stored in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Node Map and Memory Maps</a> to manage guaranteed memory allocation.</p><p>The Memory Manager updates the Node Map during the startup and runtime as follows.</p><h3 id="startup">Startup</h3><p>This occurs once a node administrator employs <code>--reserved-memory</code> (section
<a href="#reserved-memory-flag">Reserved memory flag</a>).
In this case, the Node Map becomes updated to reflect this reservation as illustrated in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples">Memory Manager KEP: Memory Maps at start-up (with examples)</a>.</p><p>The administrator must provide <code>--reserved-memory</code> flag when <code>Static</code> policy is configured.</p><h3 id="runtime">Runtime</h3><p>Reference <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples">Memory Manager KEP: Memory Maps at runtime (with examples)</a> illustrates
how a successful pod deployment affects the Node Map, and it also relates to
how potential Out-of-Memory (OOM) situations are handled further by Kubernetes or operating system.</p><p>Important topic in the context of Memory Manager operation is the management of NUMA groups.
Each time pod's memory request is in excess of single NUMA node capacity, the Memory Manager
attempts to create a group that comprises several NUMA nodes and features extend memory capacity.
The problem has been solved as elaborated in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a>.
Also, reference <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a>
illustrates how the management of groups occurs.</p><h3 id="windows-support">Windows Support</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>Windows support can be enabled via the <code>WindowsCPUAndMemoryAffinity</code> feature gate
and it requires support in the container runtime.
Only the <a href="#policy-best-effort">BestEffort Policy</a> is supported on Windows.</p><h2 id="memory-manager-configuration">Memory Manager configuration</h2><p>Other Managers should be first pre-configured. Next, the Memory Manager feature should be enabled
and be run with <code>Static</code> policy (section <a href="#policy-static">Static policy</a>).
Optionally, some amount of memory can be reserved for system or kubelet processes to increase
node stability (section <a href="#reserved-memory-flag">Reserved memory flag</a>).</p><h3 id="policies">Policies</h3><p>Memory Manager supports two policies. You can select a policy via a <code>kubelet</code> flag <code>--memory-manager-policy</code>:</p><ul><li><code>None</code> (default)</li><li><code>Static</code> (Linux only)</li><li><code>BestEffort</code> (Windows Only)</li></ul><h4 id="policy-none">None policy</h4><p>This is the default policy and does not affect the memory allocation in any way.
It acts the same as if the Memory Manager is not present at all.</p><p>The <code>None</code> policy returns default topology hint. This special hint denotes that Hint Provider
(Memory Manager in this case) has no preference for NUMA affinity with any resource.</p><h4 id="policy-static">Static policy</h4><p>In the case of the <code>Guaranteed</code> pod, the <code>Static</code> Memory Manager policy returns topology hints
relating to the set of NUMA nodes where the memory can be guaranteed,
and reserves the memory through updating the internal <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">NodeMap</a> object.</p><p>In the case of the <code>BestEffort</code> or <code>Burstable</code> pod, the <code>Static</code> Memory Manager policy sends back
the default topology hint as there is no request for the guaranteed memory,
and does not reserve the memory in the internal <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">NodeMap</a> object.</p><p>This policy is only supported on Linux.</p><h4 id="policy-best-effort">BestEffort policy</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>This policy is only supported on Windows.</p><p>On Windows, NUMA node assignment works differently than Linux.
There is no mechanism to ensure that Memory access only comes from a specific NUMA node.
Instead the Windows scheduler will select the most optimal NUMA node based on the CPU(s) assignments.
It is possible that Windows might use other NUMA nodes if deemed optimal by the Windows scheduler.</p><p>The policy does track the amount of memory available and requested through the internal <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">NodeMap</a>.
The memory manager will make a best effort at ensuring that enough memory is available on
a NUMA node before making the assignment.<br>This means that in most cases memory assignment should function as expected.</p><h3 id="reserved-memory-flag">Reserved memory flag</h3><p>The <a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Node Allocatable</a> mechanism
is commonly used by node administrators to reserve K8S node system resources for the kubelet
or operating system processes in order to enhance the node stability.
A dedicated set of flags can be used for this purpose to set the total amount of reserved memory
for a node. This pre-configured value is subsequently utilized to calculate
the real amount of node's "allocatable" memory available to pods.</p><p>The Kubernetes scheduler incorporates "allocatable" to optimise pod scheduling process.
The foregoing flags include <code>--kube-reserved</code>, <code>--system-reserved</code> and <code>--eviction-threshold</code>.
The sum of their values will account for the total amount of reserved memory.</p><p>A new <code>--reserved-memory</code> flag was added to Memory Manager to allow for this total reserved memory
to be split (by a node administrator) and accordingly reserved across many NUMA nodes.</p><p>The flag specifies a comma-separated list of memory reservations of different memory types per NUMA node.
Memory reservations across multiple NUMA nodes can be specified using semicolon as separator.
This parameter is only useful in the context of the Memory Manager feature.
The Memory Manager will not use this reserved memory for the allocation of container workloads.</p><p>For example, if you have a NUMA node "NUMA0" with <code>10Gi</code> of memory available, and
the <code>--reserved-memory</code> was specified to reserve <code>1Gi</code> of memory at "NUMA0",
the Memory Manager assumes that only <code>9Gi</code> is available for containers.</p><p>You can omit this parameter, however, you should be aware that the quantity of reserved memory
from all NUMA nodes should be equal to the quantity of memory specified by the
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Node Allocatable feature</a>.
If at least one node allocatable parameter is non-zero, you will need to specify
<code>--reserved-memory</code> for at least one NUMA node.
In fact, <code>eviction-hard</code> threshold value is equal to <code>100Mi</code> by default, so
if <code>Static</code> policy is used, <code>--reserved-memory</code> is obligatory.</p><p>Also, avoid the following configurations:</p><ol><li>duplicates, i.e. the same NUMA node or memory type, but with a different value;</li><li>setting zero limit for any of memory types;</li><li>NUMA node IDs that do not exist in the machine hardware;</li><li>memory type names different than <code>memory</code> or <code>hugepages-&lt;size&gt;</code>
(hugepages of particular <code>&lt;size&gt;</code> should also exist).</li></ol><p>Syntax:</p><p><code>--reserved-memory N:memory-type1=value1,memory-type2=value2,...</code></p><ul><li><code>N</code> (integer) - NUMA node index, e.g. <code>0</code></li><li><code>memory-type</code> (string) - represents memory type:<ul><li><code>memory</code> - conventional memory</li><li><code>hugepages-2Mi</code> or <code>hugepages-1Gi</code> - hugepages</li></ul></li><li><code>value</code> (string) - the quantity of reserved memory, e.g. <code>1Gi</code></li></ul><p>Example usage:</p><p><code>--reserved-memory 0:memory=1Gi,hugepages-1Gi=2Gi</code></p><p>or</p><p><code>--reserved-memory 0:memory=1Gi --reserved-memory 1:memory=2Gi</code></p><p>or</p><p><code>--reserved-memory '0:memory=1Gi;1:memory=2Gi'</code></p><p>When you specify values for <code>--reserved-memory</code> flag, you must comply with the setting that
you prior provided via Node Allocatable Feature flags.
That is, the following rule must be obeyed for each memory type:</p><p><code>sum(reserved-memory(i)) = kube-reserved + system-reserved + eviction-threshold</code>,</p><p>where <code>i</code> is an index of a NUMA node.</p><p>If you do not follow the formula above, the Memory Manager will show an error on startup.</p><p>In other words, the example above illustrates that for the conventional memory (<code>type=memory</code>),
we reserve <code>3Gi</code> in total, i.e.:</p><p><code>sum(reserved-memory(i)) = reserved-memory(0) + reserved-memory(1) = 1Gi + 2Gi = 3Gi</code></p><p>An example of kubelet command-line arguments relevant to the node Allocatable configuration:</p><ul><li><code>--kube-reserved=cpu=500m,memory=50Mi</code></li><li><code>--system-reserved=cpu=123m,memory=333Mi</code></li><li><code>--eviction-hard=memory.available&lt;500Mi</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The default hard eviction threshold is 100MiB, and <strong>not</strong> zero.
Remember to increase the quantity of memory that you reserve by setting <code>--reserved-memory</code>
by that hard eviction threshold. Otherwise, the kubelet will not start Memory Manager and
display an error.</div><p>Here is an example of a correct configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>--kube-reserved<span>=</span><span>cpu</span><span>=</span>4,memory<span>=</span>4Gi
</span></span><span><span>--system-reserved<span>=</span><span>cpu</span><span>=</span>1,memory<span>=</span>1Gi
</span></span><span><span>--memory-manager-policy<span>=</span>Static
</span></span><span><span>--reserved-memory <span>'0:memory=3Gi;1:memory=2148Mi'</span>
</span></span></code></pre></div><p>Prior to Kubernetes 1.32, you also need to add</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>--feature-gates<span>=</span><span>MemoryManager</span><span>=</span><span>true</span>
</span></span></code></pre></div><p>Let us validate the configuration above:</p><ol><li><code>kube-reserved + system-reserved + eviction-hard(default) = reserved-memory(0) + reserved-memory(1)</code></li><li><code>4GiB + 1GiB + 100MiB = 3GiB + 2148MiB</code></li><li><code>5120MiB + 100MiB = 3072MiB + 2148MiB</code></li><li><code>5220MiB = 5220MiB</code> (which is correct)</li></ol><h2 id="placing-a-pod-in-the-guaranteed-qos-class">Placing a Pod in the Guaranteed QoS class</h2><p>If the selected policy is anything other than <code>None</code>, the Memory Manager identifies pods
that are in the <code>Guaranteed</code> QoS class.
The Memory Manager provides specific topology hints to the Topology Manager for each <code>Guaranteed</code> pod.
For pods in a QoS class other than <code>Guaranteed</code>, the Memory Manager provides default topology hints
to the Topology Manager.</p><p>The following excerpts from pod manifests assign a pod to the <code>Guaranteed</code> QoS class.</p><p>Pod with integer CPU(s) runs in the <code>Guaranteed</code> QoS class, when <code>requests</code> are equal to <code>limits</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>Also, a pod sharing CPU(s) runs in the <code>Guaranteed</code> QoS class, when <code>requests</code> are equal to <code>limits</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"300m"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"300m"</span><span>
</span></span></span><span><span><span>        </span><span>example.com/device</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>Notice that both CPU and memory requests must be specified for a Pod to lend it to Guaranteed QoS class.</p><h2 id="troubleshooting">Troubleshooting</h2><p>The following means can be used to troubleshoot the reason why a pod could not be deployed or
became rejected at a node:</p><ul><li>pod status - indicates topology affinity errors</li><li>system logs - include valuable information for debugging, e.g., about generated hints</li><li>state file - the dump of internal state of the Memory Manager
(includes <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Node Map and Memory Maps</a>)</li><li>starting from v1.22, the <a href="#device-plugin-resource-api">device plugin resource API</a> can be used
to retrieve information about the memory reserved for containers</li></ul><h3 id="TopologyAffinityError">Pod status (TopologyAffinityError)</h3><p>This error typically occurs in the following situations:</p><ul><li>a node has not enough resources available to satisfy the pod's request</li><li>the pod's request is rejected due to particular Topology Manager policy constraints</li></ul><p>The error appears in the status of a pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME         READY   STATUS                  RESTARTS   AGE
guaranteed   0/1     TopologyAffinityError   0          113s
</code></pre><p>Use <code>kubectl describe pod &lt;id&gt;</code> or <code>kubectl get events</code> to obtain detailed error message:</p><pre tabindex="0"><code class="language-none">Warning  TopologyAffinityError  10m   kubelet, dell8  Resources cannot be allocated with Topology locality
</code></pre><h3 id="system-logs">System logs</h3><p>Search system logs with respect to a particular pod.</p><p>The set of hints that Memory Manager generated for the pod can be found in the logs.
Also, the set of hints generated by CPU Manager should be present in the logs.</p><p>Topology Manager merges these hints to calculate a single best hint.
The best hint should be also present in the logs.</p><p>The best hint indicates where to allocate all the resources.
Topology Manager tests this hint against its current policy, and based on the verdict,
it either admits the pod to the node or rejects it.</p><p>Also, search the logs for occurrences associated with the Memory Manager,
e.g. to find out information about <code>cgroups</code> and <code>cpuset.mems</code> updates.</p><h3 id="examine-the-memory-manager-state-on-a-node">Examine the memory manager state on a node</h3><p>Let us first deploy a sample <code>Guaranteed</code> pod whose specification is as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>guaranteed<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>guaranteed<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>consumer<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>150Gi<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>150Gi<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span>"infinity"</span>]<span>
</span></span></span></code></pre></div><p>Next, let us log into the node where it was deployed and examine the state file in
<code>/var/lib/kubelet/memory_manager_state</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>   <span>"policyName"</span>:<span>"Static"</span>,
</span></span><span><span>   <span>"machineState"</span>:{
</span></span><span><span>      <span>"0"</span>:{
</span></span><span><span>         <span>"numberOfAssignments"</span>:<span>1</span>,
</span></span><span><span>         <span>"memoryMap"</span>:{
</span></span><span><span>            <span>"hugepages-1Gi"</span>:{
</span></span><span><span>               <span>"total"</span>:<span>0</span>,
</span></span><span><span>               <span>"systemReserved"</span>:<span>0</span>,
</span></span><span><span>               <span>"allocatable"</span>:<span>0</span>,
</span></span><span><span>               <span>"reserved"</span>:<span>0</span>,
</span></span><span><span>               <span>"free"</span>:<span>0</span>
</span></span><span><span>            },
</span></span><span><span>            <span>"memory"</span>:{
</span></span><span><span>               <span>"total"</span>:<span>134987354112</span>,
</span></span><span><span>               <span>"systemReserved"</span>:<span>3221225472</span>,
</span></span><span><span>               <span>"allocatable"</span>:<span>131766128640</span>,
</span></span><span><span>               <span>"reserved"</span>:<span>131766128640</span>,
</span></span><span><span>               <span>"free"</span>:<span>0</span>
</span></span><span><span>            }
</span></span><span><span>         },
</span></span><span><span>         <span>"nodes"</span>:[
</span></span><span><span>            <span>0</span>,
</span></span><span><span>            <span>1</span>
</span></span><span><span>         ]
</span></span><span><span>      },
</span></span><span><span>      <span>"1"</span>:{
</span></span><span><span>         <span>"numberOfAssignments"</span>:<span>1</span>,
</span></span><span><span>         <span>"memoryMap"</span>:{
</span></span><span><span>            <span>"hugepages-1Gi"</span>:{
</span></span><span><span>               <span>"total"</span>:<span>0</span>,
</span></span><span><span>               <span>"systemReserved"</span>:<span>0</span>,
</span></span><span><span>               <span>"allocatable"</span>:<span>0</span>,
</span></span><span><span>               <span>"reserved"</span>:<span>0</span>,
</span></span><span><span>               <span>"free"</span>:<span>0</span>
</span></span><span><span>            },
</span></span><span><span>            <span>"memory"</span>:{
</span></span><span><span>               <span>"total"</span>:<span>135286722560</span>,
</span></span><span><span>               <span>"systemReserved"</span>:<span>2252341248</span>,
</span></span><span><span>               <span>"allocatable"</span>:<span>133034381312</span>,
</span></span><span><span>               <span>"reserved"</span>:<span>29295144960</span>,
</span></span><span><span>               <span>"free"</span>:<span>103739236352</span>
</span></span><span><span>            }
</span></span><span><span>         },
</span></span><span><span>         <span>"nodes"</span>:[
</span></span><span><span>            <span>0</span>,
</span></span><span><span>            <span>1</span>
</span></span><span><span>         ]
</span></span><span><span>      }
</span></span><span><span>   },
</span></span><span><span>   <span>"entries"</span>:{
</span></span><span><span>      <span>"fa9bdd38-6df9-4cf9-aa67-8c4814da37a8"</span>:{
</span></span><span><span>         <span>"guaranteed"</span>:[
</span></span><span><span>            {
</span></span><span><span>               <span>"numaAffinity"</span>:[
</span></span><span><span>                  <span>0</span>,
</span></span><span><span>                  <span>1</span>
</span></span><span><span>               ],
</span></span><span><span>               <span>"type"</span>:<span>"memory"</span>,
</span></span><span><span>               <span>"size"</span>:<span>161061273600</span>
</span></span><span><span>            }
</span></span><span><span>         ]
</span></span><span><span>      }
</span></span><span><span>   },
</span></span><span><span>   <span>"checksum"</span>:<span>4142013182</span>
</span></span><span><span>}
</span></span></code></pre></div><p>It can be deduced from the state file that the pod was pinned to both NUMA nodes, i.e.:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span><span>"numaAffinity"</span><span>:</span>[
</span></span><span><span>   <span>0</span>,
</span></span><span><span>   <span>1</span>
</span></span><span><span>]<span>,</span>
</span></span></code></pre></div><p>Pinned term means that pod's memory consumption is constrained (through <code>cgroups</code> configuration)
to these NUMA nodes.</p><p>This automatically implies that Memory Manager instantiated a new group that
comprises these two NUMA nodes, i.e. <code>0</code> and <code>1</code> indexed NUMA nodes.</p><p>Notice that the management of groups is handled in a relatively complex manner, and
further elaboration is provided in Memory Manager KEP in <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">this</a> and <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">this</a> sections.</p><p>In order to analyse memory resources available in a group,the corresponding entries from
NUMA nodes belonging to the group must be added up.</p><p>For example, the total amount of free "conventional" memory in the group can be computed
by adding up the free memory available at every NUMA node in the group,
i.e., in the <code>"memory"</code> section of NUMA node <code>0</code> (<code>"free":0</code>) and NUMA node <code>1</code> (<code>"free":103739236352</code>).
So, the total amount of free "conventional" memory in this group is equal to <code>0 + 103739236352</code> bytes.</p><p>The line <code>"systemReserved":3221225472</code> indicates that the administrator of this node reserved
<code>3221225472</code> bytes (i.e. <code>3Gi</code>) to serve kubelet and system processes at NUMA node <code>0</code>,
by using <code>--reserved-memory</code> flag.</p><h3 id="device-plugin-resource-api">Device plugin resource API</h3><p>The kubelet provides a <code>PodResourceLister</code> gRPC service to enable discovery of resources and associated metadata.
By using its <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#grpc-endpoint-list">List gRPC endpoint</a>,
information about reserved memory for each container can be retrieved, which is contained
in protobuf <code>ContainerMemory</code> message.
This information can be retrieved solely for pods in Guaranteed QoS class.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview">Memory Manager KEP: Design Overview</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples">Memory Manager KEP: Memory Maps at start-up (with examples)</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples">Memory Manager KEP: Memory Maps at runtime (with examples)</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Memory Manager KEP: The Concept of Node Map and Memory Maps</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a></li></ul></div></div><div><div class="td-content"><h1>Verify Signed Kubernetes Artifacts</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [beta]</code></div><h2 id="before-you-begin">Before you begin</h2><p>You will need to have the following tools installed:</p><ul><li><code>cosign</code> (<a href="https://docs.sigstore.dev/cosign/system_config/installation/">install guide</a>)</li><li><code>curl</code> (often provided by your operating system)</li><li><code>jq</code> (<a href="https://jqlang.github.io/jq/download/">download jq</a>)</li></ul><h2 id="verifying-binary-signatures">Verifying binary signatures</h2><p>The Kubernetes release process signs all binary artifacts (tarballs, SPDX files,
standalone binaries) by using cosign's keyless signing. To verify a particular
binary, retrieve it together with its signature and certificate:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>URL</span><span>=</span>https://dl.k8s.io/release/v1.34.0/bin/linux/amd64
</span></span><span><span><span>BINARY</span><span>=</span>kubectl
</span></span><span><span>
</span></span><span><span><span>FILES</span><span>=(</span>
</span></span><span><span>    <span>"</span><span>$BINARY</span><span>"</span>
</span></span><span><span>    <span>"</span><span>$BINARY</span><span>.sig"</span>
</span></span><span><span>    <span>"</span><span>$BINARY</span><span>.cert"</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>for</span> FILE in <span>"</span><span>${</span><span>FILES</span>[@]<span>}</span><span>"</span>; <span>do</span>
</span></span><span><span>    curl -sSfL --retry <span>3</span> --retry-delay <span>3</span> <span>"</span><span>$URL</span><span>/</span><span>$FILE</span><span>"</span> -o <span>"</span><span>$FILE</span><span>"</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>Then verify the blob by using <code>cosign verify-blob</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cosign verify-blob <span>"</span><span>$BINARY</span><span>"</span> <span>\
</span></span></span><span><span><span></span>  --signature <span>"</span><span>$BINARY</span><span>"</span>.sig <span>\
</span></span></span><span><span><span></span>  --certificate <span>"</span><span>$BINARY</span><span>"</span>.cert <span>\
</span></span></span><span><span><span></span>  --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com <span>\
</span></span></span><span><span><span></span>  --certificate-oidc-issuer https://accounts.google.com
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Cosign 2.0 requires the <code>--certificate-identity</code> and <code>--certificate-oidc-issuer</code> options.</p><p>To learn more about keyless signing, please refer to <a href="https://docs.sigstore.dev/cosign/signing/overview/">Keyless Signatures</a>.</p><p>Previous versions of Cosign required that you set <code>COSIGN_EXPERIMENTAL=1</code>.</p><p>For additional information, please refer to the <a href="https://blog.sigstore.dev/cosign-2-0-released/">sigstore Blog</a></p></div><h2 id="verifying-image-signatures">Verifying image signatures</h2><p>For a complete list of images that are signed please refer
to <a href="/releases/download/">Releases</a>.</p><p>Pick one image from this list and verify its signature using
the <code>cosign verify</code> command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cosign verify registry.k8s.io/kube-apiserver-amd64:v1.34.0 <span>\
</span></span></span><span><span><span></span>  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com <span>\
</span></span></span><span><span><span></span>  --certificate-oidc-issuer https://accounts.google.com <span>\
</span></span></span><span><span><span></span>  | jq .
</span></span></code></pre></div><h3 id="verifying-images-for-all-control-plane-components">Verifying images for all control plane components</h3><p>To verify all signed control plane images for the latest stable version
(v1.34.0), please run the following commands:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -Ls <span>"https://sbom.k8s.io/</span><span>$(</span>curl -Ls https://dl.k8s.io/release/stable.txt<span>)</span><span>/release"</span> <span>\
</span></span></span><span><span><span></span>  | grep <span>"SPDXID: SPDXRef-Package-registry.k8s.io"</span> <span>\
</span></span></span><span><span><span></span>  | grep -v sha256 | cut -d- -f3- | sed <span>'s/-/\//'</span> | sed <span>'s/-v1/:v1/'</span> <span>\
</span></span></span><span><span><span></span>  | sort &gt; images.txt
</span></span><span><span><span>input</span><span>=</span>images.txt
</span></span><span><span><span>while</span> <span>IFS</span><span>=</span> <span>read</span> -r image
</span></span><span><span><span>do</span>
</span></span><span><span>  cosign verify <span>"</span><span>$image</span><span>"</span> <span>\
</span></span></span><span><span><span></span>    --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com <span>\
</span></span></span><span><span><span></span>    --certificate-oidc-issuer https://accounts.google.com <span>\
</span></span></span><span><span><span></span>    | jq .
</span></span><span><span><span>done</span> &lt; <span>"</span><span>$input</span><span>"</span>
</span></span></code></pre></div><p>Once you have verified an image, you can specify the image by its digest in your Pod
manifests as per this example:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>registry-url/image-name@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2
</span></span></span></code></pre></div><p>For more information, please refer
to the <a href="/docs/concepts/containers/images/#image-pull-policy">Image Pull Policy</a>
section.</p><h2 id="verifying-image-signatures-with-admission-controller">Verifying Image Signatures with Admission Controller</h2><p>For non-control plane images (for example
<a href="https://github.com/kubernetes/kubernetes/blob/master/test/conformance/image/README.md">conformance image</a>),
signatures can also be verified at deploy time using
<a href="https://docs.sigstore.dev/policy-controller/overview">sigstore policy-controller</a>
admission controller.</p><p>Here are some helpful resources to get started with <code>policy-controller</code>:</p><ul><li><a href="https://github.com/sigstore/helm-charts/tree/main/charts/policy-controller">Installation</a></li><li><a href="https://github.com/sigstore/policy-controller/tree/main/config">Configuration Options</a></li></ul><h2 id="verify-the-software-bill-of-materials">Verify the Software Bill Of Materials</h2><p>You can verify the Kubernetes Software Bill of Materials (SBOM) by using the
sigstore certificate and signature, or the corresponding SHA files:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Retrieve the latest available Kubernetes release version</span>
</span></span><span><span><span>VERSION</span><span>=</span><span>$(</span>curl -Ls https://dl.k8s.io/release/stable.txt<span>)</span>
</span></span><span><span>
</span></span><span><span><span># Verify the SHA512 sum</span>
</span></span><span><span>curl -Ls <span>"https://sbom.k8s.io/</span><span>$VERSION</span><span>/release"</span> -o <span>"</span><span>$VERSION</span><span>.spdx"</span>
</span></span><span><span><span>echo</span> <span>"</span><span>$(</span>curl -Ls <span>"https://sbom.k8s.io/</span><span>$VERSION</span><span>/release.sha512"</span><span>)</span><span> </span><span>$VERSION</span><span>.spdx"</span> | sha512sum --check
</span></span><span><span>
</span></span><span><span><span># Verify the SHA256 sum</span>
</span></span><span><span><span>echo</span> <span>"</span><span>$(</span>curl -Ls <span>"https://sbom.k8s.io/</span><span>$VERSION</span><span>/release.sha256"</span><span>)</span><span> </span><span>$VERSION</span><span>.spdx"</span> | sha256sum --check
</span></span><span><span>
</span></span><span><span><span># Retrieve sigstore signature and certificate</span>
</span></span><span><span>curl -Ls <span>"https://sbom.k8s.io/</span><span>$VERSION</span><span>/release.sig"</span> -o <span>"</span><span>$VERSION</span><span>.spdx.sig"</span>
</span></span><span><span>curl -Ls <span>"https://sbom.k8s.io/</span><span>$VERSION</span><span>/release.cert"</span> -o <span>"</span><span>$VERSION</span><span>.spdx.cert"</span>
</span></span><span><span>
</span></span><span><span><span># Verify the sigstore signature</span>
</span></span><span><span>cosign verify-blob <span>\
</span></span></span><span><span><span></span>    --certificate <span>"</span><span>$VERSION</span><span>.spdx.cert"</span> <span>\
</span></span></span><span><span><span></span>    --signature <span>"</span><span>$VERSION</span><span>.spdx.sig"</span> <span>\
</span></span></span><span><span><span></span>    --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com <span>\
</span></span></span><span><span><span></span>    --certificate-oidc-issuer https://accounts.google.com <span>\
</span></span></span><span><span><span></span>    <span>"</span><span>$VERSION</span><span>.spdx"</span>
</span></span></code></pre></div></div></div><div><div class="td-content"><h1>Configure Pods and Containers</h1><div class="lead">Perform common configuration tasks for Pods and containers.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-resources/">Assign Devices to Pods and Containers</a></h5><p>Assign infrastructure resources to your Kubernetes workloads.</p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-gmsa/">Configure GMSA for Windows Pods and containers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-runasusername/">Configure RunAsUserName for Windows pods and containers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/extended-resource/">Assign Extended Resources to a Container</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-volume-storage/">Configure a Pod to Use a Volume for Storage</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">Configure a Pod to Use a PersistentVolume for Storage</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-projected-volume-storage/">Configure a Pod to Use a Projected Volume for Storage</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-service-account/">Configure Service Accounts for Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">Pull an Image from a Private Registry</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness, Readiness and Startup Probes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-pods-nodes/">Assign Pods to Nodes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">Assign Pods to Nodes using Node Affinity</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-pod-initialization/">Configure Pod Initialization</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">Attach Handlers to Container Lifecycle Events</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/share-process-namespace/">Share Process Namespace between Containers in a Pod</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/user-namespaces/">Use a User Namespace With a Pod</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/image-volumes/">Use an Image Volume With a Pod</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/static-pod/">Create static Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/translate-compose-kubernetes/">Translate a Docker Compose File to Kubernetes Resources</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">Enforce Pod Security Standards by Configuring the Built-in Admission Controller</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/migrate-from-psp/">Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Assign Memory Resources to Containers and Pods</h1><p>This page shows how to assign a memory <em>request</em> and a memory <em>limit</em> to a
Container. A Container is guaranteed to have as much memory as it requests,
but is not allowed to use more memory than its limit.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>Each node in your cluster must have at least 300 MiB of memory.</p><p>A few of the steps on this page require you to run the
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>
service in your cluster. If you have the metrics-server
running, you can skip those steps.</p><p>If you are running Minikube, run the following command to enable the
metrics-server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube addons <span>enable</span> metrics-server
</span></span></code></pre></div><p>To see whether the metrics-server is running, or another provider of the resource metrics
API (<code>metrics.k8s.io</code>), run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get apiservices
</span></span></code></pre></div><p>If the resource metrics API is available, the output includes a
reference to <code>metrics.k8s.io</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME
</span></span><span><span>v1beta1.metrics.k8s.io
</span></span></code></pre></div><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace mem-example
</span></span></code></pre></div><h2 id="specify-a-memory-request-and-a-memory-limit">Specify a memory request and a memory limit</h2><p>To specify a memory request for a Container, include the <code>resources:requests</code> field
in the Container's resource manifest. To specify a memory limit, include <code>resources:limits</code>.</p><p>In this exercise, you create a Pod that has one Container. The Container has a memory
request of 100 MiB and a memory limit of 200 MiB. Here's the configuration file
for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/memory-request-limit.yaml"><code>pods/resource/memory-request-limit.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/memory-request-limit.yaml to clipboard"></div><div class="includecode" id="pods-resource-memory-request-limit-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>memory-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>mem-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>memory-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>polinux/stress<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"stress"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"--vm"</span>,<span> </span><span>"1"</span>,<span> </span><span>"--vm-bytes"</span>,<span> </span><span>"150M"</span>,<span> </span><span>"--vm-hang"</span>,<span> </span><span>"1"</span>]<span>
</span></span></span></code></pre></div></div></div><p>The <code>args</code> section in the configuration file provides arguments for the Container when it starts.
The <code>"--vm-bytes", "150M"</code> arguments tell the Container to attempt to allocate 150 MiB of memory.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit.yaml --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>Verify that the Pod Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo --output<span>=</span>yaml --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>The output shows that the one Container in the Pod has a memory request of 100 MiB
and a memory limit of 200 MiB.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span><span>requests</span>:<span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>Run <code>kubectl top</code> to fetch the metrics for the pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl top pod memory-demo --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>The output shows that the Pod is using about 162,900,000 bytes of memory, which
is about 150 MiB. This is greater than the Pod's 100 MiB request, but within the
Pod's 200 MiB limit.</p><pre tabindex="0"><code>NAME                        CPU(cores)   MEMORY(bytes)
memory-demo                 &lt;something&gt;  162856960
</code></pre><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod memory-demo --namespace<span>=</span>mem-example
</span></span></code></pre></div><h2 id="exceed-a-container-s-memory-limit">Exceed a Container's memory limit</h2><p>A Container can exceed its memory request if the Node has memory available. But a Container
is not allowed to use more than its memory limit. If a Container allocates more memory than
its limit, the Container becomes a candidate for termination. If the Container continues to
consume memory beyond its limit, the Container is terminated. If a terminated Container can be
restarted, the kubelet restarts it, as with any other type of runtime failure.</p><p>In this exercise, you create a Pod that attempts to allocate more memory than its limit.
Here is the configuration file for a Pod that has one Container with a
memory request of 50 MiB and a memory limit of 100 MiB:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/memory-request-limit-2.yaml"><code>pods/resource/memory-request-limit-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/memory-request-limit-2.yaml to clipboard"></div><div class="includecode" id="pods-resource-memory-request-limit-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>memory-demo-2<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>mem-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>memory-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>polinux/stress<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"50Mi"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"stress"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"--vm"</span>,<span> </span><span>"1"</span>,<span> </span><span>"--vm-bytes"</span>,<span> </span><span>"250M"</span>,<span> </span><span>"--vm-hang"</span>,<span> </span><span>"1"</span>]<span>
</span></span></span></code></pre></div></div></div><p>In the <code>args</code> section of the configuration file, you can see that the Container
will attempt to allocate 250 MiB of memory, which is well above the 100 MiB limit.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-2.yaml --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo-2 --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>At this point, the Container might be running or killed. Repeat the preceding command until the Container is killed:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME            READY     STATUS      RESTARTS   AGE
</span></span><span><span>memory-demo-2   0/1       OOMKilled   <span>1</span>          24s
</span></span></code></pre></div><p>Get a more detailed view of the Container status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo-2 --output<span>=</span>yaml --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>The output shows that the Container was killed because it is out of memory (OOM):</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>lastState</span>:<span>
</span></span></span><span><span><span>   </span><span>terminated</span>:<span>
</span></span></span><span><span><span>     </span><span>containerID</span>:<span> </span>65183c1877aaec2e8427bc95609cc52677a454b56fcb24340dbd22917c23b10f<span>
</span></span></span><span><span><span>     </span><span>exitCode</span>:<span> </span><span>137</span><span>
</span></span></span><span><span><span>     </span><span>finishedAt</span>:<span> </span>2017-06-20T20:52:19Z<span>
</span></span></span><span><span><span>     </span><span>reason</span>:<span> </span>OOMKilled<span>
</span></span></span><span><span><span>     </span><span>startedAt</span>:<span> </span><span>null</span><span>
</span></span></span></code></pre></div><p>The Container in this exercise can be restarted, so the kubelet restarts it. Repeat
this command several times to see that the Container is repeatedly killed and restarted:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo-2 --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>The output shows that the Container is killed, restarted, killed again, restarted again, and so on:</p><pre tabindex="0"><code>kubectl get pod memory-demo-2 --namespace=mem-example
NAME            READY     STATUS      RESTARTS   AGE
memory-demo-2   0/1       OOMKilled   1          37s
</code></pre><pre tabindex="0"><code>
kubectl get pod memory-demo-2 --namespace=mem-example
NAME            READY     STATUS    RESTARTS   AGE
memory-demo-2   1/1       Running   2          40s
</code></pre><p>View detailed information about the Pod history:</p><pre tabindex="0"><code>kubectl describe pod memory-demo-2 --namespace=mem-example
</code></pre><p>The output shows that the Container starts and fails repeatedly:</p><pre tabindex="0"><code>... Normal  Created   Created container with id 66a3a20aa7980e61be4922780bf9d24d1a1d8b7395c09861225b0eba1b1f8511
... Warning BackOff   Back-off restarting failed container
</code></pre><p>View detailed information about your cluster's Nodes:</p><pre tabindex="0"><code>kubectl describe nodes
</code></pre><p>The output includes a record of the Container being killed because of an out-of-memory condition:</p><pre tabindex="0"><code>Warning OOMKilling Memory cgroup out of memory: Kill process 4481 (stress) score 1994 or sacrifice child
</code></pre><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod memory-demo-2 --namespace<span>=</span>mem-example
</span></span></code></pre></div><h2 id="specify-a-memory-request-that-is-too-big-for-your-nodes">Specify a memory request that is too big for your Nodes</h2><p>Memory requests and limits are associated with Containers, but it is useful to think
of a Pod as having a memory request and limit. The memory request for the Pod is the
sum of the memory requests for all the Containers in the Pod. Likewise, the memory
limit for the Pod is the sum of the limits of all the Containers in the Pod.</p><p>Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node
has enough available memory to satisfy the Pod's memory request.</p><p>In this exercise, you create a Pod that has a memory request so big that it exceeds the
capacity of any Node in your cluster. Here is the configuration file for a Pod that has one
Container with a request for 1000 GiB of memory, which likely exceeds the capacity
of any Node in your cluster.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/memory-request-limit-3.yaml"><code>pods/resource/memory-request-limit-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/memory-request-limit-3.yaml to clipboard"></div><div class="includecode" id="pods-resource-memory-request-limit-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>memory-demo-3<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>mem-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>memory-demo-3-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>polinux/stress<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"1000Gi"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"1000Gi"</span><span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"stress"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"--vm"</span>,<span> </span><span>"1"</span>,<span> </span><span>"--vm-bytes"</span>,<span> </span><span>"150M"</span>,<span> </span><span>"--vm-hang"</span>,<span> </span><span>"1"</span>]<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-3.yaml --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>View the Pod status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo-3 --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>The output shows that the Pod status is PENDING. That is, the Pod is not scheduled to run on any Node, and it will remain in the PENDING state indefinitely:</p><pre tabindex="0"><code>kubectl get pod memory-demo-3 --namespace=mem-example
NAME            READY     STATUS    RESTARTS   AGE
memory-demo-3   0/1       Pending   0          25s
</code></pre><p>View detailed information about the Pod, including events:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod memory-demo-3 --namespace<span>=</span>mem-example
</span></span></code></pre></div><p>The output shows that the Container cannot be scheduled because of insufficient memory on the Nodes:</p><pre tabindex="0"><code>Events:
  ...  Reason            Message
       ------            -------
  ...  FailedScheduling  No nodes are available that match all of the following predicates:: Insufficient memory (3).
</code></pre><h2 id="memory-units">Memory units</h2><p>The memory resource is measured in bytes. You can express memory as a plain integer or a
fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki.
For example, the following represent approximately the same value:</p><pre tabindex="0"><code>128974848, 129e6, 129M, 123Mi
</code></pre><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod memory-demo-3 --namespace<span>=</span>mem-example
</span></span></code></pre></div><h2 id="if-you-do-not-specify-a-memory-limit">If you do not specify a memory limit</h2><p>If you do not specify a memory limit for a Container, one of the following situations applies:</p><ul><li><p>The Container has no upper bound on the amount of memory it uses. The Container
could use all of the memory available on the Node where it is running which in turn could invoke the OOM Killer. Further, in case of an OOM Kill, a container with no resource limits will have a greater chance of being killed.</p></li><li><p>The Container is running in a namespace that has a default memory limit, and the
Container is automatically assigned the default limit. Cluster administrators can use a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#limitrange-v1-core">LimitRange</a>
to specify a default value for the memory limit.</p></li></ul><h2 id="motivation-for-memory-requests-and-limits">Motivation for memory requests and limits</h2><p>By configuring memory requests and limits for the Containers that run in your
cluster, you can make efficient use of the memory resources available on your cluster's
Nodes. By keeping a Pod's memory request low, you give the Pod a good chance of being
scheduled. By having a memory limit that is greater than the memory request, you accomplish two things:</p><ul><li>The Pod can have bursts of activity where it makes use of memory that happens to be available.</li><li>The amount of memory a Pod can use during a burst is limited to some reasonable amount.</li></ul><h2 id="clean-up">Clean up</h2><p>Delete your namespace. This deletes all the Pods that you created for this task:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace mem-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a></p></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a></p></li></ul></div></div><div><div class="td-content"><h1>Assign CPU Resources to Containers and Pods</h1><p>This page shows how to assign a CPU <em>request</em> and a CPU <em>limit</em> to
a container. Containers cannot use more CPU than the configured limit.
Provided the system has CPU time free, a container is guaranteed to be
allocated as much CPU as it requests.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>Your cluster must have at least 1 CPU available for use to run the task examples.</p><p>A few of the steps on this page require you to run the
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>
service in your cluster. If you have the metrics-server
running, you can skip those steps.</p><p>If you are running <a class="glossary-tooltip" title="A tool for running Kubernetes locally." href="/docs/tasks/tools/#minikube" target="_blank">Minikube</a>, run the
following command to enable metrics-server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube addons <span>enable</span> metrics-server
</span></span></code></pre></div><p>To see whether metrics-server (or another provider of the resource metrics
API, <code>metrics.k8s.io</code>) is running, type the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get apiservices
</span></span></code></pre></div><p>If the resource metrics API is available, the output will include a
reference to <code>metrics.k8s.io</code>.</p><pre tabindex="0"><code>NAME
v1beta1.metrics.k8s.io
</code></pre><h2 id="create-a-namespace">Create a namespace</h2><p>Create a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">Namespace</a> so that the resources you
create in this exercise are isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace cpu-example
</span></span></code></pre></div><h2 id="specify-a-cpu-request-and-a-cpu-limit">Specify a CPU request and a CPU limit</h2><p>To specify a CPU request for a container, include the <code>resources:requests</code> field
in the Container resource manifest. To specify a CPU limit, include <code>resources:limits</code>.</p><p>In this exercise, you create a Pod that has one container. The container has a request
of 0.5 CPU and a limit of 1 CPU. Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/cpu-request-limit.yaml"><code>pods/resource/cpu-request-limit.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/cpu-request-limit.yaml to clipboard"></div><div class="includecode" id="pods-resource-cpu-request-limit-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>cpu-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cpu-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>vish/stress<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"0.5"</span><span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- -cpus<span>
</span></span></span><span><span><span>    </span>- <span>"2"</span><span>
</span></span></span></code></pre></div></div></div><p>The <code>args</code> section of the configuration file provides arguments for the container when it starts.
The <code>-cpus "2"</code> argument tells the Container to attempt to use 2 CPUs.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit.yaml --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>Verify that the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod cpu-demo --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod cpu-demo --output<span>=</span>yaml --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>The output shows that the one container in the Pod has a CPU request of 500 milliCPU
and a CPU limit of 1 CPU.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>  </span><span>requests</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span></code></pre></div><p>Use <code>kubectl top</code> to fetch the metrics for the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl top pod cpu-demo --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>This example output shows that the Pod is using 974 milliCPU, which is
slightly less than the limit of 1 CPU specified in the Pod configuration.</p><pre tabindex="0"><code>NAME                        CPU(cores)   MEMORY(bytes)
cpu-demo                    974m         &lt;something&gt;
</code></pre><p>Recall that by setting <code>-cpu "2"</code>, you configured the Container to attempt to use 2 CPUs, but the Container is only being allowed to use about 1 CPU. The container's CPU use is being throttled, because the container is attempting to use more CPU resources than its limit.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Another possible explanation for the CPU use being below 1.0 is that the Node might not have
enough CPU resources available. Recall that the prerequisites for this exercise require your cluster to have at least 1 CPU available for use. If your Container runs on a Node that has only 1 CPU, the Container cannot use more than 1 CPU regardless of the CPU limit specified for the Container.</div><h2 id="cpu-units">CPU units</h2><p>The CPU resource is measured in <em>CPU</em> units. One CPU, in Kubernetes, is equivalent to:</p><ul><li>1 AWS vCPU</li><li>1 GCP Core</li><li>1 Azure vCore</li><li>1 Hyperthread on a bare-metal Intel processor with Hyperthreading</li></ul><p>Fractional values are allowed. A Container that requests 0.5 CPU is guaranteed half as much
CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example
100m CPU, 100 milliCPU, and 0.1 CPU are all the same. Precision finer than 1m is not allowed.</p><p>CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same
amount of CPU on a single-core, dual-core, or 48-core machine.</p><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod cpu-demo --namespace<span>=</span>cpu-example
</span></span></code></pre></div><h2 id="specify-a-cpu-request-that-is-too-big-for-your-nodes">Specify a CPU request that is too big for your Nodes</h2><p>CPU requests and limits are associated with Containers, but it is useful to think
of a Pod as having a CPU request and limit. The CPU request for a Pod is the sum
of the CPU requests for all the Containers in the Pod. Likewise, the CPU limit for
a Pod is the sum of the CPU limits for all the Containers in the Pod.</p><p>Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if
the Node has enough CPU resources available to satisfy the Pod CPU request.</p><p>In this exercise, you create a Pod that has a CPU request so big that it exceeds
the capacity of any Node in your cluster. Here is the configuration file for a Pod
that has one Container. The Container requests 100 CPU, which is likely to exceed the
capacity of any Node in your cluster.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/cpu-request-limit-2.yaml"><code>pods/resource/cpu-request-limit-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/cpu-request-limit-2.yaml to clipboard"></div><div class="includecode" id="pods-resource-cpu-request-limit-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu-demo-2<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>cpu-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cpu-demo-ctr-2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>vish/stress<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"100"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"100"</span><span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- -cpus<span>
</span></span></span><span><span><span>    </span>- <span>"2"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit-2.yaml --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>View the Pod status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod cpu-demo-2 --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>The output shows that the Pod status is Pending. That is, the Pod has not been
scheduled to run on any Node, and it will remain in the Pending state indefinitely:</p><pre tabindex="0"><code>NAME         READY     STATUS    RESTARTS   AGE
cpu-demo-2   0/1       Pending   0          7m
</code></pre><p>View detailed information about the Pod, including events:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod cpu-demo-2 --namespace<span>=</span>cpu-example
</span></span></code></pre></div><p>The output shows that the Container cannot be scheduled because of insufficient
CPU resources on the Nodes:</p><pre tabindex="0"><code>Events:
  Reason                        Message
  ------                        -------
  FailedScheduling      No nodes are available that match all of the following predicates:: Insufficient cpu (3).
</code></pre><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod cpu-demo-2 --namespace<span>=</span>cpu-example
</span></span></code></pre></div><h2 id="if-you-do-not-specify-a-cpu-limit">If you do not specify a CPU limit</h2><p>If you do not specify a CPU limit for a Container, then one of these situations applies:</p><ul><li><p>The Container has no upper bound on the CPU resources it can use. The Container
could use all of the CPU resources available on the Node where it is running.</p></li><li><p>The Container is running in a namespace that has a default CPU limit, and the
Container is automatically assigned the default limit. Cluster administrators can use a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#limitrange-v1-core/">LimitRange</a>
to specify a default value for the CPU limit.</p></li></ul><h2 id="if-you-specify-a-cpu-limit-but-do-not-specify-a-cpu-request">If you specify a CPU limit but do not specify a CPU request</h2><p>If you specify a CPU limit for a Container but do not specify a CPU request, Kubernetes automatically
assigns a CPU request that matches the limit. Similarly, if a Container specifies its own memory limit,
but does not specify a memory request, Kubernetes automatically assigns a memory request that matches
the limit.</p><h2 id="motivation-for-cpu-requests-and-limits">Motivation for CPU requests and limits</h2><p>By configuring the CPU requests and limits of the Containers that run in your
cluster, you can make efficient use of the CPU resources available on your cluster
Nodes. By keeping a Pod CPU request low, you give the Pod a good chance of being
scheduled. By having a CPU limit that is greater than the CPU request, you accomplish two things:</p><ul><li>The Pod can have bursts of activity where it makes use of CPU resources that happen to be available.</li><li>The amount of CPU resources a Pod can use during a burst is limited to some reasonable amount.</li></ul><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a></p></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a></p></li></ul></div></div><div><div class="td-content"><h1>Assign Devices to Pods and Containers</h1><div class="lead">Assign infrastructure resources to your Kubernetes workloads.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set Up DRA in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/configure-pod-container/assign-resources/allocate-devices-dra/">Allocate Devices to Workloads with DRA</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Set Up DRA in a Cluster</h1><div class="feature-state-notice feature-stable" title="Feature Gate: DynamicResourceAllocation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This page shows you how to configure <em>dynamic resource allocation (DRA)</em> in a
Kubernetes cluster by enabling API groups and configuring classes of devices.
These instructions are for cluster administrators.</p><h2 id="about-dra">About DRA</h2><p>A Kubernetes feature that lets you request and share resources among Pods.
These resources are often attached
<a class="glossary-tooltip" title="Any resource that's directly or indirectly attached your cluster's nodes, like GPUs or circuit boards." href="/docs/reference/glossary/?all=true#term-device" target="_blank">devices</a> like hardware
accelerators.</p><p>With DRA, device drivers and cluster admins define device <em>classes</em> that are
available to <em>claim</em> in workloads. Kubernetes allocates matching devices to
specific claims and places the corresponding Pods on nodes that can access the
allocated devices.</p><p>Ensure that you're familiar with how DRA works and with DRA terminology like
<a class="glossary-tooltip" title="A category of devices in the cluster. Users can claim specific devices in a DeviceClass." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass" target="_blank">DeviceClasses</a>,
<a class="glossary-tooltip" title="Describes the resources that a workload needs, such as devices. ResourceClaims can request devices from DeviceClasses." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaims-templates" target="_blank">ResourceClaims</a>, and
<a class="glossary-tooltip" title="Defines a template for Kubernetes to create ResourceClaims. Used to provide per-Pod access to separate, similar resources." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaims-templates" target="_blank">ResourceClaimTemplates</a>.
For details, see
<a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation (DRA)</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be version v1.34.<p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>Directly or indirectly attach devices to your cluster. To avoid potential
issues with drivers, wait until you set up the DRA feature for your
cluster before you install drivers.</li></ul><h2 id="enable-dra">Optional: enable legacy DRA API groups</h2><p>DRA graduated to stable in Kubernetes 1.34 and is enabled by default.
Some older DRA drivers or workloads might still need the
v1beta1 API from Kubernetes 1.30 or v1beta2 from Kubernetes 1.32.
If and only if support for those is desired, then enable the following
<a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank">API groups</a>:</p><pre><code>* `resource.k8s.io/v1beta1`
* `resource.k8s.io/v1beta2`
</code></pre><p>For more information, see
<a href="/docs/reference/using-api/#enabling-or-disabling">Enabling or disabling API groups</a>.</p><h2 id="verify">Verify that DRA is enabled</h2><p>To verify that the cluster is configured correctly, try to list DeviceClasses:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deviceclasses
</span></span></code></pre></div><p>If the component configuration was correct, the output is similar to the
following:</p><pre tabindex="0"><code>No resources found
</code></pre><p>If DRA isn't correctly configured, the output of the preceding command is
similar to the following:</p><pre tabindex="0"><code>error: the server doesn't have a resource type "deviceclasses"
</code></pre><p>Try the following troubleshooting steps:</p><ol><li><p>Reconfigure and restart the <code>kube-apiserver</code> component.</p></li><li><p>If the complete <code>.spec.resourceClaims</code> field gets removed from Pods, or if
Pods get scheduled without considering the ResourceClaims, then verify
that the <code>DynamicResourceAllocation</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is not turned off
for kube-apiserver, kube-controller-manager, kube-scheduler or the kubelet.</p></li></ol><h2 id="install-drivers">Install device drivers</h2><p>After you enable DRA for your cluster, you can install the drivers for your
attached devices. For instructions, check the documentation of your device
owner or the project that maintains the device drivers. The drivers that you
install must be compatible with DRA.</p><p>To verify that your installed drivers are working as expected, list
ResourceSlices in your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourceslices
</span></span></code></pre></div><p>The output is similar to the following:</p><pre tabindex="0"><code>NAME                                                  NODE                DRIVER               POOL                             AGE
cluster-1-device-pool-1-driver.example.com-lqx8x      cluster-1-node-1    driver.example.com   cluster-1-device-pool-1-r1gc     7s
cluster-1-device-pool-2-driver.example.com-29t7b      cluster-1-node-2    driver.example.com   cluster-1-device-pool-2-446z     8s
</code></pre><p>Try the following troubleshooting steps:</p><ol><li>Check the health of the DRA driver and look for error messages about
publishing ResourceSlices in its log output. The vendor of the driver
may have further instructions about installation and troubleshooting.</li></ol><h2 id="create-deviceclasses">Create DeviceClasses</h2><p>You can define categories of devices that your application operators can
claim in workloads by creating
<a class="glossary-tooltip" title="A category of devices in the cluster. Users can claim specific devices in a DeviceClass." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass" target="_blank">DeviceClasses</a>. Some device
driver providers might also instruct you to create DeviceClasses during driver
installation.</p><p>The ResourceSlices that your driver publishes contain information about the
devices that the driver manages, such as capacity, metadata, and attributes. You
can use <a class="glossary-tooltip" title="An expression language that's designed to be safe for executing user code." href="https://cel.dev" target="_blank">Common Expression Language</a> to filter for properties in your
DeviceClasses, which can make finding devices easier for your workload
operators.</p><ol><li><p>To find the device properties that you can select in DeviceClasses by using
CEL expressions, get the specification of a ResourceSlice:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get resourceslice &lt;resourceslice-name&gt; -o yaml
</span></span></code></pre></div><p>The output is similar to the following:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceSlice<span>
</span></span></span><span><span><span></span><span># lines omitted for clarity</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>devices</span>:<span>
</span></span></span><span><span><span>  </span>- <span>attributes</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span>
</span></span></span><span><span><span>        </span><span>string</span>:<span> </span>gpu<span>
</span></span></span><span><span><span>    </span><span>capacity</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>64Gi<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>gpu-0<span>
</span></span></span><span><span><span>  </span>- <span>attributes</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span>
</span></span></span><span><span><span>        </span><span>string</span>:<span> </span>gpu<span>
</span></span></span><span><span><span>    </span><span>capacity</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>64Gi<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>gpu-1<span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>driver.example.com<span>
</span></span></span><span><span><span>  </span><span>nodeName</span>:<span> </span>cluster-1-node-1<span>
</span></span></span><span><span><span></span><span># lines omitted for clarity</span><span>
</span></span></span></code></pre></div><p>You can also check the driver provider's documentation for available
properties and values.</p></li><li><p>Review the following example DeviceClass manifest, which selects any device
that's managed by the <code>driver.example.com</code> device driver:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/dra/deviceclass.yaml"><code>dra/deviceclass.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy dra/deviceclass.yaml to clipboard"></div><div class="includecode" id="dra-deviceclass-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DeviceClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-device-class<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selectors</span>:<span>
</span></span></span><span><span><span>  </span>- <span>cel</span>:<span>
</span></span></span><span><span><span>      </span><span>expression</span>:<span> </span>|-<span>
</span></span></span><span><span><span>        device.driver == "driver.example.com"</span><span>        
</span></span></span></code></pre></div></div></div></li><li><p>Create the DeviceClass in your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/dra/deviceclass.yaml
</span></span></code></pre></div></li></ol><h2 id="clean-up">Clean up</h2><p>To delete the DeviceClass that you created in this task, run the following
command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f https://k8s.io/examples/dra/deviceclass.yaml
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Learn more about DRA</a></li><li><a href="/docs/tasks/configure-pod-container/assign-resources/allocate-devices-dra/">Allocate Devices to Workloads with DRA</a></li></ul></div></div><div><div class="td-content"><h1>Allocate Devices to Workloads with DRA</h1><div class="feature-state-notice feature-stable" title="Feature Gate: DynamicResourceAllocation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This page shows you how to allocate devices to your Pods by using
<em>dynamic resource allocation (DRA)</em>. These instructions are for workload
operators. Before reading this page, familiarize yourself with how DRA works and
with DRA terminology like
<a class="glossary-tooltip" title="Describes the resources that a workload needs, such as devices. ResourceClaims can request devices from DeviceClasses." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaims-templates" target="_blank">ResourceClaims</a> and
<a class="glossary-tooltip" title="Defines a template for Kubernetes to create ResourceClaims. Used to provide per-Pod access to separate, similar resources." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaims-templates" target="_blank">ResourceClaimTemplates</a>.
For more information, see
<a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation (DRA)</a>.</p><h2 id="about-device-allocation-dra">About device allocation with DRA</h2><p>As a workload operator, you can <em>claim</em> devices for your workloads by creating
ResourceClaims or ResourceClaimTemplates. When you deploy your workload,
Kubernetes and the device drivers find available devices, allocate them to your
Pods, and place the Pods on nodes that can access those devices.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be version v1.34.<p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>Ensure that your cluster admin has set up DRA, attached devices, and installed
drivers. For more information, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set Up DRA in a Cluster</a>.</li></ul><h2 id="identify-devices">Identify devices to claim</h2><p>Your cluster administrator or the device drivers create
<em><a class="glossary-tooltip" title="A category of devices in the cluster. Users can claim specific devices in a DeviceClass." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass" target="_blank">DeviceClasses</a></em> that
define categories of devices. You can claim devices by using
<a class="glossary-tooltip" title="An expression language that's designed to be safe for executing user code." href="https://cel.dev" target="_blank">Common Expression Language</a> to filter for specific device properties.</p><p>Get a list of DeviceClasses in the cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deviceclasses
</span></span></code></pre></div><p>The output is similar to the following:</p><pre tabindex="0"><code>NAME                 AGE
driver.example.com   16m
</code></pre><p>If you get a permission error, you might not have access to get DeviceClasses.
Check with your cluster administrator or with the driver provider for available
device properties.</p><h2 id="claim-resources">Claim resources</h2><p>You can request resources from a DeviceClass by using
<a class="glossary-tooltip" title="Describes the resources that a workload needs, such as devices. ResourceClaims can request devices from DeviceClasses." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaims-templates" target="_blank">ResourceClaims</a>. To
create a ResourceClaim, do one of the following:</p><ul><li>Manually create a ResourceClaim if you want multiple Pods to share access to
the same devices, or if you want a claim to exist beyond the lifetime of a
Pod.</li><li>Use a
<a class="glossary-tooltip" title="Defines a template for Kubernetes to create ResourceClaims. Used to provide per-Pod access to separate, similar resources." href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaims-templates" target="_blank">ResourceClaimTemplate</a>
to let Kubernetes generate and manage per-Pod ResourceClaims. Create a
ResourceClaimTemplate if you want every Pod to have access to separate devices
that have similar configurations. For example, you might want simultaneous
access to devices for Pods in a Job that uses
<a href="/docs/concepts/workloads/controllers/job/#parallel-jobs">parallel execution</a>.</li></ul><p>If you directly reference a specific ResourceClaim in a Pod, that ResourceClaim
must already exist in the cluster. If a referenced ResourceClaim doesn't exist,
the Pod remains in a pending state until the ResourceClaim is created. You can
reference an auto-generated ResourceClaim in a Pod, but this isn't recommended
because auto-generated ResourceClaims are bound to the lifetime of the Pod that
triggered the generation.</p><p>To create a workload that claims resources, select one of the following options:</p><ul class="nav nav-tabs" id="claim-resources"><li class="nav-item"><a class="nav-link active" href="#claim-resources-0">ResourceClaimTemplate</a></li><li class="nav-item"><a class="nav-link" href="#claim-resources-1">ResourceClaim</a></li></ul><div class="tab-content" id="claim-resources"><div id="claim-resources-0" class="tab-pane show active"><p><p>Review the following example manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/dra/resourceclaimtemplate.yaml"><code>dra/resourceclaimtemplate.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy dra/resourceclaimtemplate.yaml to clipboard"></div><div class="includecode" id="dra-resourceclaimtemplate-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceClaimTemplate<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-resource-claim-template<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>devices</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>gpu-claim<span>
</span></span></span><span><span><span>        </span><span>exactly</span>:<span>
</span></span></span><span><span><span>          </span><span>deviceClassName</span>:<span> </span>example-device-class<span>
</span></span></span><span><span><span>          </span><span>selectors</span>:<span>
</span></span></span><span><span><span>            </span>- <span>cel</span>:<span>
</span></span></span><span><span><span>                </span><span>expression</span>:<span> </span>|-<span>
</span></span></span><span><span><span>                  device.attributes["driver.example.com"].type == "gpu" &amp;&amp;
</span></span></span><span><span><span>                  device.capacity["driver.example.com"].memory == quantity("64Gi")</span><span>                  
</span></span></span></code></pre></div></div></div><p>This manifest creates a ResourceClaimTemplate that requests devices in the
<code>example-device-class</code> DeviceClass that match both of the following parameters:</p><ul><li>Devices that have a <code>driver.example.com/type</code> attribute with a value of
<code>gpu</code>.</li><li>Devices that have <code>64Gi</code> of capacity.</li></ul><p>To create the ResourceClaimTemplate, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/dra/resourceclaimtemplate.yaml
</span></span></code></pre></div></p></div><div id="claim-resources-1" class="tab-pane"><p><p>Review the following example manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/dra/resourceclaim.yaml"><code>dra/resourceclaim.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy dra/resourceclaim.yaml to clipboard"></div><div class="includecode" id="dra-resourceclaim-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-resource-claim<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>devices</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>single-gpu-claim<span>
</span></span></span><span><span><span>      </span><span>exactly</span>:<span>
</span></span></span><span><span><span>        </span><span>deviceClassName</span>:<span> </span>example-device-class<span>
</span></span></span><span><span><span>        </span><span>allocationMode</span>:<span> </span>All<span>
</span></span></span><span><span><span>        </span><span>selectors</span>:<span>
</span></span></span><span><span><span>        </span>- <span>cel</span>:<span>
</span></span></span><span><span><span>            </span><span>expression</span>:<span> </span>|-<span>
</span></span></span><span><span><span>              device.attributes["driver.example.com"].type == "gpu" &amp;&amp;
</span></span></span><span><span><span>              device.capacity["driver.example.com"].memory == quantity("64Gi")</span><span>              
</span></span></span></code></pre></div></div></div><p>This manifest creates ResourceClaim that requests devices in the
<code>example-device-class</code> DeviceClass that match both of the following parameters:</p><ul><li>Devices that have a <code>driver.example.com/type</code> attribute with a value of
<code>gpu</code>.</li><li>Devices that have <code>64Gi</code> of capacity.</li></ul><p>To create the ResourceClaim, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/dra/resourceclaim.yaml
</span></span></code></pre></div></p></div></div><h2 id="request-devices-workloads">Request devices in workloads using DRA</h2><p>To request device allocation, specify a ResourceClaim or a ResourceClaimTemplate
in the <code>resourceClaims</code> field of the Pod specification. Then, request a specific
claim by name in the <code>resources.claims</code> field of a container in that Pod.
You can specify multiple entries in the <code>resourceClaims</code> field and use specific
claims in different containers.</p><ol><li><p>Review the following example Job:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/dra/dra-example-job.yaml"><code>dra/dra-example-job.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy dra/dra-example-job.yaml to clipboard"></div><div class="includecode" id="dra-dra-example-job-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-dra-job<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>container0<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>ubuntu:24.04<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"9999"</span>]<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>claims</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>separate-gpu-claim<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>container1<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>ubuntu:24.04<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"9999"</span>]<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>claims</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>shared-gpu-claim<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>container2<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>ubuntu:24.04<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"9999"</span>]<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>claims</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>shared-gpu-claim<span>
</span></span></span><span><span><span>      </span><span>resourceClaims</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>separate-gpu-claim<span>
</span></span></span><span><span><span>        </span><span>resourceClaimTemplateName</span>:<span> </span>example-resource-claim-template<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>shared-gpu-claim<span>
</span></span></span><span><span><span>        </span><span>resourceClaimName</span>:<span> </span>example-resource-claim<span>
</span></span></span></code></pre></div></div></div><p>Each Pod in this Job has the following properties:</p><ul><li>Makes a ResourceClaimTemplate named <code>separate-gpu-claim</code> and a
ResourceClaim named <code>shared-gpu-claim</code> available to containers.</li><li>Runs the following containers:<ul><li><code>container0</code> requests the devices from the <code>separate-gpu-claim</code>
ResourceClaimTemplate.</li><li><code>container1</code> and <code>container2</code> share access to the devices from the
<code>shared-gpu-claim</code> ResourceClaim.</li></ul></li></ul></li><li><p>Create the Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/dra/dra-example-job.yaml
</span></span></code></pre></div></li></ol><p>Try the following troubleshooting steps:</p><ol><li>When the workload does not start as expected, drill down from Job
to Pods to ResourceClaims and check the objects
at each level with <code>kubectl describe</code> to see whether there are any
status fields or events which might explain why the workload is
not starting.</li><li>When creating a Pod fails with <code>must specify one of: resourceClaimName, resourceClaimTemplateName</code>, check that all entries in <code>pod.spec.resourceClaims</code>
have exactly one of those fields set. If they do, then it is possible
that the cluster has a mutating Pod webhook installed which was built
against APIs from Kubernetes &lt; 1.32. Work with your cluster administrator
to check this.</li></ol><h2 id="clean-up">Clean up</h2><p>To delete the Kubernetes objects that you created in this task, follow these
steps:</p><ol><li><p>Delete the example Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f https://k8s.io/examples/dra/dra-example-job.yaml
</span></span></code></pre></div></li><li><p>To delete your resource claims, run one of the following commands:</p><ul><li><p>Delete the ResourceClaimTemplate:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f https://k8s.io/examples/dra/resourceclaimtemplate.yaml
</span></span></code></pre></div></li><li><p>Delete the ResourceClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f https://k8s.io/examples/dra/resourceclaim.yaml
</span></span></code></pre></div></li></ul></li></ol><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Learn more about DRA</a></li></ul></div></div><div><div class="td-content"><h1>Assign Pod-level CPU and memory resources</h1><div class="feature-state-notice feature-beta" title="Feature Gate: PodLevelResources"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>This page shows how to specify CPU and memory resources for a Pod at pod-level in
addition to container-level resource specifications. A Kubernetes node allocates
resources to a pod based on the pod's resource requests. These requests can be
defined at the pod level or individually for containers within the pod. When
both are present, the pod-level requests take precedence.</p><p>Similarly, a pod's resource usage is restricted by limits, which can also be set at
the pod-level or individually for containers within the pod. Again,
pod-level limits are prioritized when both are present. This allows for flexible
resource management, enabling you to control resource allocation at both the pod and
container levels.</p><p>In order to specify the resources at pod-level, it is required to enable
<code>PodLevelResources</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>For Pod Level Resources:</p><ul><li>Priority: When both pod-level and container-level resources are specified,
pod-level resources take precedence.</li><li>QoS: Pod-level resources take precedence in influencing the QoS class of the pod.</li><li>OOM Score: The OOM score adjustment calculation considers both pod-level and
container-level resources.</li><li>Compatibility: Pod-level resources are designed to be compatible with existing
features.</li></ul><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.34.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>The <code>PodLevelResources</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> must be enabled
for your control plane and for all nodes in your cluster.</p><h2 id="limitations">Limitations</h2><p>For Kubernetes 1.34, resizing pod-level resources has the
following limitations:</p><ul><li><strong>Resource Types:</strong> Only CPU, memory and hugepages resources can be specified at pod-level.</li><li><strong>Operating System:</strong> Pod-level resources are not supported for Windows
pods.</li><li><strong>Resource Managers:</strong> The Topology Manager, Memory Manager and CPU Manager do not
align pods and containers based on pod-level resources as these resource managers
don't currently support pod-level resources.</li><li><strong><a href="https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/">In-Place
Resize</a>:</strong>
In-place resize of pod-level resources is not supported. Modifying the pod-level resource
limits or requests on a pod result in a field.Forbidden error. The error message
explicitly states, "pods with pod-level resources cannot be resized."</li></ul><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace pod-resources-example
</span></span></code></pre></div><h2 id="create-a-pod-with-memory-requests-and-limits-at-pod-level">Create a pod with memory requests and limits at pod-level</h2><p>To specify memory requests for a Pod at pod-level, include the <code>resources.requests.memory</code>
field in the Pod spec manifest. To specify a memory limit, include <code>resources.limits.memory</code>.</p><p>In this exercise, you create a Pod that has one Container. The Pod has a
memory request of 100 MiB and a memory limit of 200 MiB. Here's the configuration
file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/pod-level-memory-request-limit.yaml"><code>pods/resource/pod-level-memory-request-limit.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/pod-level-memory-request-limit.yaml to clipboard"></div><div class="includecode" id="pods-resource-pod-level-memory-request-limit-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>memory-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>pod-resources-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>memory-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"stress"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"--vm"</span>,<span> </span><span>"1"</span>,<span> </span><span>"--vm-bytes"</span>,<span> </span><span>"150M"</span>,<span> </span><span>"--vm-hang"</span>,<span> </span><span>"1"</span>]<span>
</span></span></span></code></pre></div></div></div><p>The <code>args</code> section in the manifest provides arguments for the container when it starts.
The <code>"--vm-bytes", "150M"</code> arguments tell the Container to attempt to allocate 150 MiB of memory.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/pod-level-memory-request-limit.yaml --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>Verify that the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo --output<span>=</span>yaml --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>The output shows that the Pod has a memory request of 100 MiB
and a memory limit of 200 MiB.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span> 
</span></span></span><span><span><span>  </span><span>containers</span>:<span>    
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>Run <code>kubectl top</code> to fetch the metrics for the pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl top pod memory-demo --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>The output shows that the Pod is using about 162,900,000 bytes of memory, which
is about 150 MiB. This is greater than the Pod's 100 MiB request, but within the
Pod's 200 MiB limit.</p><pre tabindex="0"><code>NAME                        CPU(cores)   MEMORY(bytes)
memory-demo                 &lt;something&gt;  162856960
</code></pre><h2 id="create-a-pod-with-cpu-requests-and-limits-at-pod-level">Create a pod with CPU requests and limits at pod-level</h2><p>To specify a CPU request for a Pod, include the <code>resources.requests.cpu</code> field
in the Pod spec manifest. To specify a CPU limit, include <code>resources.limits.cpu</code>.</p><p>In this exercise, you create a Pod that has one container. The Pod has a request
of 0.5 CPU and a limit of 1 CPU. Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/pod-level-cpu-request-limit.yaml"><code>pods/resource/pod-level-cpu-request-limit.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/pod-level-cpu-request-limit.yaml to clipboard"></div><div class="includecode" id="pods-resource-pod-level-cpu-request-limit-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>pod-resources-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"0.5"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cpu-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>vish/stress<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- -cpus<span>
</span></span></span><span><span><span>    </span>- <span>"2"</span><span>
</span></span></span></code></pre></div></div></div><p>The <code>args</code> section of the configuration file provides arguments for the container when it starts.
The <code>-cpus "2"</code> argument tells the Container to attempt to use 2 CPUs.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/pod-level-cpu-request-limit.yaml --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>Verify that the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod cpu-demo --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod cpu-demo --output<span>=</span>yaml --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>The output shows that the Pod has a CPU request of 500 milliCPU
and a CPU limit of 1 CPU.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span></code></pre></div><p>Use <code>kubectl top</code> to fetch the metrics for the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl top pod cpu-demo --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>This example output shows that the Pod is using 974 milliCPU, which is
slightly less than the limit of 1 CPU specified in the Pod configuration.</p><pre tabindex="0"><code>NAME                        CPU(cores)   MEMORY(bytes)
cpu-demo                    974m         &lt;something&gt;
</code></pre><p>Recall that by setting <code>-cpu "2"</code>, you configured the Container to attempt to use 2
CPUs, but the Container is only being allowed to use about 1 CPU. The container's
CPU use is being throttled, because the container is attempting to use more CPU
resources than the Pod CPU limit.</p><h2 id="create-a-pod-with-resource-requests-and-limits-at-both-pod-level-and-container-level">Create a pod with resource requests and limits at both pod-level and container-level</h2><p>To assign CPU and memory resources to a Pod, you can specify them at both the pod
level and the container level. Include the <code>resources</code> field in the Pod spec to
define resources for the entire Pod. Additionally, include the <code>resources</code> field
within container's specification in the Pod's manifest to set container-specific
resource requirements.</p><p>In this exercise, you'll create a Pod with two containers to explore the interaction
of pod-level and container-level resource specifications. The Pod itself will have
defined CPU requests and limits, while only one of the containers will have its own
explicit resource requests and limits. The other container will inherit the resource
constraints from the pod-level settings. Here's the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/pod-level-resources.yaml"><code>pods/resource/pod-level-resources.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/pod-level-resources.yaml to clipboard"></div><div class="includecode" id="pods-resource-pod-level-resources-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-resources-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>pod-resources-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-resources-demo-ctr-1<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"0.5"</span><span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"0.5"</span><span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"50Mi"</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-resources-demo-ctr-2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>fedora<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- sleep<span>
</span></span></span><span><span><span>    </span>- inf <span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/pod-level-resources.yaml --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>Verify that the Pod Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod-resources-demo --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod memory-demo --output<span>=</span>yaml --namespace<span>=</span>pod-resources-example
</span></span></code></pre></div><p>The output shows that one container in the Pod has a memory request of 50 MiB and a
CPU request of 0.5 cores, with a memory limit of 100 MiB and a CPU limit of 0.5
cores. The Pod itself has a memory request of 100 MiB and a CPU request of
1 core, and a memory limit of 200 MiB and a CPU limit of 1 core.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-resources-demo-ctr-1<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>50Mi<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-resources-demo-ctr-2<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span> </span>{}<span>  
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>Since pod-level requests and limits are specified, the request guarantees for both
containers in the pod will be equal 1 core or CPU and 100Mi of memory. Additionally,
both containers together won't be able to use more resources than specified in the
pod-level limits, ensuring they cannot exceed a combined total of 200 MiB of memory
and 1 core of CPU.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace pod-resources-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-application-developers">For application developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure GMSA for Windows Pods and containers</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>This page shows how to configure
<a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/group-managed-service-accounts-overview">Group Managed Service Accounts</a> (GMSA)
for Pods and containers that will run on Windows nodes. Group Managed Service Accounts
are a specific type of Active Directory account that provides automatic password management,
simplified service principal name (SPN) management, and the ability to delegate the management
to other administrators across multiple servers.</p><p>In Kubernetes, GMSA credential specs are configured at a Kubernetes cluster-wide scope
as Custom Resources. Windows Pods, as well as individual containers within a Pod,
can be configured to use a GMSA for domain based functions (e.g. Kerberos authentication)
when interacting with other Windows services.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster and the <code>kubectl</code> command-line tool must be
configured to communicate with your cluster. The cluster is expected to have Windows worker nodes.
This section covers a set of initial steps required once for each cluster:</p><h3 id="install-the-gmsacredentialspec-crd">Install the GMSACredentialSpec CRD</h3><p>A <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition</a>(CRD)
for GMSA credential spec resources needs to be configured on the cluster to define
the custom resource type <code>GMSACredentialSpec</code>. Download the GMSA CRD
<a href="https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/gmsa-crd.yml">YAML</a>
and save it as gmsa-crd.yaml. Next, install the CRD with <code>kubectl apply -f gmsa-crd.yaml</code></p><h3 id="install-webhooks-to-validate-gmsa-users">Install webhooks to validate GMSA users</h3><p>Two webhooks need to be configured on the Kubernetes cluster to populate and
validate GMSA credential spec references at the Pod or container level:</p><ol><li><p>A mutating webhook that expands references to GMSAs (by name from a Pod specification)
into the full credential spec in JSON form within the Pod spec.</p></li><li><p>A validating webhook ensures all references to GMSAs are authorized to be used by the Pod service account.</p></li></ol><p>Installing the above webhooks and associated objects require the steps below:</p><ol><li><p>Create a certificate key pair (that will be used to allow the webhook container to communicate to the cluster)</p></li><li><p>Install a secret with the certificate from above.</p></li><li><p>Create a deployment for the core webhook logic.</p></li><li><p>Create the validating and mutating webhook configurations referring to the deployment.</p></li></ol><p>A <a href="https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/deploy-gmsa-webhook.sh">script</a>
can be used to deploy and configure the GMSA webhooks and associated objects
mentioned above. The script can be run with a <code>--dry-run=server</code> option to
allow you to review the changes that would be made to your cluster.</p><p>The <a href="https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/gmsa-webhook.yml.tpl">YAML template</a>
used by the script may also be used to deploy the webhooks and associated objects
manually (with appropriate substitutions for the parameters)</p><h2 id="configure-gmsas-and-windows-nodes-in-active-directory">Configure GMSAs and Windows nodes in Active Directory</h2><p>Before Pods in Kubernetes can be configured to use GMSAs, the desired GMSAs need
to be provisioned in Active Directory as described in the
<a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/getting-started-with-group-managed-service-accounts#BKMK_Step1">Windows GMSA documentation</a>.
Windows worker nodes (that are part of the Kubernetes cluster) need to be configured
in Active Directory to access the secret credentials associated with the desired GMSA as described in the
<a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/getting-started-with-group-managed-service-accounts#to-add-member-hosts-using-the-set-adserviceaccount-cmdlet">Windows GMSA documentation</a>.</p><h2 id="create-gmsa-credential-spec-resources">Create GMSA credential spec resources</h2><p>With the GMSACredentialSpec CRD installed (as described earlier), custom resources
containing GMSA credential specs can be configured. The GMSA credential spec does
not contain secret or sensitive data. It is information that a container runtime
can use to describe the desired GMSA of a container to Windows. GMSA credential
specs can be generated in YAML format with a utility
<a href="https://github.com/kubernetes-sigs/windows-gmsa/tree/master/scripts/GenerateCredentialSpecResource.ps1">PowerShell script</a>.</p><p>Following are the steps for generating a GMSA credential spec YAML manually in JSON format and then converting it:</p><ol><li><p>Import the CredentialSpec
<a href="https://github.com/MicrosoftDocs/Virtualization-Documentation/blob/live/windows-server-container-tools/ServiceAccounts/CredentialSpec.psm1">module</a>: <code>ipmo CredentialSpec.psm1</code></p></li><li><p>Create a credential spec in JSON format using <code>New-CredentialSpec</code>.
To create a GMSA credential spec named WebApp1, invoke
<code>New-CredentialSpec -Name WebApp1 -AccountName WebApp1 -Domain $(Get-ADDomain -Current LocalComputer)</code></p></li><li><p>Use <code>Get-CredentialSpec</code> to show the path of the JSON file.</p></li><li><p>Convert the credspec file from JSON to YAML format and apply the necessary
header fields <code>apiVersion</code>, <code>kind</code>, <code>metadata</code> and <code>credspec</code> to make it a
GMSACredentialSpec custom resource that can be configured in Kubernetes.</p></li></ol><p>The following YAML configuration describes a GMSA credential spec named <code>gmsa-WebApp1</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>windows.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>GMSACredentialSpec<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>gmsa-WebApp1 <span> </span><span># This is an arbitrary name but it will be used as a reference</span><span>
</span></span></span><span><span><span></span><span>credspec</span>:<span>
</span></span></span><span><span><span>  </span><span>ActiveDirectoryConfig</span>:<span>
</span></span></span><span><span><span>    </span><span>GroupManagedServiceAccounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>Name</span>:<span> </span>WebApp1  <span> </span><span># Username of the GMSA account</span><span>
</span></span></span><span><span><span>      </span><span>Scope</span>:<span> </span>CONTOSO <span> </span><span># NETBIOS Domain Name</span><span>
</span></span></span><span><span><span>    </span>- <span>Name</span>:<span> </span>WebApp1  <span> </span><span># Username of the GMSA account</span><span>
</span></span></span><span><span><span>      </span><span>Scope</span>:<span> </span>contoso.com<span> </span><span># DNS Domain Name</span><span>
</span></span></span><span><span><span>  </span><span>CmsPlugins</span>:<span>
</span></span></span><span><span><span>  </span>- ActiveDirectory<span>
</span></span></span><span><span><span>  </span><span>DomainJoinConfig</span>:<span>
</span></span></span><span><span><span>    </span><span>DnsName</span>:<span> </span>contoso.com <span> </span><span># DNS Domain Name</span><span>
</span></span></span><span><span><span>    </span><span>DnsTreeName</span>:<span> </span>contoso.com<span> </span><span># DNS Domain Name Root</span><span>
</span></span></span><span><span><span>    </span><span>Guid</span>:<span> </span>244818ae-87ac-4fcd-92ec-e79e5252348a <span> </span><span># GUID of the Domain</span><span>
</span></span></span><span><span><span>    </span><span>MachineAccountName</span>:<span> </span>WebApp1<span> </span><span># Username of the GMSA account</span><span>
</span></span></span><span><span><span>    </span><span>NetBiosName</span>:<span> </span>CONTOSO <span> </span><span># NETBIOS Domain Name</span><span>
</span></span></span><span><span><span>    </span><span>Sid</span>:<span> </span>S-1-5-21-2126449477-2524075714-3094792973<span> </span><span># SID of the Domain</span><span>
</span></span></span></code></pre></div><p>The above credential spec resource may be saved as <code>gmsa-Webapp1-credspec.yaml</code>
and applied to the cluster using: <code>kubectl apply -f gmsa-Webapp1-credspec.yml</code></p><h2 id="configure-cluster-role-to-enable-rbac-on-specific-gmsa-credential-specs">Configure cluster role to enable RBAC on specific GMSA credential specs</h2><p>A cluster role needs to be defined for each GMSA credential spec resource. This
authorizes the <code>use</code> verb on a specific GMSA resource by a subject which is typically
a service account. The following example shows a cluster role that authorizes usage
of the <code>gmsa-WebApp1</code> credential spec from above. Save the file as gmsa-webapp1-role.yaml
and apply using <code>kubectl apply -f gmsa-webapp1-role.yaml</code></p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Create the Role to read the credspec</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>webapp1-role<span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span> </span>[<span>"windows.k8s.io"</span>]<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span> </span>[<span>"gmsacredentialspecs"</span>]<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span> </span>[<span>"use"</span>]<span>
</span></span></span><span><span><span>  </span><span>resourceNames</span>:<span> </span>[<span>"gmsa-WebApp1"</span>]<span>
</span></span></span></code></pre></div><h2 id="assign-role-to-service-accounts-to-use-specific-gmsa-credspecs">Assign role to service accounts to use specific GMSA credspecs</h2><p>A service account (that Pods will be configured with) needs to be bound to the
cluster role create above. This authorizes the service account to use the desired
GMSA credential spec resource. The following shows the default service account
being bound to a cluster role <code>webapp1-role</code> to use <code>gmsa-WebApp1</code> credential spec resource created above.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>RoleBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>allow-default-svc-account-read-on-gmsa-WebApp1<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span></span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>webapp1-role<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span></code></pre></div><h2 id="configure-gmsa-credential-spec-reference-in-pod-spec">Configure GMSA credential spec reference in Pod spec</h2><p>The Pod spec field <code>securityContext.windowsOptions.gmsaCredentialSpecName</code> is used to
specify references to desired GMSA credential spec custom resources in Pod specs.
This configures all containers in the Pod spec to use the specified GMSA. A sample
Pod spec with the annotation populated to refer to <code>gmsa-WebApp1</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>run</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>        </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>          </span><span>gmsaCredentialSpecName</span>:<span> </span>gmsa-webapp1<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span>
</span></span></span><span><span><span>        </span><span>imagePullPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>iis<span>
</span></span></span><span><span><span>      </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span></code></pre></div><p>Individual containers in a Pod spec can also specify the desired GMSA credspec
using a per-container <code>securityContext.windowsOptions.gmsaCredentialSpecName</code> field. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>run</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>with-creds<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span>
</span></span></span><span><span><span>        </span><span>imagePullPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>iis<span>
</span></span></span><span><span><span>        </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>          </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>            </span><span>gmsaCredentialSpecName</span>:<span> </span>gmsa-Webapp1<span>
</span></span></span><span><span><span>      </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span></code></pre></div><p>As Pod specs with GMSA fields populated (as described above) are applied in a cluster,
the following sequence of events take place:</p><ol><li><p>The mutating webhook resolves and expands all references to GMSA credential spec
resources to the contents of the GMSA credential spec.</p></li><li><p>The validating webhook ensures the service account associated with the Pod is
authorized for the <code>use</code> verb on the specified GMSA credential spec.</p></li><li><p>The container runtime configures each Windows container with the specified GMSA
credential spec so that the container can assume the identity of the GMSA in
Active Directory and access services in the domain using that identity.</p></li></ol><h2 id="authenticating-to-network-shares-using-hostname-or-fqdn">Authenticating to network shares using hostname or FQDN</h2><p>If you are experiencing issues connecting to SMB shares from Pods using hostname or FQDN,
but are able to access the shares via their IPv4 address then make sure the following registry key is set on the Windows nodes.</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>reg add <span>"HKLM\SYSTEM\CurrentControlSet\Services\hns\State"</span> /v EnableCompartmentNamespace /t REG_DWORD /d 1
</span></span></code></pre></div><p>Running Pods will then need to be recreated to pick up the behavior changes.
More information on how this registry key is used can be found
<a href="https://github.com/microsoft/hcsshim/blob/885f896c5a8548ca36c88c4b87fd2208c8d16543/internal/uvm/create.go#L74-L83">here</a></p><h2 id="troubleshooting">Troubleshooting</h2><p>If you are having difficulties getting GMSA to work in your environment,
there are a few troubleshooting steps you can take.</p><p>First, make sure the credspec has been passed to the Pod. To do this you will need
to <code>exec</code> into one of your Pods and check the output of the <code>nltest.exe /parentdomain</code> command.</p><p>In the example below the Pod did not get the credspec correctly:</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>kubectl exec -it <span>iis-auth</span>-<span>7776966999</span>-n5nzr powershell.exe
</span></span></code></pre></div><p><code>nltest.exe /parentdomain</code> results in the following error:</p><pre tabindex="0"><code class="language-output">Getting parent domain failed: Status = 1722 0x6ba RPC_S_SERVER_UNAVAILABLE
</code></pre><p>If your Pod did get the credspec correctly, then next check communication with the domain.
First, from inside of your Pod, quickly do an nslookup to find the root of your domain.</p><p>This will tell us 3 things:</p><ol><li>The Pod can reach the DC</li><li>The DC can reach the Pod</li><li>DNS is working correctly.</li></ol><p>If the DNS and communication test passes, next you will need to check if the Pod has
established secure channel communication with the domain. To do this, again,
<code>exec</code> into your Pod and run the <code>nltest.exe /query</code> command.</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>nltest.exe /query
</span></span></code></pre></div><p>Results in the following output:</p><pre tabindex="0"><code class="language-output">I_NetLogonControl failed: Status = 1722 0x6ba RPC_S_SERVER_UNAVAILABLE
</code></pre><p>This tells us that for some reason, the Pod was unable to logon to the domain using
the account specified in the credspec. You can try to repair the secure channel by running the following:</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>nltest /sc_reset<span>:</span>domain.example
</span></span></code></pre></div><p>If the command is successful you will see and output similar to this:</p><pre tabindex="0"><code class="language-output">Flags: 30 HAS_IP  HAS_TIMESERV
Trusted DC Name \\dc10.domain.example
Trusted DC Connection Status Status = 0 0x0 NERR_Success
The command completed successfully
</code></pre><p>If the above corrects the error, you can automate the step by adding the following
lifecycle hook to your Pod spec. If it did not correct the error, you will need to
examine your credspec again and confirm that it is correct and complete.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>        </span><span>image</span>:<span> </span>registry.domain.example/iis-auth:1809v1<span>
</span></span></span><span><span><span>        </span><span>lifecycle</span>:<span>
</span></span></span><span><span><span>          </span><span>postStart</span>:<span>
</span></span></span><span><span><span>            </span><span>exec</span>:<span>
</span></span></span><span><span><span>              </span><span>command</span>:<span> </span>[<span>"powershell.exe"</span>,<span>"-command"</span>,<span>"do { Restart-Service -Name netlogon } while ( $($Result = (nltest.exe /query); if ($Result -like '*0x0 NERR_Success*') {return $true} else {return $false}) -eq $false)"</span>]<span>
</span></span></span><span><span><span>        </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div><p>If you add the <code>lifecycle</code> section show above to your Pod spec, the Pod will execute
the commands listed to restart the <code>netlogon</code> service until the <code>nltest.exe /query</code> command exits without error.</p></div></div><div><div class="td-content"><h1>Resize CPU and Memory Resources assigned to Containers</h1><div class="feature-state-notice feature-beta" title="Feature Gate: InPlacePodVerticalScaling"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>This page explains how to change the CPU and memory resource requests and limits
assigned to a container <em>without recreating the Pod</em>.</p><p>Traditionally, changing a Pod's resource requirements necessitated deleting the existing Pod
and creating a replacement, often managed by a <a href="/docs/concepts/workloads/controllers/">workload controller</a>.
In-place Pod Resize allows changing the CPU/memory allocation of container(s) within a running Pod
while potentially avoiding application disruption.</p><p><strong>Key Concepts:</strong></p><ul><li><strong>Desired Resources:</strong> A container's <code>spec.containers[*].resources</code> represent
the <em>desired</em> resources for the container, and are mutable for CPU and memory.</li><li><strong>Actual Resources:</strong> The <code>status.containerStatuses[*].resources</code> field
reflects the resources <em>currently configured</em> for a running container.
For containers that haven't started or were restarted,
it reflects the resources allocated upon their next start.</li><li><strong>Triggering a Resize:</strong> You can request a resize by updating the desired <code>requests</code>
and <code>limits</code> in the Pod's specification.
This is typically done using <code>kubectl patch</code>, <code>kubectl apply</code>, or <code>kubectl edit</code>
targeting the Pod's <code>resize</code> subresource.
When the desired resources don't match the allocated resources,
the Kubelet will attempt to resize the container.</li><li><strong>Allocated Resources (Advanced):</strong>
The <code>status.containerStatuses[*].allocatedResources</code> field tracks resource values
confirmed by the Kubelet, primarily used for internal scheduling logic.
For most monitoring and validation purposes, focus on <code>status.containerStatuses[*].resources</code>.</li></ul><p>If a node has pods with a pending or incomplete resize (see <a href="#pod-resize-status">Pod Resize Status</a> below),
the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">scheduler</a> uses
the <em>maximum</em> of a container's desired requests, allocated requests,
and actual requests from the status when making scheduling decisions.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.33.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>The <code>InPlacePodVerticalScaling</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
must be enabled
for your control plane and for all nodes in your cluster.</p><p>The <code>kubectl</code> client version must be at least v1.32 to use the <code>--subresource=resize</code> flag.</p><h2 id="pod-resize-status">Pod resize status</h2><p>The Kubelet updates the Pod's status conditions to indicate the state of a resize request:</p><ul><li><code>type: PodResizePending</code>: The Kubelet cannot immediately grant the request.
The <code>message</code> field provides an explanation of why.<ul><li><code>reason: Infeasible</code>: The requested resize is impossible on the current node
(for example, requesting more resources than the node has).</li><li><code>reason: Deferred</code>: The requested resize is currently not possible,
but might become feasible later (for example if another pod is removed).
The Kubelet will retry the resize.</li></ul></li><li><code>type: PodResizeInProgress</code>: The Kubelet has accepted the resize and allocated resources,
but the changes are still being applied.
This is usually brief but might take longer depending on the resource type and runtime behavior.
Any errors during actuation are reported in the <code>message</code> field (along with <code>reason: Error</code>).</li></ul><h3 id="how-kubelet-retries-deferred-resizes">How kubelet retries Deferred resizes</h3><p>If the requested resize is <em>Deferred</em>, the kubelet will periodically re-attempt the resize,
for example when another pod is removed or scaled down. If there are multiple deferred
resizes, they are retried according to the following priority:</p><ul><li>Pods with a higher Priority (based on PriorityClass) will have their resize request retried first.</li><li>If two pods have the same Priority, resize of guaranteed pods will be retried before the resize of burstable pods.</li><li>If all else is the same, pods that have been in the Deferred state longer will be prioritized.</li></ul><p>A higher priority resize being marked as pending will not block the remaining pending resizes from being attempted;
all remaining pending resizes will still be retried even if a higher-priority resize gets deferred again.</p><h3 id="leveraging-observedgeneration-fields">Leveraging <code>observedGeneration</code> Fields</h3><div class="feature-state-notice feature-beta" title="Feature Gate: PodObservedGenerationTracking"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><ul><li>The top-level <code>status.observedGeneration</code> field shows the <code>metadata.generation</code> corresponding to the latest pod specification that the kubelet has acknowledged. You can use this to determine the most recent resize request the kubelet has processed.</li><li>In the <code>PodResizeInProgress</code> condition, the <code>conditions[].observedGeneration</code> field indicates the <code>metadata.generation</code> of the podSpec when the current in-progress resize was initiated.</li><li>In the <code>PodResizePending</code> condition, the <code>conditions[].observedGeneration</code> field indicates the <code>metadata.generation</code> of the podSpec when the pending resize's allocation was last attempted.</li></ul><h2 id="container-resize-policies">Container resize policies</h2><p>You can control whether a container should be restarted when resizing
by setting <code>resizePolicy</code> in the container specification.
This allows fine-grained control based on resource type (CPU or memory).</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>    </span><span>resizePolicy</span>:<span>
</span></span></span><span><span><span>    </span>- <span>resourceName</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>NotRequired<span>
</span></span></span><span><span><span>    </span>- <span>resourceName</span>:<span> </span>memory<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>RestartContainer<span>
</span></span></span></code></pre></div><ul><li><code>NotRequired</code>: (Default) Apply the resource change to the running container without restarting it.</li><li><code>RestartContainer</code>: Restart the container to apply the new resource values.
This is often necessary for memory changes because many applications
and runtimes cannot adjust their memory allocation dynamically.</li></ul><p>If <code>resizePolicy[*].restartPolicy</code> is not specified for a resource, it defaults to <code>NotRequired</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If a Pod's overall <code>restartPolicy</code> is <code>Never</code>, then any container <code>resizePolicy</code> must be <code>NotRequired</code> for all resources.
You cannot configure a resize policy that would require a restart in such Pods.</div><p><strong>Example Scenario:</strong></p><p>Consider a container configured with <code>restartPolicy: NotRequired</code> for CPU and <code>restartPolicy: RestartContainer</code> for memory.</p><ul><li>If only CPU resources are changed, the container is resized in-place.</li><li>If only memory resources are changed, the container is restarted.</li><li>If <em>both</em> CPU and memory resources are changed simultaneously, the container is restarted (due to the memory policy).</li></ul><h2 id="limitations">Limitations</h2><p>For Kubernetes 1.34, resizing pod resources in-place has the following limitations:</p><ul><li><strong>Resource Types:</strong> Only CPU and memory resources can be resized.</li><li><strong>Memory Decrease:</strong> If the memory resize restart policy is <code>NotRequired</code> (or unspecified), the kubelet will make a
best-effort attempt to prevent oom-kills when decreasing memory limits, but doesn't provide any guarantees.
Before decreasing container memory limits, if memory usage exceeds the requested limit, the resize will be skipped
and the status will remain in an "In Progress" state. This is considered best-effort because it is still subject
to a race condition where memory usage may spike right after the check is performed.</li><li><strong>QoS Class:</strong> The Pod's original <a href="/docs/concepts/workloads/pods/pod-qos/">Quality of Service (QoS) class</a>
(Guaranteed, Burstable, or BestEffort) is determined at creation and <strong>cannot</strong> be changed by a resize.
The resized resource values must still adhere to the rules of the original QoS class:<ul><li><em>Guaranteed</em>: Requests must continue to equal limits for both CPU and memory after resizing.</li><li><em>Burstable</em>: Requests and limits cannot become equal for <em>both</em> CPU and memory simultaneously
(as this would change it to Guaranteed).</li><li><em>BestEffort</em>: Resource requirements (<code>requests</code> or <code>limits</code>) cannot be added
(as this would change it to Burstable or Guaranteed).</li></ul></li><li><strong>Container Types:</strong> Non-restartable <a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." href="/docs/concepts/workloads/pods/init-containers/" target="_blank">init containers</a> and
<a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank">ephemeral containers</a> cannot be resized.
<a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a> can be resized.</li><li><strong>Resource Removal:</strong> Resource requests and limits cannot be entirely removed once set;
they can only be changed to different values.</li><li><strong>Operating System:</strong> Windows pods do not support in-place resize.</li><li><strong>Node Policies:</strong> Pods managed by <a href="/docs/tasks/administer-cluster/cpu-management-policies/">static CPU or Memory manager policies</a>
cannot be resized in-place.</li><li><strong>Swap:</strong> Pods utilizing <a href="/docs/concepts/architecture/nodes/#swap-memory">swap memory</a> cannot resize memory requests
unless the <code>resizePolicy</code> for memory is <code>RestartContainer</code>.</li></ul><p>These restrictions might be relaxed in future Kubernetes versions.</p><h2 id="example-1-resizing-cpu-without-restart">Example 1: Resizing CPU without restart</h2><p>First, create a Pod designed for in-place CPU resize and restart-required memory resize.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/pod-resize.yaml"><code>pods/resource/pod-resize.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/pod-resize.yaml to clipboard"></div><div class="includecode" id="pods-resource-pod-resize-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>resize-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span><span><span><span>    </span><span>resizePolicy</span>:<span>
</span></span></span><span><span><span>    </span>- <span>resourceName</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>NotRequired<span> </span><span># Default, but explicit here</span><span>
</span></span></span><span><span><span>    </span>- <span>resourceName</span>:<span> </span>memory<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>RestartContainer<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"700m"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"700m"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f pod-resize.yaml
</span></span></code></pre></div><p>This pod starts in the Guaranteed QoS class. Verify its initial state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Wait a moment for the pod to be running</span>
</span></span><span><span>kubectl get pod resize-demo --output<span>=</span>yaml
</span></span></code></pre></div><p>Observe the <code>spec.containers[0].resources</code> and <code>status.containerStatuses[0].resources</code>.
They should match the manifest (700m CPU, 200Mi memory). Note the <code>status.containerStatuses[0].restartCount</code> (should be 0).</p><p>Now, increase the CPU request and limit to <code>800m</code>. You use <code>kubectl patch</code> with the <code>--subresource resize</code> command line argument.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pod resize-demo --subresource resize --patch <span>\
</span></span></span><span><span><span></span>  <span>'{"spec":{"containers":[{"name":"pause", "resources":{"requests":{"cpu":"800m"}, "limits":{"cpu":"800m"}}}]}}'</span>
</span></span><span><span>
</span></span><span><span><span># Alternative methods:</span>
</span></span><span><span><span># kubectl -n qos-example edit pod resize-demo --subresource resize</span>
</span></span><span><span><span># kubectl -n qos-example apply -f &lt;updated-manifest&gt; --subresource resize --server-side</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>--subresource resize</code> command line argument requires <code>kubectl</code> client version v1.32.0 or later.
Older versions will report an <code>invalid subresource</code> error.</div><p>Check the pod status again after patching:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod resize-demo --output<span>=</span>yaml --namespace<span>=</span>qos-example
</span></span></code></pre></div><p>You should see:</p><ul><li><code>spec.containers[0].resources</code> now shows <code>cpu: 800m</code>.</li><li><code>status.containerStatuses[0].resources</code> also shows <code>cpu: 800m</code>, indicating the resize was successful on the node.</li><li><code>status.containerStatuses[0].restartCount</code> remains <code>0</code>, because the CPU <code>resizePolicy</code> was <code>NotRequired</code>.</li></ul><h2 id="example-2-resizing-memory-with-restart">Example 2: Resizing memory with restart</h2><p>Now, resize the memory for the <em>same</em> pod by increasing it to <code>300Mi</code>.
Since the memory <code>resizePolicy</code> is <code>RestartContainer</code>, the container is expected to restart.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pod resize-demo --subresource resize --patch <span>\
</span></span></span><span><span><span></span>  <span>'{"spec":{"containers":[{"name":"pause", "resources":{"requests":{"memory":"300Mi"}, "limits":{"memory":"300Mi"}}}]}}'</span>
</span></span></code></pre></div><p>Check the pod status shortly after patching:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod resize-demo --output<span>=</span>yaml
</span></span></code></pre></div><p>You should now observe:</p><ul><li><code>spec.containers[0].resources</code> shows <code>memory: 300Mi</code>.</li><li><code>status.containerStatuses[0].resources</code> also shows <code>memory: 300Mi</code>.</li><li><code>status.containerStatuses[0].restartCount</code> has increased to <code>1</code> (or more, if restarts happened previously),
indicating the container was restarted to apply the memory change.</li></ul><h2 id="troubleshooting-infeasible-resize-request">Troubleshooting: Infeasible resize request</h2><p>Next, try requesting an unreasonable amount of CPU, such as 1000 full cores (written as <code>"1000"</code> instead of <code>"1000m"</code> for millicores), which likely exceeds node capacity.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Attempt to patch with an excessively large CPU request</span>
</span></span><span><span>kubectl patch pod resize-demo --subresource resize --patch <span>\
</span></span></span><span><span><span></span>  <span>'{"spec":{"containers":[{"name":"pause", "resources":{"requests":{"cpu":"1000"}, "limits":{"cpu":"1000"}}}]}}'</span>
</span></span></code></pre></div><p>Query the Pod's details:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod resize-demo --output<span>=</span>yaml
</span></span></code></pre></div><p>You'll see changes indicating the problem:</p><ul><li>The <code>spec.containers[0].resources</code> reflects the <em>desired</em> state (<code>cpu: "1000"</code>).</li><li>A condition with <code>type: PodResizePending</code> and <code>reason: Infeasible</code> was added to the Pod.</li><li>The condition's <code>message</code> will explain why (<code>Node didn't have enough capacity: cpu, requested: 800000, capacity: ...</code>)</li><li>Crucially, <code>status.containerStatuses[0].resources</code> will <em>still show the previous values</em> (<code>cpu: 800m</code>, <code>memory: 300Mi</code>),
because the infeasible resize was not applied by the Kubelet.</li><li>The <code>restartCount</code> will not have changed due to this failed attempt.</li></ul><p>To fix this, you would need to patch the pod again with feasible resource values.</p><h2 id="clean-up">Clean up</h2><p>Delete the pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod resize-demo
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-application-developers">For application developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li></ul></div></div><div><div class="td-content"><h1>Configure RunAsUserName for Windows pods and containers</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>This page shows how to use the <code>runAsUserName</code> setting for Pods and containers that will run on Windows nodes. This is roughly equivalent of the Linux-specific <code>runAsUser</code> setting, allowing you to run applications in a container as a different username than the default.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster and the kubectl command-line tool must be configured to communicate with your cluster. The cluster is expected to have Windows worker nodes where pods with containers running Windows workloads will get scheduled.</p><h2 id="set-the-username-for-a-pod">Set the Username for a Pod</h2><p>To specify the username with which to execute the Pod's container processes, include the <code>securityContext</code> field (<a href="/docs/reference/generated/kubernetes-api/v1.34/#podsecuritycontext-v1-core">PodSecurityContext</a>) in the Pod specification, and within it, the <code>windowsOptions</code> (<a href="/docs/reference/generated/kubernetes-api/v1.34/#windowssecuritycontextoptions-v1-core">WindowsSecurityContextOptions</a>) field containing the <code>runAsUserName</code> field.</p><p>The Windows security context options that you specify for a Pod apply to all Containers and init Containers in the Pod.</p><p>Here is a configuration file for a Windows Pod that has the <code>runAsUserName</code> field set:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/windows/run-as-username-pod.yaml"><code>windows/run-as-username-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy windows/run-as-username-pod.yaml to clipboard"></div><div class="includecode" id="windows-run-as-username-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>run-as-username-pod-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>      </span><span>runAsUserName</span>:<span> </span><span>"ContainerUser"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>run-as-username-demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"ping"</span>,<span> </span><span>"-t"</span>,<span> </span><span>"localhost"</span>]<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/windows/run-as-username-pod.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod run-as-username-pod-demo
</span></span></code></pre></div><p>Get a shell to the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it run-as-username-pod-demo -- powershell
</span></span></code></pre></div><p>Check that the shell is running user the correct username:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>echo </span><span>$env:USERNAME</span>
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code>ContainerUser
</code></pre><h2 id="set-the-username-for-a-container">Set the Username for a Container</h2><p>To specify the username with which to execute a Container's processes, include the <code>securityContext</code> field (<a href="/docs/reference/generated/kubernetes-api/v1.34/#securitycontext-v1-core">SecurityContext</a>) in the Container manifest, and within it, the <code>windowsOptions</code> (<a href="/docs/reference/generated/kubernetes-api/v1.34/#windowssecuritycontextoptions-v1-core">WindowsSecurityContextOptions</a>) field containing the <code>runAsUserName</code> field.</p><p>The Windows security context options that you specify for a Container apply only to that individual Container, and they override the settings made at the Pod level.</p><p>Here is the configuration file for a Pod that has one Container, and the <code>runAsUserName</code> field is set at the Pod level and the Container level:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/windows/run-as-username-container.yaml"><code>windows/run-as-username-container.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy windows/run-as-username-container.yaml to clipboard"></div><div class="includecode" id="windows-run-as-username-container-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>run-as-username-container-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>      </span><span>runAsUserName</span>:<span> </span><span>"ContainerUser"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>run-as-username-demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"ping"</span>,<span> </span><span>"-t"</span>,<span> </span><span>"localhost"</span>]<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>        </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>            </span><span>runAsUserName</span>:<span> </span><span>"ContainerAdministrator"</span><span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/windows/run-as-username-container.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod run-as-username-container-demo
</span></span></code></pre></div><p>Get a shell to the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it run-as-username-container-demo -- powershell
</span></span></code></pre></div><p>Check that the shell is running user the correct username (the one set at the Container level):</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>echo </span><span>$env:USERNAME</span>
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code>ContainerAdministrator
</code></pre><h2 id="windows-username-limitations">Windows Username limitations</h2><p>In order to use this feature, the value set in the <code>runAsUserName</code> field must be a valid username. It must have the following format: <code>DOMAIN\USER</code>, where <code>DOMAIN\</code> is optional. Windows user names are case insensitive. Additionally, there are some restrictions regarding the <code>DOMAIN</code> and <code>USER</code>:</p><ul><li>The <code>runAsUserName</code> field cannot be empty, and it cannot contain control characters (ASCII values: <code>0x00-0x1F</code>, <code>0x7F</code>)</li><li>The <code>DOMAIN</code> must be either a NetBios name, or a DNS name, each with their own restrictions:<ul><li>NetBios names: maximum 15 characters, cannot start with <code>.</code> (dot), and cannot contain the following characters: <code>\ / : * ? " &lt; &gt; |</code></li><li>DNS names: maximum 255 characters, contains only alphanumeric characters, dots, and dashes, and it cannot start or end with a <code>.</code> (dot) or <code>-</code> (dash).</li></ul></li><li>The <code>USER</code> must have at most 20 characters, it cannot contain <em>only</em> dots or spaces, and it cannot contain the following characters: <code>" / \ [ ] : ; | = , + * ? &lt; &gt; @</code>.</li></ul><p>Examples of acceptable values for the <code>runAsUserName</code> field: <code>ContainerAdministrator</code>, <code>ContainerUser</code>, <code>NT AUTHORITY\NETWORK SERVICE</code>, <code>NT AUTHORITY\LOCAL SERVICE</code>.</p><p>For more information about these limtations, check <a href="https://support.microsoft.com/en-us/help/909264/naming-conventions-in-active-directory-for-computers-domains-sites-and">here</a> and <a href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.localaccounts/new-localuser?view=powershell-5.1">here</a>.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/windows/user-guide/">Guide for scheduling Windows containers in Kubernetes</a></li><li><a href="/docs/concepts/windows/user-guide/#managing-workload-identity-with-group-managed-service-accounts">Managing Workload Identity with Group Managed Service Accounts (GMSA)</a></li><li><a href="/docs/tasks/configure-pod-container/configure-gmsa/">Configure GMSA for Windows pods and containers</a></li></ul></div></div><div><div class="td-content"><h1>Create a Windows HostProcess Pod</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Windows HostProcess containers enable you to run containerized
workloads on a Windows host. These containers operate as
normal processes but have access to the host network namespace,
storage, and devices when given the appropriate user privileges.
HostProcess containers can be used to deploy network plugins,
storage configurations, device plugins, kube-proxy, and other
components to Windows nodes without the need for dedicated proxies or
the direct installation of host services.</p><p>Administrative tasks such as installation of security patches, event
log collection, and more can be performed without requiring cluster operators to
log onto each Windows node. HostProcess containers can run as any user that is
available on the host or is in the domain of the host machine, allowing administrators
to restrict resource access through user permissions. While neither filesystem or process
isolation are supported, a new volume is created on the host upon starting the container
to give it a clean and consolidated workspace. HostProcess containers can also be built on
top of existing Windows base images and do not inherit the same
<a href="https://docs.microsoft.com/virtualization/windowscontainers/deploy-containers/version-compatibility">compatibility requirements</a>
as Windows server containers, meaning that the version of the base images does not need
to match that of the host. It is, however, recommended that you use the same base image
version as your Windows Server container workloads to ensure you do not have any unused
images taking up space on the node. HostProcess containers also support
<a href="#volume-mounts">volume mounts</a> within the container volume.</p><h3 id="when-should-i-use-a-windows-hostprocess-container">When should I use a Windows HostProcess container?</h3><ul><li>When you need to perform tasks which require the networking namespace of the host.
HostProcess containers have access to the host's network interfaces and IP addresses.</li><li>You need access to resources on the host such as the filesystem, event logs, etc.</li><li>Installation of specific device drivers or Windows services.</li><li>Consolidation of administrative tasks and security policies. This reduces the degree of
privileges needed by Windows nodes.</li></ul><h2 id="before-you-begin">Before you begin</h2><p>This task guide is specific to Kubernetes v1.34.
If you are not running Kubernetes v1.34, check the documentation for
that version of Kubernetes.</p><p>In Kubernetes 1.34, the HostProcess container feature is enabled by default. The kubelet will
communicate with containerd directly by passing the hostprocess flag via CRI. You can use the
latest version of containerd (v1.6+) to run HostProcess containers.
<a href="/docs/setup/production-environment/container-runtimes/#containerd">How to install containerd.</a></p><h2 id="limitations">Limitations</h2><p>These limitations are relevant for Kubernetes v1.34:</p><ul><li>HostProcess containers require containerd 1.6 or higher
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a> and
containerd 1.7 is recommended.</li><li>HostProcess pods can only contain HostProcess containers. This is a current limitation
of the Windows OS; non-privileged Windows containers cannot share a vNIC with the host IP namespace.</li><li>HostProcess containers run as a process on the host and do not have any degree of
isolation other than resource constraints imposed on the HostProcess user account. Neither
filesystem or Hyper-V isolation are supported for HostProcess containers.</li><li>Volume mounts are supported and are mounted under the container volume. See
<a href="#volume-mounts">Volume Mounts</a></li><li>A limited set of host user accounts are available for HostProcess containers by default.
See <a href="#choosing-a-user-account">Choosing a User Account</a>.</li><li>Resource limits (disk, memory, cpu count) are supported in the same fashion as processes
on the host.</li><li>Both Named pipe mounts and Unix domain sockets are <strong>not</strong> supported and should instead
be accessed via their path on the host (e.g. \\.\pipe\*)</li></ul><h2 id="hostprocess-pod-configuration-requirements">HostProcess Pod configuration requirements</h2><p>Enabling a Windows HostProcess pod requires setting the right configurations in the pod security
configuration. Of the policies defined in the <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>
HostProcess pods are disallowed by the baseline and restricted policies. It is therefore recommended
that HostProcess pods run in alignment with the privileged profile.</p><p>When running under the privileged policy, here are
the configurations which need to be set to enable the creation of a HostProcess pod:</p><table><caption>Privileged policy specification</caption><thead><tr><th>Control</th><th>Policy</th></tr></thead><tbody><tr><td><a href="/docs/concepts/security/pod-security-standards"><tt>securityContext.windowsOptions.hostProcess</tt></a></td><td><p>Windows pods offer the ability to run <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess containers</a> which enables privileged access to the Windows node.</p><p><strong>Allowed Values</strong></p><ul><li><code>true</code></li></ul></td></tr><tr><td><a href="/docs/concepts/security/pod-security-standards"><tt>hostNetwork</tt></a></td><td><p>Pods container HostProcess containers must use the host's network namespace.</p><p><strong>Allowed Values</strong></p><ul><li><code>true</code></li></ul></td></tr><tr><td><a href="/docs/tasks/configure-pod-container/configure-runasusername/"><tt>securityContext.windowsOptions.runAsUserName</tt></a></td><td><p>Specification of which user the HostProcess container should run as is required for the pod spec.</p><p><strong>Allowed Values</strong></p><ul><li><code>NT AUTHORITY\SYSTEM</code></li><li><code>NT AUTHORITY\Local service</code></li><li><code>NT AUTHORITY\NetworkService</code></li><li>Local usergroup names (see below)</li></ul></td></tr><tr><td><a href="/docs/concepts/security/pod-security-standards"><tt>runAsNonRoot</tt></a></td><td><p>Because HostProcess containers have privileged access to the host, the <tt>runAsNonRoot</tt> field cannot be set to true.</p><p><strong>Allowed Values</strong></p><ul><li>Undefined/Nil</li><li><code>false</code></li></ul></td></tr></tbody></table><h3 id="manifest-example">Example manifest (excerpt)</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>      </span><span>hostProcess</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>runAsUserName</span>:<span> </span><span>"NT AUTHORITY\\Local service"</span><span>
</span></span></span><span><span><span>  </span><span>hostNetwork</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>image1:latest<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>      </span>- ping<span>
</span></span></span><span><span><span>      </span>- -t<span>
</span></span></span><span><span><span>      </span>- <span>127.0.0.1</span><span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>"kubernetes.io/os": </span>windows<span>
</span></span></span></code></pre></div><h2 id="volume-mounts">Volume mounts</h2><p>HostProcess containers support the ability to mount volumes within the container volume space.
Volume mount behavior differs depending on the version of containerd runtime used by on the node.</p><h3 id="containerd-v1-6">Containerd v1.6</h3><p>Applications running inside the container can access volume mounts directly via relative or
absolute paths. An environment variable <code>$CONTAINER_SANDBOX_MOUNT_POINT</code> is set upon container
creation and provides the absolute host path to the container volume. Relative paths are based
upon the <code>.spec.containers.volumeMounts.mountPath</code> configuration.</p><p>To access service account tokens (for example) the following path structures are supported within the container:</p><ul><li><code>.\var\run\secrets\kubernetes.io\serviceaccount\</code></li><li><code>$CONTAINER_SANDBOX_MOUNT_POINT\var\run\secrets\kubernetes.io\serviceaccount\</code></li></ul><h3 id="containerd-v1-7-and-greater">Containerd v1.7 (and greater)</h3><p>Applications running inside the container can access volume mounts directly via the volumeMount's
specified <code>mountPath</code> (just like Linux and non-HostProcess Windows containers).</p><p>For backwards compatibility volumes can also be accessed via using the same relative paths configured
by containerd v1.6.</p><p>As an example, to access service account tokens within the container you would use one of the following paths:</p><ul><li><code>c:\var\run\secrets\kubernetes.io\serviceaccount</code></li><li><code>/var/run/secrets/kubernetes.io/serviceaccount/</code></li><li><code>$CONTAINER_SANDBOX_MOUNT_POINT\var\run\secrets\kubernetes.io\serviceaccount\</code></li></ul><h2 id="resource-limits">Resource limits</h2><p>Resource limits (disk, memory, cpu count) are applied to the job and are job wide.
For example, with a limit of 10MB set, the memory allocated for any HostProcess job object
will be capped at 10MB. This is the same behavior as other Windows container types.
These limits would be specified the same way they are currently for whatever orchestrator
or runtime is being used. The only difference is in the disk resource usage calculation
used for resource tracking due to the difference in how HostProcess containers are bootstrapped.</p><h2 id="choosing-a-user-account">Choosing a user account</h2><h3 id="system-accounts">System accounts</h3><p>By default, HostProcess containers support the ability to run as one of three supported Windows service accounts:</p><ul><li><strong><a href="https://docs.microsoft.com/windows/win32/services/localsystem-account">LocalSystem</a></strong></li><li><strong><a href="https://docs.microsoft.com/windows/win32/services/localservice-account">LocalService</a></strong></li><li><strong><a href="https://docs.microsoft.com/windows/win32/services/networkservice-account">NetworkService</a></strong></li></ul><p>You should select an appropriate Windows service account for each HostProcess
container, aiming to limit the degree of privileges so as to avoid accidental (or even
malicious) damage to the host. The LocalSystem service account has the highest level
of privilege of the three and should be used only if absolutely necessary. Where possible,
use the LocalService service account as it is the least privileged of the three options.</p><h3 id="local-accounts">Local accounts</h3><p>If configured, HostProcess containers can also run as local user accounts which allows for node operators to give
fine-grained access to workloads.</p><p>To run HostProcess containers as a local user; A local usergroup must first be created on the node
and the name of that local usergroup must be specified in the <code>runAsUserName</code> field in the deployment.
Prior to initializing the HostProcess container, a new <strong>ephemeral</strong> local user account to be created and joined to the specified usergroup, from which the container is run.
This provides a number a benefits including eliminating the need to manage passwords for local user accounts.
An initial HostProcess container running as a service account can be used to
prepare the user groups for later HostProcess containers.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Running HostProcess containers as local user accounts requires containerd v1.7+</div><p>Example:</p><ol><li><p>Create a local user group on the node (this can be done in another HostProcess container).</p><div class="highlight"><pre tabindex="0"><code class="language-cmd"><span><span>net localgroup hpc-localgroup /add
</span></span></code></pre></div></li><li><p>Grant access to desired resources on the node to the local usergroup.
This can be done with tools like <a href="https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/icacls">icacls</a>.</p></li><li><p>Set <code>runAsUserName</code> to the name of the local usergroup for the pod or individual containers.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>windowsOptions</span>:<span>
</span></span></span><span><span><span>    </span><span>hostProcess</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>runAsUserName</span>:<span> </span>hpc-localgroup<span>
</span></span></span></code></pre></div></li><li><p>Schedule the pod!</p></li></ol><h2 id="base-image-for-hostprocess-containers">Base Image for HostProcess Containers</h2><p>HostProcess containers can be built from any of the existing <a href="https://learn.microsoft.com/virtualization/windowscontainers/manage-containers/container-base-images">Windows Container base images</a>.</p><p>Additionally a new base mage has been created just for HostProcess containers!
For more information please check out the <a href="https://github.com/microsoft/windows-host-process-containers-base-image#overview">windows-host-process-containers-base-image github project</a>.</p><h2 id="troubleshooting-hostprocess-containers">Troubleshooting HostProcess containers</h2><ul><li><p>HostProcess containers fail to start with <code>failed to create user process token: failed to logon user: Access is denied.: unknown</code></p><p>Ensure containerd is running as <code>LocalSystem</code> or <code>LocalService</code> service accounts. User accounts (even Administrator accounts) do not have permissions to create logon tokens for any of the supported <a href="#choosing-a-user-account">user accounts</a>.</p></li></ul></div></div><div><div class="td-content"><h1>Configure Quality of Service for Pods</h1><p>This page shows how to configure Pods so that they will be assigned particular
<a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">Quality of Service (QoS) classes</a>.
Kubernetes uses QoS classes to make decisions about evicting Pods when Node resources are exceeded.</p><p>When Kubernetes creates a Pod it assigns one of these QoS classes to the Pod:</p><ul><li><a href="/docs/concepts/workloads/pods/pod-qos/#guaranteed">Guaranteed</a></li><li><a href="/docs/concepts/workloads/pods/pod-qos/#burstable">Burstable</a></li><li><a href="/docs/concepts/workloads/pods/pod-qos/#besteffort">BestEffort</a></li></ul><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You also need to be able to create and delete namespaces.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace qos-example
</span></span></code></pre></div><h2 id="create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed">Create a Pod that gets assigned a QoS class of Guaranteed</h2><p>For a Pod to be given a QoS class of <code>Guaranteed</code>:</p><ul><li>Every Container in the Pod must have a memory limit and a memory request.</li><li>For every Container in the Pod, the memory limit must equal the memory request.</li><li>Every Container in the Pod must have a CPU limit and a CPU request.</li><li>For every Container in the Pod, the CPU limit must equal the CPU request.</li></ul><p>These restrictions apply to init containers and app containers equally.
<a href="/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral containers</a>
cannot define resources so these restrictions do not apply.</p><p>Here is a manifest for a Pod that has one Container. The Container has a memory limit and a
memory request, both equal to 200 MiB. The Container has a CPU limit and a CPU request, both equal to 700 milliCPU:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/qos/qos-pod.yaml"><code>pods/qos/qos-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/qos/qos-pod.yaml to clipboard"></div><div class="includecode" id="pods-qos-qos-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>qos-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>qos-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>qos-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"700m"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"700m"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod.yaml --namespace<span>=</span>qos-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod qos-demo --namespace<span>=</span>qos-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that Kubernetes gave the Pod a QoS class of <code>Guaranteed</code>. The output also
verifies that the Pod Container has a memory request that matches its memory limit, and it has
a CPU request that matches its CPU limit.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>700m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>700m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>qosClass</span>:<span> </span>Guaranteed<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes
automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own
CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches
the limit.</div><h4 id="clean-up-guaranteed">Clean up</h4><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod qos-demo --namespace<span>=</span>qos-example
</span></span></code></pre></div><h2 id="create-a-pod-that-gets-assigned-a-qos-class-of-burstable">Create a Pod that gets assigned a QoS class of Burstable</h2><p>A Pod is given a QoS class of <code>Burstable</code> if:</p><ul><li>The Pod does not meet the criteria for QoS class <code>Guaranteed</code>.</li><li>At least one Container in the Pod has a memory or CPU request or limit.</li></ul><p>Here is a manifest for a Pod that has one Container. The Container has a memory limit of 200 MiB
and a memory request of 100 MiB.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/qos/qos-pod-2.yaml"><code>pods/qos/qos-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/qos/qos-pod-2.yaml to clipboard"></div><div class="includecode" id="pods-qos-qos-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>qos-demo-2<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>qos-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>qos-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod-2.yaml --namespace<span>=</span>qos-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod qos-demo-2 --namespace<span>=</span>qos-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that Kubernetes gave the Pod a QoS class of <code>Burstable</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>qos-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>qosClass</span>:<span> </span>Burstable<span>
</span></span></span></code></pre></div><h4 id="clean-up-burstable">Clean up</h4><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod qos-demo-2 --namespace<span>=</span>qos-example
</span></span></code></pre></div><h2 id="create-a-pod-that-gets-assigned-a-qos-class-of-besteffort">Create a Pod that gets assigned a QoS class of BestEffort</h2><p>For a Pod to be given a QoS class of <code>BestEffort</code>, the Containers in the Pod must not
have any memory or CPU limits or requests.</p><p>Here is a manifest for a Pod that has one Container. The Container has no memory or CPU
limits or requests:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/qos/qos-pod-3.yaml"><code>pods/qos/qos-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/qos/qos-pod-3.yaml to clipboard"></div><div class="includecode" id="pods-qos-qos-pod-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>qos-demo-3<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>qos-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>qos-demo-3-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod-3.yaml --namespace<span>=</span>qos-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod qos-demo-3 --namespace<span>=</span>qos-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that Kubernetes gave the Pod a QoS class of <code>BestEffort</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>qosClass</span>:<span> </span>BestEffort<span>
</span></span></span></code></pre></div><h4 id="clean-up-besteffort">Clean up</h4><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod qos-demo-3 --namespace<span>=</span>qos-example
</span></span></code></pre></div><h2 id="create-a-pod-that-has-two-containers">Create a Pod that has two Containers</h2><p>Here is a manifest for a Pod that has two Containers. One container specifies a memory
request of 200 MiB. The other Container does not specify any requests or limits.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/qos/qos-pod-4.yaml"><code>pods/qos/qos-pod-4.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/qos/qos-pod-4.yaml to clipboard"></div><div class="includecode" id="pods-qos-qos-pod-4-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>qos-demo-4<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>qos-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>qos-demo-4-ctr-1<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>qos-demo-4-ctr-2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span></code></pre></div></div></div><p>Notice that this Pod meets the criteria for QoS class <code>Burstable</code>. That is, it does not meet the
criteria for QoS class <code>Guaranteed</code>, and one of its Containers has a memory request.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod-4.yaml --namespace<span>=</span>qos-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod qos-demo-4 --namespace<span>=</span>qos-example --output<span>=</span>yaml
</span></span></code></pre></div><p>The output shows that Kubernetes gave the Pod a QoS class of <code>Burstable</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>qos-demo-4-ctr-1<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>qos-demo-4-ctr-2<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span> </span>{}<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>qosClass</span>:<span> </span>Burstable<span>
</span></span></span></code></pre></div><h2 id="retrieve-the-qos-class-for-a-pod">Retrieve the QoS class for a Pod</h2><p>Rather than see all the fields, you can view just the field you need:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl --namespace<span>=</span>qos-example get pod qos-demo-4 -o <span>jsonpath</span><span>=</span><span>'{ .status.qosClass}{"\n"}'</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Burstable
</code></pre><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete namespace qos-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li><li><p><a href="/docs/tasks/administer-cluster/topology-manager/">Control Topology Management policies on a node</a></p></li></ul></div></div><div><div class="td-content"><h1>Assign Extended Resources to a Container</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code></div><p>This page shows how to assign extended resources to a Container.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>Before you do this exercise, do the exercise in
<a href="/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a>.
That will configure one of your Nodes to advertise a dongle resource.</p><h2 id="assign-an-extended-resource-to-a-pod">Assign an extended resource to a Pod</h2><p>To request an extended resource, include the <code>resources:requests</code> field in your
Container manifest. Extended resources are fully qualified with any domain outside of
<code>*.kubernetes.io/</code>. Valid extended resource names have the form <code>example.com/foo</code> where
<code>example.com</code> is replaced with your organization's domain and <code>foo</code> is a
descriptive resource name.</p><p>Here is the configuration file for a Pod that has one Container:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/extended-resource-pod.yaml"><code>pods/resource/extended-resource-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/extended-resource-pod.yaml to clipboard"></div><div class="includecode" id="pods-resource-extended-resource-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>extended-resource-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>extended-resource-demo-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/dongle</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/dongle</span>:<span> </span><span>3</span><span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Container requests 3 dongles.</p><p>Create a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod.yaml
</span></span></code></pre></div><p>Verify that the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod extended-resource-demo
</span></span></code></pre></div><p>Describe the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod extended-resource-demo
</span></span></code></pre></div><p>The output shows dongle requests:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>Limits</span>:<span>
</span></span></span><span><span><span>  </span><span>example.com/dongle</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span></span><span>Requests</span>:<span>
</span></span></span><span><span><span>  </span><span>example.com/dongle</span>:<span> </span><span>3</span><span>
</span></span></span></code></pre></div><h2 id="attempt-to-create-a-second-pod">Attempt to create a second Pod</h2><p>Here is the configuration file for a Pod that has one Container. The Container requests
two dongles.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/extended-resource-pod-2.yaml"><code>pods/resource/extended-resource-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/extended-resource-pod-2.yaml to clipboard"></div><div class="includecode" id="pods-resource-extended-resource-pod-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>extended-resource-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>extended-resource-demo-2-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/dongle</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/dongle</span>:<span> </span><span>2</span><span>
</span></span></span></code></pre></div></div></div><p>Kubernetes will not be able to satisfy the request for two dongles, because the first Pod
used three of the four available dongles.</p><p>Attempt to create a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod-2.yaml
</span></span></code></pre></div><p>Describe the Pod</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod extended-resource-demo-2
</span></span></code></pre></div><p>The output shows that the Pod cannot be scheduled, because there is no Node that has
2 dongles available:</p><pre tabindex="0"><code>Conditions:
  Type    Status
  PodScheduled  False
...
Events:
  ...
  ... Warning   FailedScheduling  pod (extended-resource-demo-2) failed to fit in any node
fit failure summary on nodes : Insufficient example.com/dongle (1)
</code></pre><p>View the Pod status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod extended-resource-demo-2
</span></span></code></pre></div><p>The output shows that the Pod was created, but not scheduled to run on a Node.
It has a status of Pending:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>NAME                       READY     STATUS    RESTARTS   AGE<span>
</span></span></span><span><span><span></span>extended-resource-demo-2   0/1       Pending   0          6m<span>
</span></span></span></code></pre></div><h2 id="clean-up">Clean up</h2><p>Delete the Pods that you created for this exercise:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod extended-resource-demo
</span></span><span><span>kubectl delete pod extended-resource-demo-2
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-application-developers">For application developers</h3><ul><li><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></li><li><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><a href="/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a></li></ul></div></div><div><div class="td-content"><h1>Configure a Pod to Use a Volume for Storage</h1><p>This page shows how to configure a Pod to use a Volume for storage.</p><p>A Container's file system lives only as long as the Container does. So when a
Container terminates and restarts, filesystem changes are lost. For more
consistent storage that is independent of the Container, you can use a
<a href="/docs/concepts/storage/volumes/">Volume</a>. This is especially important for stateful
applications, such as key-value stores (such as Redis) and databases.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="configure-a-volume-for-a-pod">Configure a volume for a Pod</h2><p>In this exercise, you create a Pod that runs one Container. This Pod has a
Volume of type
<a href="/docs/concepts/storage/volumes/#emptydir">emptyDir</a>
that lasts for the life of the Pod, even if the Container terminates and
restarts. Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/redis.yaml"><code>pods/storage/redis.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/redis.yaml to clipboard"></div><div class="includecode" id="pods-storage-redis-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>redis<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>redis-storage<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/data/redis<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>redis-storage<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/storage/redis.yaml
</span></span></code></pre></div></li><li><p>Verify that the Pod's Container is running, and then watch for changes to
the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod redis --watch
</span></span></code></pre></div><p>The output looks like this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME      READY     STATUS    RESTARTS   AGE
</span></span></span><span><span><span>redis     1/1       Running   0          13s
</span></span></span></code></pre></div></li><li><p>In another terminal, get a shell to the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it redis -- /bin/bash
</span></span></code></pre></div></li><li><p>In your shell, go to <code>/data/redis</code>, and then create a file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>root@redis:/data# <span>cd</span> /data/redis/
</span></span><span><span>root@redis:/data/redis# <span>echo</span> Hello &gt; test-file
</span></span></code></pre></div></li><li><p>In your shell, list the running processes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>root@redis:/data/redis# apt-get update
</span></span><span><span>root@redis:/data/redis# apt-get install procps
</span></span><span><span>root@redis:/data/redis# ps aux
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span></span><span><span><span>redis        1  0.1  0.1  33308  3828 ?        Ssl  00:46   0:00 redis-server *:6379
</span></span></span><span><span><span>root        12  0.0  0.0  20228  3020 ?        Ss   00:47   0:00 /bin/bash
</span></span></span><span><span><span>root        15  0.0  0.0  17500  2072 ?        R+   00:48   0:00 ps aux
</span></span></span></code></pre></div></li><li><p>In your shell, kill the Redis process:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>root@redis:/data/redis# <span>kill</span> &lt;pid&gt;
</span></span></code></pre></div><p>where <code>&lt;pid&gt;</code> is the Redis process ID (PID).</p></li><li><p>In your original terminal, watch for changes to the Redis Pod. Eventually,
you will see something like this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME      READY     STATUS     RESTARTS   AGE
</span></span></span><span><span><span>redis     1/1       Running    0          13s
</span></span></span><span><span><span>redis     0/1       Completed  0         6m
</span></span></span><span><span><span>redis     1/1       Running    1         6m
</span></span></span></code></pre></div></li></ol><p>At this point, the Container has terminated and restarted. This is because the
Redis Pod has a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">restartPolicy</a>
of <code>Always</code>.</p><ol><li><p>Get a shell into the restarted Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it redis -- /bin/bash
</span></span></code></pre></div></li><li><p>In your shell, go to <code>/data/redis</code>, and verify that <code>test-file</code> is still there.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>root@redis:/data/redis# <span>cd</span> /data/redis/
</span></span><span><span>root@redis:/data/redis# ls
</span></span><span><span>test-file
</span></span></code></pre></div></li><li><p>Delete the Pod that you created for this exercise:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod redis
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#volume-v1-core">Volume</a>.</p></li><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#pod-v1-core">Pod</a>.</p></li><li><p>In addition to the local disk storage provided by <code>emptyDir</code>, Kubernetes
supports many different network-attached storage solutions, including PD on
GCE and EBS on EC2, which are preferred for critical data and will handle
details such as mounting and unmounting the devices on the nodes. See
<a href="/docs/concepts/storage/volumes/">Volumes</a> for more details.</p></li></ul></div></div><div><div class="td-content"><h1>Configure a Pod to Use a PersistentVolume for Storage</h1><p>This page shows you how to configure a Pod to use a
<a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PersistentVolumeClaim</a>
for storage.
Here is a summary of the process:</p><ol><li><p>You, as cluster administrator, create a PersistentVolume backed by physical
storage. You do not associate the volume with any Pod.</p></li><li><p>You, now taking the role of a developer / cluster user, create a
PersistentVolumeClaim that is automatically bound to a suitable
PersistentVolume.</p></li><li><p>You create a Pod that uses the above PersistentVolumeClaim for storage.</p></li></ol><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster that has only one Node, and the
<a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a>
command-line tool must be configured to communicate with your cluster. If you
do not already have a single-node cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/">Minikube</a>.</p></li><li><p>Familiarize yourself with the material in
<a href="/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a>.</p></li></ul><h2 id="create-an-index-html-file-on-your-node">Create an index.html file on your Node</h2><p>Open a shell to the single Node in your cluster. How you open a shell depends
on how you set up your cluster. For example, if you are using Minikube, you
can open a shell to your Node by entering <code>minikube ssh</code>.</p><p>In your shell on that Node, create a <code>/mnt/data</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This assumes that your Node uses "sudo" to run commands</span>
</span></span><span><span><span># as the superuser</span>
</span></span><span><span>sudo mkdir /mnt/data
</span></span></code></pre></div><p>In the <code>/mnt/data</code> directory, create an <code>index.html</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This again assumes that your Node uses "sudo" to run commands</span>
</span></span><span><span><span># as the superuser</span>
</span></span><span><span>sudo sh -c <span>"echo 'Hello from Kubernetes storage' &gt; /mnt/data/index.html"</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If your Node uses a tool for superuser access other than <code>sudo</code>, you can
usually make this work if you replace <code>sudo</code> with the name of the other tool.</div><p>Test that the <code>index.html</code> file exists:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /mnt/data/index.html
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code>Hello from Kubernetes storage
</code></pre><p>You can now close the shell to your Node.</p><h2 id="create-a-persistentvolume">Create a PersistentVolume</h2><p>In this exercise, you create a <em>hostPath</em> PersistentVolume. Kubernetes supports
hostPath for development and testing on a single-node cluster. A hostPath
PersistentVolume uses a file or directory on the Node to emulate network-attached storage.</p><p>In a production cluster, you would not use hostPath. Instead a cluster administrator
would provision a network resource like a Google Compute Engine persistent disk,
an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also
use <a href="/docs/reference/generated/kubernetes-api/v1.34/#storageclass-v1-storage-k8s-io">StorageClasses</a>
to set up
<a href="/docs/concepts/storage/dynamic-provisioning/">dynamic provisioning</a>.</p><p>Here is the configuration file for the hostPath PersistentVolume:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/pv-volume.yaml"><code>pods/storage/pv-volume.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/pv-volume.yaml to clipboard"></div><div class="includecode" id="pods-storage-pv-volume-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>task-pv-volume<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>local<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>manual<span>
</span></span></span><span><span><span>  </span><span>capacity</span>:<span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span><span>"/mnt/data"</span><span>
</span></span></span></code></pre></div></div></div><p>The configuration file specifies that the volume is at <code>/mnt/data</code> on the
cluster's Node. The configuration also specifies a size of 10 gibibytes and
an access mode of <code>ReadWriteOnce</code>, which means the volume can be mounted as
read-write by a single Node. It defines the <a href="/docs/concepts/storage/persistent-volumes/#class">StorageClass name</a>
<code>manual</code> for the PersistentVolume, which will be used to bind
PersistentVolumeClaim requests to this PersistentVolume.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This example uses the <code>ReadWriteOnce</code> access mode, for simplicity. For
production use, the Kubernetes project recommends using the <code>ReadWriteOncePod</code>
access mode instead.</div><p>Create the PersistentVolume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/storage/pv-volume.yaml
</span></span></code></pre></div><p>View information about the PersistentVolume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pv task-pv-volume
</span></span></code></pre></div><p>The output shows that the PersistentVolume has a <code>STATUS</code> of <code>Available</code>. This
means it has not yet been bound to a PersistentVolumeClaim.</p><pre tabindex="0"><code>NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Available             manual                   4s
</code></pre><h2 id="create-a-persistentvolumeclaim">Create a PersistentVolumeClaim</h2><p>The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims
to request physical storage. In this exercise, you create a PersistentVolumeClaim
that requests a volume of at least three gibibytes that can provide read-write
access for at most one Node at a time.</p><p>Here is the configuration file for the PersistentVolumeClaim:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/pv-claim.yaml"><code>pods/storage/pv-claim.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/pv-claim.yaml to clipboard"></div><div class="includecode" id="pods-storage-pv-claim-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>task-pv-claim<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>manual<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>3Gi<span>
</span></span></span></code></pre></div></div></div><p>Create the PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/storage/pv-claim.yaml
</span></span></code></pre></div><p>After you create the PersistentVolumeClaim, the Kubernetes control plane looks
for a PersistentVolume that satisfies the claim's requirements. If the control
plane finds a suitable PersistentVolume with the same StorageClass, it binds the
claim to the volume.</p><p>Look again at the PersistentVolume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pv task-pv-volume
</span></span></code></pre></div><p>Now the output shows a <code>STATUS</code> of <code>Bound</code>.</p><pre tabindex="0"><code>NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                   STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Bound     default/task-pv-claim   manual                   2m
</code></pre><p>Look at the PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pvc task-pv-claim
</span></span></code></pre></div><p>The output shows that the PersistentVolumeClaim is bound to your PersistentVolume,
<code>task-pv-volume</code>.</p><pre tabindex="0"><code>NAME            STATUS    VOLUME           CAPACITY   ACCESSMODES   STORAGECLASS   AGE
task-pv-claim   Bound     task-pv-volume   10Gi       RWO           manual         30s
</code></pre><h2 id="create-a-pod">Create a Pod</h2><p>The next step is to create a Pod that uses your PersistentVolumeClaim as a volume.</p><p>Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/pv-pod.yaml"><code>pods/storage/pv-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/pv-pod.yaml to clipboard"></div><div class="includecode" id="pods-storage-pv-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>task-pv-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>task-pv-storage<span>
</span></span></span><span><span><span>      </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>        </span><span>claimName</span>:<span> </span>task-pv-claim<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>task-pv-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span><span>"http-server"</span><span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>mountPath</span>:<span> </span><span>"/usr/share/nginx/html"</span><span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>task-pv-storage<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>Notice that the Pod's configuration file specifies a PersistentVolumeClaim, but
it does not specify a PersistentVolume. From the Pod's point of view, the claim
is a volume.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/storage/pv-pod.yaml
</span></span></code></pre></div><p>Verify that the container in the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod task-pv-pod
</span></span></code></pre></div><p>Get a shell to the container running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it task-pv-pod -- /bin/bash
</span></span></code></pre></div><p>In your shell, verify that nginx is serving the <code>index.html</code> file from the
hostPath volume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Be sure to run these 3 commands inside the root shell that comes from</span>
</span></span><span><span><span># running "kubectl exec" in the previous step</span>
</span></span><span><span>apt update
</span></span><span><span>apt install curl
</span></span><span><span>curl http://localhost/
</span></span></code></pre></div><p>The output shows the text that you wrote to the <code>index.html</code> file on the
hostPath volume:</p><pre tabindex="0"><code>Hello from Kubernetes storage
</code></pre><p>If you see that message, you have successfully configured a Pod to
use storage from a PersistentVolumeClaim.</p><h2 id="clean-up">Clean up</h2><p>Delete the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod task-pv-pod
</span></span></code></pre></div><h2 id="mounting-the-same-persistentvolume-in-two-places">Mounting the same PersistentVolume in two places</h2><p>You have understood how to create a PersistentVolume &amp; PersistentVolumeClaim, and how to mount
the volume to a single location in a container. Let's explore how you can mount the same PersistentVolume
at two different locations in a container. Below is an example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/pv-duplicate.yaml"><code>pods/storage/pv-duplicate.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/pv-duplicate.yaml to clipboard"></div><div class="includecode" id="pods-storage-pv-duplicate-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span><span># a mount for site-data</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/usr/share/nginx/html<span>
</span></span></span><span><span><span>          </span><span>subPath</span>:<span> </span>html<span>
</span></span></span><span><span><span>        </span><span># another mount for nginx config</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/nginx/nginx.conf<span>
</span></span></span><span><span><span>          </span><span>subPath</span>:<span> </span>nginx.conf<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>      </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>        </span><span>claimName</span>:<span> </span>task-pv-storage<span>
</span></span></span></code></pre></div></div></div><p>Here:</p><ul><li><code>subPath</code>: This field allows specific files or directories from the mounted PersistentVolume to be exposed at
different locations within the container. In this example:<ul><li><code>subPath: html</code> mounts the html directory.</li><li><code>subPath: nginx.conf</code> mounts a specific file, nginx.conf.</li></ul></li></ul><p>Since the first subPath is <code>html</code>, an <code>html</code> directory has to be created within <code>/mnt/data/</code>
on the node.</p><p>The second subPath <code>nginx.conf</code> means that a file within the <code>/mnt/data/</code> directory will be used. No other directory
needs to be created.</p><p>Two volume mounts will be made on your nginx container:</p><ul><li><code>/usr/share/nginx/html</code> for the static website</li><li><code>/etc/nginx/nginx.conf</code> for the default config</li></ul><h3 id="move-the-index-html-file-on-your-node-to-a-new-folder">Move the index.html file on your Node to a new folder</h3><p>The <code>index.html</code> file mentioned here refers to the one created in the "<a href="#create-an-index-html-file-on-your-node">Create an index.html file on your Node</a>" section.</p><p>Open a shell to the single Node in your cluster. How you open a shell depends on how you set up your cluster.
For example, if you are using Minikube, you can open a shell to your Node by entering <code>minikube ssh</code>.</p><p>Create a <code>/mnt/data/html</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This assumes that your Node uses "sudo" to run commands</span>
</span></span><span><span><span># as the superuser</span>
</span></span><span><span>sudo mkdir /mnt/data/html
</span></span></code></pre></div><p>Move index.html into the directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Move index.html from its current location to the html sub-directory</span>
</span></span><span><span>sudo mv /mnt/data/index.html html
</span></span></code></pre></div><h3 id="create-a-new-nginx-conf-file">Create a new nginx.conf file</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/nginx.conf"><code>pods/storage/nginx.conf</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/nginx.conf to clipboard"></div><div class="includecode" id="pods-storage-nginx-conf"><pre tabindex="0"><code class="language-conf">user  nginx;
worker_processes  auto;
<p>error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;</p>
<p>events {
worker_connections  1024;
}</p>
<p>http {
include       /etc/nginx/mime.types;
default_type  application/octet-stream;</p>
<pre><code>log_format  main  &amp;#39;$remote_addr - $remote_user [$time_local] &amp;#34;$request&amp;#34; &amp;#39;
                  &amp;#39;$status $body_bytes_sent &amp;#34;$http_referer&amp;#34; &amp;#39;
                  &amp;#39;&amp;#34;$http_user_agent&amp;#34; &amp;#34;$http_x_forwarded_for&amp;#34;&amp;#39;;

access_log  /var/log/nginx/access.log  main;

sendfile        on;
#tcp_nopush     on;

keepalive_timeout  60;

#gzip  on;

include /etc/nginx/conf.d/*.conf;
</code></pre><p>}</p></code></pre></div></div><p>This is a modified version of the default <code>nginx.conf</code> file. Here, the default <code>keepalive_timeout</code> has been
modified to <code>60</code></p><p>Create the nginx.conf file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt; /mnt/data/nginx.conf
</span></span></span><span><span><span>user  nginx;
</span></span></span><span><span><span>worker_processes  auto;
</span></span></span><span><span><span>error_log  /var/log/nginx/error.log notice;
</span></span></span><span><span><span>pid        /var/run/nginx.pid;
</span></span></span><span><span><span>
</span></span></span><span><span><span>events {
</span></span></span><span><span><span>    worker_connections  1024;
</span></span></span><span><span><span>}
</span></span></span><span><span><span>
</span></span></span><span><span><span>http {
</span></span></span><span><span><span>    include       /etc/nginx/mime.types;
</span></span></span><span><span><span>    default_type  application/octet-stream;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    log_format  main  '\$remote_addr - \$remote_user [\$time_local] "\$request" '
</span></span></span><span><span><span>                      '\$status \$body_bytes_sent "\$http_referer" '
</span></span></span><span><span><span>                      '"\$http_user_agent" "\$http_x_forwarded_for"';
</span></span></span><span><span><span>
</span></span></span><span><span><span>    access_log  /var/log/nginx/access.log  main;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    sendfile        on;
</span></span></span><span><span><span>    #tcp_nopush     on;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    keepalive_timeout  60;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    #gzip  on;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    include /etc/nginx/conf.d/*.conf;
</span></span></span><span><span><span>}
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><h3 id="create-a-pod-1">Create a Pod</h3><p>Here we will create a pod that uses the existing persistentVolume and persistentVolumeClaim.
However, the pod mounts only a specific file, <code>nginx.conf</code>, and directory, <code>html</code>, to the container.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/storage/pv-duplicate.yaml
</span></span></code></pre></div><p>Verify that the container in the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod <span>test</span>
</span></span></code></pre></div><p>Get a shell to the container running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it <span>test</span> -- /bin/bash
</span></span></code></pre></div><p>In your shell, verify that nginx is serving the <code>index.html</code> file from the
hostPath volume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Be sure to run these 3 commands inside the root shell that comes from</span>
</span></span><span><span><span># running "kubectl exec" in the previous step</span>
</span></span><span><span>apt update
</span></span><span><span>apt install curl
</span></span><span><span>curl http://localhost/
</span></span></code></pre></div><p>The output shows the text that you wrote to the <code>index.html</code> file on the
hostPath volume:</p><pre tabindex="0"><code>Hello from Kubernetes storage
</code></pre><p>In your shell, also verify that nginx is serving the <code>nginx.conf</code> file from the
hostPath volume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Be sure to run these commands inside the root shell that comes from</span>
</span></span><span><span><span># running "kubectl exec" in the previous step</span>
</span></span><span><span>cat /etc/nginx/nginx.conf | grep keepalive_timeout
</span></span></code></pre></div><p>The output shows the modified text that you wrote to the <code>nginx.conf</code> file on the
hostPath volume:</p><pre tabindex="0"><code>keepalive_timeout  60;
</code></pre><p>If you see these messages, you have successfully configured a Pod to
use a specific file and directory in a storage from a PersistentVolumeClaim.</p><h2 id="clean-up-1">Clean up</h2><p>Delete the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod <span>test</span>
</span></span><span><span>kubectl delete pvc task-pv-claim
</span></span><span><span>kubectl delete pv task-pv-volume
</span></span></code></pre></div><p>If you don't already have a shell open to the Node in your cluster,
open a new shell the same way that you did earlier.</p><p>In the shell on your Node, remove the file and directory that you created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This assumes that your Node uses "sudo" to run commands</span>
</span></span><span><span><span># as the superuser</span>
</span></span><span><span>sudo rm /mnt/data/html/index.html
</span></span><span><span>sudo rm /mnt/data/nginx.conf
</span></span><span><span>sudo rmdir /mnt/data
</span></span></code></pre></div><p>You can now close the shell to your Node.</p><h2 id="access-control">Access control</h2><p>Storage configured with a group ID (GID) allows writing only by Pods using the same
GID. Mismatched or missing GIDs cause permission denied errors. To reduce the
need for coordination with users, an administrator can annotate a PersistentVolume
with a GID. Then the GID is automatically added to any Pod that uses the
PersistentVolume.</p><p>Use the <code>pv.beta.kubernetes.io/gid</code> annotation as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pv1<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>pv.beta.kubernetes.io/gid</span>:<span> </span><span>"1234"</span><span>
</span></span></span></code></pre></div><p>When a Pod consumes a PersistentVolume that has a GID annotation, the annotated GID
is applied to all containers in the Pod in the same way that GIDs specified in the
Pod's security context are. Every GID, whether it originates from a PersistentVolume
annotation or the Pod's specification, is applied to the first process run in
each container.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When a Pod consumes a PersistentVolume, the GIDs associated with the
PersistentVolume are not present on the Pod resource itself.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li><li>Read the <a href="https://git.k8s.io/design-proposals-archive/storage/persistent-storage.md">Persistent Storage design document</a>.</li></ul><h3 id="reference">Reference</h3><ul><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#persistentvolume-v1-core">PersistentVolume</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#persistentvolumespec-v1-core">PersistentVolumeSpec</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#persistentvolumeclaim-v1-core">PersistentVolumeClaim</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#persistentvolumeclaimspec-v1-core">PersistentVolumeClaimSpec</a></li></ul></div></div><div><div class="td-content"><h1>Configure a Pod to Use a Projected Volume for Storage</h1><p>This page shows how to use a <a href="/docs/concepts/storage/volumes/#projected"><code>projected</code></a> Volume to mount
several existing volume sources into the same directory. Currently, <code>secret</code>, <code>configMap</code>, <code>downwardAPI</code>,
and <code>serviceAccountToken</code> volumes can be projected.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>serviceAccountToken</code> is not a volume type.</div><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="configure-a-projected-volume-for-a-pod">Configure a projected volume for a pod</h2><p>In this exercise, you create username and password <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secrets</a> from local files. You then create a Pod that runs one container, using a <a href="/docs/concepts/storage/volumes/#projected"><code>projected</code></a> Volume to mount the Secrets into the same shared directory.</p><p>Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected.yaml"><code>pods/storage/projected.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/projected.yaml to clipboard"></div><div class="includecode" id="pods-storage-projected-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-projected-volume<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>test-projected-volume<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- sleep<span>
</span></span></span><span><span><span>    </span>- <span>"86400"</span><span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>all-in-one<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/projected-volume"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>all-in-one<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>secret</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>user<span>
</span></span></span><span><span><span>      </span>- <span>secret</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>pass<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the Secrets:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create files containing the username and password:</span>
</span></span><span><span><span>echo</span> -n <span>"admin"</span> &gt; ./username.txt
</span></span><span><span><span>echo</span> -n <span>"1f2d1e2e67df"</span> &gt; ./password.txt
</span></span><span><span>
</span></span><span><span><span># Package these files into secrets:</span>
</span></span><span><span>kubectl create secret generic user --from-file<span>=</span>./username.txt
</span></span><span><span>kubectl create secret generic pass --from-file<span>=</span>./password.txt
</span></span></code></pre></div></li><li><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/storage/projected.yaml
</span></span></code></pre></div></li><li><p>Verify that the Pod's container is running, and then watch for changes to
the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get --watch pod test-projected-volume
</span></span></code></pre></div><p>The output looks like this:</p><pre tabindex="0"><code>NAME                    READY     STATUS    RESTARTS   AGE
test-projected-volume   1/1       Running   0          14s
</code></pre></li><li><p>In another terminal, get a shell to the running container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it test-projected-volume -- /bin/sh
</span></span></code></pre></div></li><li><p>In your shell, verify that the <code>projected-volume</code> directory contains your projected sources:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ls /projected-volume/
</span></span></code></pre></div></li></ol><h2 id="clean-up">Clean up</h2><p>Delete the Pod and the Secrets:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod test-projected-volume
</span></span><span><span>kubectl delete secret user pass
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/volumes/#projected"><code>projected</code></a> volumes.</li><li>Read the <a href="https://git.k8s.io/design-proposals-archive/node/all-in-one-volume.md">all-in-one volume</a> design document.</li></ul></div></div><div><div class="td-content"><h1>Configure a Security Context for a Pod or Container</h1><p>A security context defines privilege and access control settings for
a Pod or Container. Security context settings include, but are not limited to:</p><ul><li><p>Discretionary Access Control: Permission to access an object, like a file, is based on
<a href="https://wiki.archlinux.org/index.php/users_and_groups">user ID (UID) and group ID (GID)</a>.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">Security Enhanced Linux (SELinux)</a>:
Objects are assigned security labels.</p></li><li><p>Running as privileged or unprivileged.</p></li><li><p><a href="https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/">Linux Capabilities</a>:
Give a process some privileges, but not all the privileges of the root user.</p></li><li><p><a href="/docs/tutorials/security/apparmor/">AppArmor</a>:
Use program profiles to restrict the capabilities of individual programs.</p></li><li><p><a href="/docs/tutorials/security/seccomp/">Seccomp</a>: Filter a process's system calls.</p></li><li><p><code>allowPrivilegeEscalation</code>: Controls whether a process can gain more privileges than
its parent process. This bool directly controls whether the
<a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt"><code>no_new_privs</code></a>
flag gets set on the container process.
<code>allowPrivilegeEscalation</code> is always true when the container:</p><ul><li>is run as privileged, or</li><li>has <code>CAP_SYS_ADMIN</code></li></ul></li><li><p><code>readOnlyRootFilesystem</code>: Mounts the container's root filesystem as read-only.</p></li></ul><p>The above bullets are not a complete set of security context settings -- please see
<a href="/docs/reference/generated/kubernetes-api/v1.34/#securitycontext-v1-core">SecurityContext</a>
for a comprehensive list.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="set-the-security-context-for-a-pod">Set the security context for a Pod</h2><p>To specify security settings for a Pod, include the <code>securityContext</code> field
in the Pod specification. The <code>securityContext</code> field is a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podsecuritycontext-v1-core">PodSecurityContext</a> object.
The security settings that you specify for a Pod apply to all Containers in the Pod.
Here is a configuration file for a Pod that has a <code>securityContext</code> and an <code>emptyDir</code> volume:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/security/security-context.yaml"><code>pods/security/security-context.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/security/security-context.yaml to clipboard"></div><div class="includecode" id="pods-security-security-context-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>security-context-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>runAsUser</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>    </span><span>runAsGroup</span>:<span> </span><span>3000</span><span>
</span></span></span><span><span><span>    </span><span>fsGroup</span>:<span> </span><span>2000</span><span>
</span></span></span><span><span><span>    </span><span>supplementalGroups</span>:<span> </span>[<span>4000</span>]<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-vol<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span> </span><span>"sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"sleep 1h"</span><span> </span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>sec-ctx-vol<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/data/demo<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>      </span><span>allowPrivilegeEscalation</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, the <code>runAsUser</code> field specifies that for any Containers in
the Pod, all processes run with user ID 1000. The <code>runAsGroup</code> field specifies the primary group ID of 3000 for
all processes within any containers of the Pod. If this field is omitted, the primary group ID of the containers
will be root(0). Any files created will also be owned by user 1000 and group 3000 when <code>runAsGroup</code> is specified.
Since <code>fsGroup</code> field is specified, all processes of the container are also part of the supplementary group ID 2000.
The owner for volume <code>/data/demo</code> and any files created in that volume will be Group ID 2000.
Additionally, when the <code>supplementalGroups</code> field is specified, all processes of the container are also part of the
specified groups. If this field is omitted, it means empty.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/security/security-context.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod security-context-demo
</span></span></code></pre></div><p>Get a shell to the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it security-context-demo -- sh
</span></span></code></pre></div><p>In your shell, list the running processes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ps
</span></span></code></pre></div><p>The output shows that the processes are running as user 1000, which is the value of <code>runAsUser</code>:</p><pre tabindex="0"><code class="language-none">PID   USER     TIME  COMMAND
    1 1000      0:00 sleep 1h
    6 1000      0:00 sh
...
</code></pre><p>In your shell, navigate to <code>/data</code>, and list the one directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>cd</span> /data
</span></span><span><span>ls -l
</span></span></code></pre></div><p>The output shows that the <code>/data/demo</code> directory has group ID 2000, which is
the value of <code>fsGroup</code>.</p><pre tabindex="0"><code class="language-none">drwxrwsrwx 2 root 2000 4096 Jun  6 20:08 demo
</code></pre><p>In your shell, navigate to <code>/data/demo</code>, and create a file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>cd</span> demo
</span></span><span><span><span>echo</span> hello &gt; testfile
</span></span></code></pre></div><p>List the file in the <code>/data/demo</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ls -l
</span></span></code></pre></div><p>The output shows that <code>testfile</code> has group ID 2000, which is the value of <code>fsGroup</code>.</p><pre tabindex="0"><code class="language-none">-rw-r--r-- 1 1000 2000 6 Jun  6 20:08 testfile
</code></pre><p>Run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>id
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">uid=1000 gid=3000 groups=2000,3000,4000
</code></pre><p>From the output, you can see that <code>gid</code> is 3000 which is same as the <code>runAsGroup</code> field.
If the <code>runAsGroup</code> was omitted, the <code>gid</code> would remain as 0 (root) and the process will
be able to interact with files that are owned by the root(0) group and groups that have
the required group permissions for the root (0) group. You can also see that <code>groups</code>
contains the group IDs which are specified by <code>fsGroup</code> and <code>supplementalGroups</code>,
in addition to <code>gid</code>.</p><p>Exit your shell:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>exit</span>
</span></span></code></pre></div><h3 id="implicit-group-memberships-defined-in-etc-group-in-the-container-image">Implicit group memberships defined in <code>/etc/group</code> in the container image</h3><p>By default, kubernetes merges group information from the Pod with information defined in <code>/etc/group</code> in the container image.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/security/security-context-5.yaml"><code>pods/security/security-context-5.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/security/security-context-5.yaml to clipboard"></div><div class="includecode" id="pods-security-security-context-5-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>security-context-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>runAsUser</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>    </span><span>runAsGroup</span>:<span> </span><span>3000</span><span>
</span></span></span><span><span><span>    </span><span>supplementalGroups</span>:<span> </span>[<span>4000</span>]<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/e2e-test-images/agnhost:2.45<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span> </span><span>"sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"sleep 1h"</span><span> </span>]<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>      </span><span>allowPrivilegeEscalation</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div></div></div><p>This Pod security context contains <code>runAsUser</code>, <code>runAsGroup</code> and <code>supplementalGroups</code>.
However, you can see that the actual supplementary groups attached to the container process
will include group IDs which come from <code>/etc/group</code> in the container image.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/security/security-context-5.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod security-context-demo
</span></span></code></pre></div><p>Get a shell to the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it security-context-demo -- sh
</span></span></code></pre></div><p>Check the process identity:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>id
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">uid=1000 gid=3000 groups=3000,4000,50000
</code></pre><p>You can see that <code>groups</code> includes group ID <code>50000</code>. This is because the user (<code>uid=1000</code>),
which is defined in the image, belongs to the group (<code>gid=50000</code>), which is defined in <code>/etc/group</code>
inside the container image.</p><p>Check the <code>/etc/group</code> in the container image:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /etc/group
</span></span></code></pre></div><p>You can see that uid <code>1000</code> belongs to group <code>50000</code>.</p><pre tabindex="0"><code class="language-none">...
user-defined-in-image:x:1000:
group-defined-in-image:x:50000:user-defined-in-image
</code></pre><p>Exit your shell:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>exit</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><em>Implicitly merged</em> supplementary groups may cause security problems particularly when accessing
the volumes (see <a href="https://issue.k8s.io/112879">kubernetes/kubernetes#112879</a> for details).
If you want to avoid this. Please see the below section.</div><h2 id="supplementalgroupspolicy">Configure fine-grained SupplementalGroups control for a Pod</h2><div class="feature-state-notice feature-beta" title="Feature Gate: SupplementalGroupsPolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>This feature can be enabled by setting the <code>SupplementalGroupsPolicy</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> for kubelet and
kube-apiserver, and setting the <code>.spec.securityContext.supplementalGroupsPolicy</code> field for a pod.</p><p>The <code>supplementalGroupsPolicy</code> field defines the policy for calculating the
supplementary groups for the container processes in a pod. There are two valid
values for this field:</p><ul><li><p><code>Merge</code>: The group membership defined in <code>/etc/group</code> for the container's primary user will be merged.
This is the default policy if not specified.</p></li><li><p><code>Strict</code>: Only group IDs in <code>fsGroup</code>, <code>supplementalGroups</code>, or <code>runAsGroup</code> fields
are attached as the supplementary groups of the container processes.
This means no group membership from <code>/etc/group</code> for the container's primary user will be merged.</p></li></ul><p>When the feature is enabled, it also exposes the process identity attached to the first container process
in <code>.status.containerStatuses[].user.linux</code> field. It would be useful for detecting if
implicit group ID's are attached.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/security/security-context-6.yaml"><code>pods/security/security-context-6.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/security/security-context-6.yaml to clipboard"></div><div class="includecode" id="pods-security-security-context-6-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>security-context-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>runAsUser</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>    </span><span>runAsGroup</span>:<span> </span><span>3000</span><span>
</span></span></span><span><span><span>    </span><span>supplementalGroups</span>:<span> </span>[<span>4000</span>]<span>
</span></span></span><span><span><span>    </span><span>supplementalGroupsPolicy</span>:<span> </span>Strict<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/e2e-test-images/agnhost:2.45<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span> </span><span>"sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"sleep 1h"</span><span> </span>]<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>      </span><span>allowPrivilegeEscalation</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div></div></div><p>This pod manifest defines <code>supplementalGroupsPolicy=Strict</code>. You can see that no group memberships
defined in <code>/etc/group</code> are merged to the supplementary groups for container processes.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/security/security-context-6.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod security-context-demo
</span></span></code></pre></div><p>Check the process identity:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it security-context-demo -- id
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">uid=1000 gid=3000 groups=3000,4000
</code></pre><p>See the Pod's status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod security-context-demo -o yaml
</span></span></code></pre></div><p>You can see that the <code>status.containerStatuses[].user.linux</code> field exposes the process identity
attached to the first container process.</p><pre tabindex="0"><code class="language-none">...
status:
  containerStatuses:
  - name: sec-ctx-demo
    user:
      linux:
        gid: 3000
        supplementalGroups:
        - 3000
        - 4000
        uid: 1000
...
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Please note that the values in the <code>status.containerStatuses[].user.linux</code> field is <em>the first attached</em>
process identity to the first container process in the container. If the container has sufficient privilege
to make system calls related to process identity
(e.g. <a href="https://man7.org/linux/man-pages/man2/setuid.2.html"><code>setuid(2)</code></a>,
<a href="https://man7.org/linux/man-pages/man2/setgid.2.html"><code>setgid(2)</code></a> or
<a href="https://man7.org/linux/man-pages/man2/setgroups.2.html"><code>setgroups(2)</code></a>, etc.),
the container process can change its identity. Thus, the <em>actual</em> process identity will be dynamic.</div><h3 id="implementations-supplementalgroupspolicy">Implementations</h3><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The following container runtimes are known to support fine-grained SupplementalGroups control.</p><p>CRI-level:</p><ul><li><a href="https://containerd.io/">containerd</a>, since v2.0</li><li><a href="https://cri-o.io/">CRI-O</a>, since v1.31</li></ul><p>You can see if the feature is supported in the Node status.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Node<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>features</span>:<span>
</span></span></span><span><span><span>    </span><span>supplementalGroupsPolicy</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>At this alpha release(from v1.31 to v1.32), when a pod with <code>SupplementalGroupsPolicy=Strict</code> are scheduled to a node that does NOT support this feature(i.e. <code>.status.features.supplementalGroupsPolicy=false</code>), the pod's supplemental groups policy falls back to the <code>Merge</code> policy <em>silently</em>.</p><p>However, since the beta release (v1.33), to enforce the policy more strictly, <strong>such pod creation will be rejected by kubelet because the node cannot ensure the specified policy</strong>. When your pod is rejected, you will see warning events with <code>reason=SupplementalGroupsPolicyNotSupported</code> like below:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Event<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Warning<span>
</span></span></span><span><span><span></span><span>reason</span>:<span> </span>SupplementalGroupsPolicyNotSupported<span>
</span></span></span><span><span><span></span><span>message</span>:<span> </span><span>"SupplementalGroupsPolicy=Strict is not supported in this node"</span><span>
</span></span></span><span><span><span></span><span>involvedObject</span>:<span>
</span></span></span><span><span><span>  </span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div></div><h2 id="configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>By default, Kubernetes recursively changes ownership and permissions for the contents of each
volume to match the <code>fsGroup</code> specified in a Pod's <code>securityContext</code> when that volume is
mounted.
For large volumes, checking and changing ownership and permissions can take a lot of time,
slowing Pod startup. You can use the <code>fsGroupChangePolicy</code> field inside a <code>securityContext</code>
to control the way that Kubernetes checks and manages ownership and permissions
for a volume.</p><p><strong>fsGroupChangePolicy</strong> - <code>fsGroupChangePolicy</code> defines behavior for changing ownership
and permission of the volume before being exposed inside a Pod.
This field only applies to volume types that support <code>fsGroup</code> controlled ownership and permissions.
This field has two possible values:</p><ul><li><em>OnRootMismatch</em>: Only change permissions and ownership if the permission and the ownership of
root directory does not match with expected permissions of the volume.
This could help shorten the time it takes to change ownership and permission of a volume.</li><li><em>Always</em>: Always change permission and ownership of the volume when volume is mounted.</li></ul><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>runAsUser</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>  </span><span>runAsGroup</span>:<span> </span><span>3000</span><span>
</span></span></span><span><span><span>  </span><span>fsGroup</span>:<span> </span><span>2000</span><span>
</span></span></span><span><span><span>  </span><span>fsGroupChangePolicy</span>:<span> </span><span>"OnRootMismatch"</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This field has no effect on ephemeral volume types such as
<a href="/docs/concepts/storage/volumes/#secret"><code>secret</code></a>,
<a href="/docs/concepts/storage/volumes/#configmap"><code>configMap</code></a>,
and <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a>.</div><h2 id="delegating-volume-permission-and-ownership-change-to-csi-driver">Delegating volume permission and ownership change to CSI driver</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>If you deploy a <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface (CSI)</a>
driver which supports the <code>VOLUME_MOUNT_GROUP</code> <code>NodeServiceCapability</code>, the
process of setting file ownership and permissions based on the
<code>fsGroup</code> specified in the <code>securityContext</code> will be performed by the CSI driver
instead of Kubernetes. In this case, since Kubernetes doesn't perform any
ownership and permission change, <code>fsGroupChangePolicy</code> does not take effect, and
as specified by CSI, the driver is expected to mount the volume with the
provided <code>fsGroup</code>, resulting in a volume that is readable/writable by the
<code>fsGroup</code>.</p><h2 id="set-the-security-context-for-a-container">Set the security context for a Container</h2><p>To specify security settings for a Container, include the <code>securityContext</code> field
in the Container manifest. The <code>securityContext</code> field is a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#securitycontext-v1-core">SecurityContext</a> object.
Security settings that you specify for a Container apply only to
the individual Container, and they override settings made at the Pod level when
there is overlap. Container settings do not affect the Pod's Volumes.</p><p>Here is the configuration file for a Pod that has one Container. Both the Pod
and the Container have a <code>securityContext</code> field:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/security/security-context-2.yaml"><code>pods/security/security-context-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/security/security-context-2.yaml to clipboard"></div><div class="includecode" id="pods-security-security-context-2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>security-context-demo-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>runAsUser</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-demo-2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>      </span><span>runAsUser</span>:<span> </span><span>2000</span><span>
</span></span></span><span><span><span>      </span><span>allowPrivilegeEscalation</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/security/security-context-2.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod security-context-demo-2
</span></span></code></pre></div><p>Get a shell into the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it security-context-demo-2 -- sh
</span></span></code></pre></div><p>In your shell, list the running processes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ps aux
</span></span></code></pre></div><p>The output shows that the processes are running as user 2000. This is the value
of <code>runAsUser</code> specified for the Container. It overrides the value 1000 that is
specified for the Pod.</p><pre tabindex="0"><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
2000         1  0.0  0.0   4336   764 ?        Ss   20:36   0:00 /bin/sh -c node server.js
2000         8  0.1  0.5 772124 22604 ?        Sl   20:36   0:00 node server.js
...
</code></pre><p>Exit your shell:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>exit</span>
</span></span></code></pre></div><h2 id="set-capabilities-for-a-container">Set capabilities for a Container</h2><p>With <a href="https://man7.org/linux/man-pages/man7/capabilities.7.html">Linux capabilities</a>,
you can grant certain privileges to a process without granting all the privileges
of the root user. To add or remove Linux capabilities for a Container, include the
<code>capabilities</code> field in the <code>securityContext</code> section of the Container manifest.</p><p>First, see what happens when you don't include a <code>capabilities</code> field.
Here is configuration file that does not add or remove any Container capabilities:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/security/security-context-3.yaml"><code>pods/security/security-context-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/security/security-context-3.yaml to clipboard"></div><div class="includecode" id="pods-security-security-context-3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>security-context-demo-3<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-3<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/security/security-context-3.yaml
</span></span></code></pre></div><p>Verify that the Pod's Container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod security-context-demo-3
</span></span></code></pre></div><p>Get a shell into the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it security-context-demo-3 -- sh
</span></span></code></pre></div><p>In your shell, list the running processes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ps aux
</span></span></code></pre></div><p>The output shows the process IDs (PIDs) for the Container:</p><pre tabindex="0"><code>USER  PID %CPU %MEM    VSZ   RSS TTY   STAT START   TIME COMMAND
root    1  0.0  0.0   4336   796 ?     Ss   18:17   0:00 /bin/sh -c node server.js
root    5  0.1  0.5 772124 22700 ?     Sl   18:17   0:00 node server.js
</code></pre><p>In your shell, view the status for process 1:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>cd</span> /proc/1
</span></span><span><span>cat status
</span></span></code></pre></div><p>The output shows the capabilities bitmap for the process:</p><pre tabindex="0"><code>...
CapPrm:	00000000a80425fb
CapEff:	00000000a80425fb
...
</code></pre><p>Make a note of the capabilities bitmap, and then exit your shell:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>exit</span>
</span></span></code></pre></div><p>Next, run a Container that is the same as the preceding container, except
that it has additional capabilities set.</p><p>Here is the configuration file for a Pod that runs one Container. The configuration
adds the <code>CAP_NET_ADMIN</code> and <code>CAP_SYS_TIME</code> capabilities:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/security/security-context-4.yaml"><code>pods/security/security-context-4.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/security/security-context-4.yaml to clipboard"></div><div class="includecode" id="pods-security-security-context-4-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>security-context-demo-4<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>sec-ctx-4<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>      </span><span>capabilities</span>:<span>
</span></span></span><span><span><span>        </span><span>add</span>:<span> </span>[<span>"NET_ADMIN"</span>,<span> </span><span>"SYS_TIME"</span>]<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/security/security-context-4.yaml
</span></span></code></pre></div><p>Get a shell into the running Container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it security-context-demo-4 -- sh
</span></span></code></pre></div><p>In your shell, view the capabilities for process 1:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>cd</span> /proc/1
</span></span><span><span>cat status
</span></span></code></pre></div><p>The output shows capabilities bitmap for the process:</p><pre tabindex="0"><code>...
CapPrm:	00000000aa0435fb
CapEff:	00000000aa0435fb
...
</code></pre><p>Compare the capabilities of the two Containers:</p><pre tabindex="0"><code>00000000a80425fb
00000000aa0435fb
</code></pre><p>In the capability bitmap of the first container, bits 12 and 25 are clear. In the second container,
bits 12 and 25 are set. Bit 12 is <code>CAP_NET_ADMIN</code>, and bit 25 is <code>CAP_SYS_TIME</code>.
See <a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h">capability.h</a>
for definitions of the capability constants.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Linux capability constants have the form <code>CAP_XXX</code>.
But when you list capabilities in your container manifest, you must
omit the <code>CAP_</code> portion of the constant.
For example, to add <code>CAP_SYS_TIME</code>, include <code>SYS_TIME</code> in your list of capabilities.</div><h2 id="set-the-seccomp-profile-for-a-container">Set the Seccomp Profile for a Container</h2><p>To set the Seccomp profile for a Container, include the <code>seccompProfile</code> field
in the <code>securityContext</code> section of your Pod or Container manifest. The
<code>seccompProfile</code> field is a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#seccompprofile-v1-core">SeccompProfile</a> object consisting of <code>type</code> and <code>localhostProfile</code>.
Valid options for <code>type</code> include <code>RuntimeDefault</code>, <code>Unconfined</code>, and
<code>Localhost</code>. <code>localhostProfile</code> must only be set if <code>type: Localhost</code>. It
indicates the path of the pre-configured profile on the node, relative to the
kubelet's configured Seccomp profile location (configured with the <code>--root-dir</code>
flag).</p><p>Here is an example that sets the Seccomp profile to the node's container runtime
default profile:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>seccompProfile</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>RuntimeDefault<span>
</span></span></span></code></pre></div><p>Here is an example that sets the Seccomp profile to a pre-configured file at
<code>&lt;kubelet-root-dir&gt;/seccomp/my-profiles/profile-allow.json</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>seccompProfile</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Localhost<span>
</span></span></span><span><span><span>    </span><span>localhostProfile</span>:<span> </span>my-profiles/profile-allow.json<span>
</span></span></span></code></pre></div><h2 id="set-the-apparmor-profile-for-a-container">Set the AppArmor Profile for a Container</h2><p>To set the AppArmor profile for a Container, include the <code>appArmorProfile</code> field
in the <code>securityContext</code> section of your Container. The <code>appArmorProfile</code> field
is a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#apparmorprofile-v1-core">AppArmorProfile</a> object consisting of <code>type</code> and <code>localhostProfile</code>.
Valid options for <code>type</code> include <code>RuntimeDefault</code>(default), <code>Unconfined</code>, and
<code>Localhost</code>. <code>localhostProfile</code> must only be set if <code>type</code> is <code>Localhost</code>. It
indicates the name of the pre-configured profile on the node. The profile needs
to be loaded onto all nodes suitable for the Pod, since you don't know where the
pod will be scheduled.
Approaches for setting up custom profiles are discussed in
<a href="/docs/tutorials/security/apparmor/#setting-up-nodes-with-profiles">Setting up nodes with profiles</a>.</p><p>Note: If <code>containers[*].securityContext.appArmorProfile.type</code> is explicitly set
to <code>RuntimeDefault</code>, then the Pod will not be admitted if AppArmor is not
enabled on the Node. However if <code>containers[*].securityContext.appArmorProfile.type</code>
is not specified, then the default (which is also <code>RuntimeDefault</code>) will only
be applied if the node has AppArmor enabled. If the node has AppArmor disabled
the Pod will be admitted but the Container will not be restricted by the
<code>RuntimeDefault</code> profile.</p><p>Here is an example that sets the AppArmor profile to the node's container runtime
default profile:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>containers</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>container-1<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>appArmorProfile</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>RuntimeDefault<span>
</span></span></span></code></pre></div><p>Here is an example that sets the AppArmor profile to a pre-configured profile
named <code>k8s-apparmor-example-deny-write</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>containers</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>container-1<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>    </span><span>appArmorProfile</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>Localhost<span>
</span></span></span><span><span><span>      </span><span>localhostProfile</span>:<span> </span>k8s-apparmor-example-deny-write<span>
</span></span></span></code></pre></div><p>For more details please see, <a href="/docs/tutorials/security/apparmor/">Restrict a Container's Access to Resources with AppArmor</a>.</p><h2 id="assign-selinux-labels-to-a-container">Assign SELinux labels to a Container</h2><p>To assign SELinux labels to a Container, include the <code>seLinuxOptions</code> field in
the <code>securityContext</code> section of your Pod or Container manifest. The
<code>seLinuxOptions</code> field is an
<a href="/docs/reference/generated/kubernetes-api/v1.34/#selinuxoptions-v1-core">SELinuxOptions</a>
object. Here's an example that applies an SELinux level:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>seLinuxOptions</span>:<span>
</span></span></span><span><span><span>    </span><span>level</span>:<span> </span><span>"s0:c123,c456"</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To assign SELinux labels, the SELinux security module must be loaded on the host operating system.
On Windows and Linux worker nodes without SELinux support, this field and any SELinux feature gates described
below have no effect.</div><h3 id="efficient-selinux-volume-relabeling">Efficient SELinux volume relabeling</h3><div class="feature-state-notice feature-beta" title="Feature Gate: SELinuxMountReadWriteOncePod"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [beta]</code> (enabled by default: true)</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Kubernetes v1.27 introduced an early limited form of this behavior that was only applicable
to volumes (and PersistentVolumeClaims) using the <code>ReadWriteOncePod</code> access mode.</p><p>Kubernetes v1.33 promotes <code>SELinuxChangePolicy</code> and <code>SELinuxMount</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>
as beta to widen that performance improvement to other kinds of PersistentVolumeClaims,
as explained in detail below. While in beta, <code>SELinuxMount</code> is still disabled by default.</p></div><p>With <code>SELinuxMount</code> feature gate disabled (the default in Kubernetes 1.33 and any previous release),
the container runtime recursively assigns SELinux label to all
files on all Pod volumes by default. To speed up this process, Kubernetes can change the
SELinux label of a volume instantly by using a mount option
<code>-o context=&lt;label&gt;</code>.</p><p>To benefit from this speedup, all these conditions must be met:</p><ul><li>The <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
<code>SELinuxMountReadWriteOncePod</code> must be enabled.</li><li>Pod must use PersistentVolumeClaim with applicable <code>accessModes</code> and <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>:<ul><li>Either the volume has <code>accessModes: ["ReadWriteOncePod"]</code>, and feature gate <code>SELinuxMountReadWriteOncePod</code> is enabled.</li><li>Or the volume can use any other access modes and all feature gates
<code>SELinuxMountReadWriteOncePod</code>, <code>SELinuxChangePolicy</code> and <code>SELinuxMount</code> must be enabled
and the Pod has <code>spec.securityContext.seLinuxChangePolicy</code> either nil (default) or <code>MountOption</code>.</li></ul></li><li>Pod (or all its Containers that use the PersistentVolumeClaim) must
have <code>seLinuxOptions</code> set.</li><li>The corresponding PersistentVolume must be either:<ul><li>A volume that uses the legacy in-tree <code>iscsi</code>, <code>rbd</code> or <code>fc</code> volume type.</li><li>Or a volume that uses a <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.
The CSI driver must announce that it supports mounting with <code>-o context</code> by setting
<code>spec.seLinuxMount: true</code> in its CSIDriver instance.</li></ul></li></ul><p>When any of these conditions is not met, SELinux relabelling happens another way: the container
runtime recursively changes the SELinux label for all inodes (files and directories)
in the volume. Calling out explicitly, this applies to Kubernetes ephemeral volumes like
<code>secret</code>, <code>configMap</code> and <code>projected</code>, and all volumes whose CSIDriver instance does not
explicitly announce mounting with <code>-o context</code>.</p><p>When this speedup is used, all Pods that use the same applicable volume concurrently on the same node
<strong>must have the same SELinux label</strong>. A Pod with a different SELinux label will fail to start and will be
<code>ContainerCreating</code> until all Pods with other SELinux labels that use the volume are deleted.</p><p><div class="feature-state-notice feature-beta" title="Feature Gate: SELinuxChangePolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div>For Pods that want to opt-out from relabeling using mount options, they can set
<code>spec.securityContext.seLinuxChangePolicy</code> to <code>Recursive</code>. This is required
when multiple pods share a single volume on the same node, but they run with
different SELinux labels that allows simultaneous access to the volume. For example, a privileged pod
running with label <code>spc_t</code> and an unprivileged pod running with the default label <code>container_file_t</code>.
With unset <code>spec.securityContext.seLinuxChangePolicy</code> (or with the default value <code>MountOption</code>),
only one of such pods is able to run on a node, the other one gets ContainerCreating with error
<code>conflicting SELinux labels of volume &lt;name of the volume&gt;: &lt;label of the running pod&gt; and &lt;label of the pod that can't start&gt;</code>.</p><h4 id="selinuxwarningcontroller">SELinuxWarningController</h4><p>To make it easier to identify Pods that are affected by the change in SELinux volume relabeling,
a new controller called <code>SELinuxWarningController</code> has been introduced in kube-controller-manager.
It is disabled by default and can be enabled by either setting the <code>--controllers=*,selinux-warning-controller</code>
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">command line flag</a>,
or by setting <code>genericControllerManagerConfiguration.controllers</code>
<a href="/docs/reference/config-api/kube-controller-manager-config.v1alpha1/#controllermanager-config-k8s-io-v1alpha1-GenericControllerManagerConfiguration">field in KubeControllerManagerConfiguration</a>.
This controller requires <code>SELinuxChangePolicy</code> feature gate to be enabled.</p><p>When enabled, the controller observes running Pods and when it detects that two Pods use the same volume
with different SELinux labels:</p><ol><li>It emits an event to both of the Pods. <code>kubectl describe pod &lt;pod-name&gt;</code> the shows
<code>SELinuxLabel "&lt;label on the pod&gt;" conflicts with pod &lt;the other pod name&gt; that uses the same volume as this pod with SELinuxLabel "&lt;the other pod label&gt;". If both pods land on the same node, only one of them may access the volume</code>.</li><li>Raise <code>selinux_warning_controller_selinux_volume_conflict</code> metric. The metric has both pod
names + namespaces as labels to identify the affected pods easily.</li></ol><p>A cluster admin can use this information to identify pods affected by the planning change and
proactively opt-out Pods from the optimization (i.e. set <code>spec.securityContext.seLinuxChangePolicy: Recursive</code>).</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>We strongly recommend clusters that use SELinux to enable this controller and make sure that
<code>selinux_warning_controller_selinux_volume_conflict</code> metric does not report any conflicts before enabling <code>SELinuxMount</code>
feature gate or upgrading to a version where <code>SELinuxMount</code> is enabled by default.</div><h4 id="feature-gates">Feature gates</h4><p>The following feature gates control the behavior of SELinux volume relabeling:</p><ul><li><code>SELinuxMountReadWriteOncePod</code>: enables the optimization for volumes with <code>accessModes: ["ReadWriteOncePod"]</code>.
This is a very safe feature gate to enable, as it cannot happen that two pods can share one single volume with
this access mode. This feature gate is enabled by default sine v1.28.</li><li><code>SELinuxChangePolicy</code>: enables <code>spec.securityContext.seLinuxChangePolicy</code> field in Pod and related SELinuxWarningController
in kube-controller-manager. This feature can be used before enabling <code>SELinuxMount</code> to check Pods running on a cluster,
and to pro-actively opt-out Pods from the optimization.
This feature gate requires <code>SELinuxMountReadWriteOncePod</code> enabled. It is beta and enabled by default in 1.33.</li><li><code>SELinuxMount</code> enables the optimization for all eligible volumes. Since it can break existing workloads, we recommend
enabling <code>SELinuxChangePolicy</code> feature gate + SELinuxWarningController first to check the impact of the change.
This feature gate requires <code>SELinuxMountReadWriteOncePod</code> and <code>SELinuxChangePolicy</code> enabled. It is beta, but disabled
by default in 1.33.</li></ul><h2 id="proc-access">Managing access to the <code>/proc</code> filesystem</h2><div class="feature-state-notice feature-beta" title="Feature Gate: ProcMountType"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>For runtimes that follow the OCI runtime specification, containers default to running in a mode where
there are multiple paths that are both masked and read-only.
The result of this is the container has these paths present inside the container's mount namespace, and they can function similarly to if
the container was an isolated host, but the container process cannot write to
them. The list of masked and read-only paths are as follows:</p><ul><li><p>Masked Paths:</p><ul><li><code>/proc/asound</code></li><li><code>/proc/acpi</code></li><li><code>/proc/kcore</code></li><li><code>/proc/keys</code></li><li><code>/proc/latency_stats</code></li><li><code>/proc/timer_list</code></li><li><code>/proc/timer_stats</code></li><li><code>/proc/sched_debug</code></li><li><code>/proc/scsi</code></li><li><code>/sys/firmware</code></li><li><code>/sys/devices/virtual/powercap</code></li></ul></li><li><p>Read-Only Paths:</p><ul><li><code>/proc/bus</code></li><li><code>/proc/fs</code></li><li><code>/proc/irq</code></li><li><code>/proc/sys</code></li><li><code>/proc/sysrq-trigger</code></li></ul></li></ul><p>For some Pods, you might want to bypass that default masking of paths.
The most common context for wanting this is if you are trying to run containers within
a Kubernetes container (within a pod).</p><p>The <code>securityContext</code> field <code>procMount</code> allows a user to request a container's <code>/proc</code>
be <code>Unmasked</code>, or be mounted as read-write by the container process. This also
applies to <code>/sys/firmware</code> which is not in <code>/proc</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>procMount</span>:<span> </span>Unmasked<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Setting <code>procMount</code> to Unmasked requires the <code>spec.hostUsers</code> value in the pod
spec to be <code>false</code>. In other words: a container that wishes to have an Unmasked
<code>/proc</code> or unmasked <code>/sys</code> must also be in a
<a href="/docs/concepts/workloads/pods/user-namespaces/">user namespace</a>.
Kubernetes v1.12 to v1.29 did not enforce that requirement.</div><h2 id="discussion">Discussion</h2><p>The security context for a Pod applies to the Pod's Containers and also to
the Pod's Volumes when applicable. Specifically <code>fsGroup</code> and <code>seLinuxOptions</code> are
applied to Volumes as follows:</p><ul><li><p><code>fsGroup</code>: Volumes that support ownership management are modified to be owned
and writable by the GID specified in <code>fsGroup</code>. See the
<a href="https://git.k8s.io/design-proposals-archive/storage/volume-ownership-management.md">Ownership Management design document</a>
for more details.</p></li><li><p><code>seLinuxOptions</code>: Volumes that support SELinux labeling are relabeled to be accessible
by the label specified under <code>seLinuxOptions</code>. Usually you only
need to set the <code>level</code> section. This sets the
<a href="https://selinuxproject.org/page/NB_MLS">Multi-Category Security (MCS)</a>
label given to all Containers in the Pod as well as the Volumes.</p></li></ul><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>After you specify an MCS label for a Pod, all Pods with the same label can access the Volume.
If you need inter-Pod protection, you must assign a unique MCS label to each Pod.</div><h2 id="clean-up">Clean up</h2><p>Delete the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod security-context-demo
</span></span><span><span>kubectl delete pod security-context-demo-2
</span></span><span><span>kubectl delete pod security-context-demo-3
</span></span><span><span>kubectl delete pod security-context-demo-4
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#podsecuritycontext-v1-core">PodSecurityContext</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#securitycontext-v1-core">SecurityContext</a></li><li><a href="https://github.com/containerd/containerd/blob/main/docs/cri/config.md">CRI Plugin Config Guide</a></li><li><a href="https://git.k8s.io/design-proposals-archive/auth/security_context.md">Security Contexts design document</a></li><li><a href="https://git.k8s.io/design-proposals-archive/storage/volume-ownership-management.md">Ownership Management design document</a></li><li><a href="/docs/concepts/security/pod-security-admission/">PodSecurity Admission</a></li><li><a href="https://git.k8s.io/design-proposals-archive/auth/no-new-privs.md">AllowPrivilegeEscalation design
document</a></li><li>For more information about security mechanisms in Linux, see
<a href="https://www.linux.com/learn/overview-linux-kernel-security-features">Overview of Linux Kernel Security Features</a>
(Note: Some information is out of date)</li><li>Read about <a href="/docs/concepts/workloads/pods/user-namespaces/">User Namespaces</a>
for Linux pods.</li><li><a href="https://github.com/opencontainers/runtime-spec/blob/f66aad47309/config-linux.md#masked-paths">Masked Paths in the OCI Runtime
Specification</a></li></ul></div></div><div><div class="td-content"><h1>Configure Service Accounts for Pods</h1><p>Kubernetes offers two distinct ways for clients that run within your
cluster, or that otherwise have a relationship to your cluster's
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
to authenticate to the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>.</p><p>A <em>service account</em> provides an identity for processes that run in a Pod,
and maps to a ServiceAccount object. When you authenticate to the API
server, you identify yourself as a particular <em>user</em>. Kubernetes recognises
the concept of a user, however, Kubernetes itself does <strong>not</strong> have a User
API.</p><p>This task guide is about ServiceAccounts, which do exist in the Kubernetes
API. The guide shows you some ways to configure ServiceAccounts for Pods.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="use-the-default-service-account-to-access-the-api-server">Use the default service account to access the API server</h2><p>When Pods contact the API server, Pods authenticate as a particular
ServiceAccount (for example, <code>default</code>). There is always at least one
ServiceAccount in each <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.</p><p>Every Kubernetes namespace contains at least one ServiceAccount: the default
ServiceAccount for that namespace, named <code>default</code>.
If you do not specify a ServiceAccount when you create a Pod, Kubernetes
automatically assigns the ServiceAccount named <code>default</code> in that namespace.</p><p>You can fetch the details for a Pod you have created. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods/&lt;podname&gt; -o yaml
</span></span></code></pre></div><p>In the output, you see a field <code>spec.serviceAccountName</code>.
Kubernetes automatically
sets that value if you don't specify it when you create a Pod.</p><p>An application running inside a Pod can access the Kubernetes API using
automatically mounted service account credentials.
See <a href="/docs/tasks/access-application-cluster/access-cluster/">accessing the Cluster</a> to learn more.</p><p>When a Pod authenticates as a ServiceAccount, its level of access depends on the
<a href="/docs/reference/access-authn-authz/authorization/#authorization-modules">authorization plugin and policy</a>
in use.</p><p>The API credentials are automatically revoked when the Pod is deleted, even if
finalizers are in place. In particular, the API credentials are revoked 60 seconds
beyond the <code>.metadata.deletionTimestamp</code> set on the Pod (the deletion timestamp
is typically the time that the <strong>delete</strong> request was accepted plus the Pod's
termination grace period).</p><h3 id="opt-out-of-api-credential-automounting">Opt out of API credential automounting</h3><p>If you don't want the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a>
to automatically mount a ServiceAccount's API credentials, you can opt out of
the default behavior.
You can opt out of automounting API credentials on <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>
for a service account by setting <code>automountServiceAccountToken: false</code> on the ServiceAccount:</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>build-robot<span>
</span></span></span><span><span><span></span><span>automountServiceAccountToken</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>You can also opt out of automounting API credentials for a particular Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>serviceAccountName</span>:<span> </span>build-robot<span>
</span></span></span><span><span><span>  </span><span>automountServiceAccountToken</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>If both the ServiceAccount and the Pod's <code>.spec</code> specify a value for
<code>automountServiceAccountToken</code>, the Pod spec takes precedence.</p><h2 id="use-multiple-service-accounts">Use more than one ServiceAccount</h2><p>Every namespace has at least one ServiceAccount: the default ServiceAccount
resource, called <code>default</code>. You can list all ServiceAccount resources in your
<a href="/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference">current namespace</a>
with:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get serviceaccounts
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME      SECRETS    AGE
default   1          1d
</code></pre><p>You can create additional ServiceAccount objects like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f - <span>&lt;&lt;EOF
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: ServiceAccount
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: build-robot
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The name of a ServiceAccount object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>If you get a complete dump of the service account object, like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get serviceaccounts/build-robot -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2019-06-16T00:12:34Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>build-robot<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"272500"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>721ab723-13bc-11e5-aec2-42010af0021e<span>
</span></span></span></code></pre></div><p>You can use authorization plugins to
<a href="/docs/reference/access-authn-authz/rbac/#service-account-permissions">set permissions on service accounts</a>.</p><p>To use a non-default service account, set the <code>spec.serviceAccountName</code>
field of a Pod to the name of the ServiceAccount you wish to use.</p><p>You can only set the <code>serviceAccountName</code> field when creating a Pod, or in a
template for a new Pod. You cannot update the <code>.spec.serviceAccountName</code> field
of a Pod that already exists.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>.spec.serviceAccount</code> field is a deprecated alias for <code>.spec.serviceAccountName</code>.
If you want to remove the fields from a workload resource, set both fields to empty explicitly
on the <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>.</div><h3 id="cleanup-use-multiple-service-accounts">Cleanup</h3><p>If you tried creating <code>build-robot</code> ServiceAccount from the example above,
you can clean it up by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete serviceaccount/build-robot
</span></span></code></pre></div><h2 id="manually-create-an-api-token-for-a-serviceaccount">Manually create an API token for a ServiceAccount</h2><p>Suppose you have an existing service account named "build-robot" as mentioned earlier.</p><p>You can get a time-limited API token for that ServiceAccount using <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create token build-robot
</span></span></code></pre></div><p>The output from that command is a token that you can use to authenticate as that
ServiceAccount. You can request a specific token duration using the <code>--duration</code>
command line argument to <code>kubectl create token</code> (the actual duration of the issued
token might be shorter, or could even be longer).</p><div class="feature-state-notice feature-stable" title="Feature Gate: ServiceAccountTokenNodeBinding"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Using <code>kubectl</code> v1.31 or later, it is possible to create a service
account token that is directly bound to a Node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create token build-robot --bound-object-kind Node --bound-object-name node-001 --bound-object-uid 123...456
</span></span></code></pre></div><p>The token will be valid until it expires or either the associated Node or service account are deleted.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Versions of Kubernetes before v1.22 automatically created long term credentials for
accessing the Kubernetes API. This older mechanism was based on creating token Secrets
that could then be mounted into running Pods. In more recent versions, including
Kubernetes v1.34, API credentials are obtained directly by using the
<a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest</a> API,
and are mounted into Pods using a
<a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">projected volume</a>.
The tokens obtained using this method have bounded lifetimes, and are automatically
invalidated when the Pod they are mounted into is deleted.</p><p>You can still manually create a service account token Secret; for example,
if you need a token that never expires. However, using the
<a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest</a>
subresource to obtain a token to access the API is recommended instead.</p></div><h3 id="manually-create-a-long-lived-api-token-for-a-serviceaccount">Manually create a long-lived API token for a ServiceAccount</h3><p>If you want to obtain an API token for a ServiceAccount, you create a new Secret
with a special annotation, <code>kubernetes.io/service-account.name</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f - <span>&lt;&lt;EOF
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Secret
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: build-robot-secret
</span></span></span><span><span><span>  annotations:
</span></span></span><span><span><span>    kubernetes.io/service-account.name: build-robot
</span></span></span><span><span><span>type: kubernetes.io/service-account-token
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>If you view the Secret using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret/build-robot-secret -o yaml
</span></span></code></pre></div><p>you can see that the Secret now contains an API token for the "build-robot" ServiceAccount.</p><p>Because of the annotation you set, the control plane automatically generates a token for that
ServiceAccounts, and stores them into the associated Secret. The control plane also cleans up
tokens for deleted ServiceAccounts.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe secrets/build-robot-secret
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:           build-robot-secret
Namespace:      default
Labels:         &lt;none&gt;
Annotations:    kubernetes.io/service-account.name: build-robot
                kubernetes.io/service-account.uid: da68f9c6-9d26-11e7-b84e-002dc52800da

Type:   kubernetes.io/service-account-token

Data
====
ca.crt:         1338 bytes
namespace:      7 bytes
token:          ...
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The content of <code>token</code> is omitted here.</p><p>Take care not to display the contents of a <code>kubernetes.io/service-account-token</code>
Secret somewhere that your terminal / computer screen could be seen by an onlooker.</p></div><p>When you delete a ServiceAccount that has an associated Secret, the Kubernetes
control plane automatically cleans up the long-lived token from that Secret.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you view the ServiceAccount using:</p><p><code>kubectl get serviceaccount build-robot -o yaml</code></p><p>You can't see the <code>build-robot-secret</code> Secret in the ServiceAccount API objects
<a href="/docs/reference/kubernetes-api/authentication-resources/service-account-v1/"><code>.secrets</code></a> field
because that field is only populated with auto-generated Secrets.</p></div><h2 id="add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a service account</h2><p>First, <a href="/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">create an imagePullSecret</a>.
Next, verify it has been created. For example:</p><ul><li><p>Create an imagePullSecret, as described in
<a href="/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">Specifying ImagePullSecrets on a Pod</a>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret docker-registry myregistrykey --docker-server<span>=</span>&lt;registry name&gt; <span>\
</span></span></span><span><span><span></span>        --docker-username<span>=</span>DUMMY_USERNAME --docker-password<span>=</span>DUMMY_DOCKER_PASSWORD <span>\
</span></span></span><span><span><span></span>        --docker-email<span>=</span>DUMMY_DOCKER_EMAIL
</span></span></code></pre></div></li><li><p>Verify it has been created.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secrets myregistrykey
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME             TYPE                              DATA    AGE
myregistrykey &#160;  kubernetes.io/.dockerconfigjson &#160; 1 &#160; &#160; &#160; 1d
</code></pre></li></ul><h3 id="add-image-pull-secret-to-service-account">Add image pull secret to service account</h3><p>Next, modify the default service account for the namespace to use this Secret as an imagePullSecret.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch serviceaccount default -p <span>'{"imagePullSecrets": [{"name": "myregistrykey"}]}'</span>
</span></span></code></pre></div><p>You can achieve the same outcome by editing the object manually:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit serviceaccount/default
</span></span></code></pre></div><p>The output of the <code>sa.yaml</code> file is similar to this:</p><p>Your selected text editor will open with a configuration looking something like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2021-07-07T22:02:39Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"243024"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>052fb0f4-3d50-11e5-b066-42010af0d7b6<span>
</span></span></span></code></pre></div><p>Using your editor, delete the line with key <code>resourceVersion</code>, add lines for
<code>imagePullSecrets:</code> and save it. Leave the <code>uid</code> value set the same as you found it.</p><p>After you made those changes, the edited ServiceAccount looks something like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2021-07-07T22:02:39Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>052fb0f4-3d50-11e5-b066-42010af0d7b6<span>
</span></span></span><span><span><span></span><span>imagePullSecrets</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>myregistrykey<span>
</span></span></span></code></pre></div><h3 id="verify-that-imagepullsecrets-are-set-for-new-pods">Verify that imagePullSecrets are set for new Pods</h3><p>Now, when a new Pod is created in the current namespace and using the default
ServiceAccount, the new Pod has its <code>spec.imagePullSecrets</code> field set automatically:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run nginx --image<span>=</span>&lt;registry name&gt;/nginx --restart<span>=</span>Never
</span></span><span><span>kubectl get pod nginx -o<span>=</span><span>jsonpath</span><span>=</span><span>'{.spec.imagePullSecrets[0].name}{"\n"}'</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>myregistrykey
</code></pre><h2 id="serviceaccount-token-volume-projection">ServiceAccount token volume projection</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>To enable and use token request projection, you must specify each of the following
command line arguments to <code>kube-apiserver</code>:</p><dl><dt><code>--service-account-issuer</code></dt><dd>defines the Identifier of the service account token issuer. You can specify the
<code>--service-account-issuer</code> argument multiple times, this can be useful to enable
a non-disruptive change of the issuer. When this flag is specified multiple times,
the first is used to generate tokens and all are used to determine which issuers
are accepted. You must be running Kubernetes v1.22 or later to be able to specify
<code>--service-account-issuer</code> multiple times.</dd><dt><code>--service-account-key-file</code></dt><dd>specifies the path to a file containing PEM-encoded X.509 private or public keys
(RSA or ECDSA), used to verify ServiceAccount tokens. The specified file can contain
multiple keys, and the flag can be specified multiple times with different files.
If specified multiple times, tokens signed by any of the specified keys are considered
valid by the Kubernetes API server.</dd><dt><code>--service-account-signing-key-file</code></dt><dd>specifies the path to a file that contains the current private key of the service
account token issuer. The issuer signs issued ID tokens with this private key.</dd><dt><code>--api-audiences</code> (can be omitted)</dt><dd>defines audiences for ServiceAccount tokens. The service account token authenticator
validates that tokens used against the API are bound to at least one of these audiences.
If <code>api-audiences</code> is specified multiple times, tokens for any of the specified audiences
are considered valid by the Kubernetes API server. If you specify the <code>--service-account-issuer</code>
command line argument but you don't set <code>--api-audiences</code>, the control plane defaults to
a single element audience list that contains only the issuer URL.</dd></dl></div><p>The kubelet can also project a ServiceAccount token into a Pod. You can
specify desired properties of the token, such as the audience and the validity
duration. These properties are <em>not</em> configurable on the default ServiceAccount
token. The token will also become invalid against the API when either the Pod
or the ServiceAccount is deleted.</p><p>You can configure this behavior for the <code>spec</code> of a Pod using a
<a href="/docs/concepts/storage/volumes/#projected">projected volume</a> type called
<code>ServiceAccountToken</code>.</p><p>The token from this projected volume is a <a class="glossary-tooltip" title="A means of representing claims to be transferred between two parties." href="https://www.rfc-editor.org/rfc/rfc7519" target="_blank">JSON Web Token</a> (JWT).
The JSON payload of this token follows a well defined schema - an example payload for a pod bound token:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span>"aud": </span>[<span>  </span><span># matches the requested audiences, or the API server's default audiences when none are explicitly requested</span><span>
</span></span></span><span><span><span>    </span><span>"https://kubernetes.default.svc"</span><span>
</span></span></span><span><span><span>  </span>],<span>
</span></span></span><span><span><span>  </span><span>"exp": </span><span>1731613413</span>,<span>
</span></span></span><span><span><span>  </span><span>"iat": </span><span>1700077413</span>,<span>
</span></span></span><span><span><span>  </span><span>"iss": </span><span>"https://kubernetes.default.svc"</span>,<span>  </span><span># matches the first value passed to the --service-account-issuer flag</span><span>
</span></span></span><span><span><span>  </span><span>"jti": </span><span>"ea28ed49-2e11-4280-9ec5-bc3d1d84661a"</span>,<span> 
</span></span></span><span><span><span>  </span><span>"kubernetes.io": </span>{<span>
</span></span></span><span><span><span>    </span><span>"namespace": </span><span>"kube-system"</span>,<span>
</span></span></span><span><span><span>    </span><span>"node": </span>{<span>
</span></span></span><span><span><span>      </span><span>"name": </span><span>"127.0.0.1"</span>,<span>
</span></span></span><span><span><span>      </span><span>"uid": </span><span>"58456cb0-dd00-45ed-b797-5578fdceaced"</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span><span>"pod": </span>{<span>
</span></span></span><span><span><span>      </span><span>"name": </span><span>"coredns-69cbfb9798-jv9gn"</span>,<span>
</span></span></span><span><span><span>      </span><span>"uid": </span><span>"778a530c-b3f4-47c0-9cd5-ab018fb64f33"</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span><span>"serviceaccount": </span>{<span>
</span></span></span><span><span><span>      </span><span>"name": </span><span>"coredns"</span>,<span>
</span></span></span><span><span><span>      </span><span>"uid": </span><span>"a087d5a0-e1dd-43ec-93ac-f13d89cd13af"</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span><span>"warnafter": </span><span>1700081020</span><span>
</span></span></span><span><span><span>  </span>},<span>
</span></span></span><span><span><span>  </span><span>"nbf": </span><span>1700077413</span>,<span>
</span></span></span><span><span><span>  </span><span>"sub": </span><span>"system:serviceaccount:kube-system:coredns"</span><span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><h3 id="launch-a-pod-using-service-account-token-projection">Launch a Pod using service account token projection</h3><p>To provide a Pod with a token with an audience of <code>vault</code> and a validity duration
of two hours, you could define a Pod manifest that is similar to:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-projected-svc-token.yaml"><code>pods/pod-projected-svc-token.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-projected-svc-token.yaml to clipboard"></div><div class="includecode" id="pods-pod-projected-svc-token-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/var/run/secrets/tokens<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>vault-token<span>
</span></span></span><span><span><span>  </span><span>serviceAccountName</span>:<span> </span>build-robot<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>vault-token<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>serviceAccountToken</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>vault-token<span>
</span></span></span><span><span><span>          </span><span>expirationSeconds</span>:<span> </span><span>7200</span><span>
</span></span></span><span><span><span>          </span><span>audience</span>:<span> </span>vault<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/pods/pod-projected-svc-token.yaml
</span></span></code></pre></div><p>The kubelet will: request and store the token on behalf of the Pod; make
the token available to the Pod at a configurable file path; and refresh
the token as it approaches expiration. The kubelet proactively requests rotation
for the token if it is older than 80% of its total time-to-live (TTL),
or if the token is older than 24 hours.</p><p>The application is responsible for reloading the token when it rotates. It's
often good enough for the application to load the token on a schedule
(for example: once every 5 minutes), without tracking the actual expiry time.</p><h3 id="service-account-issuer-discovery">Service account issuer discovery</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>If you have enabled <a href="#serviceaccount-token-volume-projection">token projection</a>
for ServiceAccounts in your cluster, then you can also make use of the discovery
feature. Kubernetes provides a way for clients to federate as an <em>identity provider</em>,
so that one or more external systems can act as a <em>relying party</em>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The issuer URL must comply with the
<a href="https://openid.net/specs/openid-connect-discovery-1_0.html">OIDC Discovery Spec</a>. In
practice, this means it must use the <code>https</code> scheme, and should serve an OpenID
provider configuration at <code>{service-account-issuer}/.well-known/openid-configuration</code>.</p><p>If the URL does not comply, ServiceAccount issuer discovery endpoints are not
registered or accessible.</p></div><p>When enabled, the Kubernetes API server publishes an OpenID Provider
Configuration document via HTTP. The configuration document is published at
<code>/.well-known/openid-configuration</code>.
The OpenID Provider Configuration is sometimes referred to as the <em>discovery document</em>.
The Kubernetes API server publishes the related
JSON Web Key Set (JWKS), also via HTTP, at <code>/openid/v1/jwks</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The responses served at <code>/.well-known/openid-configuration</code> and
<code>/openid/v1/jwks</code> are designed to be OIDC compatible, but not strictly OIDC
compliant. Those documents contain only the parameters necessary to perform
validation of Kubernetes service account tokens.</div><p>Clusters that use <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." href="/docs/reference/access-authn-authz/rbac/" target="_blank">RBAC</a> include a
default ClusterRole called <code>system:service-account-issuer-discovery</code>.
A default ClusterRoleBinding assigns this role to the <code>system:serviceaccounts</code> group,
which all ServiceAccounts implicitly belong to.
This allows pods running on the cluster to access the service account discovery document
via their mounted service account token. Administrators may, additionally, choose to
bind the role to <code>system:authenticated</code> or <code>system:unauthenticated</code> depending on their
security requirements and which external systems they intend to federate with.</p><p>The JWKS response contains public keys that a relying party can use to validate
the Kubernetes service account tokens. Relying parties first query for the
OpenID Provider Configuration, and use the <code>jwks_uri</code> field in the response to
find the JWKS.</p><p>In many cases, Kubernetes API servers are not available on the public internet,
but public endpoints that serve cached responses from the API server can be made
available by users or by service providers. In these cases, it is possible to
override the <code>jwks_uri</code> in the OpenID Provider Configuration so that it points
to the public endpoint, rather than the API server's address, by passing the
<code>--service-account-jwks-uri</code> flag to the API server. Like the issuer URL, the
JWKS URI is required to use the <code>https</code> scheme.</p><h2 id="what-s-next">What's next</h2><p>See also:</p><ul><li>Read the <a href="/docs/reference/access-authn-authz/service-accounts-admin/">Cluster Admin Guide to Service Accounts</a></li><li>Read about <a href="/docs/reference/access-authn-authz/authorization/">Authorization in Kubernetes</a></li><li>Read about <a href="/docs/concepts/configuration/secret/">Secrets</a><ul><li>or learn to <a href="/docs/tasks/inject-data-application/distribute-credentials-secure/">distribute credentials securely using Secrets</a></li><li>but also bear in mind that using Secrets for authenticating as a ServiceAccount
is deprecated. The recommended alternative is
<a href="#serviceaccount-token-volume-projection">ServiceAccount token volume projection</a>.</li></ul></li><li>Read about <a href="/docs/tasks/configure-pod-container/configure-projected-volume-storage/">projected volumes</a>.</li><li>For background on OIDC discovery, read the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/1393-oidc-discovery">ServiceAccount signing key retrieval</a>
Kubernetes Enhancement Proposal</li><li>Read the <a href="https://openid.net/specs/openid-connect-discovery-1_0.html">OIDC Discovery Spec</a></li></ul></div></div><div><div class="td-content"><h1>Pull an Image from a Private Registry</h1><p>This page shows how to create a Pod that uses a
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a> to pull an image
from a private container image registry or repository. There are many private
registries in use. This task uses <a href="https://www.docker.com/products/docker-hub">Docker Hub</a>
as an example registry.</p><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>To do this exercise, you need the <code>docker</code> command line tool, and a
<a href="https://docs.docker.com/docker-id/">Docker ID</a> for which you know the password.</p></li><li><p>If you are using a different private container registry, you need the command
line tool for that registry and any login information for the registry.</p></li></ul><h2 id="log-in-to-docker-hub">Log in to Docker Hub</h2><p>On your laptop, you must authenticate with a registry in order to pull a private image.</p><p>Use the <code>docker</code> tool to log in to Docker Hub. See the <em>log in</em> section of
<a href="https://docs.docker.com/docker-id/#log-in">Docker ID accounts</a> for more information.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>docker login
</span></span></code></pre></div><p>When prompted, enter your Docker ID, and then the credential you want to use (access token,
or the password for your Docker ID).</p><p>The login process creates or updates a <code>config.json</code> file that holds an authorization token.
Review <a href="/docs/concepts/containers/images/#config-json">how Kubernetes interprets this file</a>.</p><p>View the <code>config.json</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat ~/.docker/config.json
</span></span></code></pre></div><p>The output contains a section similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>    <span>"auths"</span>: {
</span></span><span><span>        <span>"https://index.docker.io/v1/"</span>: {
</span></span><span><span>            <span>"auth"</span>: <span>"c3R...zE2"</span>
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you use a Docker credentials store, you won't see that <code>auth</code> entry but a <code>credsStore</code> entry with the name of the store as value.
In that case, you can create a secret directly.
See <a href="#create-a-secret-by-providing-credentials-on-the-command-line">Create a Secret by providing credentials on the command line</a>.</div><h2 id="registry-secret-existing-credentials">Create a Secret based on existing credentials</h2><p>A Kubernetes cluster uses the Secret of <code>kubernetes.io/dockerconfigjson</code> type to authenticate with
a container registry to pull a private image.</p><p>If you already ran <code>docker login</code>, you can copy
that credential into Kubernetes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic regcred <span>\
</span></span></span><span><span><span></span>    --from-file<span>=</span>.dockerconfigjson<span>=</span>&lt;path/to/.docker/config.json&gt; <span>\
</span></span></span><span><span><span></span>    --type<span>=</span>kubernetes.io/dockerconfigjson
</span></span></code></pre></div><p>If you need more control (for example, to set a namespace or a label on the new
secret) then you can customise the Secret before storing it.
Be sure to:</p><ul><li>set the name of the data item to <code>.dockerconfigjson</code></li><li>base64 encode the Docker configuration file and then paste that string, unbroken
as the value for field <code>data[".dockerconfigjson"]</code></li><li>set <code>type</code> to <code>kubernetes.io/dockerconfigjson</code></li></ul><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myregistrykey<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>awesomeapps<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>.dockerconfigjson</span>:<span> </span>UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg==<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/dockerconfigjson<span>
</span></span></span></code></pre></div><p>If you get the error message <code>error: no objects passed to create</code>, it may mean the base64 encoded string is invalid.
If you get an error message like <code>Secret "myregistrykey" is invalid: data[.dockerconfigjson]: invalid value ...</code>, it means
the base64 encoded string in the data was successfully decoded, but could not be parsed as a <code>.docker/config.json</code> file.</p><h2 id="create-a-secret-by-providing-credentials-on-the-command-line">Create a Secret by providing credentials on the command line</h2><p>Create this Secret, naming it <code>regcred</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret docker-registry regcred --docker-server<span>=</span>&lt;your-registry-server&gt; --docker-username<span>=</span>&lt;your-name&gt; --docker-password<span>=</span>&lt;your-pword&gt; --docker-email<span>=</span>&lt;your-email&gt;
</span></span></code></pre></div><p>where:</p><ul><li><code>&lt;your-registry-server&gt;</code> is your Private Docker Registry FQDN.
Use <code>https://index.docker.io/v1/</code> for DockerHub.</li><li><code>&lt;your-name&gt;</code> is your Docker username.</li><li><code>&lt;your-pword&gt;</code> is your Docker password.</li><li><code>&lt;your-email&gt;</code> is your Docker email.</li></ul><p>You have successfully set your Docker credentials in the cluster as a Secret called <code>regcred</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Typing secrets on the command line may store them in your shell history unprotected, and
those secrets might also be visible to other users on your PC during the time that
<code>kubectl</code> is running.</div><h2 id="inspecting-the-secret-regcred">Inspecting the Secret <code>regcred</code></h2><p>To understand the contents of the <code>regcred</code> Secret you created, start by viewing the Secret in YAML format:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret regcred --output<span>=</span>yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>regcred<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>.dockerconfigjson</span>:<span> </span>eyJodHRwczovL2luZGV4L ... J0QUl6RTIifX0=<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/dockerconfigjson<span>
</span></span></span></code></pre></div><p>The value of the <code>.dockerconfigjson</code> field is a base64 representation of your Docker credentials.</p><p>To understand what is in the <code>.dockerconfigjson</code> field, convert the secret data to a
readable format:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret regcred --output<span>=</span><span>"jsonpath={.data.\.dockerconfigjson}"</span> | base64 --decode
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{<span>"auths"</span>:{<span>"your.private.registry.example.com"</span>:{<span>"username"</span>:<span>"janedoe"</span>,<span>"password"</span>:<span>"xxxxxxxxxxx"</span>,<span>"email"</span>:<span>"jdoe@example.com"</span>,<span>"auth"</span>:<span>"c3R...zE2"</span>}}}
</span></span></code></pre></div><p>To understand what is in the <code>auth</code> field, convert the base64-encoded data to a readable format:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> <span>"c3R...zE2"</span> | base64 --decode
</span></span></code></pre></div><p>The output, username and password concatenated with a <code>:</code>, is similar to this:</p><pre tabindex="0"><code class="language-none">janedoe:xxxxxxxxxxx
</code></pre><p>Notice that the Secret data contains the authorization token similar to your local <code>~/.docker/config.json</code> file.</p><p>You have successfully set your Docker credentials as a Secret called <code>regcred</code> in the cluster.</p><h2 id="create-a-pod-that-uses-your-secret">Create a Pod that uses your Secret</h2><p>Here is a manifest for an example Pod that needs access to your Docker credentials in <code>regcred</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/private-reg-pod.yaml"><code>pods/private-reg-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/private-reg-pod.yaml to clipboard"></div><div class="includecode" id="pods-private-reg-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>private-reg<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>private-reg-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>&lt;your-private-image&gt;<span>
</span></span></span><span><span><span>  </span><span>imagePullSecrets</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>regcred<span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>Download the above file onto your computer:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -L -o my-private-reg-pod.yaml https://k8s.io/examples/pods/private-reg-pod.yaml
</span></span></code></pre></div><p>In file <code>my-private-reg-pod.yaml</code>, replace <code>&lt;your-private-image&gt;</code> with the path to an image in a private registry such as:</p><pre tabindex="0"><code class="language-none">your.private.registry.example.com/janedoe/jdoe-private:v1
</code></pre><p>To pull the image from the private registry, Kubernetes needs credentials.
The <code>imagePullSecrets</code> field in the configuration file specifies that
Kubernetes should get the credentials from a Secret named <code>regcred</code>.</p><p>Create a Pod that uses your Secret, and verify that the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-private-reg-pod.yaml
</span></span><span><span>kubectl get pod private-reg
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To use image pull secrets for a Pod (or a Deployment, or other object that
has a pod template that you are using), you need to make sure that the appropriate
Secret does exist in the right namespace. The namespace to use is the same
namespace where you defined the Pod.</div><p>Also, in case the Pod fails to start with the status <code>ImagePullBackOff</code>, view the Pod events:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod private-reg
</span></span></code></pre></div><p>If you then see an event with the reason set to <code>FailedToRetrieveImagePullSecret</code>,
Kubernetes can't find a Secret with name (<code>regcred</code>, in this example).</p><p>Make sure that the Secret you have specified exists, and that its name is spelled properly.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>Events:
</span></span><span><span>  ...  Reason                           ...  Message
</span></span><span><span>       ------                                -------
</span></span><span><span>  ...  FailedToRetrieveImagePullSecret  ...  Unable to retrieve some image pull secrets <span>(</span>&lt;regcred&gt;<span>)</span>; attempting to pull the image may not succeed.
</span></span></code></pre></div><h2 id="using-images-from-multiple-registries">Using images from multiple registries</h2><p>A pod can have multiple containers, each container image can be from a different registry.
You can use multiple <code>imagePullSecrets</code> with one pod, and each can contain multiple credentials.</p><p>The image pull will be attempted using each credential that matches the registry.
If no credentials match the registry, the image pull will be attempted without authorization or using custom runtime specific configuration.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/configuration/secret/">Secrets</a><ul><li>or read the API reference for
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">Secret</a></li></ul></li><li>Learn more about <a href="/docs/concepts/containers/images/#using-a-private-registry">using a private registry</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">adding image pull secrets to a service account</a>.</li><li>See <a href="/docs/reference/generated/kubectl/kubectl-commands/#-em-secret-docker-registry-em-">kubectl create secret docker-registry</a>.</li><li>See the <code>imagePullSecrets</code> field within the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#containers">container definitions</a> of a Pod</li></ul></div></div><div><div class="td-content"><h1>Configure Liveness, Readiness and Startup Probes</h1><p>This page shows how to configure liveness, readiness and startup probes for containers.</p><p>For more information about probes, see <a href="/docs/concepts/configuration/liveness-readiness-startup-probes/">Liveness, Readiness and Startup Probes</a></p><p>The <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> uses
liveness probes to know when to restart a container. For example, liveness
probes could catch a deadlock, where an application is running, but unable to
make progress. Restarting a container in such a state can help to make the
application more available despite bugs.</p><p>A common pattern for liveness probes is to use the same low-cost HTTP endpoint
as for readiness probes, but with a higher failureThreshold. This ensures that the pod
is observed as not-ready for some period of time before it is hard killed.</p><p>The kubelet uses readiness probes to know when a container is ready to start
accepting traffic. One use of this signal is to control which Pods are used as
backends for Services. A Pod is considered ready when its <code>Ready</code> <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions">condition</a>
is true. When a Pod is not ready, it is removed from Service load balancers.
A Pod's <code>Ready</code> condition is false when its Node's <code>Ready</code> condition is not true,
when one of the Pod's <code>readinessGates</code> is false, or when at least one of its containers
is not ready.</p><p>The kubelet uses startup probes to know when a container application has started.
If such a probe is configured, liveness and readiness probes do not start until
it succeeds, making sure those probes don't interfere with the application startup.
This can be used to adopt liveness checks on slow starting containers, avoiding them
getting killed by the kubelet before they are up and running.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Liveness probes can be a powerful way to recover from application failures, but
they should be used with caution. Liveness probes must be configured carefully
to ensure that they truly indicate unrecoverable application failure, for example a deadlock.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Incorrect implementation of liveness probes can lead to cascading failures. This results in
restarting of container under high load; failed client requests as your application became less
scalable; and increased workload on remaining pods due to some failed pods.
Understand the difference between readiness and liveness probes and when to apply them for your app.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="define-a-liveness-command">Define a liveness command</h2><p>Many applications running for long periods of time eventually transition to
broken states, and cannot recover except by being restarted. Kubernetes provides
liveness probes to detect and remedy such situations.</p><p>In this exercise, you create a Pod that runs a container based on the
<code>registry.k8s.io/busybox:1.27.2</code> image. Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/probe/exec-liveness.yaml"><code>pods/probe/exec-liveness.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/probe/exec-liveness.yaml to clipboard"></div><div class="includecode" id="pods-probe-exec-liveness-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>test</span>:<span> </span>liveness<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>liveness-exec<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>liveness<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- /bin/sh<span>
</span></span></span><span><span><span>    </span>- -c<span>
</span></span></span><span><span><span>    </span>- touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600<span>
</span></span></span><span><span><span>    </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>exec</span>:<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- cat<span>
</span></span></span><span><span><span>        </span>- /tmp/healthy<span>
</span></span></span><span><span><span>      </span><span>initialDelaySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>5</span><span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Pod has a single <code>Container</code>.
The <code>periodSeconds</code> field specifies that the kubelet should perform a liveness
probe every 5 seconds. The <code>initialDelaySeconds</code> field tells the kubelet that it
should wait 5 seconds before performing the first probe. To perform a probe, the
kubelet executes the command <code>cat /tmp/healthy</code> in the target container. If the
command succeeds, it returns 0, and the kubelet considers the container to be alive and
healthy. If the command returns a non-zero value, the kubelet kills the container
and restarts it.</p><p>When the container starts, it executes this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/bin/sh -c <span>"touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600"</span>
</span></span></code></pre></div><p>For the first 30 seconds of the container's life, there is a <code>/tmp/healthy</code> file.
So during the first 30 seconds, the command <code>cat /tmp/healthy</code> returns a success
code. After 30 seconds, <code>cat /tmp/healthy</code> returns a failure code.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml
</span></span></code></pre></div><p>Within 30 seconds, view the Pod events:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod liveness-exec
</span></span></code></pre></div><p>The output indicates that no liveness probes have failed yet:</p><pre tabindex="0"><code class="language-none">Type    Reason     Age   From               Message
----    ------     ----  ----               -------
Normal  Scheduled  11s   default-scheduler  Successfully assigned default/liveness-exec to node01
Normal  Pulling    9s    kubelet, node01    Pulling image "registry.k8s.io/busybox:1.27.2"
Normal  Pulled     7s    kubelet, node01    Successfully pulled image "registry.k8s.io/busybox:1.27.2"
Normal  Created    7s    kubelet, node01    Created container liveness
Normal  Started    7s    kubelet, node01    Started container liveness
</code></pre><p>After 35 seconds, view the Pod events again:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod liveness-exec
</span></span></code></pre></div><p>At the bottom of the output, there are messages indicating that the liveness
probes have failed, and the failed containers have been killed and recreated.</p><pre tabindex="0"><code class="language-none">Type     Reason     Age                From               Message
----     ------     ----               ----               -------
Normal   Scheduled  57s                default-scheduler  Successfully assigned default/liveness-exec to node01
Normal   Pulling    55s                kubelet, node01    Pulling image "registry.k8s.io/busybox:1.27.2"
Normal   Pulled     53s                kubelet, node01    Successfully pulled image "registry.k8s.io/busybox:1.27.2"
Normal   Created    53s                kubelet, node01    Created container liveness
Normal   Started    53s                kubelet, node01    Started container liveness
Warning  Unhealthy  10s (x3 over 20s)  kubelet, node01    Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
Normal   Killing    10s                kubelet, node01    Container liveness failed liveness probe, will be restarted
</code></pre><p>Wait another 30 seconds, and verify that the container has been restarted:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod liveness-exec
</span></span></code></pre></div><p>The output shows that <code>RESTARTS</code> has been incremented. Note that the <code>RESTARTS</code> counter
increments as soon as a failed container comes back to the running state:</p><pre tabindex="0"><code class="language-none">NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
</code></pre><h2 id="define-a-liveness-http-request">Define a liveness HTTP request</h2><p>Another kind of liveness probe uses an HTTP GET request. Here is the configuration
file for a Pod that runs a container based on the <code>registry.k8s.io/e2e-test-images/agnhost</code> image.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/probe/http-liveness.yaml"><code>pods/probe/http-liveness.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/probe/http-liveness.yaml to clipboard"></div><div class="includecode" id="pods-probe-http-liveness-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>test</span>:<span> </span>liveness<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>liveness-http<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>liveness<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/e2e-test-images/agnhost:2.40<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- liveness<span>
</span></span></span><span><span><span>    </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>        </span><span>httpHeaders</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>Custom-Header<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span>Awesome<span>
</span></span></span><span><span><span>      </span><span>initialDelaySeconds</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>3</span><span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Pod has a single container.
The <code>periodSeconds</code> field specifies that the kubelet should perform a liveness
probe every 3 seconds. The <code>initialDelaySeconds</code> field tells the kubelet that it
should wait 3 seconds before performing the first probe. To perform a probe, the
kubelet sends an HTTP GET request to the server that is running in the container
and listening on port 8080. If the handler for the server's <code>/healthz</code> path
returns a success code, the kubelet considers the container to be alive and
healthy. If the handler returns a failure code, the kubelet kills the container
and restarts it.</p><p>Any code greater than or equal to 200 and less than 400 indicates success. Any
other code indicates failure.</p><p>You can see the source code for the server in
<a href="https://github.com/kubernetes/kubernetes/blob/master/test/images/agnhost/liveness/server.go">server.go</a>.</p><p>For the first 10 seconds that the container is alive, the <code>/healthz</code> handler
returns a status of 200. After that, the handler returns a status of 500.</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span>http.<span>HandleFunc</span>(<span>"/healthz"</span>, <span>func</span>(w http.ResponseWriter, r <span>*</span>http.Request) {
</span></span><span><span>    duration <span>:=</span> time.<span>Now</span>().<span>Sub</span>(started)
</span></span><span><span>    <span>if</span> duration.<span>Seconds</span>() &gt; <span>10</span> {
</span></span><span><span>        w.<span>WriteHeader</span>(<span>500</span>)
</span></span><span><span>        w.<span>Write</span>([]<span>byte</span>(fmt.<span>Sprintf</span>(<span>"error: %v"</span>, duration.<span>Seconds</span>())))
</span></span><span><span>    } <span>else</span> {
</span></span><span><span>        w.<span>WriteHeader</span>(<span>200</span>)
</span></span><span><span>        w.<span>Write</span>([]<span>byte</span>(<span>"ok"</span>))
</span></span><span><span>    }
</span></span><span><span>})
</span></span></code></pre></div><p>The kubelet starts performing health checks 3 seconds after the container starts.
So the first couple of health checks will succeed. But after 10 seconds, the health
checks will fail, and the kubelet will kill and restart the container.</p><p>To try the HTTP liveness check, create a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml
</span></span></code></pre></div><p>After 10 seconds, view Pod events to verify that liveness probes have failed and
the container has been restarted:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod liveness-http
</span></span></code></pre></div><p>In releases after v1.13, local HTTP proxy environment variable settings do not
affect the HTTP liveness probe.</p><h2 id="define-a-tcp-liveness-probe">Define a TCP liveness probe</h2><p>A third type of liveness probe uses a TCP socket. With this configuration, the
kubelet will attempt to open a socket to your container on the specified port.
If it can establish a connection, the container is considered healthy, if it
can't it is considered a failure.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/probe/tcp-liveness-readiness.yaml"><code>pods/probe/tcp-liveness-readiness.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/probe/tcp-liveness-readiness.yaml to clipboard"></div><div class="includecode" id="pods-probe-tcp-liveness-readiness-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>goproxy<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>goproxy<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>goproxy<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/goproxy:0.1<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>    </span><span>readinessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>tcpSocket</span>:<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>      </span><span>initialDelaySeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>    </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>tcpSocket</span>:<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>      </span><span>initialDelaySeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span></code></pre></div></div></div><p>As you can see, configuration for a TCP check is quite similar to an HTTP check.
This example uses both readiness and liveness probes. The kubelet will run the
first liveness probe 15 seconds after the container starts. This will attempt to
connect to the <code>goproxy</code> container on port 8080. If the liveness probe fails,
the container will be restarted. The kubelet will continue to run this check
every 10 seconds.</p><p>In addition to the liveness probe, this configuration includes a readiness
probe. The kubelet will run the first readiness probe 15 seconds after the
container starts. Similar to the liveness probe, this will attempt to connect to
the <code>goproxy</code> container on port 8080. If the probe succeeds, the Pod will be
marked as ready and will receive traffic from services. If the readiness probe
fails, the pod will be marked unready and will not receive traffic from any
services.</p><p>To try the TCP liveness check, create a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/probe/tcp-liveness-readiness.yaml
</span></span></code></pre></div><p>After 15 seconds, view Pod events to verify that liveness probes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod goproxy
</span></span></code></pre></div><h2 id="define-a-grpc-liveness-probe">Define a gRPC liveness probe</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>If your application implements the
<a href="https://github.com/grpc/grpc/blob/master/doc/health-checking.md">gRPC Health Checking Protocol</a>,
this example shows how to configure Kubernetes to use it for application liveness checks.
Similarly you can configure readiness and startup probes.</p><p>Here is an example manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/probe/grpc-liveness.yaml"><code>pods/probe/grpc-liveness.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/probe/grpc-liveness.yaml to clipboard"></div><div class="includecode" id="pods-probe-grpc-liveness-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>etcd-with-grpc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>etcd<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/etcd:3.5.1-0<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span> </span><span>"/usr/local/bin/etcd"</span>,<span> </span><span>"--data-dir"</span>,<span>  </span><span>"/var/lib/etcd"</span>,<span> </span><span>"--listen-client-urls"</span>,<span> </span><span>"http://0.0.0.0:2379"</span>,<span> </span><span>"--advertise-client-urls"</span>,<span> </span><span>"http://127.0.0.1:2379"</span>,<span> </span><span>"--log-level"</span>,<span> </span><span>"debug"</span>]<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>2379</span><span>
</span></span></span><span><span><span>    </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>grpc</span>:<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span><span>2379</span><span>
</span></span></span><span><span><span>      </span><span>initialDelaySeconds</span>:<span> </span><span>10</span><span>
</span></span></span></code></pre></div></div></div><p>To use a gRPC probe, <code>port</code> must be configured. If you want to distinguish probes of different types
and probes for different features you can use the <code>service</code> field.
You can set <code>service</code> to the value <code>liveness</code> and make your gRPC Health Checking endpoint
respond to this request differently than when you set <code>service</code> set to <code>readiness</code>.
This lets you use the same endpoint for different kinds of container health check
rather than listening on two different ports.
If you want to specify your own custom service name and also specify a probe type,
the Kubernetes project recommends that you use a name that concatenates
those. For example: <code>myservice-liveness</code> (using <code>-</code> as a separator).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Unlike HTTP or TCP probes, you cannot specify the health check port by name, and you
cannot configure a custom hostname.</div><p>Configuration problems (for example: incorrect port or service, unimplemented health checking protocol)
are considered a probe failure, similar to HTTP and TCP probes.</p><p>To try the gRPC liveness check, create a Pod using the command below.
In the example below, the etcd pod is configured to use gRPC liveness probe.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/probe/grpc-liveness.yaml
</span></span></code></pre></div><p>After 15 seconds, view Pod events to verify that the liveness check has not failed:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod etcd-with-grpc
</span></span></code></pre></div><p>When using a gRPC probe, there are some technical details to be aware of:</p><ul><li>The probes run against the pod IP address or its hostname.
Be sure to configure your gRPC endpoint to listen on the Pod's IP address.</li><li>The probes do not support any authentication parameters (like <code>-tls</code>).</li><li>There are no error codes for built-in probes. All errors are considered as probe failures.</li><li>If <code>ExecProbeTimeout</code> feature gate is set to <code>false</code>, grpc-health-probe does <strong>not</strong>
respect the <code>timeoutSeconds</code> setting (which defaults to 1s), while built-in probe would fail on timeout.</li></ul><h2 id="use-a-named-port">Use a named port</h2><p>You can use a named <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#ports"><code>port</code></a>
for HTTP and TCP probes. gRPC probes do not support named ports.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>ports</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>liveness-port<span>
</span></span></span><span><span><span>  </span><span>containerPort</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span>liveness-port<span>
</span></span></span></code></pre></div><h2 id="define-startup-probes">Protect slow starting containers with startup probes</h2><p>Sometimes, you have to deal with applications that require additional startup
time on their first initialization. In such cases, it can be tricky to set up
liveness probe parameters without compromising the fast response to deadlocks
that motivated such a probe. The solution is to set up a startup probe with the
same command, HTTP or TCP check, with a <code>failureThreshold * periodSeconds</code> long
enough to cover the worst case startup time.</p><p>So, the previous example would become:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>ports</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>liveness-port<span>
</span></span></span><span><span><span>  </span><span>containerPort</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span>liveness-port<span>
</span></span></span><span><span><span>  </span><span>failureThreshold</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>periodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>startupProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span>liveness-port<span>
</span></span></span><span><span><span>  </span><span>failureThreshold</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>  </span><span>periodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span></code></pre></div><p>Thanks to the startup probe, the application will have a maximum of 5 minutes
(30 * 10 = 300s) to finish its startup.
Once the startup probe has succeeded once, the liveness probe takes over to
provide a fast response to container deadlocks.
If the startup probe never succeeds, the container is killed after 300s and
subject to the pod's <code>restartPolicy</code>.</p><h2 id="define-readiness-probes">Define readiness probes</h2><p>Sometimes, applications are temporarily unable to serve traffic.
For example, an application might need to load large data or configuration
files during startup, or depend on external services after startup.
In such cases, you don't want to kill the application,
but you don't want to send it requests either. Kubernetes provides
readiness probes to detect and mitigate these situations. A pod with containers
reporting that they are not ready does not receive traffic through Kubernetes
Services.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Readiness probes runs on the container during its whole lifecycle.</div><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>The readiness and liveness probes do not depend on each other to succeed.
If you want to wait before executing a readiness probe, you should use
<code>initialDelaySeconds</code> or a <code>startupProbe</code>.</div><p>Readiness probes are configured similarly to liveness probes. The only difference
is that you use the <code>readinessProbe</code> field instead of the <code>livenessProbe</code> field.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>readinessProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>exec</span>:<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- cat<span>
</span></span></span><span><span><span>    </span>- /tmp/healthy<span>
</span></span></span><span><span><span>  </span><span>initialDelaySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>periodSeconds</span>:<span> </span><span>5</span><span>
</span></span></span></code></pre></div><p>Configuration for HTTP and TCP readiness probes also remains identical to
liveness probes.</p><p>Readiness and liveness probes can be used in parallel for the same container.
Using both can ensure that traffic does not reach a container that is not ready
for it, and that containers are restarted when they fail.</p><h2 id="configure-probes">Configure Probes</h2><p><a href="/docs/reference/generated/kubernetes-api/v1.34/#probe-v1-core">Probes</a>
have a number of fields that you can use to more precisely control the behavior of startup,
liveness and readiness checks:</p><ul><li><code>initialDelaySeconds</code>: Number of seconds after the container has started before startup,
liveness or readiness probes are initiated. If a startup probe is defined, liveness and
readiness probe delays do not begin until the startup probe has succeeded. In some older
Kubernetes versions, the initialDelaySeconds might be ignored if periodSeconds was set to
a value higher than initialDelaySeconds. However, in current versions, initialDelaySeconds
is always honored and the probe will not start until after this initial delay. Defaults to
0 seconds. Minimum value is 0.</li><li><code>periodSeconds</code>: How often (in seconds) to perform the probe. Default to 10 seconds.
The minimum value is 1.
While a container is not Ready, the <code>ReadinessProbe</code> may be executed at times other than
the configured <code>periodSeconds</code> interval. This is to make the Pod ready faster.</li><li><code>timeoutSeconds</code>: Number of seconds after which the probe times out.
Defaults to 1 second. Minimum value is 1.</li><li><code>successThreshold</code>: Minimum consecutive successes for the probe to be considered successful
after having failed. Defaults to 1. Must be 1 for liveness and startup Probes.
Minimum value is 1.</li><li><code>failureThreshold</code>: After a probe fails <code>failureThreshold</code> times in a row, Kubernetes
considers that the overall check has failed: the container is <em>not</em> ready/healthy/live.
Defaults to 3. Minimum value is 1.
For the case of a startup or liveness probe, if at least <code>failureThreshold</code> probes have
failed, Kubernetes treats the container as unhealthy and triggers a restart for that
specific container. The kubelet honors the setting of <code>terminationGracePeriodSeconds</code>
for that container.
For a failed readiness probe, the kubelet continues running the container that failed
checks, and also continues to run more probes; because the check failed, the kubelet
sets the <code>Ready</code> <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions">condition</a>
on the Pod to <code>false</code>.</li><li><code>terminationGracePeriodSeconds</code>: configure a grace period for the kubelet to wait between
triggering a shut down of the failed container, and then forcing the container runtime to stop
that container.
The default is to inherit the Pod-level value for <code>terminationGracePeriodSeconds</code>
(30 seconds if not specified), and the minimum value is 1.
See <a href="#probe-level-terminationgraceperiodseconds">probe-level <code>terminationGracePeriodSeconds</code></a>
for more detail.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Incorrect implementation of readiness probes may result in an ever growing number
of processes in the container, and resource starvation if this is left unchecked.</div><h3 id="http-probes">HTTP probes</h3><p><a href="/docs/reference/generated/kubernetes-api/v1.34/#httpgetaction-v1-core">HTTP probes</a>
have additional fields that can be set on <code>httpGet</code>:</p><ul><li><code>host</code>: Host name to connect to, defaults to the pod IP. You probably want to
set "Host" in <code>httpHeaders</code> instead.</li><li><code>scheme</code>: Scheme to use for connecting to the host (HTTP or HTTPS). Defaults to "HTTP".</li><li><code>path</code>: Path to access on the HTTP server. Defaults to "/".</li><li><code>httpHeaders</code>: Custom headers to set in the request. HTTP allows repeated headers.</li><li><code>port</code>: Name or number of the port to access on the container. Number must be
in the range 1 to 65535.</li></ul><p>For an HTTP probe, the kubelet sends an HTTP request to the specified port and
path to perform the check. The kubelet sends the probe to the Pod's IP address,
unless the address is overridden by the optional <code>host</code> field in <code>httpGet</code>. If
<code>scheme</code> field is set to <code>HTTPS</code>, the kubelet sends an HTTPS request skipping the
certificate verification. In most scenarios, you do not want to set the <code>host</code> field.
Here's one scenario where you would set it. Suppose the container listens on 127.0.0.1
and the Pod's <code>hostNetwork</code> field is true. Then <code>host</code>, under <code>httpGet</code>, should be set
to 127.0.0.1. If your pod relies on virtual hosts, which is probably the more common
case, you should not use <code>host</code>, but rather set the <code>Host</code> header in <code>httpHeaders</code>.</p><p>For an HTTP probe, the kubelet sends two request headers in addition to the mandatory <code>Host</code> header:</p><ul><li><code>User-Agent</code>: The default value is <code>kube-probe/1.34</code>,
where <code>1.34</code> is the version of the kubelet.</li><li><code>Accept</code>: The default value is <code>*/*</code>.</li></ul><p>You can override the default headers by defining <code>httpHeaders</code> for the probe.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>httpHeaders</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>Accept<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>application/json<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>startupProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>httpHeaders</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>User-Agent<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>MyUserAgent<span>
</span></span></span></code></pre></div><p>You can also remove these two headers by defining them with an empty value.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>httpHeaders</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>Accept<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>startupProbe</span>:<span>
</span></span></span><span><span><span>  </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>    </span><span>httpHeaders</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>User-Agent<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span><span>""</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When the kubelet probes a Pod using HTTP, it only follows redirects if the redirect
is to the same host. If the kubelet receives 11 or more redirects during probing, the probe is considered successful
and a related Event is created:</p><pre tabindex="0"><code class="language-none">Events:
  Type     Reason        Age                     From               Message
  ----     ------        ----                    ----               -------
  Normal   Scheduled     29m                     default-scheduler  Successfully assigned default/httpbin-7b8bc9cb85-bjzwn to daocloud
  Normal   Pulling       29m                     kubelet            Pulling image "docker.io/kennethreitz/httpbin"
  Normal   Pulled        24m                     kubelet            Successfully pulled image "docker.io/kennethreitz/httpbin" in 5m12.402735213s
  Normal   Created       24m                     kubelet            Created container httpbin
  Normal   Started       24m                     kubelet            Started container httpbin
 Warning  ProbeWarning  4m11s (x1197 over 24m)  kubelet            Readiness probe warning: Probe terminated redirects
</code></pre><p>If the kubelet receives a redirect where the hostname is different from the request, the outcome of the probe is treated as successful and kubelet creates an event to report the redirect failure.</p></div><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>When processing an <strong>httpGet</strong> probe, the kubelet stops reading the response body after 10KiB.
The probe's success is determined solely by the response status code, which is found in the response headers.</p><p>If you probe an endpoint that returns a response body larger than <strong>10KiB</strong>,
the kubelet will still mark the probe as successful based on the status code,
but it will close the connection after reaching the 10KiB limit.
This abrupt closure can cause <strong>connection reset by peer</strong> or <strong>broken pipe errors</strong> to appear in your application's logs,
which can be difficult to distinguish from legitimate network issues.</p><p>For reliable <code>httpGet</code> probes, it is strongly recommended to use dedicated health check endpoints
that return a minimal response body. If you must use an existing endpoint with a large payload,
consider using an <code>exec</code> probe to perform a HEAD request instead.</p></div><h3 id="tcp-probes">TCP probes</h3><p>For a TCP probe, the kubelet makes the probe connection at the node, not in the Pod, which
means that you can not use a service name in the <code>host</code> parameter since the kubelet is unable
to resolve it.</p><h3 id="probe-level-terminationgraceperiodseconds">Probe-level <code>terminationGracePeriodSeconds</code></h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code></div><p>In 1.25 and above, users can specify a probe-level <code>terminationGracePeriodSeconds</code>
as part of the probe specification. When both a pod- and probe-level
<code>terminationGracePeriodSeconds</code> are set, the kubelet will use the probe-level value.</p><p>When setting the <code>terminationGracePeriodSeconds</code>, please note the following:</p><ul><li><p>The kubelet always honors the probe-level <code>terminationGracePeriodSeconds</code> field if
it is present on a Pod.</p></li><li><p>If you have existing Pods where the <code>terminationGracePeriodSeconds</code> field is set and
you no longer wish to use per-probe termination grace periods, you must delete
those existing Pods.</p></li></ul><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>3600</span><span>  </span><span># pod-level</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>...<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>liveness-port<span>
</span></span></span><span><span><span>      </span><span>containerPort</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span>liveness-port<span>
</span></span></span><span><span><span>      </span><span>failureThreshold</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span><span><span><span>      </span><span># Override pod-level terminationGracePeriodSeconds #</span><span>
</span></span></span><span><span><span>      </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p>Probe-level <code>terminationGracePeriodSeconds</code> cannot be set for readiness probes.
It will be rejected by the API server.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</li></ul><p>You can also read the API references for:</p><ul><li><a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/">Pod</a>, and specifically:<ul><li><a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">container(s)</a></li><li><a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Probe">probe(s)</a></li></ul></li></ul></div></div><div><div class="td-content"><h1>Assign Pods to Nodes</h1><p>This page shows how to assign a Kubernetes Pod to a particular node in a
Kubernetes cluster.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="add-a-label-to-a-node">Add a label to a node</h2><ol><li><p>List the <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a> in your cluster, along with their labels:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME      STATUS    ROLES    AGE     VERSION        LABELS
</span></span><span><span>worker0   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker0
</span></span><span><span>worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker1
</span></span><span><span>worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker2
</span></span></code></pre></div></li><li><p>Choose one of your nodes, and add a label to it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label nodes &lt;your-node-name&gt; <span>disktype</span><span>=</span>ssd
</span></span></code></pre></div><p>where <code>&lt;your-node-name&gt;</code> is the name of your chosen node.</p></li><li><p>Verify that your chosen node has a <code>disktype=ssd</code> label:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME      STATUS    ROLES    AGE     VERSION        LABELS
</span></span><span><span>worker0   Ready     &lt;none&gt;   1d      v1.13.0        ...,disktype<span>=</span>ssd,kubernetes.io/hostname<span>=</span>worker0
</span></span><span><span>worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker1
</span></span><span><span>worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker2
</span></span></code></pre></div><p>In the preceding output, you can see that the <code>worker0</code> node has a
<code>disktype=ssd</code> label.</p></li></ol><h2 id="create-a-pod-that-gets-scheduled-to-your-chosen-node">Create a pod that gets scheduled to your chosen node</h2><p>This pod configuration file describes a pod that has a node selector,
<code>disktype: ssd</code>. This means that the pod will get scheduled on a node that has
a <code>disktype=ssd</code> label.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-nginx.yaml"><code>pods/pod-nginx.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-nginx.yaml to clipboard"></div><div class="includecode" id="pods-pod-nginx-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span> </span>test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>disktype</span>:<span> </span>ssd<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Use the configuration file to create a pod that will get scheduled on your
chosen node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml
</span></span></code></pre></div></li><li><p>Verify that the pod is running on your chosen node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --output<span>=</span>wide
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
</span></span><span><span>nginx    1/1       Running   <span>0</span>          13s    10.200.0.4   worker0
</span></span></code></pre></div></li></ol><h2 id="create-a-pod-that-gets-scheduled-to-specific-node">Create a pod that gets scheduled to specific node</h2><p>You can also schedule a pod to one specific node via setting <code>nodeName</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-nginx-specific-node.yaml"><code>pods/pod-nginx-specific-node.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-nginx-specific-node.yaml to clipboard"></div><div class="includecode" id="pods-pod-nginx-specific-node-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeName</span>:<span> </span>foo-node<span> </span><span># schedule pod to specific node</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div></div></div><p>Use the configuration file to create a pod that will get scheduled on <code>foo-node</code> only.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/labels/">labels and selectors</a>.</li><li>Learn more about <a href="/docs/concepts/architecture/nodes/">nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Assign Pods to Nodes using Node Affinity</h1><p>This page shows how to assign a Kubernetes Pod to a particular node using Node Affinity in a
Kubernetes cluster.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.10.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="add-a-label-to-a-node">Add a label to a node</h2><ol><li><p>List the nodes in your cluster, along with their labels:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME      STATUS    ROLES    AGE     VERSION        LABELS
</span></span><span><span>worker0   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker0
</span></span><span><span>worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker1
</span></span><span><span>worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname<span>=</span>worker2
</span></span></code></pre></div></li><li><p>Choose one of your nodes, and add a label to it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label nodes &lt;your-node-name&gt; <span>disktype</span><span>=</span>ssd
</span></span></code></pre></div><p>where <code>&lt;your-node-name&gt;</code> is the name of your chosen node.</p></li><li><p>Verify that your chosen node has a <code>disktype=ssd</code> label:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME      STATUS    ROLES    AGE     VERSION        LABELS
worker0   Ready     &lt;none&gt;   1d      v1.13.0        ...,disktype=ssd,kubernetes.io/hostname=worker0
worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker1
worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker2
</code></pre><p>In the preceding output, you can see that the <code>worker0</code> node has a
<code>disktype=ssd</code> label.</p></li></ol><h2 id="schedule-a-pod-using-required-node-affinity">Schedule a Pod using required node affinity</h2><p>This manifest describes a Pod that has a <code>requiredDuringSchedulingIgnoredDuringExecution</code> node affinity,<code>disktype: ssd</code>.
This means that the pod will get scheduled only on a node that has a <code>disktype=ssd</code> label.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-nginx-required-affinity.yaml"><code>pods/pod-nginx-required-affinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-nginx-required-affinity.yaml to clipboard"></div><div class="includecode" id="pods-pod-nginx-required-affinity-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>        </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>        </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>disktype<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- ssd            <span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Apply the manifest to create a Pod that is scheduled onto your
chosen node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/pod-nginx-required-affinity.yaml
</span></span></code></pre></div></li><li><p>Verify that the pod is running on your chosen node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --output<span>=</span>wide
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre></li></ol><h2 id="schedule-a-pod-using-preferred-node-affinity">Schedule a Pod using preferred node affinity</h2><p>This manifest describes a Pod that has a <code>preferredDuringSchedulingIgnoredDuringExecution</code> node affinity,<code>disktype: ssd</code>.
This means that the pod will prefer a node that has a <code>disktype=ssd</code> label.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-nginx-preferred-affinity.yaml"><code>pods/pod-nginx-preferred-affinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-nginx-preferred-affinity.yaml to clipboard"></div><div class="includecode" id="pods-pod-nginx-preferred-affinity-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>preferredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span>- <span>weight</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>        </span><span>preference</span>:<span>
</span></span></span><span><span><span>          </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>disktype<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- ssd          <span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Apply the manifest to create a Pod that is scheduled onto your
chosen node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/pod-nginx-preferred-affinity.yaml
</span></span></code></pre></div></li><li><p>Verify that the pod is running on your chosen node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --output<span>=</span>wide
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre></li></ol><h2 id="what-s-next">What's next</h2><p>Learn more about
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">Node Affinity</a>.</p></div></div><div><div class="td-content"><h1>Configure Pod Initialization</h1><p>This page shows how to use an Init Container to initialize a Pod before an
application Container runs.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="create-a-pod-that-has-an-init-container">Create a Pod that has an Init Container</h2><p>In this exercise you create a Pod that has one application Container and one
Init Container. The init container runs to completion before the application
container starts.</p><p>Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/init-containers.yaml"><code>pods/init-containers.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/init-containers.yaml to clipboard"></div><div class="includecode" id="pods-init-containers-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>init-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>workdir<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/usr/share/nginx/html<span>
</span></span></span><span><span><span>  </span><span># These containers are run during pod initialization</span><span>
</span></span></span><span><span><span>  </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>install<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- wget<span>
</span></span></span><span><span><span>    </span>- <span>"-O"</span><span>
</span></span></span><span><span><span>    </span>- <span>"/work-dir/index.html"</span><span>
</span></span></span><span><span><span>    </span>- http://info.cern.ch<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>workdir<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/work-dir"</span><span>
</span></span></span><span><span><span>  </span><span>dnsPolicy</span>:<span> </span>Default<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>workdir<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Pod has a Volume that the init
container and the application container share.</p><p>The init container mounts the
shared Volume at <code>/work-dir</code>, and the application container mounts the shared
Volume at <code>/usr/share/nginx/html</code>. The init container runs the following command
and then terminates:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>wget -O /work-dir/index.html http://info.cern.ch
</span></span></code></pre></div><p>Notice that the init container writes the <code>index.html</code> file in the root directory
of the nginx server.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/init-containers.yaml
</span></span></code></pre></div><p>Verify that the nginx container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod init-demo
</span></span></code></pre></div><p>The output shows that the nginx container is running:</p><pre tabindex="0"><code>NAME        READY     STATUS    RESTARTS   AGE
init-demo   1/1       Running   0          1m
</code></pre><p>Get a shell into the nginx container running in the init-demo Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it init-demo -- /bin/bash
</span></span></code></pre></div><p>In your shell, send a GET request to the nginx server:</p><pre tabindex="0"><code>root@nginx:~# apt-get update
root@nginx:~# apt-get install curl
root@nginx:~# curl localhost
</code></pre><p>The output shows that nginx is serving the web page that was written by the init container:</p><div class="highlight"><pre tabindex="0"><code class="language-html"><span><span>&lt;<span>html</span>&gt;&lt;<span>head</span>&gt;&lt;/<span>head</span>&gt;&lt;<span>body</span>&gt;&lt;<span>header</span>&gt;
</span></span><span><span>&lt;<span>title</span>&gt;http://info.cern.ch&lt;/<span>title</span>&gt;
</span></span><span><span>&lt;/<span>header</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>h1</span>&gt;http://info.cern.ch - home of the first website&lt;/<span>h1</span>&gt;
</span></span><span><span>  ...
</span></span><span><span>  &lt;<span>li</span>&gt;&lt;<span>a</span> <span>href</span><span>=</span><span>"http://info.cern.ch/hypertext/WWW/TheProject.html"</span>&gt;Browse the first website&lt;/<span>a</span>&gt;&lt;/<span>li</span>&gt;
</span></span><span><span>  ...
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about
<a href="/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">communicating between Containers running in the same Pod</a>.</li><li>Learn more about <a href="/docs/concepts/workloads/pods/init-containers/">Init Containers</a>.</li><li>Learn more about <a href="/docs/concepts/storage/volumes/">Volumes</a>.</li><li>Learn more about <a href="/docs/tasks/debug/debug-application/debug-init-containers/">Debugging Init Containers</a></li></ul></div></div><div><div class="td-content"><h1>Attach Handlers to Container Lifecycle Events</h1><p>This page shows how to attach handlers to Container lifecycle events. Kubernetes supports
the postStart and preStop events. Kubernetes sends the postStart event immediately
after a Container is started, and it sends the preStop event immediately before the
Container is terminated. A Container may specify one handler per event.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="define-poststart-and-prestop-handlers">Define postStart and preStop handlers</h2><p>In this exercise, you create a Pod that has one Container. The Container has handlers
for the postStart and preStop events.</p><p>Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml"><code>pods/lifecycle-events.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/lifecycle-events.yaml to clipboard"></div><div class="includecode" id="pods-lifecycle-events-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>lifecycle-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>lifecycle-demo-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>lifecycle</span>:<span>
</span></span></span><span><span><span>      </span><span>postStart</span>:<span>
</span></span></span><span><span><span>        </span><span>exec</span>:<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"echo Hello from the postStart handler &gt; /usr/share/message"</span>]<span>
</span></span></span><span><span><span>      </span><span>preStop</span>:<span>
</span></span></span><span><span><span>        </span><span>exec</span>:<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span>"-c"</span>,<span>"nginx -s quit; while killall -0 nginx; do sleep 1; done"</span>]<span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the postStart command writes a <code>message</code>
file to the Container's <code>/usr/share</code> directory. The preStop command shuts down
nginx gracefully. This is helpful if the Container is being terminated because of a failure.</p><p>Create the Pod:</p><pre><code>kubectl apply -f https://k8s.io/examples/pods/lifecycle-events.yaml
</code></pre><p>Verify that the Container in the Pod is running:</p><pre><code>kubectl get pod lifecycle-demo
</code></pre><p>Get a shell into the Container running in your Pod:</p><pre><code>kubectl exec -it lifecycle-demo -- /bin/bash
</code></pre><p>In your shell, verify that the <code>postStart</code> handler created the <code>message</code> file:</p><pre><code>root@lifecycle-demo:/# cat /usr/share/message
</code></pre><p>The output shows the text written by the postStart handler:</p><pre><code>Hello from the postStart handler
</code></pre><h2 id="discussion">Discussion</h2><p>Kubernetes sends the postStart event immediately after the Container is created.
There is no guarantee, however, that the postStart handler is called before
the Container's entrypoint is called. The postStart handler runs asynchronously
relative to the Container's code, but Kubernetes' management of the container
blocks until the postStart handler completes. The Container's status is not
set to RUNNING until the postStart handler completes.</p><p>Kubernetes sends the preStop event immediately before the Container is terminated.
Kubernetes' management of the Container blocks until the preStop handler completes,
unless the Pod's grace period expires. For more details, see
<a href="/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes only sends the preStop event when a Pod or a container in the Pod is <em>terminated</em>.
This means that the preStop hook is not invoked when the Pod is <em>completed</em>.
About this limitation, please see <a href="/docs/concepts/containers/container-lifecycle-hooks/#container-hooks">Container hooks</a> for the detail.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/containers/container-lifecycle-hooks/">Container lifecycle hooks</a>.</li><li>Learn more about the <a href="/docs/concepts/workloads/pods/pod-lifecycle/">lifecycle of a Pod</a>.</li></ul><h3 id="reference">Reference</h3><ul><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#lifecycle-v1-core">Lifecycle</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#container-v1-core">Container</a></li><li>See <code>terminationGracePeriodSeconds</code> in <a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec</a></li></ul></div></div><div><div class="td-content"><h1>Configure a Pod to Use a ConfigMap</h1><p>Many applications rely on configuration which is used during either application initialization or runtime.
Most times, there is a requirement to adjust values assigned to configuration parameters.
ConfigMaps are a Kubernetes mechanism that let you inject configuration data into application
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">pods</a>.</p><p>The ConfigMap concept allow you to decouple configuration artifacts from image content to
keep containerized applications portable. For example, you can download and run the same
<a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." href="/docs/reference/glossary/?all=true#term-image" target="_blank">container image</a> to spin up containers for
the purposes of local development, system test, or running a live end-user workload.</p><p>This page provides a series of usage examples demonstrating how to create ConfigMaps and
configure Pods using data stored in ConfigMaps.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You need to have the <code>wget</code> tool installed. If you have a different tool
such as <code>curl</code>, and you do not have <code>wget</code>, you will need to adapt the
step that downloads example data.</p><h2 id="create-a-configmap">Create a ConfigMap</h2><p>You can use either <code>kubectl create configmap</code> or a ConfigMap generator in <code>kustomization.yaml</code>
to create a ConfigMap.</p><h3 id="create-a-configmap-using-kubectl-create-configmap">Create a ConfigMap using <code>kubectl create configmap</code></h3><p>Use the <code>kubectl create configmap</code> command to create ConfigMaps from
<a href="#create-configmaps-from-directories">directories</a>, <a href="#create-configmaps-from-files">files</a>,
or <a href="#create-configmaps-from-literal-values">literal values</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap &lt;map-name&gt; &lt;data-source&gt;
</span></span></code></pre></div><p>where &lt;map-name&gt; is the name you want to assign to the ConfigMap and &lt;data-source&gt; is the
directory, file, or literal value to draw the data from.
The name of a ConfigMap object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>When you are creating a ConfigMap based on a file, the key in the &lt;data-source&gt; defaults to
the basename of the file, and the value defaults to the file content.</p><p>You can use <a href="/docs/reference/generated/kubectl/kubectl-commands/#describe"><code>kubectl describe</code></a> or
<a href="/docs/reference/generated/kubectl/kubectl-commands/#get"><code>kubectl get</code></a> to retrieve information
about a ConfigMap.</p><h4 id="create-configmaps-from-directories">Create a ConfigMap from a directory</h4><p>You can use <code>kubectl create configmap</code> to create a ConfigMap from multiple files in the same
directory. When you are creating a ConfigMap based on a directory, kubectl identifies files
whose filename is a valid key in the directory and packages each of those files into the new
ConfigMap. Any directory entries except regular files are ignored (for example: subdirectories,
symlinks, devices, pipes, and more).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Each filename being used for ConfigMap creation must consist of only acceptable characters,
which are: letters (<code>A</code> to <code>Z</code> and <code>a</code> to <code>z</code>), digits (<code>0</code> to <code>9</code>), '-', '_', or '.'.
If you use <code>kubectl create configmap</code> with a directory where any of the file names contains
an unacceptable character, the <code>kubectl</code> command may fail.</p><p>The <code>kubectl</code> command does not print an error when it encounters an invalid filename.</p></div><p>Create the local directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>mkdir -p configure-pod-container/configmap/
</span></span></code></pre></div><p>Now, download the sample configuration and create the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Download the sample files into `configure-pod-container/configmap/` directory</span>
</span></span><span><span>wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties
</span></span><span><span>wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties
</span></span><span><span>
</span></span><span><span><span># Create the ConfigMap</span>
</span></span><span><span>kubectl create configmap game-config --from-file<span>=</span>configure-pod-container/configmap/
</span></span></code></pre></div><p>The above command packages each file, in this case, <code>game.properties</code> and <code>ui.properties</code>
in the <code>configure-pod-container/configmap/</code> directory into the game-config ConfigMap. You can
display details of the ConfigMap using the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe configmaps game-config
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:         game-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
ui.properties:
----
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
</code></pre><p>The <code>game.properties</code> and <code>ui.properties</code> files in the <code>configure-pod-container/configmap/</code>
directory are represented in the <code>data</code> section of the ConfigMap.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get configmaps game-config -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2022-02-18T18:52:05Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>game-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"516"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>b4952dc3-d670-11e5-8cd0-68f728db1985<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>game.properties</span>:<span> </span>|<span>
</span></span></span><span><span><span>    enemies=aliens
</span></span></span><span><span><span>    lives=3
</span></span></span><span><span><span>    enemies.cheat=true
</span></span></span><span><span><span>    enemies.cheat.level=noGoodRotten
</span></span></span><span><span><span>    secret.code.passphrase=UUDDLRLRBABAS
</span></span></span><span><span><span>    secret.code.allowed=true
</span></span></span><span><span><span>    secret.code.lives=30</span><span>    
</span></span></span><span><span><span>  </span><span>ui.properties</span>:<span> </span>|<span>
</span></span></span><span><span><span>    color.good=purple
</span></span></span><span><span><span>    color.bad=yellow
</span></span></span><span><span><span>    allow.textmode=true
</span></span></span><span><span><span>    how.nice.to.look=fairlyNice</span><span>    
</span></span></span></code></pre></div><h4 id="create-configmaps-from-files">Create ConfigMaps from files</h4><p>You can use <code>kubectl create configmap</code> to create a ConfigMap from an individual file, or from
multiple files.</p><p>For example,</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap game-config-2 --from-file<span>=</span>configure-pod-container/configmap/game.properties
</span></span></code></pre></div><p>would produce the following ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe configmaps game-config-2
</span></span></code></pre></div><p>where the output is similar to this:</p><pre tabindex="0"><code>Name:         game-config-2
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
</code></pre><p>You can pass in the <code>--from-file</code> argument multiple times to create a ConfigMap from multiple
data sources.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap game-config-2 --from-file<span>=</span>configure-pod-container/configmap/game.properties --from-file<span>=</span>configure-pod-container/configmap/ui.properties
</span></span></code></pre></div><p>You can display details of the <code>game-config-2</code> ConfigMap using the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe configmaps game-config-2
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:         game-config-2
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
ui.properties:
----
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
</code></pre><p>Use the option <code>--from-env-file</code> to create a ConfigMap from an env-file, for example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Env-files contain a list of environment variables.</span>
</span></span><span><span><span># These syntax rules apply:</span>
</span></span><span><span><span>#   Each line in an env file has to be in VAR=VAL format.</span>
</span></span><span><span><span>#   Lines beginning with # (i.e. comments) are ignored.</span>
</span></span><span><span><span>#   Blank lines are ignored.</span>
</span></span><span><span><span>#   There is no special handling of quotation marks (i.e. they will be part of the ConfigMap value)).</span>
</span></span><span><span>
</span></span><span><span><span># Download the sample files into `configure-pod-container/configmap/` directory</span>
</span></span><span><span>wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties
</span></span><span><span>wget https://kubernetes.io/examples/configmap/ui-env-file.properties -O configure-pod-container/configmap/ui-env-file.properties
</span></span><span><span>
</span></span><span><span><span># The env-file `game-env-file.properties` looks like below</span>
</span></span><span><span>cat configure-pod-container/configmap/game-env-file.properties
</span></span><span><span><span>enemies</span><span>=</span>aliens
</span></span><span><span><span>lives</span><span>=</span><span>3</span>
</span></span><span><span><span>allowed</span><span>=</span><span>"true"</span>
</span></span><span><span>
</span></span><span><span><span># This comment and the empty line above it are ignored</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap game-config-env-file <span>\
</span></span></span><span><span><span></span>       --from-env-file<span>=</span>configure-pod-container/configmap/game-env-file.properties
</span></span></code></pre></div><p>would produce a ConfigMap. View the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get configmap game-config-env-file -o yaml
</span></span></code></pre></div><p>the output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2019-12-27T18:36:28Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>game-config-env-file<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"809965"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>d9d1ca5b-eb34-11e7-887b-42010a8002b8<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>allowed</span>:<span> </span><span>'"true"'</span><span>
</span></span></span><span><span><span>  </span><span>enemies</span>:<span> </span>aliens<span>
</span></span></span><span><span><span>  </span><span>lives</span>:<span> </span><span>"3"</span><span>
</span></span></span></code></pre></div><p>Starting with Kubernetes v1.23, <code>kubectl</code> supports the <code>--from-env-file</code> argument to be
specified multiple times to create a ConfigMap from multiple data sources.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap config-multi-env-files <span>\
</span></span></span><span><span><span></span>        --from-env-file<span>=</span>configure-pod-container/configmap/game-env-file.properties <span>\
</span></span></span><span><span><span></span>        --from-env-file<span>=</span>configure-pod-container/configmap/ui-env-file.properties
</span></span></code></pre></div><p>would produce the following ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get configmap config-multi-env-files -o yaml
</span></span></code></pre></div><p>where the output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2019-12-27T18:38:34Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>config-multi-env-files<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"810136"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>252c4572-eb35-11e7-887b-42010a8002b8<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>allowed</span>:<span> </span><span>'"true"'</span><span>
</span></span></span><span><span><span>  </span><span>color</span>:<span> </span>purple<span>
</span></span></span><span><span><span>  </span><span>enemies</span>:<span> </span>aliens<span>
</span></span></span><span><span><span>  </span><span>how</span>:<span> </span>fairlyNice<span>
</span></span></span><span><span><span>  </span><span>lives</span>:<span> </span><span>"3"</span><span>
</span></span></span><span><span><span>  </span><span>textmode</span>:<span> </span><span>"true"</span><span>
</span></span></span></code></pre></div><h4 id="define-the-key-to-use-when-creating-a-configmap-from-a-file">Define the key to use when creating a ConfigMap from a file</h4><p>You can define a key other than the file name to use in the <code>data</code> section of your ConfigMap
when using the <code>--from-file</code> argument:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap game-config-3 --from-file<span>=</span>&lt;my-key-name&gt;<span>=</span>&lt;path-to-file&gt;
</span></span></code></pre></div><p>where <code>&lt;my-key-name&gt;</code> is the key you want to use in the ConfigMap and <code>&lt;path-to-file&gt;</code> is the
location of the data source file you want the key to represent.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap game-config-3 --from-file<span>=</span>game-special-key<span>=</span>configure-pod-container/configmap/game.properties
</span></span></code></pre></div><p>would produce the following ConfigMap:</p><pre tabindex="0"><code>kubectl get configmaps game-config-3 -o yaml
</code></pre><p>where the output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2022-02-18T18:54:22Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>game-config-3<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"530"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>05f8da22-d671-11e5-8cd0-68f728db1985<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>game-special-key</span>:<span> </span>|<span>
</span></span></span><span><span><span>    enemies=aliens
</span></span></span><span><span><span>    lives=3
</span></span></span><span><span><span>    enemies.cheat=true
</span></span></span><span><span><span>    enemies.cheat.level=noGoodRotten
</span></span></span><span><span><span>    secret.code.passphrase=UUDDLRLRBABAS
</span></span></span><span><span><span>    secret.code.allowed=true
</span></span></span><span><span><span>    secret.code.lives=30</span><span>    
</span></span></span></code></pre></div><h4 id="create-configmaps-from-literal-values">Create ConfigMaps from literal values</h4><p>You can use <code>kubectl create configmap</code> with the <code>--from-literal</code> argument to define a literal
value from the command line:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap special-config --from-literal<span>=</span>special.how<span>=</span>very --from-literal<span>=</span>special.type<span>=</span>charm
</span></span></code></pre></div><p>You can pass in multiple key-value pairs. Each pair provided on the command line is represented
as a separate entry in the <code>data</code> section of the ConfigMap.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get configmaps special-config -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2022-02-18T19:14:38Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"651"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>dadce046-d673-11e5-8cd0-68f728db1985<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>special.how</span>:<span> </span>very<span>
</span></span></span><span><span><span>  </span><span>special.type</span>:<span> </span>charm<span>
</span></span></span></code></pre></div><h3 id="create-a-configmap-from-generator">Create a ConfigMap from generator</h3><p>You can also create a ConfigMap from generators and then apply it to create the object
in the cluster's API server.
You should specify the generators in a <code>kustomization.yaml</code> file within a directory.</p><h4 id="generate-configmaps-from-files">Generate ConfigMaps from files</h4><p>For example, to generate a ConfigMap from files <code>configure-pod-container/configmap/game.properties</code></p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a kustomization.yaml file with ConfigMapGenerator</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>configMapGenerator:
</span></span></span><span><span><span>- name: game-config-4
</span></span></span><span><span><span>  options:
</span></span></span><span><span><span>    labels:
</span></span></span><span><span><span>      game-config: config-4
</span></span></span><span><span><span>  files:
</span></span></span><span><span><span>  - configure-pod-container/configmap/game.properties
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Apply the kustomization directory to create the ConfigMap object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k .
</span></span></code></pre></div><pre tabindex="0"><code>configmap/game-config-4-m9dm2f92bt created
</code></pre><p>You can check that the ConfigMap was created like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get configmap
</span></span></code></pre></div><pre tabindex="0"><code>NAME                       DATA   AGE
game-config-4-m9dm2f92bt   1      37s
</code></pre><p>and also:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe configmaps/game-config-4-m9dm2f92bt
</span></span></code></pre></div><pre tabindex="0"><code>Name:         game-config-4-m9dm2f92bt
Namespace:    default
Labels:       game-config=config-4
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","data":{"game.properties":"enemies=aliens\nlives=3\nenemies.cheat=true\nenemies.cheat.level=noGoodRotten\nsecret.code.p...

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
Events:  &lt;none&gt;
</code></pre><p>Notice that the generated ConfigMap name has a suffix appended by hashing the contents. This
ensures that a new ConfigMap is generated each time the content is modified.</p><h4 id="define-the-key-to-use-when-generating-a-configmap-from-a-file">Define the key to use when generating a ConfigMap from a file</h4><p>You can define a key other than the file name to use in the ConfigMap generator.
For example, to generate a ConfigMap from files <code>configure-pod-container/configmap/game.properties</code>
with the key <code>game-special-key</code></p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a kustomization.yaml file with ConfigMapGenerator</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>configMapGenerator:
</span></span></span><span><span><span>- name: game-config-5
</span></span></span><span><span><span>  options:
</span></span></span><span><span><span>    labels:
</span></span></span><span><span><span>      game-config: config-5
</span></span></span><span><span><span>  files:
</span></span></span><span><span><span>  - game-special-key=configure-pod-container/configmap/game.properties
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Apply the kustomization directory to create the ConfigMap object.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k .
</span></span></code></pre></div><pre tabindex="0"><code>configmap/game-config-5-m67dt67794 created
</code></pre><h4 id="generate-configmaps-from-literals">Generate ConfigMaps from literals</h4><p>This example shows you how to create a <code>ConfigMap</code> from two literal key/value pairs:
<code>special.type=charm</code> and <code>special.how=very</code>, using Kustomize and kubectl. To achieve
this, you can specify the <code>ConfigMap</code> generator. Create (or replace)
<code>kustomization.yaml</code> so that it has the following contents:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span># kustomization.yaml contents for creating a ConfigMap from literals</span><span>
</span></span></span><span><span><span></span><span>configMapGenerator</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>special-config-2<span>
</span></span></span><span><span><span>  </span><span>literals</span>:<span>
</span></span></span><span><span><span>  </span>- special.how=very<span>
</span></span></span><span><span><span>  </span>- special.type=charm<span>
</span></span></span></code></pre></div><p>Apply the kustomization directory to create the ConfigMap object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k .
</span></span></code></pre></div><pre tabindex="0"><code>configmap/special-config-2-c92b5mmcf2 created
</code></pre><h2 id="interim-cleanup">Interim cleanup</h2><p>Before proceeding, clean up some of the ConfigMaps you made:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl delete configmap special-config
</span></span><span><span>kubectl delete configmap env-config
</span></span><span><span>kubectl delete configmap -l <span>'game-config in (config-4,config-5)'</span>
</span></span></code></pre></div><p>Now that you have learned to define ConfigMaps, you can move on to the next
section, and learn how to use these objects with Pods.</p><hr><h2 id="define-container-environment-variables-using-configmap-data">Define container environment variables using ConfigMap data</h2><h3 id="define-a-container-environment-variable-with-data-from-a-single-configmap">Define a container environment variable with data from a single ConfigMap</h3><ol><li><p>Define an environment variable as a key-value pair in a ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap special-config --from-literal<span>=</span>special.how<span>=</span>very
</span></span></code></pre></div></li><li><p>Assign the <code>special.how</code> value defined in the ConfigMap to the <code>SPECIAL_LEVEL_KEY</code>
environment variable in the Pod specification.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-single-configmap-env-variable.yaml"><code>pods/pod-single-configmap-env-variable.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-single-configmap-env-variable.yaml to clipboard"></div><div class="includecode" id="pods-pod-single-configmap-env-variable-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"env"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span><span># Define the environment variable</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SPECIAL_LEVEL_KEY<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span># The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY</span><span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>              </span><span># Specify the key associated with the value</span><span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>special.how<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/pods/pod-single-configmap-env-variable.yaml
</span></span></code></pre></div><p>Now, the Pod's output includes environment variable <code>SPECIAL_LEVEL_KEY=very</code>.</p></li></ol><h3 id="define-container-environment-variables-with-data-from-multiple-configmaps">Define container environment variables with data from multiple ConfigMaps</h3><p>As with the previous example, create the ConfigMaps first.
Here is the manifest you will use:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/configmaps.yaml"><code>configmap/configmaps.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy configmap/configmaps.yaml to clipboard"></div><div class="includecode" id="configmap-configmaps-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>special.how</span>:<span> </span>very<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>env-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>log_level</span>:<span> </span>INFO<span>
</span></span></span></code></pre></div></div></div><ul><li><p>Create the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/configmap/configmaps.yaml
</span></span></code></pre></div></li><li><p>Define the environment variables in the Pod specification.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-multiple-configmap-env-variable.yaml"><code>pods/pod-multiple-configmap-env-variable.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-multiple-configmap-env-variable.yaml to clipboard"></div><div class="includecode" id="pods-pod-multiple-configmap-env-variable-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"env"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SPECIAL_LEVEL_KEY<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>special.how<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>LOG_LEVEL<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>env-config<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>log_level<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/pods/pod-multiple-configmap-env-variable.yaml
</span></span></code></pre></div><p>Now, the Pod's output includes environment variables <code>SPECIAL_LEVEL_KEY=very</code> and <code>LOG_LEVEL=INFO</code>.</p><p>Once you're happy to move on, delete that Pod and ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod dapi-test-pod --now
</span></span><span><span>kubectl delete configmap special-config
</span></span><span><span>kubectl delete configmap env-config
</span></span></code></pre></div></li></ul><h2 id="configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables">Configure all key-value pairs in a ConfigMap as container environment variables</h2><ul><li><p>Create a ConfigMap containing multiple key-value pairs.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/configmap-multikeys.yaml"><code>configmap/configmap-multikeys.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy configmap/configmap-multikeys.yaml to clipboard"></div><div class="includecode" id="configmap-configmap-multikeys-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>SPECIAL_LEVEL</span>:<span> </span>very<span>
</span></span></span><span><span><span>  </span><span>SPECIAL_TYPE</span>:<span> </span>charm<span>
</span></span></span></code></pre></div></div></div><p>Create the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml
</span></span></code></pre></div></li><li><p>Use <code>envFrom</code> to define all of the ConfigMap's data as container environment variables. The
key from the ConfigMap becomes the environment variable name in the Pod.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-configmap-envFrom.yaml"><code>pods/pod-configmap-envFrom.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-configmap-envFrom.yaml to clipboard"></div><div class="includecode" id="pods-pod-configmap-envfrom-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"env"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>envFrom</span>:<span>
</span></span></span><span><span><span>      </span>- <span>configMapRef</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-envFrom.yaml
</span></span></code></pre></div><p>Now, the Pod's output includes environment variables <code>SPECIAL_LEVEL=very</code> and
<code>SPECIAL_TYPE=charm</code>.</p><p>Once you're happy to move on, delete that Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod dapi-test-pod --now
</span></span></code></pre></div></li></ul><h2 id="use-configmap-defined-environment-variables-in-pod-commands">Use ConfigMap-defined environment variables in Pod commands</h2><p>You can use ConfigMap-defined environment variables in the <code>command</code> and <code>args</code> of a container
using the <code>$(VAR_NAME)</code> Kubernetes substitution syntax.</p><p>For example, the following Pod manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-configmap-env-var-valueFrom.yaml"><code>pods/pod-configmap-env-var-valueFrom.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-configmap-env-var-valueFrom.yaml to clipboard"></div><div class="includecode" id="pods-pod-configmap-env-var-valuefrom-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/echo"</span>,<span> </span><span>"$(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SPECIAL_LEVEL_KEY<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>SPECIAL_LEVEL<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SPECIAL_TYPE_KEY<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>SPECIAL_TYPE<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>Create that Pod, by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-env-var-valueFrom.yaml
</span></span></code></pre></div><p>That pod produces the following output from the <code>test-container</code> container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs dapi-test-pod
</span></span></code></pre></div><pre tabindex="0"><code>very charm
</code></pre><p>Once you're happy to move on, delete that Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod dapi-test-pod --now
</span></span></code></pre></div><h2 id="add-configmap-data-to-a-volume">Add ConfigMap data to a Volume</h2><p>As explained in <a href="#create-configmaps-from-files">Create ConfigMaps from files</a>, when you create
a ConfigMap using <code>--from-file</code>, the filename becomes a key stored in the <code>data</code> section of
the ConfigMap. The file contents become the key's value.</p><p>The examples in this section refer to a ConfigMap named <code>special-config</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/configmap-multikeys.yaml"><code>configmap/configmap-multikeys.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy configmap/configmap-multikeys.yaml to clipboard"></div><div class="includecode" id="configmap-configmap-multikeys-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>SPECIAL_LEVEL</span>:<span> </span>very<span>
</span></span></span><span><span><span>  </span><span>SPECIAL_TYPE</span>:<span> </span>charm<span>
</span></span></span></code></pre></div></div></div><p>Create the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml
</span></span></code></pre></div><h3 id="populate-a-volume-with-data-stored-in-a-configmap">Populate a Volume with data stored in a ConfigMap</h3><p>Add the ConfigMap name under the <code>volumes</code> section of the Pod specification.
This adds the ConfigMap data to the directory specified as <code>volumeMounts.mountPath</code> (in this
case, <code>/etc/config</code>). The <code>command</code> section lists directory files with names that match the
keys in ConfigMap.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-configmap-volume.yaml"><code>pods/pod-configmap-volume.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-configmap-volume.yaml to clipboard"></div><div class="includecode" id="pods-pod-configmap-volume-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"ls /etc/config/"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>        </span><span>mountPath</span>:<span> </span>/etc/config<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>      </span><span>configMap</span>:<span>
</span></span></span><span><span><span>        </span><span># Provide the name of the ConfigMap containing the files you want</span><span>
</span></span></span><span><span><span>        </span><span># to add to the container</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume.yaml
</span></span></code></pre></div><p>When the pod runs, the command <code>ls /etc/config/</code> produces the output below:</p><pre tabindex="0"><code>SPECIAL_LEVEL
SPECIAL_TYPE
</code></pre><p>Text data is exposed as files using the UTF-8 character encoding. To use some other
character encoding, use <code>binaryData</code>
(see <a href="/docs/concepts/configuration/configmap/#configmap-object">ConfigMap object</a> for more details).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If there are any files in the <code>/etc/config</code> directory of that container image, the volume
mount will make those files from the image inaccessible.</div><p>Once you're happy to move on, delete that Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod dapi-test-pod --now
</span></span></code></pre></div><h3 id="add-configmap-data-to-a-specific-path-in-the-volume">Add ConfigMap data to a specific path in the Volume</h3><p>Use the <code>path</code> field to specify the desired file path for specific ConfigMap items.
In this case, the <code>SPECIAL_LEVEL</code> item will be mounted in the <code>config-volume</code> volume at <code>/etc/config/keys</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-configmap-volume-specific-key.yaml"><code>pods/pod-configmap-volume-specific-key.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-configmap-volume-specific-key.yaml to clipboard"></div><div class="includecode" id="pods-pod-configmap-volume-specific-key-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/sh"</span>,<span>"-c"</span>,<span>"cat /etc/config/keys"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>        </span><span>mountPath</span>:<span> </span>/etc/config<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>      </span><span>configMap</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>special-config<span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>        </span>- <span>key</span>:<span> </span>SPECIAL_LEVEL<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>keys<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume-specific-key.yaml
</span></span></code></pre></div><p>When the pod runs, the command <code>cat /etc/config/keys</code> produces the output below:</p><pre tabindex="0"><code>very
</code></pre><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Like before, all previous files in the <code>/etc/config/</code> directory will be deleted.</div><p>Delete that Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod dapi-test-pod --now
</span></span></code></pre></div><h3 id="project-keys-to-specific-paths-and-file-permissions">Project keys to specific paths and file permissions</h3><p>You can project keys to specific paths. Refer to the corresponding section in the <a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#project-secret-keys-to-specific-file-paths">Secrets</a> guide for the syntax.<br>You can set POSIX permissions for keys. Refer to the corresponding section in the <a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#set-posix-permissions-for-secret-keys">Secrets</a> guide for the syntax.</p><h3 id="optional-references">Optional references</h3><p>A ConfigMap reference may be marked <em>optional</em>. If the ConfigMap is non-existent, the mounted
volume will be empty. If the ConfigMap exists, but the referenced key is non-existent, the path
will be absent beneath the mount point. See <a href="#optional-configmaps">Optional ConfigMaps</a> for more
details.</p><h3 id="mounted-configmaps-are-updated-automatically">Mounted ConfigMaps are updated automatically</h3><p>When a mounted ConfigMap is updated, the projected content is eventually updated too.
This applies in the case where an optionally referenced ConfigMap comes into
existence after a pod has started.</p><p>Kubelet checks whether the mounted ConfigMap is fresh on every periodic sync. However,
it uses its local TTL-based cache for getting the current value of the ConfigMap. As a
result, the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the pod can be as long as kubelet sync period (1
minute by default) + TTL of ConfigMaps cache (1 minute by default) in kubelet. You
can trigger an immediate refresh by updating one of the pod's annotations.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container using a ConfigMap as a <a href="/docs/concepts/storage/volumes/#using-subpath">subPath</a>
volume will not receive ConfigMap updates.</div><h2 id="understanding-configmaps-and-pods">Understanding ConfigMaps and Pods</h2><p>The ConfigMap API resource stores configuration data as key-value pairs. The data can be consumed
in pods or provide the configurations for system components such as controllers. ConfigMap is
similar to <a href="/docs/concepts/configuration/secret/">Secrets</a>, but provides a means of working
with strings that don't contain sensitive information. Users and system components alike can
store configuration data in ConfigMap.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>ConfigMaps should reference properties files, not replace them. Think of the ConfigMap as
representing something similar to the Linux <code>/etc</code> directory and its contents. For example,
if you create a <a href="/docs/concepts/storage/volumes/">Kubernetes Volume</a> from a ConfigMap, each
data item in the ConfigMap is represented by an individual file in the volume.</div><p>The ConfigMap's <code>data</code> field contains the configuration data. As shown in the example below,
this can be simple (like individual properties defined using <code>--from-literal</code>) or complex
(like configuration files or JSON blobs defined using <code>--from-file</code>).</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2016-02-18T19:14:38Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span># example of a simple property defined using --from-literal</span><span>
</span></span></span><span><span><span>  </span><span>example.property.1</span>:<span> </span>hello<span>
</span></span></span><span><span><span>  </span><span>example.property.2</span>:<span> </span>world<span>
</span></span></span><span><span><span>  </span><span># example of a complex property defined using --from-file</span><span>
</span></span></span><span><span><span>  </span><span>example.property.file</span>:<span> </span>|-<span>
</span></span></span><span><span><span>    property.1=value-1
</span></span></span><span><span><span>    property.2=value-2
</span></span></span><span><span><span>    property.3=value-3</span><span>    
</span></span></span></code></pre></div><p>When <code>kubectl</code> creates a ConfigMap from inputs that are not ASCII or UTF-8, the tool puts
these into the <code>binaryData</code> field of the ConfigMap, and not in <code>data</code>. Both text and binary
data sources can be combined in one ConfigMap.</p><p>If you want to view the <code>binaryData</code> keys (and their values) in a ConfigMap, you can run
<code>kubectl get configmap -o jsonpath='{.binaryData}' &lt;name&gt;</code>.</p><p>Pods can load data from a ConfigMap that uses either <code>data</code> or <code>binaryData</code>.</p><h2 id="optional-configmaps">Optional ConfigMaps</h2><p>You can mark a reference to a ConfigMap as <em>optional</em> in a Pod specification.
If the ConfigMap doesn't exist, the configuration for which it provides data in the Pod
(for example: environment variable, mounted volume) will be empty.
If the ConfigMap exists, but the referenced key is non-existent the data is also empty.</p><p>For example, the following Pod specification marks an environment variable from a ConfigMap
as optional:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>gcr.io/google_containers/busybox<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"env"</span>]<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SPECIAL_LEVEL_KEY<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>a-config<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>akey<span>
</span></span></span><span><span><span>              </span><span>optional</span>:<span> </span><span>true</span><span> </span><span># mark the variable as optional</span><span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div><p>If you run this pod, and there is no ConfigMap named <code>a-config</code>, the output is empty.
If you run this pod, and there is a ConfigMap named <code>a-config</code> but that ConfigMap doesn't have
a key named <code>akey</code>, the output is also empty. If you do set a value for <code>akey</code> in the <code>a-config</code>
ConfigMap, this pod prints that value and then terminates.</p><p>You can also mark the volumes and files provided by a ConfigMap as optional. Kubernetes always
creates the mount paths for the volume, even if the referenced ConfigMap or key doesn't exist. For
example, the following Pod specification marks a volume that references a ConfigMap as optional:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>gcr.io/google_containers/busybox<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"ls /etc/config"</span>]<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>        </span><span>mountPath</span>:<span> </span>/etc/config<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>      </span><span>configMap</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span><span>no</span>-config<span>
</span></span></span><span><span><span>        </span><span>optional</span>:<span> </span><span>true</span><span> </span><span># mark the source ConfigMap as optional</span><span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div><h2 id="restrictions">Restrictions</h2><ul><li><p>You must create the <code>ConfigMap</code> object before you reference it in a Pod
specification. Alternatively, mark the ConfigMap reference as <code>optional</code> in the Pod spec (see
<a href="#optional-configmaps">Optional ConfigMaps</a>). If you reference a ConfigMap that doesn't exist
and you don't mark the reference as <code>optional</code>, the Pod won't start. Similarly, references
to keys that don't exist in the ConfigMap will also prevent the Pod from starting, unless
you mark the key references as <code>optional</code>.</p></li><li><p>If you use <code>envFrom</code> to define environment variables from ConfigMaps, keys that are considered
invalid will be skipped. The pod will be allowed to start, but the invalid names will be
recorded in the event log (<code>InvalidVariableNames</code>). The log message lists each skipped
key. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get events
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>LASTSEEN FIRSTSEEN COUNT NAME          KIND  SUBOBJECT  TYPE      REASON                            SOURCE                MESSAGE
0s       0s        1     dapi-test-pod Pod              Warning   InvalidEnvironmentVariableNames   {kubelet, 127.0.0.1}  Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names.
</code></pre></li><li><p>ConfigMaps reside in a specific <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">Namespace</a>.
Pods can only refer to ConfigMaps that are in the same namespace as the Pod.</p></li><li><p>You can't use ConfigMaps for
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static pods</a>, because the
kubelet does not support this.</p></li></ul><h2 id="cleaning-up">Cleaning up</h2><p>Delete the ConfigMaps and Pods that you made:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl delete configmaps/game-config configmaps/game-config-2 configmaps/game-config-3 <span>\
</span></span></span><span><span><span></span>               configmaps/game-config-env-file
</span></span><span><span>kubectl delete pod dapi-test-pod --now
</span></span><span><span>
</span></span><span><span><span># You might already have removed the next set</span>
</span></span><span><span>kubectl delete configmaps/special-config configmaps/env-config
</span></span><span><span>kubectl delete configmap -l <span>'game-config in (config-4,config-5)'</span>
</span></span></code></pre></div><p>Remove the <code>kustomization.yaml</code> file that you used to generate the ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>rm kustomization.yaml
</span></span></code></pre></div><p>If you created a directory <code>configure-pod-container</code> and no longer need it, you should remove that too,
or move it into the trash can / deleted files location.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>rm -r configure-pod-container
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Follow a real world example of
<a href="/docs/tutorials/configuration/configure-redis-using-configmap/">Configuring Redis using a ConfigMap</a>.</li><li>Follow an example of <a href="/docs/tutorials/configuration/updating-configuration-via-a-configmap/">Updating configuration via a ConfigMap</a>.</li></ul></div></div><div><div class="td-content"><h1>Share Process Namespace between Containers in a Pod</h1><p>This page shows how to configure process namespace sharing for a pod. When
process namespace sharing is enabled, processes in a container are visible
to all other containers in the same pod.</p><p>You can use this feature to configure cooperating containers, such as a log
handler sidecar container, or to troubleshoot container images that don't
include debugging utilities like a shell.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="configure-a-pod">Configure a Pod</h2><p>Process namespace sharing is enabled using the <code>shareProcessNamespace</code> field of
<code>.spec</code> for a Pod. For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/share-process-namespace.yaml"><code>pods/share-process-namespace.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/share-process-namespace.yaml to clipboard"></div><div class="includecode" id="pods-share-process-namespace-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>shareProcessNamespace</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shell<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"3600"</span>]<span>
</span></span></span><span><span><span>    </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>      </span><span>capabilities</span>:<span>
</span></span></span><span><span><span>        </span><span>add</span>:<span>
</span></span></span><span><span><span>        </span>- SYS_PTRACE<span>
</span></span></span><span><span><span>    </span><span>stdin</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>tty</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the pod <code>nginx</code> on your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/share-process-namespace.yaml
</span></span></code></pre></div></li><li><p>Attach to the <code>shell</code> container and run <code>ps</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it nginx -c shell -- /bin/sh
</span></span></code></pre></div><p>If you don't see a command prompt, try pressing enter. In the container shell:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># run this inside the "shell" container</span>
</span></span><span><span>ps ax
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   14 101       0:00 nginx: worker process
   15 root      0:00 sh
   21 root      0:00 ps ax
</code></pre></li></ol><p>You can signal processes in other containers. For example, send <code>SIGHUP</code> to
<code>nginx</code> to restart the worker process. This requires the <code>SYS_PTRACE</code> capability.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># run this inside the "shell" container</span>
</span></span><span><span><span>kill</span> -HUP <span>8</span>   <span># change "8" to match the PID of the nginx leader process, if necessary</span>
</span></span><span><span>ps ax
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   15 root      0:00 sh
   22 101       0:00 nginx: worker process
   23 root      0:00 ps ax
</code></pre><p>It's even possible to access the file system of another container using the
<code>/proc/$pid/root</code> link.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># run this inside the "shell" container</span>
</span></span><span><span><span># change "8" to the PID of the Nginx process, if necessary</span>
</span></span><span><span>head /proc/8/root/etc/nginx/nginx.conf
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
</code></pre><h2 id="understanding-process-namespace-sharing">Understanding process namespace sharing</h2><p>Pods share many resources so it makes sense they would also share a process
namespace. Some containers may expect to be isolated from others, though,
so it's important to understand the differences:</p><ol><li><p><strong>The container process no longer has PID 1.</strong> Some containers refuse
to start without PID 1 (for example, containers using <code>systemd</code>) or run
commands like <code>kill -HUP 1</code> to signal the container process. In pods with a
shared process namespace, <code>kill -HUP 1</code> will signal the pod sandbox
(<code>/pause</code> in the above example).</p></li><li><p><strong>Processes are visible to other containers in the pod.</strong> This includes all
information visible in <code>/proc</code>, such as passwords that were passed as arguments
or environment variables. These are protected only by regular Unix permissions.</p></li><li><p><strong>Container filesystems are visible to other containers in the pod through the
<code>/proc/$pid/root</code> link.</strong> This makes debugging easier, but it also means
that filesystem secrets are protected only by filesystem permissions.</p></li></ol></div></div><div><div class="td-content"><h1>Use a User Namespace With a Pod</h1><div class="feature-state-notice feature-beta" title="Feature Gate: UserNamespacesSupport"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>This page shows how to configure a user namespace for pods. This allows you to
isolate the user running inside the container from the one in the host.</p><p>A process running as root in a container can run as a different (non-root) user
in the host; in other words, the process has full privileges for operations
inside the user namespace, but is unprivileged for operations outside the
namespace.</p><p>You can use this feature to reduce the damage a compromised container can do to
the host or other pods in the same node. There are <a href="https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation">several security
vulnerabilities</a> rated either <strong>HIGH</strong> or <strong>CRITICAL</strong> that were not
exploitable when user namespaces is active. It is expected user namespace will
mitigate some future vulnerabilities too.</p><p>Without using a user namespace a container running as root, in the case of a
container breakout, has root privileges on the node. And if some capability were
granted to the container, the capabilities are valid on the host too. None of
this is true when user namespaces are used.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.25.<p>To check the version, enter <code>kubectl version</code>.</p></p><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><ul><li>The node OS needs to be Linux</li><li>You need to exec commands in the host</li><li>You need to be able to exec into pods</li><li>You need to enable the <code>UserNamespacesSupport</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The feature gate to enable user namespaces was previously named
<code>UserNamespacesStatelessPodsSupport</code>, when only stateless pods were supported.
Only Kubernetes v1.25 through to v1.27 recognise <code>UserNamespacesStatelessPodsSupport</code>.</div><p>The cluster that you're using <strong>must</strong> include at least one node that meets the
<a href="/docs/concepts/workloads/pods/user-namespaces/#before-you-begin">requirements</a>
for using user namespaces with Pods.</p><p>If you have a mixture of nodes and only some of the nodes provide user namespace support for
Pods, you also need to ensure that the user namespace Pods are
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">scheduled</a> to suitable nodes.</p><h2 id="create-pod">Run a Pod that uses a user namespace</h2><p>A user namespace for a pod is enabled setting the <code>hostUsers</code> field of <code>.spec</code>
to <code>false</code>. For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/user-namespaces-stateless.yaml"><code>pods/user-namespaces-stateless.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/user-namespaces-stateless.yaml to clipboard"></div><div class="includecode" id="pods-user-namespaces-stateless-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>userns<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hostUsers</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shell<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"infinity"</span>]<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the pod on your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/user-namespaces-stateless.yaml
</span></span></code></pre></div></li><li><p>Exec into the pod and run <code>readlink /proc/self/ns/user</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -ti userns -- bash
</span></span></code></pre></div></li></ol><p>Run this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>readlink /proc/self/ns/user
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>user:<span>[</span>4026531837<span>]</span>
</span></span></code></pre></div><p>Also run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /proc/self/uid_map
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>0</span>  <span>833617920</span>      <span>65536</span>
</span></span></code></pre></div><p>Then, open a shell in the host and run the same commands.</p><p>The <code>readlink</code> command shows the user namespace the process is running in. It
should be different when it is run on the host and inside the container.</p><p>The last number of the <code>uid_map</code> file inside the container must be 65536, on the
host it must be a bigger number.</p><p>If you are running the kubelet inside a user namespace, you need to compare the
output from running the command in the pod to the output of running in the host:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>readlink /proc/<span>$pid</span>/ns/user
</span></span></code></pre></div><p>replacing <code>$pid</code> with the kubelet PID.</p></div></div><div><div class="td-content"><h1>Use an Image Volume With a Pod</h1><div class="feature-state-notice feature-beta" title="Feature Gate: ImageVolume"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><p>This page shows how to configure a pod using image volumes. This allows you to
mount content from OCI registries inside containers.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.31.<p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>The container runtime needs to support the image volumes feature</li><li>You need to exec commands in the host</li><li>You need to be able to exec into pods</li><li>You need to enable the <code>ImageVolume</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a></li></ul><h2 id="create-pod">Run a Pod that uses an image volume</h2><p>An image volume for a pod is enabled by setting the <code>volumes.[*].image</code> field of <code>.spec</code>
to a valid reference and consuming it in the <code>volumeMounts</code> of the container. For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/image-volumes.yaml"><code>pods/image-volumes.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/image-volumes.yaml to clipboard"></div><div class="includecode" id="pods-image-volumes-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>image-volume<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shell<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"infinity"</span>]<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>volume<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/volume<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>volume<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span>
</span></span></span><span><span><span>      </span><span>reference</span>:<span> </span>quay.io/crio/artifact:v2<span>
</span></span></span><span><span><span>      </span><span>pullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the pod on your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/image-volumes.yaml
</span></span></code></pre></div></li><li><p>Attach to the container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl attach -it image-volume bash
</span></span></code></pre></div></li><li><p>Check the content of a file in the volume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /volume/dir/file
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">1
</code></pre><p>You can also check another file in a different path:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /volume/file
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">2
</code></pre></li></ol><h2 id="use-subpath-or-subpathexpr">Use <code>subPath</code> (or <code>subPathExpr</code>)</h2><p>It is possible to utilize
<a href="/docs/concepts/storage/volumes/#using-subpath"><code>subPath</code></a> or
<a href="/docs/concepts/storage/volumes/#using-subpath-expanded-environment"><code>subPathExpr</code></a>
from Kubernetes v1.33 when using the image volume feature.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/image-volumes-subpath.yaml"><code>pods/image-volumes-subpath.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/image-volumes-subpath.yaml to clipboard"></div><div class="includecode" id="pods-image-volumes-subpath-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>image-volume<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shell<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"infinity"</span>]<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>volume<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/volume<span>
</span></span></span><span><span><span>      </span><span>subPath</span>:<span> </span>dir<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>volume<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span>
</span></span></span><span><span><span>      </span><span>reference</span>:<span> </span>quay.io/crio/artifact:v2<span>
</span></span></span><span><span><span>      </span><span>pullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the pod on your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/image-volumes-subpath.yaml
</span></span></code></pre></div></li><li><p>Attach to the container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl attach -it image-volume bash
</span></span></code></pre></div></li><li><p>Check the content of the file from the <code>dir</code> sub path in the volume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /volume/file
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">1
</code></pre></li></ol><h2 id="further-reading">Further reading</h2><ul><li><a href="/docs/concepts/storage/volumes/#image"><code>image</code> volumes</a></li></ul></div></div><div><div class="td-content"><h1>Create static Pods</h1><p><em>Static Pods</em> are managed directly by the kubelet daemon on a specific node,
without the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>
observing them.
Unlike Pods that are managed by the control plane (for example, a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>);
instead, the kubelet watches each static Pod (and restarts it if it fails).</p><p>Static Pods are always bound to one <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">Kubelet</a> on a specific node.</p><p>The kubelet automatically tries to create a <a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank">mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there.
The Pod names will be suffixed with the node hostname with a leading hyphen.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you are running clustered Kubernetes and are using static
Pods to run a Pod on every node, you should probably be using a
<a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a> instead.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>spec</code> of a static Pod cannot refer to other API objects
(e.g., <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank">ServiceAccount</a>,
<a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a>,
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a>, etc).</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Static pods do not support <a href="/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers</a>.</div><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>This page assumes you're using <a class="glossary-tooltip" title="A lightweight container runtime specifically for Kubernetes" href="https://cri-o.io/#what-is-cri-o" target="_blank">CRI-O</a> to run Pods,
and that your nodes are running the Fedora operating system.
Instructions for other distributions or Kubernetes installations may vary.</p><h2 id="static-pod-creation">Create a static pod</h2><p>You can configure a static Pod with either a
<a href="/docs/tasks/configure-pod-container/static-pod/#configuration-files">file system hosted configuration file</a>
or a <a href="/docs/tasks/configure-pod-container/static-pod/#pods-created-via-http">web hosted configuration file</a>.</p><h3 id="configuration-files">Filesystem-hosted static Pod manifest</h3><p>Manifests are standard Pod definitions in JSON or YAML format in a specific directory.
Use the <code>staticPodPath: &lt;the directory&gt;</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration file</a>,
which periodically scans the directory and creates/deletes static Pods as YAML/JSON files appear/disappear there.
Note that the kubelet will ignore files starting with dots when scanning the specified directory.</p><p>For example, this is how to start a simple web server as a static Pod:</p><ol><li><p>Choose a node where you want to run the static Pod. In this example, it's <code>my-node1</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ssh my-node1
</span></span></code></pre></div></li><li><p>Choose a directory, say <code>/etc/kubernetes/manifests</code> and place a web server
Pod definition there, for example <code>/etc/kubernetes/manifests/static-web.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this command on the node where kubelet is running</span>
</span></span><span><span>mkdir -p /etc/kubernetes/manifests/
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;/etc/kubernetes/manifests/static-web.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Pod
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: static-web
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    role: myrole
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  containers:
</span></span></span><span><span><span>    - name: web
</span></span></span><span><span><span>      image: nginx
</span></span></span><span><span><span>      ports:
</span></span></span><span><span><span>        - name: web
</span></span></span><span><span><span>          containerPort: 80
</span></span></span><span><span><span>          protocol: TCP
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div></li><li><p>Configure the kubelet on that node to set a <code>staticPodPath</code> value in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration file</a>.<br>See <a href="/docs/tasks/administer-cluster/kubelet-config-file/">Set Kubelet Parameters Via A Configuration File</a>
for more information.</p><p>An alternative and deprecated method is to configure the kubelet on that node
to look for static Pod manifests locally, using a command line argument.
To use the deprecated approach, start the kubelet with the<br><code>--pod-manifest-path=/etc/kubernetes/manifests/</code> argument.</p></li><li><p>Restart the kubelet. On Fedora, you would run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this command on the node where the kubelet is running</span>
</span></span><span><span>systemctl restart kubelet
</span></span></code></pre></div></li></ol><h3 id="pods-created-via-http">Web-hosted static pod manifest</h3><p>Kubelet periodically downloads a file specified by <code>--manifest-url=&lt;URL&gt;</code> argument
and interprets it as a JSON/YAML file that contains Pod definitions.
Similar to how <a href="#configuration-files">filesystem-hosted manifests</a> work, the kubelet
refetches the manifest on a schedule. If there are changes to the list of static
Pods, the kubelet applies them.</p><p>To use this approach:</p><ol><li><p>Create a YAML file and store it on a web server so that you can pass the URL of that file to the kubelet.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>static-web<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>role</span>:<span> </span>myrole<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>web<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>web<span>
</span></span></span><span><span><span>          </span><span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>          </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span></code></pre></div></li><li><p>Configure the kubelet on your selected node to use this web manifest by
running it with <code>--manifest-url=&lt;manifest-url&gt;</code>.
On Fedora, edit <code>/etc/kubernetes/kubelet</code> to include this line:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>KUBELET_ARGS</span><span>=</span><span>"--cluster-dns=10.254.0.10 --cluster-domain=kube.local --manifest-url=&lt;manifest-url&gt;"</span>
</span></span></code></pre></div></li><li><p>Restart the kubelet. On Fedora, you would run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this command on the node where the kubelet is running</span>
</span></span><span><span>systemctl restart kubelet
</span></span></code></pre></div></li></ol><h2 id="behavior-of-static-pods">Observe static pod behavior</h2><p>When the kubelet starts, it automatically starts all defined static Pods. As you have
defined a static Pod and restarted the kubelet, the new static Pod should
already be running.</p><p>You can view running containers (including static Pods) by running (on the node):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this command on the node where the kubelet is running</span>
</span></span><span><span>crictl ps
</span></span></code></pre></div><p>The output might be something like:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>CONTAINER       IMAGE                                 CREATED           STATE      NAME    ATTEMPT    POD ID
</span></span></span><span><span><span>129fd7d382018   docker.io/library/nginx@sha256:...    11 minutes ago    Running    web     0          34533c6729106
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>crictl</code> outputs the image URI and SHA-256 checksum. <code>NAME</code> will look more like:
<code>docker.io/library/nginx@sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31</code>.</div><p>You can see the mirror Pod on the API server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME                  READY   STATUS    RESTARTS        AGE
</span></span></span><span><span><span>static-web-my-node1   1/1     Running   0               2m
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure the kubelet has permission to create the mirror Pod in the API server.
If not, the creation request is rejected by the API server.</div><p><a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">Labels</a> from the static Pod are
propagated into the mirror Pod. You can use those labels as normal via
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selectors</a>, etc.</p><p>If you try to use <code>kubectl</code> to delete the mirror Pod from the API server,
the kubelet <em>doesn't</em> remove the static Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod static-web-my-node1
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>pod "static-web-my-node1" deleted
</span></span></span></code></pre></div><p>You can see that the Pod is still running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAME                  READY   STATUS    RESTARTS   AGE
</span></span></span><span><span><span>static-web-my-node1   1/1     Running   0          4s
</span></span></span></code></pre></div><p>Back on your node where the kubelet is running, you can try to stop the container manually.
You'll see that, after a time, the kubelet will notice and will restart the Pod
automatically:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run these commands on the node where the kubelet is running</span>
</span></span><span><span>crictl stop 129fd7d382018 <span># replace with the ID of your container</span>
</span></span><span><span>sleep <span>20</span>
</span></span><span><span>crictl ps
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>CONTAINER       IMAGE                                 CREATED           STATE      NAME    ATTEMPT    POD ID
</span></span></span><span><span><span>89db4553e1eeb   docker.io/library/nginx@sha256:...    19 seconds ago    Running    web     1          34533c6729106
</span></span></span></code></pre></div><p>Once you identify the right container, you can get the logs for that container with <code>crictl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run these commands on the node where the container is running</span>
</span></span><span><span>crictl logs &lt;container_id&gt;
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>10.240.0.48 - - [16/Nov/2022:12:45:49 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
</span></span></span><span><span><span>10.240.0.48 - - [16/Nov/2022:12:45:50 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
</span></span></span><span><span><span>10.240.0.48 - - [16/Nove/2022:12:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
</span></span></span></code></pre></div><p>To find more about how to debug using <code>crictl</code>, please visit
<a href="/docs/tasks/debug/debug-cluster/crictl/"><em>Debugging Kubernetes nodes with crictl</em></a>.</p><h2 id="dynamic-addition-and-removal-of-static-pods">Dynamic addition and removal of static pods</h2><p>The running kubelet periodically scans the configured directory
(<code>/etc/kubernetes/manifests</code> in our example) for changes and
adds/removes Pods as files appear/disappear in this directory.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This assumes you are using filesystem-hosted static Pod configuration</span>
</span></span><span><span><span># Run these commands on the node where the container is running</span>
</span></span><span><span><span>#</span>
</span></span><span><span>mv /etc/kubernetes/manifests/static-web.yaml /tmp
</span></span><span><span>sleep <span>20</span>
</span></span><span><span>crictl ps
</span></span><span><span><span># You see that no nginx container is running</span>
</span></span><span><span>mv /tmp/static-web.yaml  /etc/kubernetes/manifests/
</span></span><span><span>sleep <span>20</span>
</span></span><span><span>crictl ps
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>CONTAINER       IMAGE                                 CREATED           STATE      NAME    ATTEMPT    POD ID
</span></span></span><span><span><span>f427638871c35   docker.io/library/nginx@sha256:...    19 seconds ago    Running    web     1          34533c6729106
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/setup-tools/kubeadm/implementation-details/#generate-static-pod-manifests-for-control-plane-components">Generate static Pod manifests for control plane components</a></li><li><a href="/docs/reference/setup-tools/kubeadm/implementation-details/#generate-static-pod-manifest-for-local-etcd">Generate static Pod manifest for local etcd</a></li><li><a href="/docs/tasks/debug/debug-cluster/crictl/">Debugging Kubernetes nodes with <code>crictl</code></a></li><li><a href="https://github.com/kubernetes-sigs/cri-tools">Learn more about <code>crictl</code></a></li><li><a href="/docs/reference/tools/map-crictl-dockercli/">Map <code>docker</code> CLI commands to <code>crictl</code></a></li><li><a href="/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">Set up etcd instances as static pods managed by a kubelet</a></li></ul></div></div><div><div class="td-content"><h1>Translate a Docker Compose File to Kubernetes Resources</h1><p>What's Kompose? It's a conversion tool for all things compose (namely Docker Compose) to container orchestrators (Kubernetes or OpenShift).</p><p>More information can be found on the Kompose website at <a href="https://kompose.io/">https://kompose.io/</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="install-kompose">Install Kompose</h2><p>We have multiple ways to install Kompose. Our preferred method is downloading the binary from the latest GitHub release.</p><ul class="nav nav-tabs" id="install-ways"><li class="nav-item"><a class="nav-link active" href="#install-ways-0">GitHub download</a></li><li class="nav-item"><a class="nav-link" href="#install-ways-1">Build from source</a></li><li class="nav-item"><a class="nav-link" href="#install-ways-2">Homebrew (macOS)</a></li></ul><div class="tab-content" id="install-ways"><div id="install-ways-0" class="tab-pane show active"><p><p>Kompose is released via GitHub on a three-week cycle, you can see all current releases on the <a href="https://github.com/kubernetes/kompose/releases">GitHub release page</a>.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span># Linux</span>
</span></span><span><span>curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-linux-amd64 -o kompose
</span></span><span><span>
</span></span><span><span><span># macOS</span>
</span></span><span><span>curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-darwin-amd64 -o kompose
</span></span><span><span>
</span></span><span><span><span># Windows</span>
</span></span><span><span>curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-windows-amd64.exe -o kompose.exe
</span></span><span><span>
</span></span><span><span>chmod +x kompose
</span></span><span><span>sudo mv ./kompose /usr/local/bin/kompose
</span></span></code></pre></div><p>Alternatively, you can download the <a href="https://github.com/kubernetes/kompose/releases">tarball</a>.</p></p></div><div id="install-ways-1" class="tab-pane"><p><p>Installing using <code>go get</code> pulls from the master branch with the latest development changes.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>go get -u github.com/kubernetes/kompose
</span></span></code></pre></div></p></div><div id="install-ways-2" class="tab-pane"><p><p>On macOS you can install the latest release via <a href="https://brew.sh">Homebrew</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>brew install kompose
</span></span></code></pre></div></p></div></div><h2 id="use-kompose">Use Kompose</h2><p>In a few steps, we'll take you from Docker Compose to Kubernetes. All
you need is an existing <code>docker-compose.yml</code> file.</p><ol><li><p>Go to the directory containing your <code>docker-compose.yml</code> file. If you don't have one, test using this one.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>
</span></span></span><span><span><span></span><span>services</span>:<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>redis-leader</span>:<span>
</span></span></span><span><span><span>    </span><span>container_name</span>:<span> </span>redis-leader<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"6379"</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>redis-replica</span>:<span>
</span></span></span><span><span><span>    </span><span>container_name</span>:<span> </span>redis-replica<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"6379"</span><span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>redis-server --replicaof redis-leader 6379 --dir /tmp<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>web</span>:<span>
</span></span></span><span><span><span>    </span><span>container_name</span>:<span> </span>web<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>quay.io/kompose/web<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"8080:8080"</span><span>
</span></span></span><span><span><span>    </span><span>environment</span>:<span>
</span></span></span><span><span><span>      </span>- GET_HOSTS_FROM=dns<span>
</span></span></span><span><span><span>    </span><span>labels</span>:<span>
</span></span></span><span><span><span>      </span><span>kompose.service.type</span>:<span> </span>LoadBalancer<span>
</span></span></span></code></pre></div></li><li><p>To convert the <code>docker-compose.yml</code> file to files that you can use with
<code>kubectl</code>, run <code>kompose convert</code> and then <code>kubectl apply -f &lt;output file&gt;</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kompose convert
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">INFO Kubernetes file "redis-leader-service.yaml" created
INFO Kubernetes file "redis-replica-service.yaml" created
INFO Kubernetes file "web-tcp-service.yaml" created
INFO Kubernetes file "redis-leader-deployment.yaml" created
INFO Kubernetes file "redis-replica-deployment.yaml" created
INFO Kubernetes file "web-deployment.yaml" created
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span> kubectl apply -f web-tcp-service.yaml,redis-leader-service.yaml,redis-replica-service.yaml,web-deployment.yaml,redis-leader-deployment.yaml,redis-replica-deployment.yaml
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">deployment.apps/redis-leader created
deployment.apps/redis-replica created
deployment.apps/web created
service/redis-leader created
service/redis-replica created
service/web-tcp created
</code></pre><p>Your deployments are running in Kubernetes.</p></li><li><p>Access your application.</p><p>If you're already using <code>minikube</code> for your development process:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>minikube service web-tcp
</span></span></code></pre></div><p>Otherwise, let's look up what IP your service is using!</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl describe svc web-tcp
</span></span></code></pre></div><pre tabindex="0"><code class="language-none"> Name:                     web-tcp
 Namespace:                default
 Labels:                   io.kompose.service=web-tcp
 Annotations:              kompose.cmd: kompose convert
                           kompose.service.type: LoadBalancer
                           kompose.version: 1.33.0 (3ce457399)
 Selector:                 io.kompose.service=web
 Type:                     LoadBalancer
 IP Family Policy:         SingleStack
 IP Families:              IPv4
 IP:                       10.102.30.3
 IPs:                      10.102.30.3
 Port:                     8080  8080/TCP
 TargetPort:               8080/TCP
 NodePort:                 8080  31624/TCP
 Endpoints:                10.244.0.5:8080
 Session Affinity:         None
 External Traffic Policy:  Cluster
 Events:                   &lt;none&gt;
</code></pre><p>If you're using a cloud provider, your IP will be listed next to <code>LoadBalancer Ingress</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>curl http://192.0.2.89
</span></span></code></pre></div></li><li><p>Clean-up.</p><p>After you are finished testing out the example application deployment, simply run the following command in your shell to delete the
resources used.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete -f web-tcp-service.yaml,redis-leader-service.yaml,redis-replica-service.yaml,web-deployment.yaml,redis-leader-deployment.yaml,redis-replica-deployment.yaml
</span></span></code></pre></div></li></ol><h2 id="user-guide">User Guide</h2><ul><li>CLI<ul><li><a href="#kompose-convert"><code>kompose convert</code></a></li></ul></li><li>Documentation<ul><li><a href="#alternative-conversions">Alternative Conversions</a></li><li><a href="#labels">Labels</a></li><li><a href="#restart">Restart</a></li><li><a href="#docker-compose-versions">Docker Compose Versions</a></li></ul></li></ul><p>Kompose has support for two providers: OpenShift and Kubernetes.
You can choose a targeted provider using global option <code>--provider</code>. If no provider is specified, Kubernetes is set by default.</p><h2 id="kompose-convert"><code>kompose convert</code></h2><p>Kompose supports conversion of V1, V2, and V3 Docker Compose files into Kubernetes and OpenShift objects.</p><h3 id="kubernetes-kompose-convert-example">Kubernetes <code>kompose convert</code> example</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kompose --file docker-voting.yml convert
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">WARN Unsupported key networks - ignoring
WARN Unsupported key build - ignoring
INFO Kubernetes file "worker-svc.yaml" created
INFO Kubernetes file "db-svc.yaml" created
INFO Kubernetes file "redis-svc.yaml" created
INFO Kubernetes file "result-svc.yaml" created
INFO Kubernetes file "vote-svc.yaml" created
INFO Kubernetes file "redis-deployment.yaml" created
INFO Kubernetes file "result-deployment.yaml" created
INFO Kubernetes file "vote-deployment.yaml" created
INFO Kubernetes file "worker-deployment.yaml" created
INFO Kubernetes file "db-deployment.yaml" created
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ls
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">db-deployment.yaml  docker-compose.yml         docker-gitlab.yml  redis-deployment.yaml  result-deployment.yaml  vote-deployment.yaml  worker-deployment.yaml
db-svc.yaml         docker-voting.yml          redis-svc.yaml     result-svc.yaml        vote-svc.yaml           worker-svc.yaml
</code></pre><p>You can also provide multiple docker-compose files at the same time:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kompose -f docker-compose.yml -f docker-guestbook.yml convert
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">INFO Kubernetes file "frontend-service.yaml" created         
INFO Kubernetes file "mlbparks-service.yaml" created         
INFO Kubernetes file "mongodb-service.yaml" created          
INFO Kubernetes file "redis-master-service.yaml" created     
INFO Kubernetes file "redis-slave-service.yaml" created      
INFO Kubernetes file "frontend-deployment.yaml" created      
INFO Kubernetes file "mlbparks-deployment.yaml" created      
INFO Kubernetes file "mongodb-deployment.yaml" created       
INFO Kubernetes file "mongodb-claim0-persistentvolumeclaim.yaml" created
INFO Kubernetes file "redis-master-deployment.yaml" created  
INFO Kubernetes file "redis-slave-deployment.yaml" created   
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ls
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">mlbparks-deployment.yaml  mongodb-service.yaml                       redis-slave-service.jsonmlbparks-service.yaml  
frontend-deployment.yaml  mongodb-claim0-persistentvolumeclaim.yaml  redis-master-service.yaml
frontend-service.yaml     mongodb-deployment.yaml                    redis-slave-deployment.yaml
redis-master-deployment.yaml
</code></pre><p>When multiple docker-compose files are provided the configuration is merged. Any configuration that is common will be overridden by subsequent file.</p><h3 id="openshift-kompose-convert-example">OpenShift <code>kompose convert</code> example</h3><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kompose --provider openshift --file docker-voting.yml convert
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">WARN [worker] Service cannot be created because of missing port.
INFO OpenShift file "vote-service.yaml" created             
INFO OpenShift file "db-service.yaml" created               
INFO OpenShift file "redis-service.yaml" created            
INFO OpenShift file "result-service.yaml" created           
INFO OpenShift file "vote-deploymentconfig.yaml" created    
INFO OpenShift file "vote-imagestream.yaml" created         
INFO OpenShift file "worker-deploymentconfig.yaml" created  
INFO OpenShift file "worker-imagestream.yaml" created       
INFO OpenShift file "db-deploymentconfig.yaml" created      
INFO OpenShift file "db-imagestream.yaml" created           
INFO OpenShift file "redis-deploymentconfig.yaml" created   
INFO OpenShift file "redis-imagestream.yaml" created        
INFO OpenShift file "result-deploymentconfig.yaml" created  
INFO OpenShift file "result-imagestream.yaml" created  
</code></pre><p>It also supports creating buildconfig for build directive in a service. By default, it uses the remote repo for the current git branch as the source repo, and the current branch as the source branch for the build. You can specify a different source repo and branch using <code>--build-repo</code> and <code>--build-branch</code> options respectively.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kompose --provider openshift --file buildconfig/docker-compose.yml convert
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">WARN [foo] Service cannot be created because of missing port.
INFO OpenShift Buildconfig using git@github.com:rtnpro/kompose.git::master as source.
INFO OpenShift file "foo-deploymentconfig.yaml" created     
INFO OpenShift file "foo-imagestream.yaml" created          
INFO OpenShift file "foo-buildconfig.yaml" created
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you are manually pushing the OpenShift artifacts using <code>oc create -f</code>, you need to ensure that you push the imagestream artifact before the buildconfig artifact, to workaround this OpenShift issue: <a href="https://github.com/openshift/origin/issues/4518">https://github.com/openshift/origin/issues/4518</a> .</div><h2 id="alternative-conversions">Alternative Conversions</h2><p>The default <code>kompose</code> transformation will generate Kubernetes <a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a> and <a href="/docs/concepts/services-networking/service/">Services</a>, in yaml format. You have alternative option to generate json with <code>-j</code>. Also, you can alternatively generate <a href="/docs/concepts/workloads/controllers/replicationcontroller/">Replication Controllers</a> objects, <a href="/docs/concepts/workloads/controllers/daemonset/">Daemon Sets</a>, or <a href="https://github.com/helm/helm">Helm</a> charts.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kompose convert -j
</span></span><span><span>INFO Kubernetes file <span>"redis-svc.json"</span> created
</span></span><span><span>INFO Kubernetes file <span>"web-svc.json"</span> created
</span></span><span><span>INFO Kubernetes file <span>"redis-deployment.json"</span> created
</span></span><span><span>INFO Kubernetes file <span>"web-deployment.json"</span> created
</span></span></code></pre></div><p>The <code>*-deployment.json</code> files contain the Deployment objects.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kompose convert --replication-controller
</span></span><span><span>INFO Kubernetes file <span>"redis-svc.yaml"</span> created
</span></span><span><span>INFO Kubernetes file <span>"web-svc.yaml"</span> created
</span></span><span><span>INFO Kubernetes file <span>"redis-replicationcontroller.yaml"</span> created
</span></span><span><span>INFO Kubernetes file <span>"web-replicationcontroller.yaml"</span> created
</span></span></code></pre></div><p>The <code>*-replicationcontroller.yaml</code> files contain the Replication Controller objects. If you want to specify replicas (default is 1), use <code>--replicas</code> flag: <code>kompose convert --replication-controller --replicas 3</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kompose convert --daemon-set
</span></span><span><span>INFO Kubernetes file <span>"redis-svc.yaml"</span> created
</span></span><span><span>INFO Kubernetes file <span>"web-svc.yaml"</span> created
</span></span><span><span>INFO Kubernetes file <span>"redis-daemonset.yaml"</span> created
</span></span><span><span>INFO Kubernetes file <span>"web-daemonset.yaml"</span> created
</span></span></code></pre></div><p>The <code>*-daemonset.yaml</code> files contain the DaemonSet objects.</p><p>If you want to generate a Chart to be used with <a href="https://github.com/kubernetes/helm">Helm</a> run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kompose convert -c
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">INFO Kubernetes file "web-svc.yaml" created
INFO Kubernetes file "redis-svc.yaml" created
INFO Kubernetes file "web-deployment.yaml" created
INFO Kubernetes file "redis-deployment.yaml" created
chart created in "./docker-compose/"
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>tree docker-compose/
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">docker-compose
&#9500;&#9472;&#9472; Chart.yaml
&#9500;&#9472;&#9472; README.md
&#9492;&#9472;&#9472; templates
    &#9500;&#9472;&#9472; redis-deployment.yaml
    &#9500;&#9472;&#9472; redis-svc.yaml
    &#9500;&#9472;&#9472; web-deployment.yaml
    &#9492;&#9472;&#9472; web-svc.yaml
</code></pre><p>The chart structure is aimed at providing a skeleton for building your Helm charts.</p><h2 id="labels">Labels</h2><p><code>kompose</code> supports Kompose-specific labels within the <code>docker-compose.yml</code> file in order to explicitly define a service's behavior upon conversion.</p><ul><li><p><code>kompose.service.type</code> defines the type of service to be created.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>version</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span></span><span>services</span>:<span>
</span></span></span><span><span><span>  </span><span>nginx</span>:<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>dockerfile</span>:<span> </span>foobar<span>
</span></span></span><span><span><span>    </span><span>build</span>:<span> </span>./foobar<span>
</span></span></span><span><span><span>    </span><span>cap_add</span>:<span>
</span></span></span><span><span><span>      </span>- ALL<span>
</span></span></span><span><span><span>    </span><span>container_name</span>:<span> </span>foobar<span>
</span></span></span><span><span><span>    </span><span>labels</span>:<span>
</span></span></span><span><span><span>      </span><span>kompose.service.type</span>:<span> </span>nodeport<span>
</span></span></span></code></pre></div></li><li><p><code>kompose.service.expose</code> defines if the service needs to be made accessible from outside the cluster or not. If the value is set to "true", the provider sets the endpoint automatically, and for any other value, the value is set as the hostname. If multiple ports are defined in a service, the first one is chosen to be the exposed.</p><ul><li>For the Kubernetes provider, an ingress resource is created and it is assumed that an ingress controller has already been configured.</li><li>For the OpenShift provider, a route is created.</li></ul><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>version</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span></span><span>services</span>:<span>
</span></span></span><span><span><span>  </span><span>web</span>:<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>tuna/docker-counter23<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>"5000:5000"</span><span>
</span></span></span><span><span><span>    </span><span>links</span>:<span>
</span></span></span><span><span><span>    </span>- redis<span>
</span></span></span><span><span><span>    </span><span>labels</span>:<span>
</span></span></span><span><span><span>      </span><span>kompose.service.expose</span>:<span> </span><span>"counter.example.com"</span><span>
</span></span></span><span><span><span>  </span><span>redis</span>:<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis:3.0<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>"6379"</span><span>
</span></span></span></code></pre></div></li></ul><p>The currently supported options are:</p><table><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>kompose.service.type</td><td>nodeport / clusterip / loadbalancer</td></tr><tr><td>kompose.service.expose</td><td>true / hostname</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>kompose.service.type</code> label should be defined with <code>ports</code> only, otherwise <code>kompose</code> will fail.</div><h2 id="restart">Restart</h2><p>If you want to create normal pods without controllers you can use <code>restart</code> construct of docker-compose to define that. Follow table below to see what happens on the <code>restart</code> value.</p><table><thead><tr><th><code>docker-compose</code> <code>restart</code></th><th>object created</th><th>Pod <code>restartPolicy</code></th></tr></thead><tbody><tr><td><code>""</code></td><td>controller object</td><td><code>Always</code></td></tr><tr><td><code>always</code></td><td>controller object</td><td><code>Always</code></td></tr><tr><td><code>on-failure</code></td><td>Pod</td><td><code>OnFailure</code></td></tr><tr><td><code>no</code></td><td>Pod</td><td><code>Never</code></td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The controller object could be <code>deployment</code> or <code>replicationcontroller</code>.</div><p>For example, the <code>pival</code> service will become pod down here. This container calculated value of <code>pi</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>version</span>:<span> </span><span>'2'</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>services</span>:<span>
</span></span></span><span><span><span>  </span><span>pival</span>:<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>perl<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"perl"</span>,<span>  </span><span>"-Mbignum=bpi"</span>,<span> </span><span>"-wle"</span>,<span> </span><span>"print bpi(2000)"</span>]<span>
</span></span></span><span><span><span>    </span><span>restart</span>:<span> </span><span>"on-failure"</span><span>
</span></span></span></code></pre></div><h3 id="warning-about-deployment-configurations">Warning about Deployment Configurations</h3><p>If the Docker Compose file has a volume specified for a service, the Deployment (Kubernetes) or DeploymentConfig (OpenShift) strategy is changed to "Recreate" instead of "RollingUpdate" (default). This is done to avoid multiple instances of a service from accessing a volume at the same time.</p><p>If the Docker Compose file has service name with <code>_</code> in it (for example, <code>web_service</code>), then it will be replaced by <code>-</code> and the service name will be renamed accordingly (for example, <code>web-service</code>). Kompose does this because "Kubernetes" doesn't allow <code>_</code> in object name.</p><p>Please note that changing service name might break some <code>docker-compose</code> files.</p><h2 id="docker-compose-versions">Docker Compose Versions</h2><p>Kompose supports Docker Compose versions: 1, 2 and 3. We have limited support on versions 2.1 and 3.2 due to their experimental nature.</p><p>A full list on compatibility between all three versions is listed in our <a href="https://github.com/kubernetes/kompose/blob/master/docs/conversion.md">conversion document</a> including a list of all incompatible Docker Compose keys.</p></div></div><div><div class="td-content"><h1>Enforce Pod Security Standards by Configuring the Built-in Admission Controller</h1><p>Kubernetes provides a built-in <a href="/docs/reference/access-authn-authz/admission-controllers/#podsecurity">admission controller</a>
to enforce the <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>.
You can configure this admission controller to set cluster-wide defaults and <a href="/docs/concepts/security/pod-security-admission/#exemptions">exemptions</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>Following an alpha release in Kubernetes v1.22,
Pod Security Admission became available by default in Kubernetes v1.23, as
a beta. From version 1.25 onwards, Pod Security Admission is generally
available.</p><p>To check the version, enter <code>kubectl version</code>.</p><p>If you are not running Kubernetes 1.34, you can switch
to viewing this page in the documentation for the Kubernetes version that you
are running.</p><h2 id="configure-the-admission-controller">Configure the Admission Controller</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>pod-security.admission.config.k8s.io/v1</code> configuration requires v1.25+.
For v1.23 and v1.24, use <a href="https://v1-24.docs.kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">v1beta1</a>.
For v1.22, use <a href="https://v1-22.docs.kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">v1alpha1</a>.</div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>AdmissionConfiguration<span>
</span></span></span><span><span><span></span><span>plugins</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>PodSecurity<span>
</span></span></span><span><span><span>  </span><span>configuration</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>pod-security.admission.config.k8s.io/v1<span> </span><span># see compatibility note</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>PodSecurityConfiguration<span>
</span></span></span><span><span><span>    </span><span># Defaults applied when a mode label is not set.</span><span>
</span></span></span><span><span><span>    </span><span>#</span><span>
</span></span></span><span><span><span>    </span><span># Level label values must be one of:</span><span>
</span></span></span><span><span><span>    </span><span># - "privileged" (default)</span><span>
</span></span></span><span><span><span>    </span><span># - "baseline"</span><span>
</span></span></span><span><span><span>    </span><span># - "restricted"</span><span>
</span></span></span><span><span><span>    </span><span>#</span><span>
</span></span></span><span><span><span>    </span><span># Version label values must be one of:</span><span>
</span></span></span><span><span><span>    </span><span># - "latest" (default) </span><span>
</span></span></span><span><span><span>    </span><span># - specific version like "v1.34"</span><span>
</span></span></span><span><span><span>    </span><span>defaults</span>:<span>
</span></span></span><span><span><span>      </span><span>enforce</span>:<span> </span><span>"privileged"</span><span>
</span></span></span><span><span><span>      </span><span>enforce-version</span>:<span> </span><span>"latest"</span><span>
</span></span></span><span><span><span>      </span><span>audit</span>:<span> </span><span>"privileged"</span><span>
</span></span></span><span><span><span>      </span><span>audit-version</span>:<span> </span><span>"latest"</span><span>
</span></span></span><span><span><span>      </span><span>warn</span>:<span> </span><span>"privileged"</span><span>
</span></span></span><span><span><span>      </span><span>warn-version</span>:<span> </span><span>"latest"</span><span>
</span></span></span><span><span><span>    </span><span>exemptions</span>:<span>
</span></span></span><span><span><span>      </span><span># Array of authenticated usernames to exempt.</span><span>
</span></span></span><span><span><span>      </span><span>usernames</span>:<span> </span>[]<span>
</span></span></span><span><span><span>      </span><span># Array of runtime class names to exempt.</span><span>
</span></span></span><span><span><span>      </span><span>runtimeClasses</span>:<span> </span>[]<span>
</span></span></span><span><span><span>      </span><span># Array of namespaces to exempt.</span><span>
</span></span></span><span><span><span>      </span><span>namespaces</span>:<span> </span>[]<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The above manifest needs to be specified via the <code>--admission-control-config-file</code> to kube-apiserver.</div></div></div><div><div class="td-content"><h1>Enforce Pod Security Standards with Namespace Labels</h1><p>Namespaces can be labeled to enforce the <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>. The three policies
<a href="/docs/concepts/security/pod-security-standards/#privileged">privileged</a>, <a href="/docs/concepts/security/pod-security-standards/#baseline">baseline</a>
and <a href="/docs/concepts/security/pod-security-standards/#restricted">restricted</a> broadly cover the security spectrum
and are implemented by the <a href="/docs/concepts/security/pod-security-admission/">Pod Security</a> <a class="glossary-tooltip" title="A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object." href="/docs/reference/access-authn-authz/admission-controllers/" target="_blank">admission controller</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>Pod Security Admission was available by default in Kubernetes v1.23, as
a beta. From version 1.25 onwards, Pod Security Admission is generally
available.</p><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="requiring-the-baseline-pod-security-standard-with-namespace-labels">Requiring the <code>baseline</code> Pod Security Standard with namespace labels</h2><p>This manifest defines a Namespace <code>my-baseline-namespace</code> that:</p><ul><li><em>Blocks</em> any pods that don't satisfy the <code>baseline</code> policy requirements.</li><li>Generates a user-facing warning and adds an audit annotation to any created pod that does not
meet the <code>restricted</code> policy requirements.</li><li>Pins the versions of the <code>baseline</code> and <code>restricted</code> policies to v1.34.</li></ul><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Namespace<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-baseline-namespace<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>pod-security.kubernetes.io/enforce</span>:<span> </span>baseline<span>
</span></span></span><span><span><span>    </span><span>pod-security.kubernetes.io/enforce-version</span>:<span> </span>v1.34<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span># We are setting these to our _desired_ `enforce` level.</span><span>
</span></span></span><span><span><span>    </span><span>pod-security.kubernetes.io/audit</span>:<span> </span>restricted<span>
</span></span></span><span><span><span>    </span><span>pod-security.kubernetes.io/audit-version</span>:<span> </span>v1.34<span>
</span></span></span><span><span><span>    </span><span>pod-security.kubernetes.io/warn</span>:<span> </span>restricted<span>
</span></span></span><span><span><span>    </span><span>pod-security.kubernetes.io/warn-version</span>:<span> </span>v1.34<span>
</span></span></span></code></pre></div><h2 id="add-labels-to-existing-namespaces-with-kubectl-label">Add labels to existing namespaces with <code>kubectl label</code></h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When an <code>enforce</code> policy (or version) label is added or changed, the admission plugin will test
each pod in the namespace against the new policy. Violations are returned to the user as warnings.</div><p>It is helpful to apply the <code>--dry-run</code> flag when initially evaluating security profile changes for
namespaces. The Pod Security Standard checks will still be run in <em>dry run</em> mode, giving you
information about how the new policy would treat existing pods, without actually updating a policy.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label --dry-run<span>=</span>server --overwrite ns --all <span>\
</span></span></span><span><span><span></span>    pod-security.kubernetes.io/enforce<span>=</span>baseline
</span></span></code></pre></div><h3 id="applying-to-all-namespaces">Applying to all namespaces</h3><p>If you're just getting started with the Pod Security Standards, a suitable first step would be to
configure all namespaces with audit annotations for a stricter level such as <code>baseline</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label --overwrite ns --all <span>\
</span></span></span><span><span><span></span>  pod-security.kubernetes.io/audit<span>=</span>baseline <span>\
</span></span></span><span><span><span></span>  pod-security.kubernetes.io/warn<span>=</span>baseline
</span></span></code></pre></div><p>Note that this is not setting an enforce level, so that namespaces that haven't been explicitly
evaluated can be distinguished. You can list namespaces without an explicitly set enforce level
using this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get namespaces --selector<span>=</span><span>'!pod-security.kubernetes.io/enforce'</span>
</span></span></code></pre></div><h3 id="applying-to-a-single-namespace">Applying to a single namespace</h3><p>You can update a specific namespace as well. This command adds the <code>enforce=restricted</code>
policy to <code>my-existing-namespace</code>, pinning the restricted policy version to v1.34.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label --overwrite ns my-existing-namespace <span>\
</span></span></span><span><span><span></span>  pod-security.kubernetes.io/enforce<span>=</span>restricted <span>\
</span></span></span><span><span><span></span>  pod-security.kubernetes.io/enforce-version<span>=</span>v1.34
</span></span></code></pre></div></div></div><div><div class="td-content"><h1>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</h1><p>This page describes the process of migrating from PodSecurityPolicies to the built-in PodSecurity
admission controller. This can be done effectively using a combination of dry-run and <code>audit</code> and
<code>warn</code> modes, although this becomes harder if mutating PSPs are used.</p><h2 id="before-you-begin">Before you begin</h2><p>Your Kubernetes server must be at or later than version v1.22.</p><p>To check the version, enter <code>kubectl version</code>.</p><p>If you are currently running a version of Kubernetes other than
1.34, you may want to switch to viewing this
page in the documentation for the version of Kubernetes that you
are actually running.</p><p>This page assumes you are already familiar with the basic <a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission</a>
concepts.</p><h2 id="overall-approach">Overall approach</h2><p>There are multiple strategies you can take for migrating from PodSecurityPolicy to Pod Security
Admission. The following steps are one possible migration path, with a goal of minimizing both the
risks of a production outage and of a security gap.</p><ol start="0"><li>Decide whether Pod Security Admission is the right fit for your use case.</li><li>Review namespace permissions</li><li>Simplify &amp; standardize PodSecurityPolicies</li><li>Update namespaces<ol><li>Identify an appropriate Pod Security level</li><li>Verify the Pod Security level</li><li>Enforce the Pod Security level</li><li>Bypass PodSecurityPolicy</li></ol></li><li>Review namespace creation processes</li><li>Disable PodSecurityPolicy</li></ol><h2 id="is-psa-right-for-you">0. Decide whether Pod Security Admission is right for you</h2><p>Pod Security Admission was designed to meet the most common security needs out of the box, and to
provide a standard set of security levels across clusters. However, it is less flexible than
PodSecurityPolicy. Notably, the following features are supported by PodSecurityPolicy but not Pod
Security Admission:</p><ul><li><strong>Setting default security constraints</strong> - Pod Security Admission is a non-mutating admission
controller, meaning it won't modify pods before validating them. If you were relying on this
aspect of PSP, you will need to either modify your workloads to meet the Pod Security constraints,
or use a <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Mutating Admission Webhook</a>
to make those changes. See <a href="#simplify-psps">Simplify &amp; Standardize PodSecurityPolicies</a> below for more detail.</li><li><strong>Fine-grained control over policy definition</strong> - Pod Security Admission only supports
<a href="/docs/concepts/security/pod-security-standards/">3 standard levels</a>.
If you require more control over specific constraints, then you will need to use a
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Validating Admission Webhook</a>
to enforce those policies.</li><li><strong>Sub-namespace policy granularity</strong> - PodSecurityPolicy lets you bind different policies to
different Service Accounts or users, even within a single namespace. This approach has many
pitfalls and is not recommended, but if you require this feature anyway you will
need to use a 3rd party webhook instead. The exception to this is if you only need to completely exempt
specific users or <a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a>, in which case Pod
Security Admission does expose some
<a href="/docs/concepts/security/pod-security-admission/#exemptions">static configuration for exemptions</a>.</li></ul><p>Even if Pod Security Admission does not meet all of your needs it was designed to be <em>complementary</em>
to other policy enforcement mechanisms, and can provide a useful fallback running alongside other
admission webhooks.</p><h2 id="review-namespace-permissions">1. Review namespace permissions</h2><p>Pod Security Admission is controlled by <a href="/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces">labels on
namespaces</a>.
This means that anyone who can update (or patch or create) a namespace can also modify the Pod
Security level for that namespace, which could be used to bypass a more restrictive policy. Before
proceeding, ensure that only trusted, privileged users have these namespace permissions. It is not
recommended to grant these powerful permissions to users that shouldn't have elevated permissions,
but if you must you will need to use an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>
to place additional restrictions on setting Pod Security labels on Namespace objects.</p><h2 id="simplify-psps">2. Simplify &amp; standardize PodSecurityPolicies</h2><p>In this section, you will reduce mutating PodSecurityPolicies and remove options that are outside
the scope of the Pod Security Standards. You should make the changes recommended here to an offline
copy of the original PodSecurityPolicy being modified. The cloned PSP should have a different
name that is alphabetically before the original (for example, prepend a <code>0</code> to it). Do not create the
new policies in Kubernetes yet - that will be covered in the <a href="#psp-update-rollout">Rollout the updated
policies</a> section below.</p><h3 id="eliminate-mutating-fields">2.a. Eliminate purely mutating fields</h3><p>If a PodSecurityPolicy is mutating pods, then you could end up with pods that don't meet the Pod
Security level requirements when you finally turn PodSecurityPolicy off. In order to avoid this, you
should eliminate all PSP mutation prior to switching over. Unfortunately PSP does not cleanly
separate mutating &amp; validating fields, so this is not a straightforward migration.</p><p>You can start by eliminating the fields that are purely mutating, and don't have any bearing on the
validating policy. These fields (also listed in the
<a href="/docs/reference/access-authn-authz/psp-to-pod-security-standards/">Mapping PodSecurityPolicies to Pod Security Standards</a>
reference) are:</p><ul><li><code>.spec.defaultAllowPrivilegeEscalation</code></li><li><code>.spec.runtimeClass.defaultRuntimeClassName</code></li><li><code>.metadata.annotations['seccomp.security.alpha.kubernetes.io/defaultProfileName']</code></li><li><code>.metadata.annotations['apparmor.security.beta.kubernetes.io/defaultProfileName']</code></li><li><code>.spec.defaultAddCapabilities</code> - Although technically a mutating &amp; validating field, these should
be merged into <code>.spec.allowedCapabilities</code> which performs the same validation without mutation.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Removing these could result in workloads missing required configuration, and cause problems. See
<a href="#psp-update-rollout">Rollout the updated policies</a> below for advice on how to roll these changes
out safely.</div><h3 id="eliminate-non-standard-options">2.b. Eliminate options not covered by the Pod Security Standards</h3><p>There are several fields in PodSecurityPolicy that are not covered by the Pod Security Standards. If
you must enforce these options, you will need to supplement Pod Security Admission with an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>,
which is outside the scope of this guide.</p><p>First, you can remove the purely validating fields that the Pod Security Standards do not cover.
These fields (also listed in the
<a href="/docs/reference/access-authn-authz/psp-to-pod-security-standards/">Mapping PodSecurityPolicies to Pod Security Standards</a>
reference with "no opinion") are:</p><ul><li><code>.spec.allowedHostPaths</code></li><li><code>.spec.allowedFlexVolumes</code></li><li><code>.spec.allowedCSIDrivers</code></li><li><code>.spec.forbiddenSysctls</code></li><li><code>.spec.runtimeClass</code></li></ul><p>You can also remove the following fields, that are related to POSIX / UNIX group controls.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>If any of these use the <code>MustRunAs</code> strategy they may be mutating! Removing these could result in
workloads not setting the required groups, and cause problems. See
<a href="#psp-update-rollout">Rollout the updated policies</a> below for advice on how to roll these changes
out safely.</div><ul><li><code>.spec.runAsGroup</code></li><li><code>.spec.supplementalGroups</code></li><li><code>.spec.fsGroup</code></li></ul><p>The remaining mutating fields are required to properly support the Pod Security Standards, and will
need to be handled on a case-by-case basis later:</p><ul><li><code>.spec.requiredDropCapabilities</code> - Required to drop <code>ALL</code> for the Restricted profile.</li><li><code>.spec.seLinux</code> - (Only mutating with the <code>MustRunAs</code> rule) required to enforce the SELinux
requirements of the Baseline &amp; Restricted profiles.</li><li><code>.spec.runAsUser</code> - (Non-mutating with the <code>RunAsAny</code> rule) required to enforce <code>RunAsNonRoot</code> for
the Restricted profile.</li><li><code>.spec.allowPrivilegeEscalation</code> - (Only mutating if set to <code>false</code>) required for the Restricted
profile.</li></ul><h3 id="psp-update-rollout">2.c. Rollout the updated PSPs</h3><p>Next, you can rollout the updated policies to your cluster. You should proceed with caution, as
removing the mutating options may result in workloads missing required configuration.</p><p>For each updated PodSecurityPolicy:</p><ol><li>Identify pods running under the original PSP. This can be done using the <code>kubernetes.io/psp</code>
annotation. For example, using kubectl:<div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span>PSP_NAME</span><span>=</span><span>"original"</span> <span># Set the name of the PSP you're checking for</span>
</span></span><span><span>kubectl get pods --all-namespaces -o <span>jsonpath</span><span>=</span><span>"{range .items[?(@.metadata.annotations.kubernetes\.io\/psp=='</span><span>$PSP_NAME</span><span>')]}{.metadata.namespace} {.metadata.name}{'\n'}{end}"</span>
</span></span></code></pre></div></li><li>Compare these running pods against the original pod spec to determine whether PodSecurityPolicy
has modified the pod. For pods created by a <a href="/docs/concepts/workloads/controllers/">workload resource</a>
you can compare the pod with the PodTemplate in the controller resource. If any changes are
identified, the original Pod or PodTemplate should be updated with the desired configuration.
The fields to review are:<ul><li><code>.metadata.annotations['container.apparmor.security.beta.kubernetes.io/*']</code> (replace * with each container name)</li><li><code>.spec.runtimeClassName</code></li><li><code>.spec.securityContext.fsGroup</code></li><li><code>.spec.securityContext.seccompProfile</code></li><li><code>.spec.securityContext.seLinuxOptions</code></li><li><code>.spec.securityContext.supplementalGroups</code></li><li>On containers, under <code>.spec.containers[*]</code> and <code>.spec.initContainers[*]</code>:<ul><li><code>.securityContext.allowPrivilegeEscalation</code></li><li><code>.securityContext.capabilities.add</code></li><li><code>.securityContext.capabilities.drop</code></li><li><code>.securityContext.readOnlyRootFilesystem</code></li><li><code>.securityContext.runAsGroup</code></li><li><code>.securityContext.runAsNonRoot</code></li><li><code>.securityContext.runAsUser</code></li><li><code>.securityContext.seccompProfile</code></li><li><code>.securityContext.seLinuxOptions</code></li></ul></li></ul></li><li>Create the new PodSecurityPolicies. If any Roles or ClusterRoles are granting <code>use</code> on all PSPs
this could cause the new PSPs to be used instead of their mutating counter-parts.</li><li>Update your authorization to grant access to the new PSPs. In RBAC this means updating any Roles
or ClusterRoles that grant the <code>use</code> permission on the original PSP to also grant it to the
updated PSP.</li><li>Verify: after some soak time, rerun the command from step 1 to see if any pods are still using
the original PSPs. Note that pods need to be recreated after the new policies have been rolled
out before they can be fully verified.</li><li>(optional) Once you have verified that the original PSPs are no longer in use, you can delete
them.</li></ol><h2 id="update-namespaces">3. Update Namespaces</h2><p>The following steps will need to be performed on every namespace in the cluster. Commands referenced
in these steps use the <code>$NAMESPACE</code> variable to refer to the namespace being updated.</p><h3 id="identify-appropriate-level">3.a. Identify an appropriate Pod Security level</h3><p>Start reviewing the <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> and
familiarizing yourself with the 3 different levels.</p><p>There are several ways to choose a Pod Security level for your namespace:</p><ol><li><strong>By security requirements for the namespace</strong> - If you are familiar with the expected access
level for the namespace, you can choose an appropriate level based on those requirements, similar
to how one might approach this on a new cluster.</li><li><strong>By existing PodSecurityPolicies</strong> - Using the
<a href="/docs/reference/access-authn-authz/psp-to-pod-security-standards/">Mapping PodSecurityPolicies to Pod Security Standards</a>
reference you can map each
PSP to a Pod Security Standard level. If your PSPs aren't based on the Pod Security Standards, you
may need to decide between choosing a level that is at least as permissive as the PSP, and a
level that is at least as restrictive. You can see which PSPs are in use for pods in a given
namespace with this command:<div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get pods -n <span>$NAMESPACE</span> -o <span>jsonpath</span><span>=</span><span>"{.items[*].metadata.annotations.kubernetes\.io\/psp}"</span> | tr <span>" "</span> <span>"\n"</span> | sort -u
</span></span></code></pre></div></li><li><strong>By existing pods</strong> - Using the strategies under <a href="#verify-pss-level">Verify the Pod Security level</a>,
you can test out both the Baseline and Restricted levels to see
whether they are sufficiently permissive for existing workloads, and chose the least-privileged
valid level.</li></ol><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Options 2 &amp; 3 above are based on <em>existing</em> pods, and may miss workloads that aren't currently
running, such as CronJobs, scale-to-zero workloads, or other workloads that haven't rolled out.</div><h3 id="verify-pss-level">3.b. Verify the Pod Security level</h3><p>Once you have selected a Pod Security level for the namespace (or if you're trying several), it's a
good idea to test it out first (you can skip this step if using the Privileged level). Pod Security
includes several tools to help test and safely roll out profiles.</p><p>First, you can dry-run the policy, which will evaluate pods currently running in the namespace
against the applied policy, without making the new policy take effect:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span># $LEVEL is the level to dry-run, either "baseline" or "restricted".</span>
</span></span><span><span>kubectl label --dry-run<span>=</span>server --overwrite ns <span>$NAMESPACE</span> pod-security.kubernetes.io/enforce<span>=</span><span>$LEVEL</span>
</span></span></code></pre></div><p>This command will return a warning for any <em>existing</em> pods that are not valid under the proposed
level.</p><p>The second option is better for catching workloads that are not currently running: audit mode. When
running under audit-mode (as opposed to enforcing), pods that violate the policy level are recorded
in the audit logs, which can be reviewed later after some soak time, but are not forbidden. Warning
mode works similarly, but returns the warning to the user immediately. You can set the audit level
on a namespace with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl label --overwrite ns <span>$NAMESPACE</span> pod-security.kubernetes.io/audit<span>=</span><span>$LEVEL</span>
</span></span></code></pre></div><p>If either of these approaches yield unexpected violations, you will need to either update the
violating workloads to meet the policy requirements, or relax the namespace Pod Security level.</p><h3 id="enforce-pod-security-level">3.c. Enforce the Pod Security level</h3><p>When you are satisfied that the chosen level can safely be enforced on the namespace, you can update
the namespace to enforce the desired level:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl label --overwrite ns <span>$NAMESPACE</span> pod-security.kubernetes.io/enforce<span>=</span><span>$LEVEL</span>
</span></span></code></pre></div><h3 id="bypass-psp">3.d. Bypass PodSecurityPolicy</h3><p>Finally, you can effectively bypass PodSecurityPolicy at the namespace level by binding the fully
<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/privileged-psp.yaml">privileged PSP</a>
to all service
accounts in the namespace.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span># The following cluster-scoped commands are only needed once.</span>
</span></span><span><span>kubectl apply -f privileged-psp.yaml
</span></span><span><span>kubectl create clusterrole privileged-psp --verb use --resource podsecuritypolicies.policy --resource-name privileged
</span></span><span><span>
</span></span><span><span><span># Per-namespace disable</span>
</span></span><span><span>kubectl create -n <span>$NAMESPACE</span> rolebinding disable-psp --clusterrole privileged-psp --group system:serviceaccounts:<span>$NAMESPACE</span>
</span></span></code></pre></div><p>Since the privileged PSP is non-mutating, and the PSP admission controller always
prefers non-mutating PSPs, this will ensure that pods in this namespace are no longer being modified
or restricted by PodSecurityPolicy.</p><p>The advantage to disabling PodSecurityPolicy on a per-namespace basis like this is if a problem
arises you can easily roll the change back by deleting the RoleBinding. Just make sure the
pre-existing PodSecurityPolicies are still in place!</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span># Undo PodSecurityPolicy disablement.</span>
</span></span><span><span>kubectl delete -n <span>$NAMESPACE</span> rolebinding disable-psp
</span></span></code></pre></div><h2 id="review-namespace-creation-process">4. Review namespace creation processes</h2><p>Now that existing namespaces have been updated to enforce Pod Security Admission, you should ensure
that your processes and/or policies for creating new namespaces are updated to ensure that an
appropriate Pod Security profile is applied to new namespaces.</p><p>You can also statically configure the Pod Security admission controller to set a default enforce,
audit, and/or warn level for unlabeled namespaces. See
<a href="/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller">Configure the Admission Controller</a>
for more information.</p><h2 id="disable-psp">5. Disable PodSecurityPolicy</h2><p>Finally, you're ready to disable PodSecurityPolicy. To do so, you will need to modify the admission
configuration of the API server:
<a href="/docs/reference/access-authn-authz/admission-controllers/#how-do-i-turn-off-an-admission-controller">How do I turn off an admission controller?</a>.</p><p>To verify that the PodSecurityPolicy admission controller is no longer enabled, you can manually run
a test by impersonating a user without access to any PodSecurityPolicies (see the
<a href="/docs/concepts/security/pod-security-policy/#example">PodSecurityPolicy example</a>), or by verifying in
the API server logs. At startup, the API server outputs log lines listing the loaded admission
controller plugins:</p><pre tabindex="0"><code>I0218 00:59:44.903329      13 plugins.go:158] Loaded 16 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,ExtendedResourceToleration,PersistentVolumeLabel,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0218 00:59:44.903350      13 plugins.go:161] Loaded 14 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,DenyServiceExternalIPs,ValidatingAdmissionWebhook,ResourceQuota.
</code></pre><p>You should see <code>PodSecurity</code> (in the validating admission controllers), and neither list should
contain <code>PodSecurityPolicy</code>.</p><p>Once you are certain the PSP admission controller is disabled (and after sufficient soak time to be
confident you won't need to roll back), you are free to delete your PodSecurityPolicies and any
associated Roles, ClusterRoles, RoleBindings and ClusterRoleBindings (just make sure they don't
grant any other unrelated permissions).</p></div></div><div><div class="td-content"><h1>Monitoring, Logging, and Debugging</h1><div class="lead">Set up monitoring and logging to troubleshoot a cluster, or debug a containerized application.</div><p>Sometimes things go wrong. This guide helps you gather the relevant information and resolve issues. It has four sections:</p><ul><li><a href="/docs/tasks/debug/debug-application/">Debugging your application</a> - Useful
for users who are deploying code into Kubernetes and wondering why it is not working.</li><li><a href="/docs/tasks/debug/debug-cluster/">Debugging your cluster</a> - Useful
for cluster administrators and operators troubleshooting issues with the Kubernetes cluster itself.</li><li><a href="/docs/tasks/debug/logging/">Logging in Kubernetes</a> - Useful
for cluster administrators who want to set up and manage logging in Kubernetes.</li><li><a href="/docs/tasks/debug/monitoring/">Monitoring in Kubernetes</a> - Useful
for cluster administrators who want to enable monitoring in a Kubernetes cluster.</li></ul><p>You should also check the known issues for the <a href="https://github.com/kubernetes/kubernetes/releases">release</a>
you're using.</p><h2 id="getting-help">Getting help</h2><p>If your problem isn't answered by any of the guides above, there are variety of
ways for you to get help from the Kubernetes community.</p><h3 id="questions">Questions</h3><p>The documentation on this site has been structured to provide answers to a wide
range of questions. <a href="/docs/concepts/">Concepts</a> explain the Kubernetes
architecture and how each component works, while <a href="/docs/setup/">Setup</a> provides
practical instructions for getting started. <a href="/docs/tasks/">Tasks</a> show how to
accomplish commonly used tasks, and <a href="/docs/tutorials/">Tutorials</a> are more
comprehensive walkthroughs of real-world, industry-specific, or end-to-end
development scenarios. The <a href="/docs/reference/">Reference</a> section provides
detailed documentation on the <a href="/docs/reference/generated/kubernetes-api/v1.34/">Kubernetes API</a>
and command-line interfaces (CLIs), such as <a href="/docs/reference/kubectl/"><code>kubectl</code></a>.</p><h2 id="help-my-question-isn-t-covered-i-need-help-now">Help! My question isn't covered! I need help now!</h2><h3 id="stack-exchange">Stack Exchange, Stack Overflow, or Server Fault</h3><p>If you have questions related to <em>software development</em> for your containerized app,
you can ask those on <a href="https://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow</a>.</p><p>If you have Kubernetes questions related to <em>cluster management</em> or <em>configuration</em>,
you can ask those on
<a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault</a>.</p><p>There are also several more specific Stack Exchange network sites which might
be the right place to ask Kubernetes questions in areas such as
<a href="https://devops.stackexchange.com/questions/tagged/kubernetes">DevOps</a>,
<a href="https://softwareengineering.stackexchange.com/questions/tagged/kubernetes">Software Engineering</a>,
or <a href="https://security.stackexchange.com/questions/tagged/kubernetes">InfoSec</a>.</p><p>Someone else from the community may have already asked a similar question or
may be able to help with your problem.</p><p>The Kubernetes team will also monitor
<a href="https://stackoverflow.com/questions/tagged/kubernetes">posts tagged Kubernetes</a>.
If there aren't any existing questions that help, <strong>please ensure that your question
is <a href="https://stackoverflow.com/help/on-topic">on-topic on Stack Overflow</a>,
<a href="https://serverfault.com/help/on-topic">Server Fault</a>, or the Stack Exchange
Network site you're asking on</strong>, and read through the guidance on
<a href="https://stackoverflow.com/help/how-to-ask">how to ask a new question</a>,
before asking a new one!</p><h3 id="slack">Slack</h3><p>Many people from the Kubernetes community hang out on Kubernetes Slack in the <code>#kubernetes-users</code> channel.
Slack requires registration; you can <a href="https://slack.kubernetes.io">request an invitation</a>,
and registration is open to everyone). Feel free to come and ask any and all questions.
Once registered, access the <a href="https://kubernetes.slack.com">Kubernetes organisation in Slack</a>
via your web browser or via Slack's own dedicated app.</p><p>Once you are registered, browse the growing list of channels for various subjects of
interest. For example, people new to Kubernetes may also want to join the
<a href="https://kubernetes.slack.com/messages/kubernetes-novice"><code>#kubernetes-novice</code></a> channel. As another example, developers should join the
<a href="https://kubernetes.slack.com/messages/kubernetes-contributors"><code>#kubernetes-contributors</code></a> channel.</p><p>There are also many country specific / local language channels. Feel free to join
these channels for localized support and info:</p><table><caption>Country / language specific Slack channels</caption><thead><tr><th>Country</th><th>Channels</th></tr></thead><tbody><tr><td>China</td><td><a href="https://kubernetes.slack.com/messages/cn-users"><code>#cn-users</code></a>, <a href="https://kubernetes.slack.com/messages/cn-events"><code>#cn-events</code></a></td></tr><tr><td>Finland</td><td><a href="https://kubernetes.slack.com/messages/fi-users"><code>#fi-users</code></a></td></tr><tr><td>France</td><td><a href="https://kubernetes.slack.com/messages/fr-users"><code>#fr-users</code></a>, <a href="https://kubernetes.slack.com/messages/fr-events"><code>#fr-events</code></a></td></tr><tr><td>Germany</td><td><a href="https://kubernetes.slack.com/messages/de-users"><code>#de-users</code></a>, <a href="https://kubernetes.slack.com/messages/de-events"><code>#de-events</code></a></td></tr><tr><td>India</td><td><a href="https://kubernetes.slack.com/messages/in-users"><code>#in-users</code></a>, <a href="https://kubernetes.slack.com/messages/in-events"><code>#in-events</code></a></td></tr><tr><td>Italy</td><td><a href="https://kubernetes.slack.com/messages/it-users"><code>#it-users</code></a>, <a href="https://kubernetes.slack.com/messages/it-events"><code>#it-events</code></a></td></tr><tr><td>Japan</td><td><a href="https://kubernetes.slack.com/messages/jp-users"><code>#jp-users</code></a>, <a href="https://kubernetes.slack.com/messages/jp-events"><code>#jp-events</code></a></td></tr><tr><td>Korea</td><td><a href="https://kubernetes.slack.com/messages/kr-users"><code>#kr-users</code></a></td></tr><tr><td>Netherlands</td><td><a href="https://kubernetes.slack.com/messages/nl-users"><code>#nl-users</code></a></td></tr><tr><td>Norway</td><td><a href="https://kubernetes.slack.com/messages/norw-users"><code>#norw-users</code></a></td></tr><tr><td>Poland</td><td><a href="https://kubernetes.slack.com/messages/pl-users"><code>#pl-users</code></a></td></tr><tr><td>Russia</td><td><a href="https://kubernetes.slack.com/messages/ru-users"><code>#ru-users</code></a></td></tr><tr><td>Spain</td><td><a href="https://kubernetes.slack.com/messages/es-users"><code>#es-users</code></a></td></tr><tr><td>Sweden</td><td><a href="https://kubernetes.slack.com/messages/se-users"><code>#se-users</code></a></td></tr><tr><td>Turkey</td><td><a href="https://kubernetes.slack.com/messages/tr-users"><code>#tr-users</code></a>, <a href="https://kubernetes.slack.com/messages/tr-events"><code>#tr-events</code></a></td></tr></tbody></table><h3 id="forum">Forum</h3><p>You're welcome to join the official Kubernetes Forum: <a href="https://discuss.kubernetes.io">discuss.kubernetes.io</a>.</p><h3 id="bugs-and-feature-requests">Bugs and feature requests</h3><p>If you have what looks like a bug, or you would like to make a feature request,
please use the <a href="https://github.com/kubernetes/kubernetes/issues">GitHub issue tracking system</a>.</p><p>Before you file an issue, please search existing issues to see if your issue is
already covered.</p><p>If filing a bug, please include detailed information about how to reproduce the
problem, such as:</p><ul><li>Kubernetes version: <code>kubectl version</code></li><li>Cloud provider, OS distro, network configuration, and container runtime version</li><li>Steps to reproduce the problem</li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Logging in Kubernetes</h1><div class="lead">Logging architecture and system logs.</div><p>This page provides resources that describe logging in Kubernetes. You can learn how to collect, access, and analyze logs using built-in tools and popular logging stacks:</p><ul><li><a href="/docs/concepts/cluster-administration/logging/">Logging Architecture</a></li><li><a href="/docs/concepts/cluster-administration/system-logs/">System Logs</a></li><li><a href="https://www.cncf.io/blog/2020/10/05/a-practical-guide-to-kubernetes-logging">A Practical Guide to Kubernetes Logging</a></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Monitoring in Kubernetes</h1><div class="lead">Monitoring kubernetes system components.</div><p>This page provides resources that describe monitoring in Kubernetes. You can learn how to collect system metrics and traces for Kubernetes system components:</p><ul><li><a href="/docs/concepts/cluster-administration/system-metrics/">Metrics For Kubernetes System Components</a></li><li><a href="/docs/concepts/cluster-administration/system-traces/">Traces For Kubernetes System Components</a></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Troubleshooting Applications</h1><div class="lead">Debugging common containerized application issues.</div><p>This doc contains a set of resources for fixing issues with containerized applications. It covers things like common issues with Kubernetes resources (like Pods, Services, or StatefulSets), advice on making sense of container termination messages, and ways to debug running containers.</p><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/debug-pods/">Debug Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/debug-service/">Debug Services</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/debug-statefulset/">Debug a StatefulSet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/determine-reason-pod-failure/">Determine the Reason for Pod Failure</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/debug-init-containers/">Debug Init Containers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/debug-running-pod/">Debug Running Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/debug/debug-application/get-shell-running-container/">Get a Shell to a Running Container</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Debug Pods</h1><p>This guide is to help users debug applications that are deployed into Kubernetes
and not behaving correctly. This is <em>not</em> a guide for people who want to debug their cluster.
For that you should check out <a href="/docs/tasks/debug/debug-cluster/">this guide</a>.</p><h2 id="diagnosing-the-problem">Diagnosing the problem</h2><p>The first step in troubleshooting is triage. What is the problem?
Is it your Pods, your Replication Controller or your Service?</p><ul><li><a href="#debugging-pods">Debugging Pods</a></li><li><a href="#debugging-replication-controllers">Debugging Replication Controllers</a></li><li><a href="#debugging-services">Debugging Services</a></li></ul><h3 id="debugging-pods">Debugging Pods</h3><p>The first step in debugging a Pod is taking a look at it. Check the current
state of the Pod and recent events with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pods <span>${</span><span>POD_NAME</span><span>}</span>
</span></span></code></pre></div><p>Look at the state of the containers in the pod. Are they all <code>Running</code>?
Have there been recent restarts?</p><p>Continue debugging depending on the state of the pods.</p><h4 id="my-pod-stays-pending">My pod stays pending</h4><p>If a Pod is stuck in <code>Pending</code> it means that it can not be scheduled onto a node.
Generally this is because there are insufficient resources of one type or another
that prevent scheduling. Look at the output of the <code>kubectl describe ...</code> command above.
There should be messages from the scheduler about why it can not schedule your pod.
Reasons include:</p><ul><li><p><strong>You don't have enough resources</strong>: You may have exhausted the supply of CPU
or Memory in your cluster, in this case you need to delete Pods, adjust resource
requests, or add new nodes to your cluster. See <a href="/docs/concepts/configuration/manage-resources-containers/">Compute Resources document</a>
for more information.</p></li><li><p><strong>You are using <code>hostPort</code></strong>: When you bind a Pod to a <code>hostPort</code> there are a
limited number of places that pod can be scheduled. In most cases, <code>hostPort</code>
is unnecessary, try using a Service object to expose your Pod. If you do require
<code>hostPort</code> then you can only schedule as many Pods as there are nodes in your Kubernetes cluster.</p></li></ul><h4 id="my-pod-stays-waiting">My pod stays waiting</h4><p>If a Pod is stuck in the <code>Waiting</code> state, then it has been scheduled to a worker node,
but it can't run on that machine. Again, the information from <code>kubectl describe ...</code>
should be informative. The most common cause of <code>Waiting</code> pods is a failure to pull the image.
There are three things to check:</p><ul><li>Make sure that you have the name of the image correct.</li><li>Have you pushed the image to the registry?</li><li>Try to manually pull the image to see if the image can be pulled. For example,
if you use Docker on your PC, run <code>docker pull &lt;image&gt;</code>.</li></ul><h4 id="my-pod-stays-terminating">My pod stays terminating</h4><p>If a Pod is stuck in the <code>Terminating</code> state, it means that a deletion has been
issued for the Pod, but the control plane is unable to delete the Pod object.</p><p>This typically happens if the Pod has a <a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizer</a>
and there is an <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>
installed in the cluster that prevents the control plane from removing the
finalizer.</p><p>To identify this scenario, check if your cluster has any
ValidatingWebhookConfiguration or MutatingWebhookConfiguration that target
<code>UPDATE</code> operations for <code>pods</code> resources.</p><p>If the webhook is provided by a third-party:</p><ul><li>Make sure you are using the latest version.</li><li>Disable the webhook for <code>UPDATE</code> operations.</li><li>Report an issue with the corresponding provider.</li></ul><p>If you are the author of the webhook:</p><ul><li>For a mutating webhook, make sure it never changes immutable fields on
<code>UPDATE</code> operations. For example, changes to containers are usually not allowed.</li><li>For a validating webhook, make sure that your validation policies only apply
to new changes. In other words, you should allow Pods with existing violations
to pass validation. This allows Pods that were created before the validating
webhook was installed to continue running.</li></ul><h4 id="my-pod-is-crashing-or-otherwise-unhealthy">My pod is crashing or otherwise unhealthy</h4><p>Once your pod has been scheduled, the methods described in
<a href="/docs/tasks/debug/debug-application/debug-running-pod/">Debug Running Pods</a>
are available for debugging.</p><h4 id="my-pod-is-running-but-not-doing-what-i-told-it-to-do">My pod is running but not doing what I told it to do</h4><p>If your pod is not behaving as you expected, it may be that there was an error in your
pod description (e.g. <code>mypod.yaml</code> file on your local machine), and that the error
was silently ignored when you created the pod. Often a section of the pod description
is nested incorrectly, or a key name is typed incorrectly, and so the key is ignored.
For example, if you misspelled <code>command</code> as <code>commnd</code> then the pod will be created but
will not use the command line you intended it to use.</p><p>The first thing to do is to delete your pod and try creating it again with the <code>--validate</code> option.
For example, run <code>kubectl apply --validate -f mypod.yaml</code>.
If you misspelled <code>command</code> as <code>commnd</code> then will give an error like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>I0805 10:43:25.129850   <span>46757</span> schema.go:126<span>]</span> unknown field: commnd
</span></span><span><span>I0805 10:43:25.129973   <span>46757</span> schema.go:129<span>]</span> this may be a <span>false</span> alarm, see https://github.com/kubernetes/kubernetes/issues/6842
</span></span><span><span>pods/mypod
</span></span></code></pre></div><p>The next thing to check is whether the pod on the apiserver
matches the pod you meant to create (e.g. in a yaml file on your local machine).
For example, run <code>kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</code> and then
manually compare the original pod description, <code>mypod.yaml</code> with the one you got
back from apiserver, <code>mypod-on-apiserver.yaml</code>. There will typically be some
lines on the "apiserver" version that are not on the original version. This is
expected. However, if there are lines on the original that are not on the apiserver
version, then this may indicate a problem with your pod spec.</p><h3 id="debugging-replication-controllers">Debugging Replication Controllers</h3><p>Replication controllers are fairly straightforward. They can either create Pods or they can't.
If they can't create pods, then please refer to the
<a href="#debugging-pods">instructions above</a> to debug your pods.</p><p>You can also use <code>kubectl describe rc ${CONTROLLER_NAME}</code> to introspect events
related to the replication controller.</p><h3 id="debugging-services">Debugging Services</h3><p>Services provide load balancing across a set of pods. There are several common problems that can make Services
not work properly. The following instructions should help debug Service problems.</p><p>First, verify that there are endpoints for the service. For every Service object,
the apiserver makes one or more <code>EndpointSlice</code> resources available.</p><p>You can view these resources with:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get endpointslices -l kubernetes.io/service-name<span>=</span><span>${</span><span>SERVICE_NAME</span><span>}</span>
</span></span></code></pre></div><p>Make sure that the endpoints in the EndpointSlices match up with the number of pods that you expect to be members of your service.
For example, if your Service is for an nginx container with 3 replicas, you would expect to see three different
IP addresses in the Service's endpoint slices.</p><h4 id="my-service-is-missing-endpoints">My service is missing endpoints</h4><p>If you are missing endpoints, try listing pods using the labels that Service uses.
Imagine that you have a Service where the labels are:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>- <span>selector</span>:<span>
</span></span></span><span><span><span>     </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>     </span><span>type</span>:<span> </span>frontend<span>
</span></span></span></code></pre></div><p>You can use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --selector<span>=</span><span>name</span><span>=</span>nginx,type<span>=</span>frontend
</span></span></code></pre></div><p>to list pods that match this selector. Verify that the list matches the Pods that you expect to provide your Service.
Verify that the pod's <code>containerPort</code> matches up with the Service's <code>targetPort</code></p><h4 id="network-traffic-is-not-forwarded">Network traffic is not forwarded</h4><p>Please see <a href="/docs/tasks/debug/debug-application/debug-service/">debugging service</a> for more information.</p><h2 id="what-s-next">What's next</h2><p>If none of the above solves your problem, follow the instructions in
<a href="/docs/tasks/debug/debug-application/debug-service/">Debugging Service document</a>
to make sure that your <code>Service</code> is running, has <code>Endpoints</code>, and your <code>Pods</code> are
actually serving; you have DNS working, iptables rules installed, and kube-proxy
does not seem to be misbehaving.</p><p>You may also visit <a href="/docs/tasks/debug/">troubleshooting document</a> for more information.</p></div></div><div><div class="td-content"><h1>Debug Services</h1><p>An issue that comes up rather frequently for new installations of Kubernetes is
that a Service is not working properly. You've run your Pods through a
Deployment (or other workload controller) and created a Service, but you
get no response when you try to access it. This document will hopefully help
you to figure out what's going wrong.</p><h2 id="running-commands-in-a-pod">Running commands in a Pod</h2><p>For many steps here you will want to see what a Pod running in the cluster
sees. The simplest way to do this is to run an interactive busybox Pod:</p><pre tabindex="0"><code class="language-none">kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox sh
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you don't see a command prompt, try pressing enter.</div><p>If you already have a running Pod that you prefer to use, you can run a
command in it using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> &lt;POD-NAME&gt; -c &lt;CONTAINER-NAME&gt; -- &lt;COMMAND&gt;
</span></span></code></pre></div><h2 id="setup">Setup</h2><p>For the purposes of this walk-through, let's run some Pods. Since you're
probably debugging your own Service you can substitute your own details, or you
can follow along and get a second data point.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment hostnames --image<span>=</span>registry.k8s.io/serve_hostname
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/hostnames created
</code></pre><p><code>kubectl</code> commands will print the type and name of the resource created or mutated, which can then be used in subsequent commands.</p><p>Let's scale the deployment to 3 replicas.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale deployment hostnames --replicas<span>=</span><span>3</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/hostnames scaled
</code></pre><p>Note that this is the same as if you had started the Deployment with the following
YAML:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/serve_hostname<span>
</span></span></span></code></pre></div><p>The label "app" is automatically set by <code>kubectl create deployment</code> to the name of the
Deployment.</p><p>You can confirm your Pods are running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                        READY     STATUS    RESTARTS   AGE
hostnames-632524106-bbpiw   1/1       Running   0          2m
hostnames-632524106-ly40y   1/1       Running   0          2m
hostnames-632524106-tlaok   1/1       Running   0          2m
</code></pre><p>You can also confirm that your Pods are serving. You can get the list of
Pod IP addresses and test them directly.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>hostnames <span>\
</span></span></span><span><span><span></span>    -o go-template<span>=</span><span>'{{range .items}}{{.status.podIP}}{{"\n"}}{{end}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">10.244.0.5
10.244.0.6
10.244.0.7
</code></pre><p>The example container used for this walk-through serves its own hostname
via HTTP on port 9376, but if you are debugging your own app, you'll want to
use whatever port number your Pods are listening on.</p><p>From within a pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; <span>do</span>
</span></span><span><span>    wget -qO- <span>$ep</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>This should produce something like:</p><pre tabindex="0"><code>hostnames-632524106-bbpiw
hostnames-632524106-ly40y
hostnames-632524106-tlaok
</code></pre><p>If you are not getting the responses you expect at this point, your Pods
might not be healthy or might not be listening on the port you think they are.
You might find <code>kubectl logs</code> to be useful for seeing what is happening, or
perhaps you need to <code>kubectl exec</code> directly into your Pods and debug from
there.</p><p>Assuming everything has gone to plan so far, you can start to investigate why
your Service doesn't work.</p><h2 id="does-the-service-exist">Does the Service exist?</h2><p>The astute reader will have noticed that you did not actually create a Service
yet - that is intentional. This is a step that sometimes gets forgotten, and
is the first thing to check.</p><p>What would happen if you tried to access a non-existent Service? If
you have another Pod that consumes this Service by name you would get
something like:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>wget -O- hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Resolving hostnames (hostnames)... failed: Name or service not known.
wget: unable to resolve host address 'hostnames'
</code></pre><p>The first thing to check is whether that Service actually exists:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">No resources found.
Error from server (NotFound): services "hostnames" not found
</code></pre><p>Let's create the Service. As before, this is for the walk-through - you can
use your own Service's details here.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl expose deployment hostnames --port<span>=</span><span>80</span> --target-port<span>=</span><span>9376</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">service/hostnames exposed
</code></pre><p>And read it back:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
hostnames   ClusterIP   10.0.1.175   &lt;none&gt;        80/TCP    5s
</code></pre><p>Now you know that the Service exists.</p><p>As before, this is the same as if you had started the Service with YAML:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>hostnames<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span></code></pre></div><p>In order to highlight the full range of configuration, the Service you created
here uses a different port number than the Pods. For many real-world
Services, these values might be the same.</p><h2 id="any-network-policy-ingress-rules-affecting-the-target-pods">Any Network Policy Ingress rules affecting the target Pods?</h2><p>If you have deployed any Network Policy Ingress rules which may affect incoming
traffic to <code>hostnames-*</code> Pods, these need to be reviewed.</p><p>Please refer to <a href="/docs/concepts/services-networking/network-policies/">Network Policies</a> for more details.</p><h2 id="does-the-service-work-by-dns-name">Does the Service work by DNS name?</h2><p>One of the most common ways that clients consume a Service is through a DNS
name.</p><p>From a Pod in the same Namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nslookup hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      hostnames
Address 1: 10.0.1.175 hostnames.default.svc.cluster.local
</code></pre><p>If this fails, perhaps your Pod and Service are in different
Namespaces, try a namespace-qualified name (again, from within a Pod):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nslookup hostnames.default
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      hostnames.default
Address 1: 10.0.1.175 hostnames.default.svc.cluster.local
</code></pre><p>If this works, you'll need to adjust your app to use a cross-namespace name, or
run your app and Service in the same Namespace. If this still fails, try a
fully-qualified name:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nslookup hostnames.default.svc.cluster.local
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      hostnames.default.svc.cluster.local
Address 1: 10.0.1.175 hostnames.default.svc.cluster.local
</code></pre><p>Note the suffix here: "default.svc.cluster.local". The "default" is the
Namespace you're operating in. The "svc" denotes that this is a Service.
The "cluster.local" is your cluster domain, which COULD be different in your
own cluster.</p><p>You can also try this from a Node in the cluster:</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>10.0.0.10 is the cluster's DNS Service IP, yours might be different.</div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nslookup hostnames.default.svc.cluster.local 10.0.0.10
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Server:         10.0.0.10
Address:        10.0.0.10#53

Name:   hostnames.default.svc.cluster.local
Address: 10.0.1.175
</code></pre><p>If you are able to do a fully-qualified name lookup but not a relative one, you
need to check that your <code>/etc/resolv.conf</code> file in your Pod is correct. From
within a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /etc/resolv.conf
</span></span></code></pre></div><p>You should see something like:</p><pre tabindex="0"><code>nameserver 10.0.0.10
search default.svc.cluster.local svc.cluster.local cluster.local example.com
options ndots:5
</code></pre><p>The <code>nameserver</code> line must indicate your cluster's DNS Service. This is
passed into <code>kubelet</code> with the <code>--cluster-dns</code> flag.</p><p>The <code>search</code> line must include an appropriate suffix for you to find the
Service name. In this case it is looking for Services in the local
Namespace ("default.svc.cluster.local"), Services in all Namespaces
("svc.cluster.local"), and lastly for names in the cluster ("cluster.local").
Depending on your own install you might have additional records after that (up
to 6 total). The cluster suffix is passed into <code>kubelet</code> with the
<code>--cluster-domain</code> flag. Throughout this document, the cluster suffix is
assumed to be "cluster.local". Your own clusters might be configured
differently, in which case you should change that in all of the previous
commands.</p><p>The <code>options</code> line must set <code>ndots</code> high enough that your DNS client library
considers search paths at all. Kubernetes sets this to 5 by default, which is
high enough to cover all of the DNS names it generates.</p><h3 id="does-any-service-exist-in-dns">Does any Service work by DNS name?</h3><p>If the above still fails, DNS lookups are not working for your Service. You
can take a step back and see what else is not working. The Kubernetes master
Service should always work. From within a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local
</code></pre><p>If this fails, please see the <a href="#is-the-kube-proxy-working">kube-proxy</a> section
of this document, or even go back to the top of this document and start over,
but instead of debugging your own Service, debug the DNS Service.</p><h2 id="does-the-service-work-by-ip">Does the Service work by IP?</h2><p>Assuming you have confirmed that DNS works, the next thing to test is whether your
Service works by its IP address. From a Pod in your cluster, access the
Service's IP (from <code>kubectl get</code> above).</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> i in <span>$(</span>seq <span>1</span> 3<span>)</span>; <span>do</span> 
</span></span><span><span>    wget -qO- 10.0.1.175:80
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>This should produce something like:</p><pre tabindex="0"><code>hostnames-632524106-bbpiw
hostnames-632524106-ly40y
hostnames-632524106-tlaok
</code></pre><p>If your Service is working, you should get correct responses. If not, there
are a number of things that could be going wrong. Read on.</p><h2 id="is-the-service-defined-correctly">Is the Service defined correctly?</h2><p>It might sound silly, but you should really double and triple check that your
Service is correct and matches your Pod's port. Read back your Service
and verify it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get service hostnames -o json
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>    <span>"kind"</span>: <span>"Service"</span>,
</span></span><span><span>    <span>"apiVersion"</span>: <span>"v1"</span>,
</span></span><span><span>    <span>"metadata"</span>: {
</span></span><span><span>        <span>"name"</span>: <span>"hostnames"</span>,
</span></span><span><span>        <span>"namespace"</span>: <span>"default"</span>,
</span></span><span><span>        <span>"uid"</span>: <span>"428c8b6c-24bc-11e5-936d-42010af0a9bc"</span>,
</span></span><span><span>        <span>"resourceVersion"</span>: <span>"347189"</span>,
</span></span><span><span>        <span>"creationTimestamp"</span>: <span>"2015-07-07T15:24:29Z"</span>,
</span></span><span><span>        <span>"labels"</span>: {
</span></span><span><span>            <span>"app"</span>: <span>"hostnames"</span>
</span></span><span><span>        }
</span></span><span><span>    },
</span></span><span><span>    <span>"spec"</span>: {
</span></span><span><span>        <span>"ports"</span>: [
</span></span><span><span>            {
</span></span><span><span>                <span>"name"</span>: <span>"default"</span>,
</span></span><span><span>                <span>"protocol"</span>: <span>"TCP"</span>,
</span></span><span><span>                <span>"port"</span>: <span>80</span>,
</span></span><span><span>                <span>"targetPort"</span>: <span>9376</span>,
</span></span><span><span>                <span>"nodePort"</span>: <span>0</span>
</span></span><span><span>            }
</span></span><span><span>        ],
</span></span><span><span>        <span>"selector"</span>: {
</span></span><span><span>            <span>"app"</span>: <span>"hostnames"</span>
</span></span><span><span>        },
</span></span><span><span>        <span>"clusterIP"</span>: <span>"10.0.1.175"</span>,
</span></span><span><span>        <span>"type"</span>: <span>"ClusterIP"</span>,
</span></span><span><span>        <span>"sessionAffinity"</span>: <span>"None"</span>
</span></span><span><span>    },
</span></span><span><span>    <span>"status"</span>: {
</span></span><span><span>        <span>"loadBalancer"</span>: {}
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><ul><li>Is the Service port you are trying to access listed in <code>spec.ports[]</code>?</li><li>Is the <code>targetPort</code> correct for your Pods (some Pods use a different port than the Service)?</li><li>If you meant to use a numeric port, is it a number (9376) or a string "9376"?</li><li>If you meant to use a named port, do your Pods expose a port with the same name?</li><li>Is the port's <code>protocol</code> correct for your Pods?</li></ul><h2 id="does-the-service-have-any-endpointslices">Does the Service have any EndpointSlices?</h2><p>If you got this far, you have confirmed that your Service is correctly
defined and is resolved by DNS. Now let's check that the Pods you ran are
actually being selected by the Service.</p><p>Earlier you saw that the Pods were running. You can re-check that:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                        READY     STATUS    RESTARTS   AGE
hostnames-632524106-bbpiw   1/1       Running   0          1h
hostnames-632524106-ly40y   1/1       Running   0          1h
hostnames-632524106-tlaok   1/1       Running   0          1h
</code></pre><p>The <code>-l app=hostnames</code> argument is a label selector configured on the Service.</p><p>The "AGE" column says that these Pods are about an hour old, which implies that
they are running fine and not crashing.</p><p>The "RESTARTS" column says that these pods are not crashing frequently or being
restarted. Frequent restarts could lead to intermittent connectivity issues.
If the restart count is high, read more about how to <a href="/docs/tasks/debug/debug-application/debug-pods/">debug pods</a>.</p><p>Inside the Kubernetes system is a control loop which evaluates the selector of
every Service and saves the results into one or more EndpointSlice objects.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get endpointslices -l k8s.io/service-name<span>=</span>hostnames
</span></span><span><span>
</span></span><span><span>NAME              ADDRESSTYPE   PORTS   ENDPOINTS
</span></span><span><span>hostnames-ytpni   IPv4          <span>9376</span>    10.244.0.5,10.244.0.6,10.244.0.7
</span></span></code></pre></div><p>This confirms that the EndpointSlice controller has found the correct Pods for
your Service. If the <code>ENDPOINTS</code> column is <code>&lt;none&gt;</code>, you should check that
the <code>spec.selector</code> field of your Service actually selects for
<code>metadata.labels</code> values on your Pods. A common mistake is to have a typo or
other error, such as the Service selecting for <code>app=hostnames</code>, but the
Deployment specifying <code>run=hostnames</code>, as in versions previous to 1.18, where
the <code>kubectl run</code> command could have been also used to create a Deployment.</p><h2 id="are-the-pods-working">Are the Pods working?</h2><p>At this point, you know that your Service exists and has selected your Pods.
At the beginning of this walk-through, you verified the Pods themselves.
Let's check again that the Pods are actually working - you can bypass the
Service mechanism and go straight to the Pods, as listed by the Endpoints
above.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>These commands use the Pod port (9376), rather than the Service port (80).</div><p>From within a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; <span>do</span>
</span></span><span><span>    wget -qO- <span>$ep</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>This should produce something like:</p><pre tabindex="0"><code>hostnames-632524106-bbpiw
hostnames-632524106-ly40y
hostnames-632524106-tlaok
</code></pre><p>You expect each Pod in the endpoints list to return its own hostname. If
this is not what happens (or whatever the correct behavior is for your own
Pods), you should investigate what's happening there.</p><h2 id="is-the-kube-proxy-working">Is the kube-proxy working?</h2><p>If you get here, your Service is running, has EndpointSlices, and your Pods
are actually serving. At this point, the whole Service proxy mechanism is
suspect. Let's confirm it, piece by piece.</p><p>The default implementation of Services, and the one used on most clusters, is
kube-proxy. This is a program that runs on every node and configures one of a
small set of mechanisms for providing the Service abstraction. If your
cluster does not use kube-proxy, the following sections will not apply, and you
will have to investigate whatever implementation of Services you are using.</p><h3 id="is-kube-proxy-running">Is kube-proxy running?</h3><p>Confirm that <code>kube-proxy</code> is running on your Nodes. Running directly on a
Node, you should get something like the below:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ps auxw | grep kube-proxy
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">root  4194  0.4  0.1 101864 17696 ?    Sl Jul04  25:43 /usr/local/bin/kube-proxy --master=https://kubernetes-master --kubeconfig=/var/lib/kube-proxy/kubeconfig --v=2
</code></pre><p>Next, confirm that it is not failing something obvious, like contacting the
master. To do this, you'll have to look at the logs. Accessing the logs
depends on your Node OS. On some OSes it is a file, such as
/var/log/kube-proxy.log, while other OSes use <code>journalctl</code> to access logs. You
should see something like:</p><pre tabindex="0"><code class="language-none">I1027 22:14:53.995134    5063 server.go:200] Running in resource-only container "/kube-proxy"
I1027 22:14:53.998163    5063 server.go:247] Using iptables Proxier.
I1027 22:14:54.038140    5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns-tcp" to [10.244.1.3:53]
I1027 22:14:54.038164    5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns" to [10.244.1.3:53]
I1027 22:14:54.038209    5063 proxier.go:352] Setting endpoints for "default/kubernetes:https" to [10.240.0.2:443]
I1027 22:14:54.038238    5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master
I1027 22:14:54.040048    5063 proxier.go:294] Adding new service "default/kubernetes:https" at 10.0.0.1:443/TCP
I1027 22:14:54.040154    5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns" at 10.0.0.10:53/UDP
I1027 22:14:54.040223    5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns-tcp" at 10.0.0.10:53/TCP
</code></pre><p>If you see error messages about not being able to contact the master, you
should double-check your Node configuration and installation steps.</p><p>Kube-proxy can run in one of a few modes. In the log listed above, the
line <code>Using iptables Proxier</code> indicates that kube-proxy is running in
"iptables" mode. The most common other mode is "ipvs".</p><h4 id="iptables-mode">Iptables mode</h4><p>In "iptables" mode, you should see something like the following on a Node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>iptables-save | grep hostnames
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.3.6:9376
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.1.7:9376
-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.2.3:9376
-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR
</code></pre><p>For each port of each Service, there should be 1 rule in <code>KUBE-SERVICES</code> and
one <code>KUBE-SVC-&lt;hash&gt;</code> chain. For each Pod endpoint, there should be a small
number of rules in that <code>KUBE-SVC-&lt;hash&gt;</code> and one <code>KUBE-SEP-&lt;hash&gt;</code> chain with
a small number of rules in it. The exact rules will vary based on your exact
config (including node-ports and load-balancers).</p><h4 id="ipvs-mode">IPVS mode</h4><p>In "ipvs" mode, you should see something like the following on a Node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ipvsadm -ln
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
...
TCP  10.0.1.175:80 rr
  -&gt; 10.244.0.5:9376               Masq    1      0          0
  -&gt; 10.244.0.6:9376               Masq    1      0          0
  -&gt; 10.244.0.7:9376               Masq    1      0          0
...
</code></pre><p>For each port of each Service, plus any NodePorts, external IPs, and
load-balancer IPs, kube-proxy will create a virtual server. For each Pod
endpoint, it will create corresponding real servers. In this example, service
hostnames(<code>10.0.1.175:80</code>) has 3 endpoints(<code>10.244.0.5:9376</code>,
<code>10.244.0.6:9376</code>, <code>10.244.0.7:9376</code>).</p><h3 id="is-kube-proxy-proxying">Is kube-proxy proxying?</h3><p>Assuming you do see one the above cases, try again to access your Service by
IP from one of your Nodes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl 10.0.1.175:80
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">hostnames-632524106-bbpiw
</code></pre><p>If this still fails, look at the <code>kube-proxy</code> logs for specific lines like:</p><pre tabindex="0"><code class="language-none">Setting endpoints for default/hostnames:default to [10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376]
</code></pre><p>If you don't see those, try restarting <code>kube-proxy</code> with the <code>-v</code> flag set to 4, and
then look at the logs again.</p><h3 id="a-pod-fails-to-reach-itself-via-the-service-ip">Edge case: A Pod fails to reach itself via the Service IP</h3><p>This might sound unlikely, but it does happen and it is supposed to work.</p><p>This can happen when the network is not properly configured for "hairpin"
traffic, usually when <code>kube-proxy</code> is running in <code>iptables</code> mode and Pods
are connected with bridge network. The <code>Kubelet</code> exposes a <code>hairpin-mode</code>
<a href="/docs/reference/command-line-tools-reference/kubelet/">flag</a> that allows endpoints of a Service to loadbalance
back to themselves if they try to access their own Service VIP. The
<code>hairpin-mode</code> flag must either be set to <code>hairpin-veth</code> or
<code>promiscuous-bridge</code>.</p><p>The common steps to trouble shoot this are as follows:</p><ul><li>Confirm <code>hairpin-mode</code> is set to <code>hairpin-veth</code> or <code>promiscuous-bridge</code>.
You should see something like the below. <code>hairpin-mode</code> is set to
<code>promiscuous-bridge</code> in the following example.</li></ul><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ps auxw | grep kubelet
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">root      3392  1.1  0.8 186804 65208 ?        Sl   00:51  11:11 /usr/local/bin/kubelet --enable-debugging-handlers=true --config=/etc/kubernetes/manifests --allow-privileged=True --v=4 --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --configure-cbr0=true --cgroup-root=/ --system-cgroups=/system --hairpin-mode=promiscuous-bridge --runtime-cgroups=/docker-daemon --kubelet-cgroups=/kubelet --babysit-daemons=true --max-pods=110 --serialize-image-pulls=false --outofdisk-transition-frequency=0
</code></pre><ul><li>Confirm the effective <code>hairpin-mode</code>. To do this, you'll have to look at
kubelet log. Accessing the logs depends on your Node OS. On some OSes it
is a file, such as /var/log/kubelet.log, while other OSes use <code>journalctl</code>
to access logs. Please be noted that the effective hairpin mode may not
match <code>--hairpin-mode</code> flag due to compatibility. Check if there is any log
lines with key word <code>hairpin</code> in kubelet.log. There should be log lines
indicating the effective hairpin mode, like something below.</li></ul><pre tabindex="0"><code class="language-none">I0629 00:51:43.648698    3252 kubelet.go:380] Hairpin mode set to "promiscuous-bridge"
</code></pre><ul><li>If the effective hairpin mode is <code>hairpin-veth</code>, ensure the <code>Kubelet</code> has
the permission to operate in <code>/sys</code> on node. If everything works properly,
you should see something like:</li></ul><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> intf in /sys/devices/virtual/net/cbr0/brif/*; <span>do</span> cat <span>$intf</span>/hairpin_mode; <span>done</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">1
1
1
1
</code></pre><ul><li>If the effective hairpin mode is <code>promiscuous-bridge</code>, ensure <code>Kubelet</code>
has the permission to manipulate linux bridge on node. If <code>cbr0</code> bridge is
used and configured properly, you should see:</li></ul><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ifconfig cbr0 |grep PROMISC
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1460  Metric:1
</code></pre><ul><li>Seek help if none of above works out.</li></ul><h2 id="seek-help">Seek help</h2><p>If you get this far, something very strange is happening. Your Service is
running, has EndpointSlices, and your Pods are actually serving. You have DNS
working, and <code>kube-proxy</code> does not seem to be misbehaving. And yet your
Service is not working. Please let us know what is going on, so we can help
investigate!</p><p>Contact us on
<a href="https://slack.k8s.io/">Slack</a> or
<a href="https://discuss.kubernetes.io">Forum</a> or
<a href="https://github.com/kubernetes/kubernetes">GitHub</a>.</p><h2 id="what-s-next">What's next</h2><p>Visit the <a href="/docs/tasks/debug/">troubleshooting overview document</a>
for more information.</p></div></div><div><div class="td-content"><h1>Debug a StatefulSet</h1><p>This task shows you how to debug a StatefulSet.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster.</li><li>You should have a StatefulSet running that you want to investigate.</li></ul><h2 id="debugging-a-statefulset">Debugging a StatefulSet</h2><p>In order to list all the pods which belong to a StatefulSet, which have a label <code>app.kubernetes.io/name=MyApp</code> set on them,
you can use the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l app.kubernetes.io/name<span>=</span>MyApp
</span></span></code></pre></div><p>If you find that any Pods listed are in <code>Unknown</code> or <code>Terminating</code> state for an extended period of time,
refer to the <a href="/docs/tasks/run-application/delete-stateful-set/">Deleting StatefulSet Pods</a> task for
instructions on how to deal with them.
You can debug individual Pods in a StatefulSet using the
<a href="/docs/tasks/debug/debug-application/debug-pods/">Debugging Pods</a> guide.</p><h2 id="what-s-next">What's next</h2><p>Learn more about <a href="/docs/tasks/debug/debug-application/debug-init-containers/">debugging an init-container</a>.</p></div></div><div><div class="td-content"><h1>Determine the Reason for Pod Failure</h1><p>This page shows how to write and read a Container termination message.</p><p>Termination messages provide a way for containers to write
information about fatal events to a location where it can
be easily retrieved and surfaced by tools like dashboards
and monitoring software. In most cases, information that you
put in a termination message should also be written to
the general
<a href="/docs/concepts/cluster-administration/logging/">Kubernetes logs</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="writing-and-reading-a-termination-message">Writing and reading a termination message</h2><p>In this exercise, you create a Pod that runs one container.
The manifest for that Pod specifies a command that runs when the container starts:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/termination.yaml"><code>debug/termination.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy debug/termination.yaml to clipboard"></div><div class="includecode" id="debug-termination-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>termination-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>termination-demo-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"-c"</span>,<span> </span><span>"sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log"</span>]<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create a Pod based on the YAML configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/debug/termination.yaml
</span></span></code></pre></div><p>In the YAML file, in the <code>command</code> and <code>args</code> fields, you can see that the
container sleeps for 10 seconds and then writes "Sleep expired" to
the <code>/dev/termination-log</code> file. After the container writes
the "Sleep expired" message, it terminates.</p></li><li><p>Display information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod termination-demo
</span></span></code></pre></div><p>Repeat the preceding command until the Pod is no longer running.</p></li><li><p>Display detailed information about the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod termination-demo --output<span>=</span>yaml
</span></span></code></pre></div><p>The output includes the "Sleep expired" message:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span>    </span><span>lastState</span>:<span>
</span></span></span><span><span><span>      </span><span>terminated</span>:<span>
</span></span></span><span><span><span>        </span><span>containerID</span>:<span> </span>...<span>
</span></span></span><span><span><span>        </span><span>exitCode</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>        </span><span>finishedAt</span>:<span> </span>...<span>
</span></span></span><span><span><span>        </span><span>message</span>:<span> </span>|<span>
</span></span></span><span><span><span>          Sleep expired</span><span>          
</span></span></span><span><span><span>        </span>...<span>
</span></span></span></code></pre></div></li><li><p>Use a Go template to filter the output so that it includes only the termination message:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod termination-demo -o go-template<span>=</span><span>"{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}"</span>
</span></span></code></pre></div></li></ol><p>If you are running a multi-container Pod, you can use a Go template to include the container's name.
By doing so, you can discover which of the containers is failing:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod multi-container-pod -o go-template<span>=</span><span>'{{range .status.containerStatuses}}{{printf "%s:\n%s\n\n" .name .lastState.terminated.message}}{{end}}'</span>
</span></span></code></pre></div><h2 id="customizing-the-termination-message">Customizing the termination message</h2><p>Kubernetes retrieves termination messages from the termination message file
specified in the <code>terminationMessagePath</code> field of a Container, which has a default
value of <code>/dev/termination-log</code>. By customizing this field, you can tell Kubernetes
to use a different file. Kubernetes use the contents from the specified file to
populate the Container's status message on both success and failure.</p><p>The termination message is intended to be brief final status, such as an assertion failure message.
The kubelet truncates messages that are longer than 4096 bytes.</p><p>The total message length across all containers is limited to 12KiB, divided equally among each container.
For example, if there are 12 containers (<code>initContainers</code> or <code>containers</code>), each has 1024 bytes of available termination message space.</p><p>The default termination message path is <code>/dev/termination-log</code>.
You cannot set the termination message path after a Pod is launched.</p><p>In the following example, the container writes termination messages to
<code>/tmp/my-log</code> for Kubernetes to retrieve:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>msg-path-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>msg-path-demo-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>terminationMessagePath</span>:<span> </span><span>"/tmp/my-log"</span><span>
</span></span></span></code></pre></div><p>Moreover, users can set the <code>terminationMessagePolicy</code> field of a Container for
further customization. This field defaults to "<code>File</code>" which means the termination
messages are retrieved only from the termination message file. By setting the
<code>terminationMessagePolicy</code> to "<code>FallbackToLogsOnError</code>", you can tell Kubernetes
to use the last chunk of container log output if the termination message file
is empty and the container exited with an error. The log output is limited to
2048 bytes or 80 lines, whichever is smaller.</p><h2 id="what-s-next">What's next</h2><ul><li>See the <code>terminationMessagePath</code> field in
<a href="/docs/reference/generated/kubernetes-api/v1.34/#container-v1-core">Container</a>.</li><li>See <a href="/docs/concepts/containers/images/#imagepullbackoff">ImagePullBackOff</a> in <a href="/docs/concepts/containers/images/">Images</a>.</li><li>Learn about <a href="/docs/concepts/cluster-administration/logging/">retrieving logs</a>.</li><li>Learn about <a href="https://pkg.go.dev/text/template">Go templates</a>.</li><li>Learn about <a href="/docs/tasks/debug/debug-application/debug-init-containers/#understanding-pod-status">Pod status</a> and <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">Pod phase</a>.</li><li>Learn about <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-states">container states</a>.</li></ul></div></div><div><div class="td-content"><h1>Debug Init Containers</h1><p>This page shows how to investigate problems related to the execution of
Init Containers. The example command lines below refer to the Pod as
<code>&lt;pod-name&gt;</code> and the Init Containers as <code>&lt;init-container-1&gt;</code> and
<code>&lt;init-container-2&gt;</code>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>You should be familiar with the basics of
<a href="/docs/concepts/workloads/pods/init-containers/">Init Containers</a>.</li><li>You should have <a href="/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">Configured an Init Container</a>.</li></ul><h2 id="checking-the-status-of-init-containers">Checking the status of Init Containers</h2><p>Display the status of your pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod &lt;pod-name&gt;
</span></span></code></pre></div><p>For example, a status of <code>Init:1/2</code> indicates that one of two Init Containers
has completed successfully:</p><pre tabindex="0"><code>NAME         READY     STATUS     RESTARTS   AGE
&lt;pod-name&gt;   0/1       Init:1/2   0          7s
</code></pre><p>See <a href="#understanding-pod-status">Understanding Pod status</a> for more examples of
status values and their meanings.</p><h2 id="getting-details-about-init-containers">Getting details about Init Containers</h2><p>View more detailed information about Init Container execution:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod &lt;pod-name&gt;
</span></span></code></pre></div><p>For example, a Pod with two Init Containers might show the following:</p><pre tabindex="0"><code>Init Containers:
  &lt;init-container-1&gt;:
    Container ID:    ...
    ...
    State:           Terminated
      Reason:        Completed
      Exit Code:     0
      Started:       ...
      Finished:      ...
    Ready:           True
    Restart Count:   0
    ...
  &lt;init-container-2&gt;:
    Container ID:    ...
    ...
    State:           Waiting
      Reason:        CrashLoopBackOff
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       ...
      Finished:      ...
    Ready:           False
    Restart Count:   3
    ...
</code></pre><p>You can also access the Init Container statuses programmatically by reading the
<code>status.initContainerStatuses</code> field on the Pod Spec:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod &lt;pod-name&gt; --template <span>'{{.status.initContainerStatuses}}'</span>
</span></span></code></pre></div><p>This command will return the same information as above, formatted using a <a href="https://pkg.go.dev/text/template">Go template</a>.</p><h2 id="accessing-logs-from-init-containers">Accessing logs from Init Containers</h2><p>Pass the Init Container name along with the Pod name
to access its logs.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs &lt;pod-name&gt; -c &lt;init-container-2&gt;
</span></span></code></pre></div><p>Init Containers that run a shell script print
commands as they're executed. For example, you can do this in Bash by running
<code>set -x</code> at the beginning of the script.</p><h2 id="understanding-pod-status">Understanding Pod status</h2><p>A Pod status beginning with <code>Init:</code> summarizes the status of Init Container
execution. The table below describes some example status values that you might
see while debugging Init Containers.</p><table><thead><tr><th>Status</th><th>Meaning</th></tr></thead><tbody><tr><td><code>Init:N/M</code></td><td>The Pod has <code>M</code> Init Containers, and <code>N</code> have completed so far.</td></tr><tr><td><code>Init:Error</code></td><td>An Init Container has failed to execute.</td></tr><tr><td><code>Init:CrashLoopBackOff</code></td><td>An Init Container has failed repeatedly.</td></tr><tr><td><code>Pending</code></td><td>The Pod has not yet begun executing Init Containers.</td></tr><tr><td><code>PodInitializing</code> or <code>Running</code></td><td>The Pod has already finished executing Init Containers.</td></tr></tbody></table></div></div><div><div class="td-content"><h1>Debug Running Pods</h1><p>This page explains how to debug Pods running (or crashing) on a Node.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Your <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> should already be
scheduled and running. If your Pod is not yet running, start with <a href="/docs/tasks/debug/debug-application/">Debugging
Pods</a>.</li><li>For some of the advanced debugging steps you need to know on which Node the
Pod is running and have shell access to run commands on that Node. You don't
need that access to run the standard debug steps that use <code>kubectl</code>.</li></ul><h2 id="using-kubectl-describe-pod-to-fetch-details-about-pods">Using <code>kubectl describe pod</code> to fetch details about pods</h2><p>For this example we'll use a Deployment to create two pods, similar to the earlier example.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx-with-request.yaml"><code>application/nginx-with-request.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/nginx-with-request.yaml to clipboard"></div><div class="includecode" id="application-nginx-with-request-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"128Mi"</span><span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Create deployment by running following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/nginx-with-request.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/nginx-deployment created
</code></pre><p>Check pod status by following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-67d4bdd6f5-cx2nz   1/1     Running   0          13s
nginx-deployment-67d4bdd6f5-w6kd7   1/1     Running   0          13s
</code></pre><p>We can retrieve a lot more information about each of these pods using <code>kubectl describe pod</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod nginx-deployment-67d4bdd6f5-w6kd7
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Name:         nginx-deployment-67d4bdd6f5-w6kd7
Namespace:    default
Priority:     0
Node:         kube-worker-1/192.168.0.113
Start Time:   Thu, 17 Feb 2022 16:51:01 -0500
Labels:       app=nginx
              pod-template-hash=67d4bdd6f5
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.88.0.3
IPs:
  IP:           10.88.0.3
  IP:           2001:db8::1
Controlled By:  ReplicaSet/nginx-deployment-67d4bdd6f5
Containers:
  nginx:
    Container ID:   containerd://5403af59a2b46ee5a23fb0ae4b1e077f7ca5c5fb7af16e1ab21c00e0e616462a
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 17 Feb 2022 16:51:05 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        500m
      memory:     128Mi
    Environment:  &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bgsgp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-bgsgp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  34s   default-scheduler  Successfully assigned default/nginx-deployment-67d4bdd6f5-w6kd7 to kube-worker-1
  Normal  Pulling    31s   kubelet            Pulling image "nginx"
  Normal  Pulled     30s   kubelet            Successfully pulled image "nginx" in 1.146417389s
  Normal  Created    30s   kubelet            Created container nginx
  Normal  Started    30s   kubelet            Started container nginx
</code></pre><p>Here you can see configuration information about the container(s) and Pod (labels, resource requirements, etc.), as well as status information about the container(s) and Pod (state, readiness, restart count, events, etc.).</p><p>The container state is one of Waiting, Running, or Terminated. Depending on the state, additional information will be provided - here you can see that for a container in Running state, the system tells you when the container started.</p><p>Ready tells you whether the container passed its last readiness probe. (In this case, the container does not have a readiness probe configured; the container is assumed to be ready if no readiness probe is configured.)</p><p>Restart Count tells you how many times the container has been restarted; this information can be useful for detecting crash loops in containers that are configured with a restart policy of <code>Always</code>.</p><p>Currently the only Condition associated with a Pod is the binary Ready condition, which indicates that the pod is able to service requests and should be added to the load balancing pools of all matching services.</p><p>Lastly, you see a log of recent events related to your Pod. "From" indicates the component that is logging the event. "Reason" and "Message" tell you what happened.</p><h2 id="example-debugging-pending-pods">Example: debugging Pending Pods</h2><p>A common scenario that you can detect using events is when you've created a Pod that won't fit on any node. For example, the Pod might request more resources than are free on any node, or it might specify a label selector that doesn't match any nodes. Let's say we created the previous Deployment with 5 replicas (instead of 2) and requesting 600 millicores instead of 500, on a four-node cluster where each (virtual) machine has 1 CPU. In that case one of the Pods will not be able to schedule. (Note that because of the cluster addon pods such as fluentd, skydns, etc., that run on each node, if we requested 1000 millicores then none of the Pods would be able to schedule.)</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1006230814-6winp   1/1       Running   0          7m
nginx-deployment-1006230814-fmgu3   1/1       Running   0          7m
nginx-deployment-1370807587-6ekbw   1/1       Running   0          1m
nginx-deployment-1370807587-fg172   0/1       Pending   0          1m
nginx-deployment-1370807587-fz9sd   0/1       Pending   0          1m
</code></pre><p>To find out why the nginx-deployment-1370807587-fz9sd pod is not running, we can use <code>kubectl describe pod</code> on the pending Pod and look at its events:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod nginx-deployment-1370807587-fz9sd
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">  Name:		nginx-deployment-1370807587-fz9sd
  Namespace:	default
  Node:		/
  Labels:		app=nginx,pod-template-hash=1370807587
  Status:		Pending
  IP:
  Controllers:	ReplicaSet/nginx-deployment-1370807587
  Containers:
    nginx:
      Image:	nginx
      Port:	80/TCP
      QoS Tier:
        memory:	Guaranteed
        cpu:	Guaranteed
      Limits:
        cpu:	1
        memory:	128Mi
      Requests:
        cpu:	1
        memory:	128Mi
      Environment Variables:
  Volumes:
    default-token-4bcbi:
      Type:	Secret (a volume populated by a Secret)
      SecretName:	default-token-4bcbi
  Events:
    FirstSeen	LastSeen	Count	From			        SubobjectPath	Type		Reason			    Message
    ---------	--------	-----	----			        -------------	--------	------			    -------
    1m		    48s		    7	    {default-scheduler }			        Warning		FailedScheduling	pod (nginx-deployment-1370807587-fz9sd) failed to fit in any node
  fit failure on node (kubernetes-node-6ta5): Node didn't have enough resource: CPU, requested: 1000, used: 1420, capacity: 2000
  fit failure on node (kubernetes-node-wul5): Node didn't have enough resource: CPU, requested: 1000, used: 1100, capacity: 2000
</code></pre><p>Here you can see the event generated by the scheduler saying that the Pod failed to schedule for reason <code>FailedScheduling</code> (and possibly others). The message tells us that there were not enough resources for the Pod on any of the nodes.</p><p>To correct this situation, you can use <code>kubectl scale</code> to update your Deployment to specify four or fewer replicas. (Or you could leave the one Pod pending, which is harmless.)</p><p>Events such as the ones you saw at the end of <code>kubectl describe pod</code> are persisted in etcd and provide high-level information on what is happening in the cluster. To list all events you can use</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get events
</span></span></code></pre></div><p>but you have to remember that events are namespaced. This means that if you're interested in events for some namespaced object (e.g. what happened with Pods in namespace <code>my-namespace</code>) you need to explicitly provide a namespace to the command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get events --namespace<span>=</span>my-namespace
</span></span></code></pre></div><p>To see events from all namespaces, you can use the <code>--all-namespaces</code> argument.</p><p>In addition to <code>kubectl describe pod</code>, another way to get extra information about a pod (beyond what is provided by <code>kubectl get pod</code>) is to pass the <code>-o yaml</code> output format flag to <code>kubectl get pod</code>. This will give you, in YAML format, even more information than <code>kubectl describe pod</code> - essentially all of the information the system has about the Pod. Here you will see things like annotations (which are key-value metadata without the label restrictions, that is used internally by Kubernetes system components), restart policy, ports, and volumes.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod nginx-deployment-1006230814-6winp -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2022-02-17T21:51:01Z"</span><span>
</span></span></span><span><span><span>  </span><span>generateName</span>:<span> </span>nginx-deployment-67d4bdd6f5-<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>pod-template-hash</span>:<span> </span>67d4bdd6f5<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment-67d4bdd6f5-w6kd7<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>ownerReferences</span>:<span>
</span></span></span><span><span><span>  </span>- <span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>    </span><span>blockOwnerDeletion</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>controller</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ReplicaSet<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>nginx-deployment-67d4bdd6f5<span>
</span></span></span><span><span><span>    </span><span>uid</span>:<span> </span>7d41dfd4-84c0-4be4-88ab-cedbe626ad82<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"1364"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>a6501da1-0447-4262-98eb-c03d4002222e<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>128Mi<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>128Mi<span>
</span></span></span><span><span><span>    </span><span>terminationMessagePath</span>:<span> </span>/dev/termination-log<span>
</span></span></span><span><span><span>    </span><span>terminationMessagePolicy</span>:<span> </span>File<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/var/run/secrets/kubernetes.io/serviceaccount<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>kube-api-access-bgsgp<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>dnsPolicy</span>:<span> </span>ClusterFirst<span>
</span></span></span><span><span><span>  </span><span>enableServiceLinks</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>nodeName</span>:<span> </span>kube-worker-1<span>
</span></span></span><span><span><span>  </span><span>preemptionPolicy</span>:<span> </span>PreemptLowerPriority<span>
</span></span></span><span><span><span>  </span><span>priority</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>  </span><span>schedulerName</span>:<span> </span>default-scheduler<span>
</span></span></span><span><span><span>  </span><span>securityContext</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>serviceAccount</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>serviceAccountName</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>  </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>  </span>- <span>effect</span>:<span> </span>NoExecute<span>
</span></span></span><span><span><span>    </span><span>key</span>:<span> </span>node.kubernetes.io/not-ready<span>
</span></span></span><span><span><span>    </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>    </span><span>tolerationSeconds</span>:<span> </span><span>300</span><span>
</span></span></span><span><span><span>  </span>- <span>effect</span>:<span> </span>NoExecute<span>
</span></span></span><span><span><span>    </span><span>key</span>:<span> </span>node.kubernetes.io/unreachable<span>
</span></span></span><span><span><span>    </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>    </span><span>tolerationSeconds</span>:<span> </span><span>300</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>kube-api-access-bgsgp<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>defaultMode</span>:<span> </span><span>420</span><span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>serviceAccountToken</span>:<span>
</span></span></span><span><span><span>          </span><span>expirationSeconds</span>:<span> </span><span>3607</span><span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>token<span>
</span></span></span><span><span><span>      </span>- <span>configMap</span>:<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>ca.crt<span>
</span></span></span><span><span><span>            </span><span>path</span>:<span> </span>ca.crt<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>kube-root-ca.crt<span>
</span></span></span><span><span><span>      </span>- <span>downwardAPI</span>:<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span>- <span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>metadata.namespace<span>
</span></span></span><span><span><span>            </span><span>path</span>:<span> </span>namespace<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>lastProbeTime</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T21:51:01Z"</span><span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Initialized<span>
</span></span></span><span><span><span>  </span>- <span>lastProbeTime</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T21:51:06Z"</span><span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Ready<span>
</span></span></span><span><span><span>  </span>- <span>lastProbeTime</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T21:51:06Z"</span><span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>ContainersReady<span>
</span></span></span><span><span><span>  </span>- <span>lastProbeTime</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T21:51:01Z"</span><span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>PodScheduled<span>
</span></span></span><span><span><span>  </span><span>containerStatuses</span>:<span>
</span></span></span><span><span><span>  </span>- <span>containerID</span>:<span> </span>containerd://5403af59a2b46ee5a23fb0ae4b1e077f7ca5c5fb7af16e1ab21c00e0e616462a<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>docker.io/library/nginx:latest<span>
</span></span></span><span><span><span>    </span><span>imageID</span>:<span> </span>docker.io/library/nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767<span>
</span></span></span><span><span><span>    </span><span>lastState</span>:<span> </span>{}<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>ready</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>restartCount</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>    </span><span>started</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>state</span>:<span>
</span></span></span><span><span><span>      </span><span>running</span>:<span>
</span></span></span><span><span><span>        </span><span>startedAt</span>:<span> </span><span>"2022-02-17T21:51:05Z"</span><span>
</span></span></span><span><span><span>  </span><span>hostIP</span>:<span> </span><span>192.168.0.113</span><span>
</span></span></span><span><span><span>  </span><span>phase</span>:<span> </span>Running<span>
</span></span></span><span><span><span>  </span><span>podIP</span>:<span> </span><span>10.88.0.3</span><span>
</span></span></span><span><span><span>  </span><span>podIPs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>ip</span>:<span> </span><span>10.88.0.3</span><span>
</span></span></span><span><span><span>  </span>- <span>ip</span>:<span> </span><span>2001</span>:db8::1<span>
</span></span></span><span><span><span>  </span><span>qosClass</span>:<span> </span>Guaranteed<span>
</span></span></span><span><span><span>  </span><span>startTime</span>:<span> </span><span>"2022-02-17T21:51:01Z"</span><span>
</span></span></span></code></pre></div><h2 id="examine-pod-logs">Examining pod logs</h2><p>First, look at the logs of the affected container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs <span>${</span><span>POD_NAME</span><span>}</span> -c <span>${</span><span>CONTAINER_NAME</span><span>}</span>
</span></span></code></pre></div><p>If your container has previously crashed, you can access the previous container's crash log with:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs <span>${</span><span>POD_NAME</span><span>}</span> -c <span>${</span><span>CONTAINER_NAME</span><span>}</span> --previous
</span></span></code></pre></div><h2 id="container-exec">Debugging with container exec</h2><p>If the <a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." href="/docs/reference/glossary/?all=true#term-image" target="_blank">container image</a> includes
debugging utilities, as is the case with images built from Linux and Windows OS
base images, you can run commands inside a specific container with
<code>kubectl exec</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> <span>${</span><span>POD_NAME</span><span>}</span> -c <span>${</span><span>CONTAINER_NAME</span><span>}</span> -- <span>${</span><span>CMD</span><span>}</span> <span>${</span><span>ARG1</span><span>}</span> <span>${</span><span>ARG2</span><span>}</span> ... <span>${</span><span>ARGN</span><span>}</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>-c ${CONTAINER_NAME}</code> is optional. You can omit it for Pods that only contain a single container.</div><p>As an example, to look at the logs from a running Cassandra pod, you might run</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> cassandra -- cat /var/log/cassandra/system.log
</span></span></code></pre></div><p>You can run a shell that's connected to your terminal using the <code>-i</code> and <code>-t</code>
arguments to <code>kubectl exec</code>, for example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it cassandra -- sh
</span></span></code></pre></div><p>For more details, see <a href="/docs/tasks/debug/debug-application/get-shell-running-container/">Get a Shell to a Running Container</a>.</p><h2 id="ephemeral-container">Debugging with an ephemeral debug container</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p><a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank">Ephemeral containers</a>
are useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient
because a container has crashed or a container image doesn't include debugging
utilities, such as with <a href="https://github.com/GoogleContainerTools/distroless">distroless images</a>.</p><h3 id="ephemeral-container-example">Example debugging using ephemeral containers</h3><p>You can use the <code>kubectl debug</code> command to add ephemeral containers to a
running Pod. First, create a pod for the example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run ephemeral-demo --image<span>=</span>registry.k8s.io/pause:3.1 --restart<span>=</span>Never
</span></span></code></pre></div><p>The examples in this section use the <code>pause</code> container image because it does not
contain debugging utilities, but this method works with all container
images.</p><p>If you attempt to use <code>kubectl exec</code> to create a shell you will see an error
because there is no shell in this container image.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it ephemeral-demo -- sh
</span></span></code></pre></div><pre tabindex="0"><code>OCI runtime exec failed: exec failed: container_linux.go:346: starting container process caused "exec: \"sh\": executable file not found in $PATH": unknown
</code></pre><p>You can instead add a debugging container using <code>kubectl debug</code>. If you
specify the <code>-i</code>/<code>--interactive</code> argument, <code>kubectl</code> will automatically attach
to the console of the Ephemeral Container.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl debug -it ephemeral-demo --image<span>=</span>busybox:1.28 --target<span>=</span>ephemeral-demo
</span></span></code></pre></div><pre tabindex="0"><code>Defaulting debug container name to debugger-8xzrl.
If you don't see a command prompt, try pressing enter.
/ #
</code></pre><p>This command adds a new busybox container and attaches to it. The <code>--target</code>
parameter targets the process namespace of another container. It's necessary
here because <code>kubectl run</code> does not enable <a href="/docs/tasks/configure-pod-container/share-process-namespace/">process namespace sharing</a> in the pod it
creates.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>--target</code> parameter must be supported by the <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">Container Runtime</a>. When not supported,
the Ephemeral Container may not be started, or it may be started with an
isolated process namespace so that <code>ps</code> does not reveal processes in other
containers.</div><p>You can view the state of the newly created ephemeral container using <code>kubectl describe</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod ephemeral-demo
</span></span></code></pre></div><pre tabindex="0"><code>...
Ephemeral Containers:
  debugger-8xzrl:
    Container ID:   docker://b888f9adfd15bd5739fefaa39e1df4dd3c617b9902082b1cfdc29c4028ffb2eb
    Image:          busybox
    Image ID:       docker-pullable://busybox@sha256:1828edd60c5efd34b2bf5dd3282ec0cc04d47b2ff9caa0b6d4f07a21d1c08084
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Wed, 12 Feb 2020 14:25:42 +0100
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:         &lt;none&gt;
...
</code></pre><p>Use <code>kubectl delete</code> to remove the Pod when you're finished:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod ephemeral-demo
</span></span></code></pre></div><h2 id="debugging-using-a-copy-of-the-pod">Debugging using a copy of the Pod</h2><p>Sometimes Pod configuration options make it difficult to troubleshoot in certain
situations. For example, you can't run <code>kubectl exec</code> to troubleshoot your
container if your container image does not include a shell or if your application
crashes on startup. In these situations you can use <code>kubectl debug</code> to create a
copy of the Pod with configuration values changed to aid debugging.</p><h3 id="copying-a-pod-while-adding-a-new-container">Copying a Pod while adding a new container</h3><p>Adding a new container can be useful when your application is running but not
behaving as you expect and you'd like to add additional troubleshooting
utilities to the Pod.</p><p>For example, maybe your application's container images are built on <code>busybox</code>
but you need debugging utilities not included in <code>busybox</code>. You can simulate
this scenario using <code>kubectl run</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run myapp --image<span>=</span>busybox:1.28 --restart<span>=</span>Never -- sleep 1d
</span></span></code></pre></div><p>Run this command to create a copy of <code>myapp</code> named <code>myapp-debug</code> that adds a
new Ubuntu container for debugging:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl debug myapp -it --image<span>=</span>ubuntu --share-processes --copy-to<span>=</span>myapp-debug
</span></span></code></pre></div><pre tabindex="0"><code>Defaulting debug container name to debugger-w7xmf.
If you don't see a command prompt, try pressing enter.
root@myapp-debug:/#
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li><code>kubectl debug</code> automatically generates a container name if you don't choose
one using the <code>--container</code> flag.</li><li>The <code>-i</code> flag causes <code>kubectl debug</code> to attach to the new container by
default. You can prevent this by specifying <code>--attach=false</code>. If your session
becomes disconnected you can reattach using <code>kubectl attach</code>.</li><li>The <code>--share-processes</code> allows the containers in this Pod to see processes
from the other containers in the Pod. For more information about how this
works, see <a href="/docs/tasks/configure-pod-container/share-process-namespace/">Share Process Namespace between Containers in a Pod</a>.</li></ul></div><p>Don't forget to clean up the debugging Pod when you're finished with it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod myapp myapp-debug
</span></span></code></pre></div><h3 id="copying-a-pod-while-changing-its-command">Copying a Pod while changing its command</h3><p>Sometimes it's useful to change the command for a container, for example to
add a debugging flag or because the application is crashing.</p><p>To simulate a crashing application, use <code>kubectl run</code> to create a container
that immediately exits:</p><pre tabindex="0"><code>kubectl run --image=busybox:1.28 myapp -- false
</code></pre><p>You can see using <code>kubectl describe pod myapp</code> that this container is crashing:</p><pre tabindex="0"><code>Containers:
  myapp:
    Image:         busybox
    ...
    Args:
      false
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
</code></pre><p>You can use <code>kubectl debug</code> to create a copy of this Pod with the command
changed to an interactive shell:</p><pre tabindex="0"><code>kubectl debug myapp -it --copy-to=myapp-debug --container=myapp -- sh
</code></pre><pre tabindex="0"><code>If you don't see a command prompt, try pressing enter.
/ #
</code></pre><p>Now you have an interactive shell that you can use to perform tasks like
checking filesystem paths or running the container command manually.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>To change the command of a specific container you must
specify its name using <code>--container</code> or <code>kubectl debug</code> will instead
create a new container to run the command you specified.</li><li>The <code>-i</code> flag causes <code>kubectl debug</code> to attach to the container by default.
You can prevent this by specifying <code>--attach=false</code>. If your session becomes
disconnected you can reattach using <code>kubectl attach</code>.</li></ul></div><p>Don't forget to clean up the debugging Pod when you're finished with it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod myapp myapp-debug
</span></span></code></pre></div><h3 id="copying-a-pod-while-changing-container-images">Copying a Pod while changing container images</h3><p>In some situations you may want to change a misbehaving Pod from its normal
production container images to an image containing a debugging build or
additional utilities.</p><p>As an example, create a Pod using <code>kubectl run</code>:</p><pre tabindex="0"><code>kubectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d
</code></pre><p>Now use <code>kubectl debug</code> to make a copy and change its container image
to <code>ubuntu</code>:</p><pre tabindex="0"><code>kubectl debug myapp --copy-to=myapp-debug --set-image=*=ubuntu
</code></pre><p>The syntax of <code>--set-image</code> uses the same <code>container_name=image</code> syntax as
<code>kubectl set image</code>. <code>*=ubuntu</code> means change the image of all containers
to <code>ubuntu</code>.</p><p>Don't forget to clean up the debugging Pod when you're finished with it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod myapp myapp-debug
</span></span></code></pre></div><h2 id="node-shell-session">Debugging via a shell on the node</h2><p>If none of these approaches work, you can find the Node on which the Pod is
running and create a Pod running on the Node. To create
an interactive shell on a Node using <code>kubectl debug</code>, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl debug node/mynode -it --image<span>=</span>ubuntu
</span></span></code></pre></div><pre tabindex="0"><code>Creating debugging pod node-debugger-mynode-pdx84 with container debugger on node mynode.
If you don't see a command prompt, try pressing enter.
root@ek8s:/#
</code></pre><p>When creating a debugging session on a node, keep in mind that:</p><ul><li><code>kubectl debug</code> automatically generates the name of the new Pod based on
the name of the Node.</li><li>The root filesystem of the Node will be mounted at <code>/host</code>.</li><li>The container runs in the host IPC, Network, and PID namespaces, although
the pod isn't privileged, so reading some process information may fail,
and <code>chroot /host</code> may fail.</li><li>If you need a privileged pod, create it manually or use the <code>--profile=sysadmin</code> flag.</li></ul><p>Don't forget to clean up the debugging Pod when you're finished with it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod node-debugger-mynode-pdx84
</span></span></code></pre></div><h2 id="debugging-profiles">Debugging a Pod or Node while applying a profile</h2><p>When using <code>kubectl debug</code> to debug a node via a debugging Pod, a Pod via an ephemeral container,
or a copied Pod, you can apply a profile to them.
By applying a profile, specific properties such as <a href="/docs/tasks/configure-pod-container/security-context/">securityContext</a>
are set, allowing for adaptation to various scenarios.
There are two types of profiles, static profile and custom profile.</p><h3 id="static-profile">Applying a Static Profile</h3><p>A static profile is a set of predefined properties, and you can apply them using the <code>--profile</code> flag.
The available profiles are as follows:</p><table><thead><tr><th>Profile</th><th>Description</th></tr></thead><tbody><tr><td>legacy</td><td>A set of properties backwards compatibility with 1.22 behavior</td></tr><tr><td>general</td><td>A reasonable set of generic properties for each debugging journey</td></tr><tr><td>baseline</td><td>A set of properties compatible with <a href="/docs/concepts/security/pod-security-standards/#baseline">PodSecurityStandard baseline policy</a></td></tr><tr><td>restricted</td><td>A set of properties compatible with <a href="/docs/concepts/security/pod-security-standards/#restricted">PodSecurityStandard restricted policy</a></td></tr><tr><td>netadmin</td><td>A set of properties including Network Administrator privileges</td></tr><tr><td>sysadmin</td><td>A set of properties including System Administrator (root) privileges</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you don't specify <code>--profile</code>, the <code>legacy</code> profile is used by default, but it is planned to be deprecated in the near future.
So it is recommended to use other profiles such as <code>general</code>.</div><p>Assume that you create a Pod and debug it.
First, create a Pod named <code>myapp</code> as an example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run myapp --image<span>=</span>busybox:1.28 --restart<span>=</span>Never -- sleep 1d
</span></span></code></pre></div><p>Then, debug the Pod using an ephemeral container.
If the ephemeral container needs to have privilege, you can use the <code>sysadmin</code> profile:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl debug -it myapp --image<span>=</span>busybox:1.28 --target<span>=</span>myapp --profile<span>=</span>sysadmin
</span></span></code></pre></div><pre tabindex="0"><code>Targeting container "myapp". If you don't see processes from this container it may be because the container runtime doesn't support this feature.
Defaulting debug container name to debugger-6kg4x.
If you don't see a command prompt, try pressing enter.
/ #
</code></pre><p>Check the capabilities of the ephemeral container process by running the following command inside the container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/ <span># grep Cap /proc/$$/status</span>
</span></span></code></pre></div><pre tabindex="0"><code>...
CapPrm:	000001ffffffffff
CapEff:	000001ffffffffff
...
</code></pre><p>This means the container process is granted full capabilities as a privileged container by applying <code>sysadmin</code> profile.
See more details about <a href="/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">capabilities</a>.</p><p>You can also check that the ephemeral container was created as a privileged container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod myapp -o <span>jsonpath</span><span>=</span><span>'{.spec.ephemeralContainers[0].securityContext}'</span>
</span></span></code></pre></div><pre tabindex="0"><code>{"privileged":true}
</code></pre><p>Clean up the Pod when you're finished with it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod myapp
</span></span></code></pre></div><h3 id="custom-profile">Applying Custom Profile</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code></div><p>You can define a partial container spec for debugging as a custom profile in either YAML or JSON format,
and apply it using the <code>--custom</code> flag.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Custom profile only supports the modification of the container spec,
but modifications to <code>name</code>, <code>image</code>, <code>command</code>, <code>lifecycle</code> and <code>volumeDevices</code> fields of the container spec
are not allowed.
It does not support the modification of the Pod spec.</div><p>Create a Pod named myapp as an example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run myapp --image<span>=</span>busybox:1.28 --restart<span>=</span>Never -- sleep 1d
</span></span></code></pre></div><p>Create a custom profile in YAML or JSON format.
Here, create a YAML format file named <code>custom-profile.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>env</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>ENV_VAR_1<span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span>value_1<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>ENV_VAR_2<span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span>value_2<span>
</span></span></span><span><span><span></span><span>securityContext</span>:<span>
</span></span></span><span><span><span>  </span><span>capabilities</span>:<span>
</span></span></span><span><span><span>    </span><span>add</span>:<span>
</span></span></span><span><span><span>    </span>- NET_ADMIN<span>
</span></span></span><span><span><span>    </span>- SYS_TIME<span>
</span></span></span></code></pre></div><p>Run this command to debug the Pod using an ephemeral container with the custom profile:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl debug -it myapp --image<span>=</span>busybox:1.28 --target<span>=</span>myapp --profile<span>=</span>general --custom<span>=</span>custom-profile.yaml
</span></span></code></pre></div><p>You can check that the ephemeral container has been added to the target Pod with the custom profile applied:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod <span>m</span>yapp -o <span>jsonpath</span><span>=</span><span>'{.spec.ephemeralContainers[0].env}'</span>
</span></span></code></pre></div><pre tabindex="0"><code>[{"name":"ENV_VAR_1","value":"value_1"},{"name":"ENV_VAR_2","value":"value_2"}]
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod myapp -o <span>jsonpath</span><span>=</span><span>'{.spec.ephemeralContainers[0].securityContext}'</span>
</span></span></code></pre></div><pre tabindex="0"><code>{"capabilities":{"add":["NET_ADMIN","SYS_TIME"]}}
</code></pre><p>Clean up the Pod when you're finished with it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod myapp
</span></span></code></pre></div></div></div><div><div class="td-content"><h1>Get a Shell to a Running Container</h1><p>This page shows how to use <code>kubectl exec</code> to get a shell to a
running container.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="getting-a-shell-to-a-container">Getting a shell to a container</h2><p>In this exercise, you create a Pod that has one container. The container
runs the nginx image. Here is the configuration file for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/shell-demo.yaml"><code>application/shell-demo.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/shell-demo.yaml to clipboard"></div><div class="includecode" id="application-shell-demo-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>shell-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shared-data<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>shared-data<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/usr/share/nginx/html<span>
</span></span></span><span><span><span>  </span><span>hostNetwork</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>dnsPolicy</span>:<span> </span>Default<span>
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/shell-demo.yaml
</span></span></code></pre></div><p>Verify that the container is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod shell-demo
</span></span></code></pre></div><p>Get a shell to the running container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> --stdin --tty shell-demo -- /bin/bash
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The double dash (<code>--</code>) separates the arguments you want to pass to the command from the kubectl arguments.</div><p>In your shell, list the root directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this inside the container</span>
</span></span><span><span>ls /
</span></span></code></pre></div><p>In your shell, experiment with other commands. Here are
some examples:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># You can run these example commands inside the container</span>
</span></span><span><span>ls /
</span></span><span><span>cat /proc/mounts
</span></span><span><span>cat /proc/1/maps
</span></span><span><span>apt-get update
</span></span><span><span>apt-get install -y tcpdump
</span></span><span><span>tcpdump
</span></span><span><span>apt-get install -y lsof
</span></span><span><span>lsof
</span></span><span><span>apt-get install -y procps
</span></span><span><span>ps aux
</span></span><span><span>ps aux | grep nginx
</span></span></code></pre></div><h2 id="writing-the-root-page-for-nginx">Writing the root page for nginx</h2><p>Look again at the configuration file for your Pod. The Pod
has an <code>emptyDir</code> volume, and the container mounts the volume
at <code>/usr/share/nginx/html</code>.</p><p>In your shell, create an <code>index.html</code> file in the <code>/usr/share/nginx/html</code>
directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this inside the container</span>
</span></span><span><span><span>echo</span> <span>'Hello shell demo'</span> &gt; /usr/share/nginx/html/index.html
</span></span></code></pre></div><p>In your shell, send a GET request to the nginx server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this in the shell inside your container</span>
</span></span><span><span>apt-get update
</span></span><span><span>apt-get install curl
</span></span><span><span>curl http://localhost/
</span></span></code></pre></div><p>The output shows the text that you wrote to the <code>index.html</code> file:</p><pre tabindex="0"><code>Hello shell demo
</code></pre><p>When you are finished with your shell, enter <code>exit</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>exit</span> <span># To quit the shell in the container</span>
</span></span></code></pre></div><h2 id="running-individual-commands-in-a-container">Running individual commands in a container</h2><p>In an ordinary command window, not your shell, list the environment
variables in the running container:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> shell-demo -- env
</span></span></code></pre></div><p>Experiment with running other commands. Here are some examples:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> shell-demo -- ps aux
</span></span><span><span>kubectl <span>exec</span> shell-demo -- ls /
</span></span><span><span>kubectl <span>exec</span> shell-demo -- cat /proc/1/mounts
</span></span></code></pre></div><h2 id="opening-a-shell-when-a-pod-has-more-than-one-container">Opening a shell when a Pod has more than one container</h2><p>If a Pod has more than one container, use <code>--container</code> or <code>-c</code> to
specify a container in the <code>kubectl exec</code> command. For example,
suppose you have a Pod named my-pod, and the Pod has two containers
named <em>main-app</em> and <em>helper-app</em>. The following command would open a
shell to the <em>main-app</em> container.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t my-pod --container main-app -- /bin/bash
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The short options <code>-i</code> and <code>-t</code> are the same as the long options <code>--stdin</code> and <code>--tty</code></div><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/reference/generated/kubectl/kubectl-commands/#exec">kubectl exec</a></li></ul></div></div><div><div class="td-content"><h1>Troubleshooting Clusters</h1><div class="lead">Debugging common cluster issues.</div><p>This doc is about cluster troubleshooting; we assume you have already ruled out your application as the root cause of the
problem you are experiencing. See
the <a href="/docs/tasks/debug/debug-application/">application troubleshooting guide</a> for tips on application debugging.
You may also visit the <a href="/docs/tasks/debug/">troubleshooting overview document</a> for more information.</p><p>For troubleshooting <a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a>, refer to
<a href="/docs/tasks/debug/debug-cluster/troubleshoot-kubectl/">Troubleshooting kubectl</a>.</p><h2 id="listing-your-cluster">Listing your cluster</h2><p>The first thing to debug in your cluster is if your nodes are all registered correctly.</p><p>Run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes
</span></span></code></pre></div><p>And verify that all of the nodes you expect to see are present and that they are all in the <code>Ready</code> state.</p><p>To get detailed information about the overall health of your cluster, you can run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info dump
</span></span></code></pre></div><h3 id="example-debugging-a-down-unreachable-node">Example: debugging a down/unreachable node</h3><p>Sometimes when debugging it can be useful to look at the status of a node -- for example, because
you've noticed strange behavior of a Pod that's running on the node, or to find out why a Pod
won't schedule onto the node. As with Pods, you can use <code>kubectl describe node</code> and <code>kubectl get node -o yaml</code> to retrieve detailed information about nodes. For example, here's what you'll see if
a node is down (disconnected from the network, or kubelet dies and won't restart, etc.). Notice
the events that show the node is NotReady, and also notice that the pods are no longer running
(they are evicted after five minutes of NotReady status).</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                     STATUS       ROLES     AGE     VERSION
kube-worker-1            NotReady     &lt;none&gt;    1h      v1.23.3
kubernetes-node-bols     Ready        &lt;none&gt;    1h      v1.23.3
kubernetes-node-st6x     Ready        &lt;none&gt;    1h      v1.23.3
kubernetes-node-unaj     Ready        &lt;none&gt;    1h      v1.23.3
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe node kube-worker-1
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Name:               kube-worker-1
Roles:              &lt;none&gt;
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=kube-worker-1
                    kubernetes.io/os=linux
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 17 Feb 2022 16:46:30 -0500
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  kube-worker-1
  AcquireTime:     &lt;unset&gt;
  RenewTime:       Thu, 17 Feb 2022 17:13:09 -0500
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------    -----------------                 ------------------                ------              -------
  NetworkUnavailable   False     Thu, 17 Feb 2022 17:09:13 -0500   Thu, 17 Feb 2022 17:09:13 -0500   WeaveIsUp           Weave pod has set this
  MemoryPressure       Unknown   Thu, 17 Feb 2022 17:12:40 -0500   Thu, 17 Feb 2022 17:13:52 -0500   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Thu, 17 Feb 2022 17:12:40 -0500   Thu, 17 Feb 2022 17:13:52 -0500   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Thu, 17 Feb 2022 17:12:40 -0500   Thu, 17 Feb 2022 17:13:52 -0500   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Thu, 17 Feb 2022 17:12:40 -0500   Thu, 17 Feb 2022 17:13:52 -0500   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  192.168.0.113
  Hostname:    kube-worker-1
Capacity:
  cpu:                2
  ephemeral-storage:  15372232Ki
  hugepages-2Mi:      0
  memory:             2025188Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  14167048988
  hugepages-2Mi:      0
  memory:             1922788Ki
  pods:               110
System Info:
  Machine ID:                 9384e2927f544209b5d7b67474bbf92b
  System UUID:                aa829ca9-73d7-064d-9019-df07404ad448
  Boot ID:                    5a295a03-aaca-4340-af20-1327fa5dab5c
  Kernel Version:             5.13.0-28-generic
  OS Image:                   Ubuntu 21.10
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.5.9
  Kubelet Version:            v1.23.3
  Kube-Proxy Version:         v1.23.3
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     nginx-deployment-67d4bdd6f5-cx2nz    500m (25%)    500m (25%)  128Mi (6%)       128Mi (6%)     23m
  default                     nginx-deployment-67d4bdd6f5-w6kd7    500m (25%)    500m (25%)  128Mi (6%)       128Mi (6%)     23m
  kube-system                 kube-proxy-dnxbz                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m
  kube-system                 weave-net-gjxxp                      100m (5%)     0 (0%)      200Mi (10%)      0 (0%)         28m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1100m (55%)  1 (50%)
  memory             456Mi (24%)  256Mi (13%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
...
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get node kube-worker-1 -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Node<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>node.alpha.kubernetes.io/ttl</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>    </span><span>volumes.kubernetes.io/controller-managed-attach-detach</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2022-02-17T21:46:30Z"</span><span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>beta.kubernetes.io/arch</span>:<span> </span>amd64<span>
</span></span></span><span><span><span>    </span><span>beta.kubernetes.io/os</span>:<span> </span>linux<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/arch</span>:<span> </span>amd64<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/hostname</span>:<span> </span>kube-worker-1<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>linux<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kube-worker-1<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"4026"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>98efe7cb-2978-4a0b-842a-1a7bf12c05f8<span>
</span></span></span><span><span><span></span><span>spec</span>:<span> </span>{}<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>addresses</span>:<span>
</span></span></span><span><span><span>  </span>- <span>address</span>:<span> </span><span>192.168.0.113</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>InternalIP<span>
</span></span></span><span><span><span>  </span>- <span>address</span>:<span> </span>kube-worker-1<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Hostname<span>
</span></span></span><span><span><span>  </span><span>allocatable</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>    </span><span>ephemeral-storage</span>:<span> </span><span>"14167048988"</span><span>
</span></span></span><span><span><span>    </span><span>hugepages-2Mi</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span>1922788Ki<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"110"</span><span>
</span></span></span><span><span><span>  </span><span>capacity</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>    </span><span>ephemeral-storage</span>:<span> </span>15372232Ki<span>
</span></span></span><span><span><span>    </span><span>hugepages-2Mi</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span>2025188Ki<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"110"</span><span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>lastHeartbeatTime</span>:<span> </span><span>"2022-02-17T22:20:32Z"</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T22:20:32Z"</span><span>
</span></span></span><span><span><span>    </span><span>message</span>:<span> </span>Weave pod has set this<span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>WeaveIsUp<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>NetworkUnavailable<span>
</span></span></span><span><span><span>  </span>- <span>lastHeartbeatTime</span>:<span> </span><span>"2022-02-17T22:20:15Z"</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T22:13:25Z"</span><span>
</span></span></span><span><span><span>    </span><span>message</span>:<span> </span>kubelet has sufficient memory available<span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>KubeletHasSufficientMemory<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>MemoryPressure<span>
</span></span></span><span><span><span>  </span>- <span>lastHeartbeatTime</span>:<span> </span><span>"2022-02-17T22:20:15Z"</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T22:13:25Z"</span><span>
</span></span></span><span><span><span>    </span><span>message</span>:<span> </span>kubelet has no disk pressure<span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>KubeletHasNoDiskPressure<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>DiskPressure<span>
</span></span></span><span><span><span>  </span>- <span>lastHeartbeatTime</span>:<span> </span><span>"2022-02-17T22:20:15Z"</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T22:13:25Z"</span><span>
</span></span></span><span><span><span>    </span><span>message</span>:<span> </span>kubelet has sufficient PID available<span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>KubeletHasSufficientPID<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>PIDPressure<span>
</span></span></span><span><span><span>  </span>- <span>lastHeartbeatTime</span>:<span> </span><span>"2022-02-17T22:20:15Z"</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2022-02-17T22:15:15Z"</span><span>
</span></span></span><span><span><span>    </span><span>message</span>:<span> </span>kubelet is posting ready status<span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>KubeletReady<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Ready<span>
</span></span></span><span><span><span>  </span><span>daemonEndpoints</span>:<span>
</span></span></span><span><span><span>    </span><span>kubeletEndpoint</span>:<span>
</span></span></span><span><span><span>      </span><span>Port</span>:<span> </span><span>10250</span><span>
</span></span></span><span><span><span>  </span><span>nodeInfo</span>:<span>
</span></span></span><span><span><span>    </span><span>architecture</span>:<span> </span>amd64<span>
</span></span></span><span><span><span>    </span><span>bootID</span>:<span> </span><span>22333234</span>-7a6b-44d4-9ce1-67e31dc7e369<span>
</span></span></span><span><span><span>    </span><span>containerRuntimeVersion</span>:<span> </span>containerd://1.5.9<span>
</span></span></span><span><span><span>    </span><span>kernelVersion</span>:<span> </span><span>5.13.0-28</span>-generic<span>
</span></span></span><span><span><span>    </span><span>kubeProxyVersion</span>:<span> </span>v1.23.3<span>
</span></span></span><span><span><span>    </span><span>kubeletVersion</span>:<span> </span>v1.23.3<span>
</span></span></span><span><span><span>    </span><span>machineID</span>:<span> </span>9384e2927f544209b5d7b67474bbf92b<span>
</span></span></span><span><span><span>    </span><span>operatingSystem</span>:<span> </span>linux<span>
</span></span></span><span><span><span>    </span><span>osImage</span>:<span> </span>Ubuntu 21.10<span>
</span></span></span><span><span><span>    </span><span>systemUUID</span>:<span> </span>aa829ca9-73d7-064d-9019-df07404ad448<span>
</span></span></span></code></pre></div><h2 id="looking-at-logs">Looking at logs</h2><p>For now, digging deeper into the cluster requires logging into the relevant machines. Here are the locations
of the relevant log files. On systemd-based systems, you may need to use <code>journalctl</code> instead of examining log files.</p><h3 id="control-plane-nodes">Control Plane nodes</h3><ul><li><code>/var/log/kube-apiserver.log</code> - API Server, responsible for serving the API</li><li><code>/var/log/kube-scheduler.log</code> - Scheduler, responsible for making scheduling decisions</li><li><code>/var/log/kube-controller-manager.log</code> - a component that runs most Kubernetes built-in
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a>, with the notable exception of scheduling
(the kube-scheduler handles scheduling).</li></ul><h3 id="worker-nodes">Worker Nodes</h3><ul><li><code>/var/log/kubelet.log</code> - logs from the kubelet, responsible for running containers on the node</li><li><code>/var/log/kube-proxy.log</code> - logs from <code>kube-proxy</code>, which is responsible for directing traffic to Service endpoints</li></ul><h2 id="cluster-failure-modes">Cluster failure modes</h2><p>This is an incomplete list of things that could go wrong, and how to adjust your cluster setup to mitigate the problems.</p><h3 id="contributing-causes">Contributing causes</h3><ul><li>VM(s) shutdown</li><li>Network partition within cluster, or between cluster and users</li><li>Crashes in Kubernetes software</li><li>Data loss or unavailability of persistent storage (e.g. GCE PD or AWS EBS volume)</li><li>Operator error, for example, misconfigured Kubernetes software or application software</li></ul><h3 id="specific-scenarios">Specific scenarios</h3><ul><li>API server VM shutdown or apiserver crashing<ul><li>Results<ul><li>unable to stop, update, or start new pods, services, replication controller</li><li>existing pods and services should continue to work normally unless they depend on the Kubernetes API</li></ul></li></ul></li><li>API server backing storage lost<ul><li>Results<ul><li>the kube-apiserver component fails to start successfully and become healthy</li><li>kubelets will not be able to reach it but will continue to run the same pods and provide the same service proxying</li><li>manual recovery or recreation of apiserver state necessary before apiserver is restarted</li></ul></li></ul></li><li>Supporting services (node controller, replication controller manager, scheduler, etc) VM shutdown or crashes<ul><li>currently those are colocated with the apiserver, and their unavailability has similar consequences as apiserver</li><li>in future, these will be replicated as well and may not be co-located</li><li>they do not have their own persistent state</li></ul></li><li>Individual node (VM or physical machine) shuts down<ul><li>Results<ul><li>pods on that Node stop running</li></ul></li></ul></li><li>Network partition<ul><li>Results<ul><li>partition A thinks the nodes in partition B are down; partition B thinks the apiserver is down.
(Assuming the master VM ends up in partition A.)</li></ul></li></ul></li><li>Kubelet software fault<ul><li>Results<ul><li>crashing kubelet cannot start new pods on the node</li><li>kubelet might delete the pods or not</li><li>node marked unhealthy</li><li>replication controllers start new pods elsewhere</li></ul></li></ul></li><li>Cluster operator error<ul><li>Results<ul><li>loss of pods, services, etc</li><li>lost of apiserver backing store</li><li>users unable to read API</li><li>etc.</li></ul></li></ul></li></ul><h3 id="mitigations">Mitigations</h3><ul><li><p>Action: Use the IaaS provider's automatic VM restarting feature for IaaS VMs</p><ul><li>Mitigates: Apiserver VM shutdown or apiserver crashing</li><li>Mitigates: Supporting services VM shutdown or crashes</li></ul></li><li><p>Action: Use IaaS providers reliable storage (e.g. GCE PD or AWS EBS volume) for VMs with apiserver+etcd</p><ul><li>Mitigates: Apiserver backing storage lost</li></ul></li><li><p>Action: Use <a href="/docs/setup/production-environment/tools/kubeadm/high-availability/">high-availability</a> configuration</p><ul><li>Mitigates: Control plane node shutdown or control plane components (scheduler, API server, controller-manager) crashing<ul><li>Will tolerate one or more simultaneous node or component failures</li></ul></li><li>Mitigates: API server backing storage (i.e., etcd's data directory) lost<ul><li>Assumes HA (highly-available) etcd configuration</li></ul></li></ul></li><li><p>Action: Snapshot apiserver PDs/EBS-volumes periodically</p><ul><li>Mitigates: Apiserver backing storage lost</li><li>Mitigates: Some cases of operator error</li><li>Mitigates: Some cases of Kubernetes software fault</li></ul></li><li><p>Action: use replication controller and services in front of pods</p><ul><li>Mitigates: Node shutdown</li><li>Mitigates: Kubelet software fault</li></ul></li><li><p>Action: applications (containers) designed to tolerate unexpected restarts</p><ul><li>Mitigates: Node shutdown</li><li>Mitigates: Kubelet software fault</li></ul></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn about the metrics available in the
<a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/">Resource Metrics Pipeline</a></li><li>Discover additional tools for
<a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">monitoring resource usage</a></li><li>Use Node Problem Detector to
<a href="/docs/tasks/debug/debug-cluster/monitor-node-health/">monitor node health</a></li><li>Use <code>kubectl debug node</code> to <a href="/docs/tasks/debug/debug-cluster/kubectl-node-debug/">debug Kubernetes nodes</a></li><li>Use <code>crictl</code> to <a href="/docs/tasks/debug/debug-cluster/crictl/">debug Kubernetes nodes</a></li><li>Get more information about <a href="/docs/tasks/debug/debug-cluster/audit/">Kubernetes auditing</a></li><li>Use <code>telepresence</code> to <a href="/docs/tasks/debug/debug-cluster/local-debugging/">develop and debug services locally</a></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Troubleshooting kubectl</h1><p>This documentation is about investigating and diagnosing
<a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a> related issues.
If you encounter issues accessing <code>kubectl</code> or connecting to your cluster, this
document outlines various common scenarios and potential solutions to help
identify and address the likely cause.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to have a Kubernetes cluster.</li><li>You also need to have <code>kubectl</code> installed - see <a href="/docs/tasks/tools/#kubectl">install tools</a></li></ul><h2 id="verify-kubectl-setup">Verify kubectl setup</h2><p>Make sure you have installed and configured <code>kubectl</code> correctly on your local machine.
Check the <code>kubectl</code> version to ensure it is up-to-date and compatible with your cluster.</p><p>Check kubectl version:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl version
</span></span></code></pre></div><p>You'll see a similar output:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4",GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean",BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
</span></span></span><span><span><span>Kustomize Version: v5.0.1
</span></span></span><span><span><span>Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3",GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean",BuildDate:"2023-06-14T09:47:40Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
</span></span></span></code></pre></div><p>If you see <code>Unable to connect to the server: dial tcp &lt;server-ip&gt;:8443: i/o timeout</code>,
instead of <code>Server Version</code>, you need to troubleshoot kubectl connectivity with your cluster.</p><p>Make sure you have installed the kubectl by following the
<a href="/docs/tasks/tools/#kubectl">official documentation for installing kubectl</a>, and you have
properly configured the <code>$PATH</code> environment variable.</p><h2 id="check-kubeconfig">Check kubeconfig</h2><p>The <code>kubectl</code> requires a <code>kubeconfig</code> file to connect to a Kubernetes cluster. The
<code>kubeconfig</code> file is usually located under the <code>~/.kube/config</code> directory. Make sure
that you have a valid <code>kubeconfig</code> file. If you don't have a <code>kubeconfig</code> file, you can
obtain it from your Kubernetes administrator, or you can copy it from your Kubernetes
control plane's <code>/etc/kubernetes/admin.conf</code> directory. If you have deployed your
Kubernetes cluster on a cloud platform and lost your <code>kubeconfig</code> file, you can
re-generate it using your cloud provider's tools. Refer the cloud provider's
documentation for re-generating a <code>kubeconfig</code> file.</p><p>Check if the <code>$KUBECONFIG</code> environment variable is configured correctly. You can set
<code>$KUBECONFIG</code>environment variable or use the <code>--kubeconfig</code> parameter with the kubectl
to specify the directory of a <code>kubeconfig</code> file.</p><h2 id="check-vpn-connectivity">Check VPN connectivity</h2><p>If you are using a Virtual Private Network (VPN) to access your Kubernetes cluster,
make sure that your VPN connection is active and stable. Sometimes, VPN disconnections
can lead to connection issues with the cluster. Reconnect to the VPN and try accessing
the cluster again.</p><h2 id="authentication-and-authorization">Authentication and authorization</h2><p>If you are using the token based authentication and the kubectl is returning an error
regarding the authentication token or authentication server address, validate the
Kubernetes authentication token and the authentication server address are configured
properly.</p><p>If kubectl is returning an error regarding the authorization, make sure that you are
using the valid user credentials. And you have the permission to access the resource
that you have requested.</p><h2 id="verify-contexts">Verify contexts</h2><p>Kubernetes supports <a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">multiple clusters and contexts</a>.
Ensure that you are using the correct context to interact with your cluster.</p><p>List available contexts:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config get-contexts
</span></span></code></pre></div><p>Switch to the appropriate context:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config use-context &lt;context-name&gt;
</span></span></code></pre></div><h2 id="api-server-and-load-balancer">API server and load balancer</h2><p>The <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">kube-apiserver</a> server is the
central component of a Kubernetes cluster. If the API server or the load balancer that
runs in front of your API servers is not reachable or not responding, you won't be able
to interact with the cluster.</p><p>Check the if the API server's host is reachable by using <code>ping</code> command. Check cluster's
network connectivity and firewall. If your are using a cloud provider for deploying
the cluster, check your cloud provider's health check status for the cluster's
API server.</p><p>Verify the status of the load balancer (if used) to ensure it is healthy and forwarding
traffic to the API server.</p><h2 id="tls-problems">TLS problems</h2><ul><li>Additional tools required - <code>base64</code> and <code>openssl</code> version 3.0 or above.</li></ul><p>The Kubernetes API server only serves HTTPS requests by default. In that case TLS problems
may occur due to various reasons, such as certificate expiry or chain of trust validity.</p><p>You can find the TLS certificate in the kubeconfig file, located in the <code>~/.kube/config</code>
directory. The <code>certificate-authority</code> attribute contains the CA certificate and the
<code>client-certificate</code> attribute contains the client certificate.</p><p>Verify the expiry of these certificates:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view --flatten --output <span>'jsonpath={.clusters[0].cluster.certificate-authority-data}'</span> | base64 -d | openssl x509 -noout -dates
</span></span></code></pre></div><p>output:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>notBefore=Feb 13 05:57:47 2024 GMT
</span></span></span><span><span><span>notAfter=Feb 10 06:02:47 2034 GMT
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view --flatten --output <span>'jsonpath={.users[0].user.client-certificate-data}'</span>| base64 -d | openssl x509 -noout -dates
</span></span></code></pre></div><p>output:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>notBefore=Feb 13 05:57:47 2024 GMT
</span></span></span><span><span><span>notAfter=Feb 12 06:02:50 2025 GMT
</span></span></span></code></pre></div><h2 id="verify-kubectl-helpers">Verify kubectl helpers</h2><p>Some kubectl authentication helpers provide easy access to Kubernetes clusters. If you
have used such helpers and are facing connectivity issues, ensure that the necessary
configurations are still present.</p><p>Check kubectl configuration for authentication details:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view
</span></span></code></pre></div><p>If you previously used a helper tool (for example, <code>kubectl-oidc-login</code>), ensure that it is still
installed and configured correctly.</p></div></div><div><div class="td-content"><h1>Resource metrics pipeline</h1><p>For Kubernetes, the <em>Metrics API</em> offers a basic set of metrics to support automatic scaling and
similar use cases. This API makes information available about resource usage for node and pod,
including metrics for CPU and memory. If you deploy the Metrics API into your cluster, clients of
the Kubernetes API can then query for this information, and you can use Kubernetes' access control
mechanisms to manage permissions to do so.</p><p>The <a href="/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a> (HPA) and
<a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme">VerticalPodAutoscaler</a> (VPA)
use data from the metrics API to adjust workload replicas and resources to meet customer demand.</p><p>You can also view the resource metrics using the
<a href="/docs/reference/generated/kubectl/kubectl-commands#top"><code>kubectl top</code></a>
command.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The Metrics API, and the metrics pipeline that it enables, only offers the minimum
CPU and memory metrics to enable automatic scaling using HPA and / or VPA.
If you would like to provide a more complete set of metrics, you can complement
the simpler Metrics API by deploying a second
<a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/#full-metrics-pipeline">metrics pipeline</a>
that uses the <em>Custom Metrics API</em>.</div><p>Figure 1 illustrates the architecture of the resource metrics pipeline.</p><figure><div class="mermaid">flowchart RL
subgraph cluster[Cluster]
direction RL
S[<br><br>]
A[Metrics-<br>Server]
subgraph B[Nodes]
direction TB
D[cAdvisor] --&gt; C[kubelet]
E[Container<br>runtime] --&gt; D
E1[Container<br>runtime] --&gt; D
P[pod data] -.- C
end
L[API<br>server]
W[HPA]
C ----&gt;|node level<br>resource metrics| A --&gt;|metrics<br>API| L --&gt; W
end
L ---&gt; K[kubectl<br>top]
classDef box fill:#fff,stroke:#000,stroke-width:1px,color:#000;
class W,B,P,K,cluster,D,E,E1 box
classDef spacewhite fill:#ffffff,stroke:#fff,stroke-width:0px,color:#000
class S spacewhite
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:1px,color:#fff;
class A,L,C k8s</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>Figure 1. Resource Metrics Pipeline</p><p>The architecture components, from right to left in the figure, consist of the following:</p><ul><li><p><a href="https://github.com/google/cadvisor">cAdvisor</a>: Daemon for collecting, aggregating and exposing
container metrics included in Kubelet.</p></li><li><p><a href="/docs/concepts/architecture/#kubelet">kubelet</a>: Node agent for managing container
resources. Resource metrics are accessible using the <code>/metrics/resource</code> and <code>/stats</code> kubelet
API endpoints.</p></li><li><p><a href="/docs/reference/instrumentation/node-metrics/">node level resource metrics</a>: API provided by the kubelet for discovering and retrieving
per-node summarized stats available through the <code>/metrics/resource</code> endpoint.</p></li><li><p><a href="#metrics-server">metrics-server</a>: Cluster addon component that collects and aggregates resource
metrics pulled from each kubelet. The API server serves Metrics API for use by HPA, VPA, and by
the <code>kubectl top</code> command. Metrics Server is a reference implementation of the Metrics API.</p></li><li><p><a href="#metrics-api">Metrics API</a>: Kubernetes API supporting access to CPU and memory used for
workload autoscaling. To make this work in your cluster, you need an API extension server that
provides the Metrics API.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>cAdvisor supports reading metrics from cgroups, which works with typical container runtimes on Linux.
If you use a container runtime that uses another resource isolation mechanism, for example
virtualization, then that container runtime must support
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-container-stats.md">CRI Container Metrics</a>
in order for metrics to be available to the kubelet.</div></li></ul><h2 id="metrics-api">Metrics API</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes 1.8 [beta]</code></div><p>The metrics-server implements the Metrics API. This API allows you to access CPU and memory usage
for the nodes and pods in your cluster. Its primary role is to feed resource usage metrics to K8s
autoscaler components.</p><p>Here is an example of the Metrics API request for a <code>minikube</code> node piped through <code>jq</code> for easier
reading:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get --raw <span>"/apis/metrics.k8s.io/v1beta1/nodes/minikube"</span> | jq <span>'.'</span>
</span></span></code></pre></div><p>Here is the same API call using <code>curl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://localhost:8080/apis/metrics.k8s.io/v1beta1/nodes/minikube
</span></span></code></pre></div><p>Sample response:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"NodeMetrics"</span>,
</span></span><span><span>  <span>"apiVersion"</span>: <span>"metrics.k8s.io/v1beta1"</span>,
</span></span><span><span>  <span>"metadata"</span>: {
</span></span><span><span>    <span>"name"</span>: <span>"minikube"</span>,
</span></span><span><span>    <span>"selfLink"</span>: <span>"/apis/metrics.k8s.io/v1beta1/nodes/minikube"</span>,
</span></span><span><span>    <span>"creationTimestamp"</span>: <span>"2022-01-27T18:48:43Z"</span>
</span></span><span><span>  },
</span></span><span><span>  <span>"timestamp"</span>: <span>"2022-01-27T18:48:33Z"</span>,
</span></span><span><span>  <span>"window"</span>: <span>"30s"</span>,
</span></span><span><span>  <span>"usage"</span>: {
</span></span><span><span>    <span>"cpu"</span>: <span>"487558164n"</span>,
</span></span><span><span>    <span>"memory"</span>: <span>"732212Ki"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>Here is an example of the Metrics API request for a <code>kube-scheduler-minikube</code> pod contained in the
<code>kube-system</code> namespace and piped through <code>jq</code> for easier reading:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get --raw <span>"/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-minikube"</span> | jq <span>'.'</span>
</span></span></code></pre></div><p>Here is the same API call using <code>curl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://localhost:8080/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-minikube
</span></span></code></pre></div><p>Sample response:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"PodMetrics"</span>,
</span></span><span><span>  <span>"apiVersion"</span>: <span>"metrics.k8s.io/v1beta1"</span>,
</span></span><span><span>  <span>"metadata"</span>: {
</span></span><span><span>    <span>"name"</span>: <span>"kube-scheduler-minikube"</span>,
</span></span><span><span>    <span>"namespace"</span>: <span>"kube-system"</span>,
</span></span><span><span>    <span>"selfLink"</span>: <span>"/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-minikube"</span>,
</span></span><span><span>    <span>"creationTimestamp"</span>: <span>"2022-01-27T19:25:00Z"</span>
</span></span><span><span>  },
</span></span><span><span>  <span>"timestamp"</span>: <span>"2022-01-27T19:24:31Z"</span>,
</span></span><span><span>  <span>"window"</span>: <span>"30s"</span>,
</span></span><span><span>  <span>"containers"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"name"</span>: <span>"kube-scheduler"</span>,
</span></span><span><span>      <span>"usage"</span>: {
</span></span><span><span>        <span>"cpu"</span>: <span>"9559630n"</span>,
</span></span><span><span>        <span>"memory"</span>: <span>"22244Ki"</span>
</span></span><span><span>      }
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>The Metrics API is defined in the <a href="https://github.com/kubernetes/metrics">k8s.io/metrics</a>
repository. You must enable the <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">API aggregation layer</a>
and register an <a href="/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService</a>
for the <code>metrics.k8s.io</code> API.</p><p>To learn more about the Metrics API, see <a href="https://git.k8s.io/design-proposals-archive/instrumentation/resource-metrics-api.md">resource metrics API design</a>,
the <a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server repository</a> and the
<a href="https://github.com/kubernetes/metrics#resource-metrics-api">resource metrics API</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You must deploy the metrics-server or alternative adapter that serves the Metrics API to be able
to access it.</div><h2 id="measuring-resource-usage">Measuring resource usage</h2><h3 id="cpu">CPU</h3><p>CPU is reported as the average core usage measured in cpu units. One cpu, in Kubernetes, is
equivalent to 1 vCPU/Core for cloud providers, and 1 hyper-thread on bare-metal Intel processors.</p><p>This value is derived by taking a rate over a cumulative CPU counter provided by the kernel (in
both Linux and Windows kernels). The time window used to calculate CPU is shown under window field
in Metrics API.</p><p>To learn more about how Kubernetes allocates and measures CPU resources, see
<a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>.</p><h3 id="memory">Memory</h3><p>Memory is reported as the working set, measured in bytes, at the instant the metric was collected.</p><p>In an ideal world, the "working set" is the amount of memory in-use that cannot be freed under
memory pressure. However, calculation of the working set varies by host OS, and generally makes
heavy use of heuristics to produce an estimate.</p><p>The Kubernetes model for a container's working set expects that the container runtime counts
anonymous memory associated with the container in question. The working set metric typically also
includes some cached (file-backed) memory, because the host OS cannot always reclaim pages.</p><p>To learn more about how Kubernetes allocates and measures memory resources, see
<a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory">meaning of memory</a>.</p><h2 id="metrics-server">Metrics Server</h2><p>The metrics-server fetches resource metrics from the kubelets and exposes them in the Kubernetes
API server through the Metrics API for use by the HPA and VPA. You can also view these metrics
using the <code>kubectl top</code> command.</p><p>The metrics-server uses the Kubernetes API to track nodes and pods in your cluster. The
metrics-server queries each node over HTTP to fetch metrics. The metrics-server also builds an
internal view of pod metadata, and keeps a cache of pod health. That cached pod health information
is available via the extension API that the metrics-server makes available.</p><p>For example with an HPA query, the metrics-server needs to identify which pods fulfill the label
selectors in the deployment.</p><p>The metrics-server calls the <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> API
to collect metrics from each node. Depending on the metrics-server version it uses:</p><ul><li>Metrics resource endpoint <code>/metrics/resource</code> in version v0.6.0+ or</li><li>Summary API endpoint <code>/stats/summary</code> in older versions</li></ul><h2 id="what-s-next">What's next</h2><p>To learn more about the metrics-server, see the
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server repository</a>.</p><p>You can also check out the following:</p><ul><li><a href="https://git.k8s.io/design-proposals-archive/instrumentation/metrics-server.md">metrics-server design</a></li><li><a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md">metrics-server FAQ</a></li><li><a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/KNOWN_ISSUES.md">metrics-server known issues</a></li><li><a href="https://github.com/kubernetes-sigs/metrics-server/releases">metrics-server releases</a></li><li><a href="/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaling</a></li></ul><p>To learn about how the kubelet serves node metrics, and how you can access those via
the Kubernetes API, read <a href="/docs/reference/instrumentation/node-metrics/">Node Metrics Data</a>.</p></div></div><div><div class="td-content"><h1>Tools for Monitoring Resources</h1><p>To scale an application and provide a reliable service, you need to
understand how the application behaves when it is deployed. You can examine
application performance in a Kubernetes cluster by examining the containers,
<a href="/docs/concepts/workloads/pods/">pods</a>,
<a href="/docs/concepts/services-networking/service/">services</a>, and
the characteristics of the overall cluster. Kubernetes provides detailed
information about an application's resource usage at each of these levels.
This information allows you to evaluate your application's performance and
where bottlenecks can be removed to improve overall performance.</p><p>In Kubernetes, application monitoring does not depend on a single monitoring solution.
On new clusters, you can use <a href="#resource-metrics-pipeline">resource metrics</a> or
<a href="#full-metrics-pipeline">full metrics</a> pipelines to collect monitoring statistics.</p><h2 id="resource-metrics-pipeline">Resource metrics pipeline</h2><p>The resource metrics pipeline provides a limited set of metrics related to
cluster components such as the
<a href="/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a>
controller, as well as the <code>kubectl top</code> utility.
These metrics are collected by the lightweight, short-term, in-memory
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a> and
are exposed via the <code>metrics.k8s.io</code> API.</p><p>metrics-server discovers all nodes on the cluster and
queries each node's
<a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> for CPU and
memory usage. The kubelet acts as a bridge between the Kubernetes master and
the nodes, managing the pods and containers running on a machine. The kubelet
translates each pod into its constituent containers and fetches individual
container usage statistics from the container runtime through the container
runtime interface. If you use a container runtime that uses Linux cgroups and
namespaces to implement containers, and the container runtime does not publish
usage statistics, then the kubelet can look up those statistics directly
(using code from <a href="https://github.com/google/cadvisor">cAdvisor</a>).
No matter how those statistics arrive, the kubelet then exposes the aggregated pod
resource usage statistics through the metrics-server Resource Metrics API.
This API is served at <code>/metrics/resource/v1beta1</code> on the kubelet's authenticated and
read-only ports.</p><h2 id="full-metrics-pipeline">Full metrics pipeline</h2><p>A full metrics pipeline gives you access to richer metrics. Kubernetes can
respond to these metrics by automatically scaling or adapting the cluster
based on its current state, using mechanisms such as the Horizontal Pod
Autoscaler. The monitoring pipeline fetches metrics from the kubelet and
then exposes them to Kubernetes via an adapter by implementing either the
<code>custom.metrics.k8s.io</code> or <code>external.metrics.k8s.io</code> API.</p><p>Kubernetes is designed to work with <a href="https://openmetrics.io/">OpenMetrics</a>,
which is one of the
<a href="https://landscape.cncf.io/?group=projects-and-products&amp;view-mode=card#observability-and-analysis--monitoring">CNCF Observability and Analysis - Monitoring Projects</a>,
built upon and carefully extending <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus exposition format</a>
in almost 100% backwards-compatible ways.</p><p>If you glance over at the
<a href="https://landscape.cncf.io/?group=projects-and-products&amp;view-mode=card#observability-and-analysis--monitoring">CNCF Landscape</a>,
you can see a number of monitoring projects that can work with Kubernetes by <em>scraping</em>
metric data and using that to help you observe your cluster. It is up to you to select the tool
or tools that suit your needs. The CNCF landscape for observability and analytics includes a
mix of open-source software, paid-for software-as-a-service, and other commercial products.</p><p>When you design and implement a full metrics pipeline you can make that monitoring data
available back to Kubernetes. For example, a HorizontalPodAutoscaler can use the processed
metrics to work out how many Pods to run for a component of your workload.</p><p>Integration of a full metrics pipeline into your Kubernetes implementation is outside
the scope of Kubernetes documentation because of the very wide scope of possible
solutions.</p><p>The choice of monitoring platform depends heavily on your needs, budget, and technical resources.
Kubernetes does not recommend any specific metrics pipeline; <a href="https://landscape.cncf.io/?group=projects-and-products&amp;view-mode=card#observability-and-analysis--monitoring">many options</a> are available.
Your monitoring system should be capable of handling the <a href="https://openmetrics.io/">OpenMetrics</a> metrics
transmission standard and needs to be chosen to best fit into your overall design and deployment of
your infrastructure platform.</p><h2 id="what-s-next">What's next</h2><p>Learn about additional debugging tools, including:</p><ul><li><a href="/docs/concepts/cluster-administration/logging/">Logging</a></li><li><a href="/docs/tasks/debug/debug-application/get-shell-running-container/">Getting into containers via <code>exec</code></a></li><li><a href="/docs/tasks/extend-kubernetes/http-proxy-access-api/">Connecting to containers via proxies</a></li><li><a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Connecting to containers via port forwarding</a></li><li><a href="/docs/tasks/debug/debug-cluster/crictl/">Inspect Kubernetes node with crictl</a></li></ul></div></div><div><div class="td-content"><h1>Monitor Node Health</h1><p><em>Node Problem Detector</em> is a daemon for monitoring and reporting about a node's health.
You can run Node Problem Detector as a <code>DaemonSet</code> or as a standalone daemon.
Node Problem Detector collects information about node problems from various daemons
and reports these conditions to the API server as Node <a href="/docs/concepts/architecture/nodes/#condition">Condition</a>s
or as <a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">Event</a>s.</p><p>To learn how to install and use Node Problem Detector, see
<a href="https://github.com/kubernetes/node-problem-detector">Node Problem Detector project documentation</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="limitations">Limitations</h2><ul><li>Node Problem Detector uses the kernel log format for reporting kernel issues.
To learn how to extend the kernel log format, see <a href="#support-other-log-format">Add support for another log format</a>.</li></ul><h2 id="enabling-node-problem-detector">Enabling Node Problem Detector</h2><p>Some cloud providers enable Node Problem Detector as an <a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." href="/docs/concepts/cluster-administration/addons/" target="_blank">Addon</a>.
You can also enable Node Problem Detector with <code>kubectl</code> or by creating an Addon DaemonSet.</p><h3 id="using-kubectl">Using kubectl to enable Node Problem Detector</h3><p><code>kubectl</code> provides the most flexible management of Node Problem Detector.
You can overwrite the default configuration to fit it into your environment or
to detect customized node problems. For example:</p><ol><li><p>Create a Node Problem Detector configuration similar to <code>node-problem-detector.yaml</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/node-problem-detector.yaml"><code>debug/node-problem-detector.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy debug/node-problem-detector.yaml to clipboard"></div><div class="includecode" id="debug-node-problem-detector-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>node-problem-detector-v0.1<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>node-problem-detector<span>
</span></span></span><span><span><span>    </span><span>version</span>:<span> </span>v0.1<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>k8s-app</span>:<span> </span>node-problem-detector  <span>
</span></span></span><span><span><span>      </span><span>version</span>:<span> </span>v0.1<span>
</span></span></span><span><span><span>      </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>k8s-app</span>:<span> </span>node-problem-detector<span>
</span></span></span><span><span><span>        </span><span>version</span>:<span> </span>v0.1<span>
</span></span></span><span><span><span>        </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>hostNetwork</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>node-problem-detector<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/node-problem-detector:v0.1<span>
</span></span></span><span><span><span>        </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>          </span><span>privileged</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>"200m"</span><span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>"20m"</span><span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"20Mi"</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>log<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/log<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>log<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/log/</span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You should verify that the system log directory is right for your operating system distribution.</div></li><li><p>Start node problem detector with <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/debug/node-problem-detector.yaml
</span></span></code></pre></div></li></ol><h3 id="using-addon-pod">Using an Addon pod to enable Node Problem Detector</h3><p>If you are using a custom cluster bootstrap solution and don't need
to overwrite the default configuration, you can leverage the Addon pod to
further automate the deployment.</p><p>Create <code>node-problem-detector.yaml</code>, and save the configuration in the Addon pod's
directory <code>/etc/kubernetes/addons/node-problem-detector</code> on a control plane node.</p><h2 id="overwrite-the-configuration">Overwrite the configuration</h2><p>The <a href="https://github.com/kubernetes/node-problem-detector/tree/v0.8.12/config">default configuration</a>
is embedded when building the Docker image of Node Problem Detector.</p><p>However, you can use a <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/"><code>ConfigMap</code></a>
to overwrite the configuration:</p><ol><li><p>Change the configuration files in <code>config/</code></p></li><li><p>Create the <code>ConfigMap</code> <code>node-problem-detector-config</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap node-problem-detector-config --from-file<span>=</span>config/
</span></span></code></pre></div></li><li><p>Change the <code>node-problem-detector.yaml</code> to use the <code>ConfigMap</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/node-problem-detector-configmap.yaml"><code>debug/node-problem-detector-configmap.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy debug/node-problem-detector-configmap.yaml to clipboard"></div><div class="includecode" id="debug-node-problem-detector-configmap-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>node-problem-detector-v0.1<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>node-problem-detector<span>
</span></span></span><span><span><span>    </span><span>version</span>:<span> </span>v0.1<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>k8s-app</span>:<span> </span>node-problem-detector  <span>
</span></span></span><span><span><span>      </span><span>version</span>:<span> </span>v0.1<span>
</span></span></span><span><span><span>      </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>k8s-app</span>:<span> </span>node-problem-detector<span>
</span></span></span><span><span><span>        </span><span>version</span>:<span> </span>v0.1<span>
</span></span></span><span><span><span>        </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>hostNetwork</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>node-problem-detector<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/node-problem-detector:v0.1<span>
</span></span></span><span><span><span>        </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>          </span><span>privileged</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>"200m"</span><span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>"20m"</span><span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span><span>"20Mi"</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>log<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/log<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config<span> </span><span># Overwrite the config/ directory with ConfigMap volume</span><span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/config<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>log<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/log/<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config<span> </span><span># Define ConfigMap volume</span><span>
</span></span></span><span><span><span>        </span><span>configMap</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>node-problem-detector-config</span></span></code></pre></div></div></div></li><li><p>Recreate the Node Problem Detector with the new configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If you have a node-problem-detector running, delete before recreating</span>
</span></span><span><span>kubectl delete -f https://k8s.io/examples/debug/node-problem-detector.yaml
</span></span><span><span>kubectl apply -f https://k8s.io/examples/debug/node-problem-detector-configmap.yaml
</span></span></code></pre></div></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This approach only applies to a Node Problem Detector started with <code>kubectl</code>.</div><p>Overwriting a configuration is not supported if a Node Problem Detector runs as a cluster Addon.
The Addon manager does not support <code>ConfigMap</code>.</p><h2 id="problem-daemons">Problem Daemons</h2><p>A problem daemon is a sub-daemon of the Node Problem Detector. It monitors specific kinds of node
problems and reports them to the Node Problem Detector.
There are several types of supported problem daemons.</p><ul><li><p>A <code>SystemLogMonitor</code> type of daemon monitors the system logs and reports problems and metrics
according to predefined rules. You can customize the configurations for different log sources
such as <a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/kernel-monitor-filelog.json">filelog</a>,
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/kernel-monitor.json">kmsg</a>,
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/kernel-monitor-counter.json">kernel</a>,
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/abrt-adaptor.json">abrt</a>,
and <a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/systemd-monitor-counter.json">systemd</a>.</p></li><li><p>A <code>SystemStatsMonitor</code> type of daemon collects various health-related system stats as metrics.
You can customize its behavior by updating its
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/system-stats-monitor.json">configuration file</a>.</p></li><li><p>A <code>CustomPluginMonitor</code> type of daemon invokes and checks various node problems by running
user-defined scripts. You can use different custom plugin monitors to monitor different
problems and customize the daemon behavior by updating the
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/custom-plugin-monitor.json">configuration file</a>.</p></li><li><p>A <code>HealthChecker</code> type of daemon checks the health of the kubelet and container runtime on a node.</p></li></ul><h3 id="support-other-log-format">Adding support for other log format</h3><p>The system log monitor currently supports file-based logs, journald, and kmsg.
Additional sources can be added by implementing a new
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/pkg/systemlogmonitor/logwatchers/types/log_watcher.go">log watcher</a>.</p><h3 id="adding-custom-plugin-monitors">Adding custom plugin monitors</h3><p>You can extend the Node Problem Detector to execute any monitor scripts written in any language by
developing a custom plugin. The monitor scripts must conform to the plugin protocol in exit code
and standard output. For more information, please refer to the
<a href="https://docs.google.com/document/d/1jK_5YloSYtboj-DtfjmYKxfNnUxCAvohLnsH5aGCAYQ/edit">plugin interface proposal</a>.</p><h2 id="exporter">Exporter</h2><p>An exporter reports the node problems and/or metrics to certain backends.
The following exporters are supported:</p><ul><li><p><strong>Kubernetes exporter</strong>: this exporter reports node problems to the Kubernetes API server.
Temporary problems are reported as Events and permanent problems are reported as Node Conditions.</p></li><li><p><strong>Prometheus exporter</strong>: this exporter reports node problems and metrics locally as Prometheus
(or OpenMetrics) metrics. You can specify the IP address and port for the exporter using command
line arguments.</p></li><li><p><strong>Stackdriver exporter</strong>: this exporter reports node problems and metrics to the Stackdriver
Monitoring API. The exporting behavior can be customized using a
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.8.12/config/exporter/stackdriver-exporter.json">configuration file</a>.</p></li></ul><h2 id="recommendations-and-restrictions">Recommendations and restrictions</h2><p>It is recommended to run the Node Problem Detector in your cluster to monitor node health.
When running the Node Problem Detector, you can expect extra resource overhead on each node.
Usually this is fine, because:</p><ul><li>The kernel log grows relatively slowly.</li><li>A resource limit is set for the Node Problem Detector.</li><li>Even under high load, the resource usage is acceptable. For more information, see the Node Problem Detector
<a href="https://github.com/kubernetes/node-problem-detector/issues/2#issuecomment-220255629">benchmark result</a>.</li></ul></div></div><div><div class="td-content"><h1>Debugging Kubernetes nodes with crictl</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.11 [stable]</code></div><p><code>crictl</code> is a command-line interface for CRI-compatible container runtimes.
You can use it to inspect and debug container runtimes and applications on a
Kubernetes node. <code>crictl</code> and its source are hosted in the
<a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools</a> repository.</p><h2 id="before-you-begin">Before you begin</h2><p><code>crictl</code> requires a Linux operating system with a CRI runtime.</p><h2 id="installing-crictl">Installing crictl</h2><p>You can download a compressed archive <code>crictl</code> from the cri-tools
<a href="https://github.com/kubernetes-sigs/cri-tools/releases">release page</a>, for several
different architectures. Download the version that corresponds to your version
of Kubernetes. Extract it and move it to a location on your system path, such as
<code>/usr/local/bin/</code>.</p><h2 id="general-usage">General usage</h2><p>The <code>crictl</code> command has several subcommands and runtime flags. Use
<code>crictl help</code> or <code>crictl &lt;subcommand&gt; help</code> for more details.</p><p>You can set the endpoint for <code>crictl</code> by doing one of the following:</p><ul><li>Set the <code>--runtime-endpoint</code> and <code>--image-endpoint</code> flags.</li><li>Set the <code>CONTAINER_RUNTIME_ENDPOINT</code> and <code>IMAGE_SERVICE_ENDPOINT</code> environment
variables.</li><li>Set the endpoint in the configuration file <code>/etc/crictl.yaml</code>. To specify a
different file, use the <code>--config=PATH_TO_FILE</code> flag when you run <code>crictl</code>.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you don't set an endpoint, <code>crictl</code> attempts to connect to a list of known
endpoints, which might result in an impact to performance.</div><p>You can also specify timeout values when connecting to the server and enable or
disable debugging, by specifying <code>timeout</code> or <code>debug</code> values in the configuration
file or using the <code>--timeout</code> and <code>--debug</code> command-line flags.</p><p>To view or edit the current configuration, view or edit the contents of
<code>/etc/crictl.yaml</code>. For example, the configuration when using the <code>containerd</code>
container runtime would be similar to this:</p><pre tabindex="0"><code>runtime-endpoint: unix:///var/run/containerd/containerd.sock
image-endpoint: unix:///var/run/containerd/containerd.sock
timeout: 10
debug: true
</code></pre><p>To learn more about <code>crictl</code>, refer to the <a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md"><code>crictl</code>
documentation</a>.</p><h2 id="example-crictl-commands">Example crictl commands</h2><p>The following examples show some <code>crictl</code> commands and example output.</p><h3 id="list-pods">List pods</h3><p>List all pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>POD ID              CREATED              STATE               NAME                         NAMESPACE           ATTEMPT
926f1b5a1d33a       About a minute ago   Ready               sh-84d7dcf559-4r2gq          default             0
4dccb216c4adb       About a minute ago   Ready               nginx-65899c769f-wv2gp       default             0
a86316e96fa89       17 hours ago         Ready               kube-proxy-gblk4             kube-system         0
919630b8f81f1       17 hours ago         Ready               nvidia-device-plugin-zgbbv   kube-system         0
</code></pre><p>List pods by name:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl pods --name nginx-65899c769f-wv2gp
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>POD ID              CREATED             STATE               NAME                     NAMESPACE           ATTEMPT
4dccb216c4adb       2 minutes ago       Ready               nginx-65899c769f-wv2gp   default             0
</code></pre><p>List pods by label:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl pods --label <span>run</span><span>=</span>nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>POD ID              CREATED             STATE               NAME                     NAMESPACE           ATTEMPT
4dccb216c4adb       2 minutes ago       Ready               nginx-65899c769f-wv2gp   default             0
</code></pre><h3 id="list-images">List images</h3><p>List all images:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl images
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>IMAGE                                     TAG                 IMAGE ID            SIZE
busybox                                   latest              8c811b4aec35f       1.15MB
k8s-gcrio.azureedge.net/hyperkube-amd64   v1.10.3             e179bbfe5d238       665MB
k8s-gcrio.azureedge.net/pause-amd64       3.1                 da86e6ba6ca19       742kB
nginx                                     latest              cd5239a0906a6       109MB
</code></pre><p>List images by repository:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl images nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>IMAGE               TAG                 IMAGE ID            SIZE
nginx               latest              cd5239a0906a6       109MB
</code></pre><p>Only list image IDs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl images -q
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>sha256:8c811b4aec35f259572d0f79207bc0678df4c736eeec50bc9fec37ed936a472a
sha256:e179bbfe5d238de6069f3b03fccbecc3fb4f2019af741bfff1233c4d7b2970c5
sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e
sha256:cd5239a0906a6ccf0562354852fae04bc5b52d72a2aff9a871ddb6bd57553569
</code></pre><h3 id="list-containers">List containers</h3><p>List all containers:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl ps -a
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>CONTAINER ID        IMAGE                                                                                                             CREATED             STATE               NAME                       ATTEMPT
1f73f2d81bf98       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   7 minutes ago       Running             sh                         1
9c5951df22c78       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   8 minutes ago       Exited              sh                         0
87d3992f84f74       nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f                                     8 minutes ago       Running             nginx                      0
1941fb4da154f       k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a   18 hours ago        Running             kube-proxy                 0
</code></pre><p>List running containers:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl ps
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>CONTAINER ID        IMAGE                                                                                                             CREATED             STATE               NAME                       ATTEMPT
1f73f2d81bf98       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   6 minutes ago       Running             sh                         1
87d3992f84f74       nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f                                     7 minutes ago       Running             nginx                      0
1941fb4da154f       k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a   17 hours ago        Running             kube-proxy                 0
</code></pre><h3 id="execute-a-command-in-a-running-container">Execute a command in a running container</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl <span>exec</span> -i -t 1f73f2d81bf98 ls
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>bin   dev   etc   home  proc  root  sys   tmp   usr   var
</code></pre><h3 id="get-a-container-s-logs">Get a container's logs</h3><p>Get all container logs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl logs 87d3992f84f74
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>10.240.0.96 - - [06/Jun/2018:02:45:49 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
10.240.0.96 - - [06/Jun/2018:02:45:50 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
</code></pre><p>Get only the latest <code>N</code> lines of logs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>crictl logs --tail<span>=</span><span>1</span> 87d3992f84f74
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
</code></pre><h2 id="what-s-next">What's next</h2><ul><li><a href="https://github.com/kubernetes-sigs/cri-tools">Learn more about <code>crictl</code></a>.</li></ul></div></div><div><div class="td-content"><h1>Auditing</h1><p>Kubernetes <em>auditing</em> provides a security-relevant, chronological set of records documenting
the sequence of actions in a cluster. The cluster audits the activities generated by users,
by applications that use the Kubernetes API, and by the control plane itself.</p><p>Auditing allows cluster administrators to answer the following questions:</p><ul><li>what happened?</li><li>when did it happen?</li><li>who initiated it?</li><li>on what did it happen?</li><li>where was it observed?</li><li>from where was it initiated?</li><li>to where was it going?</li></ul><p>Audit records begin their lifecycle inside the
<a href="/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>
component. Each request on each stage
of its execution generates an audit event, which is then pre-processed according to
a certain policy and written to a backend. The policy determines what's recorded
and the backends persist the records. The current backend implementations
include logs files and webhooks.</p><p>Each request can be recorded with an associated <em>stage</em>. The defined stages are:</p><ul><li><code>RequestReceived</code> - The stage for events generated as soon as the audit
handler receives the request, and before it is delegated down the handler
chain.</li><li><code>ResponseStarted</code> - Once the response headers are sent, but before the
response body is sent. This stage is only generated for long-running requests
(e.g. watch).</li><li><code>ResponseComplete</code> - The response body has been completed and no more bytes
will be sent.</li><li><code>Panic</code> - Events generated when a panic occurred.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The configuration of an
<a href="/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event">Audit Event configuration</a>
is different from the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#event-v1-core">Event</a>
API object.</div><p>The audit logging feature increases the memory consumption of the API server
because some context required for auditing is stored for each request.
Memory consumption depends on the audit logging configuration.</p><h2 id="audit-policy">Audit policy</h2><p>Audit policy defines rules about what events should be recorded and what data
they should include. The audit policy object structure is defined in the
<a href="/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy"><code>audit.k8s.io</code> API group</a>.
When an event is processed, it's
compared against the list of rules in order. The first matching rule sets the
<em>audit level</em> of the event. The defined audit levels are:</p><ul><li><code>None</code> - don't log events that match this rule.</li><li><code>Metadata</code> - log events with metadata (requesting user, timestamp, resource,
verb, etc.) but not request or response body.</li><li><code>Request</code> - log events with request metadata and body but not response body.
This does not apply for non-resource requests.</li><li><code>RequestResponse</code> - log events with request metadata, request body and response body.
This does not apply for non-resource requests.</li></ul><p>You can pass a file with the policy to <code>kube-apiserver</code>
using the <code>--audit-policy-file</code> flag. If the flag is omitted, no events are logged.
Note that the <code>rules</code> field <strong>must</strong> be provided in the audit policy file.
A policy with no (0) rules is treated as illegal.</p><p>Below is an example audit policy file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/audit/audit-policy.yaml"><code>audit/audit-policy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy audit/audit-policy.yaml to clipboard"></div><div class="includecode" id="audit-audit-policy-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>audit.k8s.io/v1<span> </span><span># This is required.</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Policy<span>
</span></span></span><span><span><span></span><span># Don't generate audit events for all requests in RequestReceived stage.</span><span>
</span></span></span><span><span><span></span><span>omitStages</span>:<span>
</span></span></span><span><span><span>  </span>- <span>"RequestReceived"</span><span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span><span># Log pod changes at RequestResponse level</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>RequestResponse<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>      </span><span># Resource "pods" doesn't match requests to any subresource of pods,</span><span>
</span></span></span><span><span><span>      </span><span># which is consistent with the RBAC policy.</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span> </span>[<span>"pods"</span>]<span>
</span></span></span><span><span><span>  </span><span># Log "pods/log", "pods/status" at Metadata level</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>Metadata<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span> </span>[<span>"pods/log"</span>,<span> </span><span>"pods/status"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># Don't log requests to a configmap called "controller-leader"</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>None<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span> </span>[<span>"configmaps"</span>]<span>
</span></span></span><span><span><span>      </span><span>resourceNames</span>:<span> </span>[<span>"controller-leader"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># Don't log watch requests by the "system:kube-proxy" on endpoints or services</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>None<span>
</span></span></span><span><span><span>    </span><span>users</span>:<span> </span>[<span>"system:kube-proxy"</span>]<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span> </span>[<span>"watch"</span>]<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span> </span><span># core API group</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span> </span>[<span>"endpoints"</span>,<span> </span><span>"services"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># Don't log authenticated requests to certain non-resource URL paths.</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>None<span>
</span></span></span><span><span><span>    </span><span>userGroups</span>:<span> </span>[<span>"system:authenticated"</span>]<span>
</span></span></span><span><span><span>    </span><span>nonResourceURLs</span>:<span>
</span></span></span><span><span><span>    </span>- <span>"/api*"</span><span> </span><span># Wildcard matching.</span><span>
</span></span></span><span><span><span>    </span>- <span>"/version"</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># Log the request body of configmap changes in kube-system.</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>Request<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span> </span><span># core API group</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span> </span>[<span>"configmaps"</span>]<span>
</span></span></span><span><span><span>    </span><span># This rule only applies to resources in the "kube-system" namespace.</span><span>
</span></span></span><span><span><span>    </span><span># The empty string "" can be used to select non-namespaced resources.</span><span>
</span></span></span><span><span><span>    </span><span>namespaces</span>:<span> </span>[<span>"kube-system"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># Log configmap and secret changes in all other namespaces at the Metadata level.</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>Metadata<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span> </span><span># core API group</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span> </span>[<span>"secrets"</span>,<span> </span><span>"configmaps"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># Log all other resources in core and extensions at the Request level.</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>Request<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>""</span><span> </span><span># core API group</span><span>
</span></span></span><span><span><span>    </span>- <span>group</span>:<span> </span><span>"extensions"</span><span> </span><span># Version of group should NOT be included.</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># A catch-all rule to log all other requests at the Metadata level.</span><span>
</span></span></span><span><span><span>  </span>- <span>level</span>:<span> </span>Metadata<span>
</span></span></span><span><span><span>    </span><span># Long-running requests like watches that fall under this rule will not</span><span>
</span></span></span><span><span><span>    </span><span># generate an audit event in RequestReceived.</span><span>
</span></span></span><span><span><span>    </span><span>omitStages</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"RequestReceived"</span><span>
</span></span></span></code></pre></div></div></div><p>You can use a minimal audit policy file to log all requests at the <code>Metadata</code> level:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Log all requests at the Metadata level.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>audit.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Policy<span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span></span>- <span>level</span>:<span> </span>Metadata<span>
</span></span></span></code></pre></div><p>If you're crafting your own audit profile, you can use the audit profile for Google Container-Optimized OS as a starting point. You can check the
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh">configure-helper.sh</a>
script, which generates an audit policy file. You can see most of the audit policy file by looking directly at the script.</p><p>You can also refer to the <a href="/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy"><code>Policy</code> configuration reference</a>
for details about the fields defined.</p><h2 id="audit-backends">Audit backends</h2><p>Audit backends persist audit events to an external storage.
Out of the box, the kube-apiserver provides two backends:</p><ul><li>Log backend, which writes events into the filesystem</li><li>Webhook backend, which sends events to an external HTTP API</li></ul><p>In all cases, audit events follow a structure defined by the Kubernetes API in the
<a href="/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event"><code>audit.k8s.io</code> API group</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In case of patches, request body is a JSON array with patch operations, not a JSON object
with an appropriate Kubernetes API object. For example, the following request body is a valid patch
request to <code>/apis/batch/v1/namespaces/some-namespace/jobs/some-job-name</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>[
</span></span><span><span>  {
</span></span><span><span>    <span>"op"</span>: <span>"replace"</span>,
</span></span><span><span>    <span>"path"</span>: <span>"/spec/parallelism"</span>,
</span></span><span><span>    <span>"value"</span>: <span>0</span>
</span></span><span><span>  },
</span></span><span><span>  {
</span></span><span><span>    <span>"op"</span>: <span>"remove"</span>,
</span></span><span><span>    <span>"path"</span>: <span>"/spec/template/spec/containers/0/terminationMessagePolicy"</span>
</span></span><span><span>  }
</span></span><span><span>]
</span></span></code></pre></div></div><h3 id="log-backend">Log backend</h3><p>The log backend writes audit events to a file in <a href="https://jsonlines.org/">JSONlines</a> format.
You can configure the log audit backend using the following <code>kube-apiserver</code> flags:</p><ul><li><code>--audit-log-path</code> specifies the log file path that log backend uses to write
audit events. Not specifying this flag disables log backend. <code>-</code> means standard out</li><li><code>--audit-log-maxage</code> defined the maximum number of days to retain old audit log files</li><li><code>--audit-log-maxbackup</code> defines the maximum number of audit log files to retain</li><li><code>--audit-log-maxsize</code> defines the maximum size in megabytes of the audit log file before it gets rotated</li></ul><p>If your cluster's control plane runs the kube-apiserver as a Pod, remember to mount the <code>hostPath</code>
to the location of the policy file and log file, so that audit records are persisted. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span>- --audit-policy-file=/etc/kubernetes/audit-policy.yaml<span>
</span></span></span><span><span><span>  </span>- --audit-log-path=/var/log/kubernetes/audit/audit.log<span>
</span></span></span></code></pre></div><p>then mount the volumes:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>  </span>- <span>mountPath</span>:<span> </span>/etc/kubernetes/audit-policy.yaml<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>audit<span>
</span></span></span><span><span><span>    </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span>- <span>mountPath</span>:<span> </span>/var/log/kubernetes/audit/<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>audit-log<span>
</span></span></span><span><span><span>    </span><span>readOnly</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div><p>and finally configure the <code>hostPath</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>volumes</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>audit<span>
</span></span></span><span><span><span>  </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/etc/kubernetes/audit-policy.yaml<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>File<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>audit-log<span>
</span></span></span><span><span><span>  </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/var/log/kubernetes/audit/<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>DirectoryOrCreate<span>
</span></span></span></code></pre></div><h3 id="webhook-backend">Webhook backend</h3><p>The webhook audit backend sends audit events to a remote web API, which is assumed to
be a form of the Kubernetes API, including means of authentication. You can configure
a webhook audit backend using the following kube-apiserver flags:</p><ul><li><code>--audit-webhook-config-file</code> specifies the path to a file with a webhook
configuration. The webhook configuration is effectively a specialized
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">kubeconfig</a>.</li><li><code>--audit-webhook-initial-backoff</code> specifies the amount of time to wait after the first failed
request before retrying. Subsequent requests are retried with exponential backoff.</li></ul><p>The webhook config file uses the kubeconfig format to specify the remote address of
the service and credentials used to connect to it.</p><h2 id="batching">Event batching</h2><p>Both <code>log</code> and <code>webhook</code> backends support batching. Below is a list of
available flags specific to each backend.
By default, batching and throttling are <strong>enabled</strong> for the <code>webhook</code> backend and <strong>disabled</strong> for the <code>log</code> backend.</p><ul class="nav nav-tabs" id="tab-with-md"><li class="nav-item"><a class="nav-link active" href="#tab-with-md-0">webhook</a></li><li class="nav-item"><a class="nav-link" href="#tab-with-md-1">log</a></li></ul><div class="tab-content" id="tab-with-md"><div id="tab-with-md-0" class="tab-pane show active"><p><ul><li><code>--audit-webhook-mode</code> defines the buffering strategy. One of the following:<ul><li><code>batch</code> - buffer events and asynchronously process them in batches. This is the default mode for the <code>webhook</code> backend.</li><li><code>blocking</code> - block API server responses on processing each individual event.</li><li><code>blocking-strict</code> - Same as blocking, but when there is a failure during audit logging at the
RequestReceived stage, the whole request to the kube-apiserver fails.</li></ul></li></ul><p>The following flags are used only in the <code>batch</code> mode:</p><ul><li><code>--audit-webhook-batch-buffer-size</code> defines the number of events to buffer before batching.
If the rate of incoming events overflows the buffer, events are dropped. The default value is 10000.</li><li><code>--audit-webhook-batch-max-size</code> defines the maximum number of events in one batch. The default value is 400.</li><li><code>--audit-webhook-batch-max-wait</code> defines the maximum amount of time to wait before unconditionally
batching events in the queue. The default value is 30 seconds.</li><li><code>--audit-webhook-batch-throttle-enable</code> defines whether batching throttling is enabled. Throttling is enabled by default.</li><li><code>--audit-webhook-batch-throttle-qps</code> defines the maximum average number of batches generated
per second. The default value is 10.</li><li><code>--audit-webhook-batch-throttle-burst</code> defines the maximum number of batches generated at the same
moment if the allowed QPS was underutilized previously. The default value is 15.</li></ul></p></div><div id="tab-with-md-1" class="tab-pane"><p><ul><li><code>--audit-log-mode</code> defines the buffering strategy. One of the following:<ul><li><code>batch</code> - buffer events and asynchronously process them in batches. Batching is not recommended for the <code>log</code> backend.</li><li><code>blocking</code> - block API server responses on processing each individual event. This is the default mode for the <code>log</code> backend.</li><li><code>blocking-strict</code> - Same as blocking, but when there is a failure during audit logging at the
RequestReceived stage, the whole request to the kube-apiserver fails.</li></ul></li></ul><p>The following flags are used only in the <code>batch</code> mode (batching is <strong>disabled</strong> by default for the <code>log</code> backend, and when batching is disabled, all batching-related flags are ignored):</p><ul><li><code>--audit-log-batch-buffer-size</code> defines the number of events to buffer before batching.
If the rate of incoming events overflows the buffer, events are dropped.</li><li><code>--audit-log-batch-max-size</code> defines the maximum number of events in one batch.</li><li><code>--audit-log-batch-max-wait</code> defines the maximum amount of time to wait before unconditionally
batching events in the queue.</li><li><code>--audit-log-batch-throttle-enable</code> defines whether batching throttling is enabled.</li><li><code>--audit-log-batch-throttle-qps</code> defines the maximum average number of batches generated
per second.</li><li><code>--audit-log-batch-throttle-burst</code> defines the maximum number of batches generated at the same
moment if the allowed QPS was underutilized previously.</li></ul></p></div></div><h2 id="parameter-tuning">Parameter tuning</h2><p>Parameters should be set to accommodate the load on the API server.</p><p>For example, if kube-apiserver receives 100 requests each second, and each request is audited only
on <code>ResponseStarted</code> and <code>ResponseComplete</code> stages, you should account for &#8773;200 audit
events being generated each second. Assuming that there are up to 100 events in a batch,
you should set throttling level at least 2 queries per second. Assuming that the backend can take up to
5 seconds to write events, you should set the buffer size to hold up to 5 seconds of events;
that is: 10 batches, or 1000 events.</p><p>In most cases however, the default parameters should be sufficient and you don't have to worry about
setting them manually. You can look at the following Prometheus metrics exposed by kube-apiserver
and in the logs to monitor the state of the auditing subsystem.</p><ul><li><code>apiserver_audit_event_total</code> metric contains the total number of audit events exported.</li><li><code>apiserver_audit_error_total</code> metric contains the total number of events dropped due to an error
during exporting.</li></ul><h3 id="truncate">Log entry truncation</h3><p>Both log and webhook backends support limiting the size of events that are logged.
As an example, the following is the list of flags available for the log backend:</p><ul><li><code>audit-log-truncate-enabled</code> whether event and batch truncating is enabled.</li><li><code>audit-log-truncate-max-batch-size</code> maximum size in bytes of the batch sent to the underlying backend.</li><li><code>audit-log-truncate-max-event-size</code> maximum size in bytes of the audit event sent to the underlying backend.</li></ul><p>By default truncate is disabled in both <code>webhook</code> and <code>log</code>, a cluster administrator should set
<code>audit-log-truncate-enabled</code> or <code>audit-webhook-truncate-enabled</code> to enable the feature.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#mutating-webhook-auditing-annotations">Mutating webhook auditing annotations</a>.</li><li>Learn more about <a href="/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event"><code>Event</code></a>
and the <a href="/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy"><code>Policy</code></a>
resource types by reading the Audit configuration reference.</li></ul></div></div><div><div class="td-content"><h1>Debugging Kubernetes Nodes With Kubectl</h1><p>This page shows how to debug a <a href="/docs/concepts/architecture/nodes/">node</a>
running on the Kubernetes cluster using <code>kubectl debug</code> command.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.2.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>You need to have permission to create Pods and to assign those new Pods to arbitrary nodes.
You also need to be authorized to create Pods that access filesystems from the host.</p><h2 id="debugging-a-node-using-kubectl-debug-node">Debugging a Node using <code>kubectl debug node</code></h2><p>Use the <code>kubectl debug node</code> command to deploy a Pod to a Node that you want to troubleshoot.
This command is helpful in scenarios where you can't access your Node by using an SSH connection.
When the Pod is created, the Pod opens an interactive shell on the Node.
To create an interactive shell on a Node named &#8220;mynode&#8221;, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl debug node/mynode -it --image<span>=</span>ubuntu
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Creating debugging pod node-debugger-mynode-pdx84 with container debugger on node mynode.
</span></span></span><span><span><span>If you don't see a command prompt, try pressing enter.
</span></span></span><span><span><span>root@mynode:/#
</span></span></span></code></pre></div><p>The debug command helps to gather information and troubleshoot issues. Commands
that you might use include <code>ip</code>, <code>ifconfig</code>, <code>nc</code>, <code>ping</code>, and <code>ps</code> and so on. You can also
install other tools, such as <code>mtr</code>, <code>tcpdump</code>, and <code>curl</code>, from the respective package manager.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The debug commands may differ based on the image the debugging pod is using and
these commands might need to be installed.</div><p>The debugging Pod can access the root filesystem of the Node, mounted at <code>/host</code> in the Pod.
If you run your kubelet in a filesystem namespace,
the debugging Pod sees the root for that namespace, not for the entire node. For a typical Linux node,
you can look at the following paths to find relevant logs:</p><dl><dt><code>/host/var/log/kubelet.log</code></dt><dd>Logs from the <code>kubelet</code>, responsible for running containers on the node.</dd><dt><code>/host/var/log/kube-proxy.log</code></dt><dd>Logs from <code>kube-proxy</code>, which is responsible for directing traffic to Service endpoints.</dd><dt><code>/host/var/log/containerd.log</code></dt><dd>Logs from the <code>containerd</code> process running on the node.</dd><dt><code>/host/var/log/syslog</code></dt><dd>Shows general messages and information regarding the system.</dd><dt><code>/host/var/log/kern.log</code></dt><dd>Shows kernel logs.</dd></dl><p>When creating a debugging session on a Node, keep in mind that:</p><ul><li><code>kubectl debug</code> automatically generates the name of the new pod, based on
the name of the node.</li><li>The root filesystem of the Node will be mounted at <code>/host</code>.</li><li>Although the container runs in the host IPC, Network, and PID namespaces,
the pod isn't privileged. This means that reading some process information might fail
because access to that information is restricted to superusers. For example, <code>chroot /host</code> will fail.
If you need a privileged pod, create it manually or use the <code>--profile=sysadmin</code> flag.</li><li>By applying <a href="/docs/tasks/debug/debug-application/debug-running-pod/#debugging-profiles">Debugging Profiles</a>, you can set specific properties such as <a href="/docs/tasks/configure-pod-container/security-context/">securityContext</a> to a debugging Pod.</li></ul><h2 id="cleaning-up">Cleaning up</h2><p>When you finish using the debugging Pod, delete it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                          READY   STATUS       RESTARTS   AGE
node-debugger-mynode-pdx84    0/1     Completed    0          8m1s
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Change the pod name accordingly</span>
</span></span><span><span>kubectl delete pod node-debugger-mynode-pdx84 --now
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">pod "node-debugger-mynode-pdx84" deleted
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>kubectl debug node</code> command won't work if the Node is down (disconnected
from the network, or kubelet dies and won't restart, etc.).
Check <a href="/docs/tasks/debug/debug-cluster/#example-debugging-a-down-unreachable-node">debugging a down/unreachable node </a>in that case.</div></div></div><div><div class="td-content"><h1>Developing and debugging services locally using telepresence</h1><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Kubernetes applications usually consist of multiple, separate services,
each running in its own container. Developing and debugging these services
on a remote Kubernetes cluster can be cumbersome, requiring you to
<a href="/docs/tasks/debug/debug-application/get-shell-running-container/">get a shell on a running container</a>
in order to run debugging tools.</p><p><code>telepresence</code> is a tool to ease the process of developing and debugging
services locally while proxying the service to a remote Kubernetes cluster.
Using <code>telepresence</code> allows you to use custom tools, such as a debugger and
IDE, for a local service and provides the service full access to ConfigMap,
secrets, and the services running on the remote cluster.</p><p>This document describes using <code>telepresence</code> to develop and debug services
running on a remote cluster locally.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Kubernetes cluster is installed</li><li><code>kubectl</code> is configured to communicate with the cluster</li><li><a href="https://www.telepresence.io/docs/latest/quick-start/">Telepresence</a> is installed</li></ul><h2 id="connecting-your-local-machine-to-a-remote-kubernetes-cluster">Connecting your local machine to a remote Kubernetes cluster</h2><p>After installing <code>telepresence</code>, run <code>telepresence connect</code> to launch
its Daemon and connect your local workstation to the cluster.</p><pre tabindex="0"><code>$ telepresence connect
 
Launching Telepresence Daemon
...
Connected to context default (https://&lt;cluster public IP&gt;)
</code></pre><p>You can curl services using the Kubernetes syntax e.g. <code>curl -ik https://kubernetes.default</code></p><h2 id="developing-or-debugging-an-existing-service">Developing or debugging an existing service</h2><p>When developing an application on Kubernetes, you typically program
or debug a single service. The service might require access to other
services for testing and debugging. One option is to use the continuous
deployment pipeline, but even the fastest deployment pipeline introduces
a delay in the program or debug cycle.</p><p>Use the <code>telepresence intercept $SERVICE_NAME --port $LOCAL_PORT:$REMOTE_PORT</code>
command to create an "intercept" for rerouting remote service traffic.</p><p>Where:</p><ul><li><code>$SERVICE_NAME</code> is the name of your local service</li><li><code>$LOCAL_PORT</code> is the port that your service is running on your local workstation</li><li>And <code>$REMOTE_PORT</code> is the port your service listens to in the cluster</li></ul><p>Running this command tells Telepresence to send remote traffic to your
local service instead of the service in the remote Kubernetes cluster.
Make edits to your service source code locally, save, and see the corresponding
changes when accessing your remote application take effect immediately.
You can also run your local service using a debugger or any other local development tool.</p><h2 id="how-does-telepresence-work">How does Telepresence work?</h2><p>Telepresence installs a traffic-agent sidecar next to your existing
application's container running in the remote cluster. It then captures
all traffic requests going into the Pod, and instead of forwarding this
to the application in the remote cluster, it routes all traffic (when you
create a <a href="https://www.getambassador.io/docs/telepresence/latest/concepts/intercepts/#global-intercept">global intercept</a>
or a subset of the traffic (when you create a
<a href="https://www.getambassador.io/docs/telepresence/latest/concepts/intercepts/#personal-intercept">personal intercept</a>)
to your local development environment.</p><h2 id="what-s-next">What's next</h2><p>If you're interested in a hands-on tutorial, check out
<a href="https://cloud.google.com/community/tutorials/developing-services-with-k8s">this tutorial</a>
that walks through locally developing the Guestbook application on Google Kubernetes Engine.</p><p>For further reading, visit the <a href="https://www.telepresence.io">Telepresence website</a>.</p></div></div><div><div class="td-content"><h1>Windows debugging tips</h1><h2 id="troubleshooting-node">Node-level troubleshooting</h2><ol><li><p>My Pods are stuck at "Container Creating" or restarting over and over</p><p>Ensure that your pause image is compatible with your Windows OS version.
See <a href="/docs/concepts/windows/intro/#pause-container">Pause container</a>
to see the latest / recommended pause image and/or get more information.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If using containerd as your container runtime the pause image is specified in the
<code>plugins.plugins.cri.sandbox_image</code> field of the of config.toml configuration file.</div></li><li><p>My pods show status as <code>ErrImgPull</code> or <code>ImagePullBackOff</code></p><p>Ensure that your Pod is getting scheduled to a
<a href="https://docs.microsoft.com/virtualization/windowscontainers/deploy-containers/version-compatibility">compatible</a>
Windows Node.</p><p>More information on how to specify a compatible node for your Pod can be found in
<a href="/docs/concepts/windows/user-guide/#ensuring-os-specific-workloads-land-on-the-appropriate-container-host">this guide</a>.</p></li></ol><h2 id="troubleshooting-network">Network troubleshooting</h2><ol><li><p>My Windows Pods do not have network connectivity</p><p>If you are using virtual machines, ensure that MAC spoofing is <strong>enabled</strong> on all
the VM network adapter(s).</p></li><li><p>My Windows Pods cannot ping external resources</p><p>Windows Pods do not have outbound rules programmed for the ICMP protocol. However,
TCP/UDP is supported. When trying to demonstrate connectivity to resources
outside of the cluster, substitute <code>ping &lt;IP&gt;</code> with corresponding
<code>curl &lt;IP&gt;</code> commands.</p><p>If you are still facing problems, most likely your network configuration in
<a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf">cni.conf</a>
deserves some extra attention. You can always edit this static file. The
configuration update will apply to any new Kubernetes resources.</p><p>One of the Kubernetes networking requirements
(see <a href="/docs/concepts/cluster-administration/networking/">Kubernetes model</a>) is
for cluster communication to occur without
NAT internally. To honor this requirement, there is an
<a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20">ExceptionList</a>
for all the communication where you do not want outbound NAT to occur. However,
this also means that you need to exclude the external IP you are trying to query
from the <code>ExceptionList</code>. Only then will the traffic originating from your Windows
pods be SNAT'ed correctly to receive a response from the outside world. In this
regard, your <code>ExceptionList</code> in <code>cni.conf</code> should look as follows:</p><pre tabindex="0"><code class="language-conf">"ExceptionList": [
                "10.244.0.0/16",  # Cluster subnet
                "10.96.0.0/12",   # Service subnet
                "10.127.130.0/24" # Management (host) subnet
            ]
</code></pre></li><li><p>My Windows node cannot access <code>NodePort</code> type Services</p><p>Local NodePort access from the node itself fails. This is a known
limitation. NodePort access works from other nodes or external clients.</p></li><li><p>vNICs and HNS endpoints of containers are being deleted</p><p>This issue can be caused when the <code>hostname-override</code> parameter is not passed to
<a href="/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>. To resolve
it, users need to pass the hostname to kube-proxy as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>C:\k\<span>kube-proxy</span>.exe --hostname-override=$(hostname)
</span></span></code></pre></div></li><li><p>My Windows node cannot access my services using the service IP</p><p>This is a known limitation of the networking stack on Windows. However, Windows Pods can access the Service IP.</p></li><li><p>No network adapter is found when starting the kubelet</p><p>The Windows networking stack needs a virtual adapter for Kubernetes networking to work.
If the following commands return no results (in an admin shell),
virtual network creation &#8212; a necessary prerequisite for the kubelet to work &#8212; has failed:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>Get-HnsNetwork</span> | ? Name <span>-ieq</span> <span>"cbr0"</span>
</span></span><span><span><span>Get-NetAdapter</span> | ? Name <span>-Like</span> <span>"vEthernet (Ethernet*"</span>
</span></span></code></pre></div><p>Often it is worthwhile to modify the <a href="https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L7">InterfaceName</a>
parameter of the <code>start.ps1</code> script, in cases where the host's network adapter isn't "Ethernet".
Otherwise, consult the output of the <code>start-kubelet.ps1</code> script to see if there are errors during virtual network creation.</p></li><li><p>DNS resolution is not properly working</p><p>Check the DNS limitations for Windows in this <a href="/docs/concepts/services-networking/dns-pod-service/#dns-windows">section</a>.</p></li><li><p><code>kubectl port-forward</code> fails with "unable to do port forwarding: wincat not found"</p><p>This was implemented in Kubernetes 1.15 by including <code>wincat.exe</code> in the pause infrastructure container
<code>mcr.microsoft.com/oss/kubernetes/pause:3.6</code>.
Be sure to use a supported version of Kubernetes.
If you would like to build your own pause infrastructure container be sure to include
<a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause/windows/wincat">wincat</a>.</p></li><li><p>My Kubernetes installation is failing because my Windows Server node is behind a proxy</p><p>If you are behind a proxy, the following PowerShell environment variables must be defined:</p><div class="highlight"><pre tabindex="0"><code class="language-PowerShell"><span><span>[<span>Environment</span>]::SetEnvironmentVariable(<span>"HTTP_PROXY"</span>, <span>"http://proxy.example.com:80/"</span>, [<span>EnvironmentVariableTarget</span>]::Machine)
</span></span><span><span>[<span>Environment</span>]::SetEnvironmentVariable(<span>"HTTPS_PROXY"</span>, <span>"http://proxy.example.com:443/"</span>, [<span>EnvironmentVariableTarget</span>]::Machine)
</span></span></code></pre></div></li></ol><h3 id="flannel-troubleshooting">Flannel troubleshooting</h3><ol><li><p>With Flannel, my nodes are having issues after rejoining a cluster</p><p>Whenever a previously deleted node is being re-joined to the cluster, flannelD
tries to assign a new pod subnet to the node. Users should remove the old pod
subnet configuration files in the following paths:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>Remove-Item</span> C:\k\SourceVip.json
</span></span><span><span><span>Remove-Item</span> C:\k\SourceVipRequest.json
</span></span></code></pre></div></li><li><p>Flanneld is stuck in "Waiting for the Network to be created"</p><p>There are numerous reports of this <a href="https://github.com/coreos/flannel/issues/1066">issue</a>;
most likely it is a timing issue for when the management IP of the flannel network is set.
A workaround is to relaunch <code>start.ps1</code> or relaunch it manually as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span>[<span>Environment</span>]::SetEnvironmentVariable(<span>"NODE_NAME"</span>, <span>"&lt;Windows_Worker_Hostname&gt;"</span>)
</span></span><span><span>C:\flannel\flanneld.exe --kubeconfig<span>-file</span>=c:\k\config --iface=&lt;Windows_Worker_Node_IP&gt; --ip-masq=<span>1</span> --kube-subnet-mgr=<span>1</span>
</span></span></code></pre></div></li><li><p>My Windows Pods cannot launch because of missing <code>/run/flannel/subnet.env</code></p><p>This indicates that Flannel didn't launch correctly. You can either try
to restart <code>flanneld.exe</code> or you can copy the files over manually from
<code>/run/flannel/subnet.env</code> on the Kubernetes master to <code>C:\run\flannel\subnet.env</code>
on the Windows worker node and modify the <code>FLANNEL_SUBNET</code> row to a different
number. For example, if node subnet 10.244.4.1/24 is desired:</p><div class="highlight"><pre tabindex="0"><code class="language-env"><span><span><span>FLANNEL_NETWORK</span><span>=</span>10.244.0.0/16
</span></span><span><span><span>FLANNEL_SUBNET</span><span>=</span>10.244.4.1/24
</span></span><span><span><span>FLANNEL_MTU</span><span>=</span><span>1500</span>
</span></span><span><span><span>FLANNEL_IPMASQ</span><span>=</span><span>true</span>
</span></span></code></pre></div></li></ol><h3 id="further-investigation">Further investigation</h3><p>If these steps don't resolve your problem, you can get help running Windows containers on Windows nodes in Kubernetes through:</p><ul><li>StackOverflow <a href="https://stackoverflow.com/questions/tagged/windows-server-container">Windows Server Container</a> topic</li><li>Kubernetes Official Forum <a href="https://discuss.kubernetes.io/">discuss.kubernetes.io</a></li><li>Kubernetes Slack <a href="https://kubernetes.slack.com/messages/sig-windows">#SIG-Windows Channel</a></li></ul></div></div><div><div class="td-content"><h1>Manage Kubernetes Objects</h1><div class="lead">Declarative and imperative paradigms for interacting with the Kubernetes API.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/declarative-config/">Declarative Management of Kubernetes Objects Using Configuration Files</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/kustomization/">Declarative Management of Kubernetes Objects Using Kustomize</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch</a></h5><p>Use kubectl patch to update Kubernetes API objects in place. Do a strategic merge patch or a JSON merge patch.</p></div><div class="entry"><h5><a href="/docs/tasks/manage-kubernetes-objects/storage-version-migration/">Migrate Kubernetes Objects Using Storage Version Migration</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Declarative Management of Kubernetes Objects Using Configuration Files</h1><p>Kubernetes objects can be created, updated, and deleted by storing multiple
object configuration files in a directory and using <code>kubectl apply</code> to
recursively create and update those objects as needed. This method
retains writes made to live objects without merging the changes
back into the object configuration files. <code>kubectl diff</code> also gives you a
preview of what changes <code>apply</code> will make.</p><h2 id="before-you-begin">Before you begin</h2><p>Install <a href="/docs/tasks/tools/"><code>kubectl</code></a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="trade-offs">Trade-offs</h2><p>The <code>kubectl</code> tool supports three kinds of object management:</p><ul><li>Imperative commands</li><li>Imperative object configuration</li><li>Declarative object configuration</li></ul><p>See <a href="/docs/concepts/overview/working-with-objects/object-management/">Kubernetes Object Management</a>
for a discussion of the advantages and disadvantage of each kind of object management.</p><h2 id="overview">Overview</h2><p>Declarative object configuration requires a firm understanding of
the Kubernetes object definitions and configuration. Read and complete
the following documents if you have not already:</p><ul><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li></ul><p>Following are definitions for terms used in this document:</p><ul><li><em>object configuration file / configuration file</em>: A file that defines the
configuration for a Kubernetes object. This topic shows how to pass configuration
files to <code>kubectl apply</code>. Configuration files are typically stored in source control, such as Git.</li><li><em>live object configuration / live configuration</em>: The live configuration
values of an object, as observed by the Kubernetes cluster. These are kept in the Kubernetes
cluster storage, typically etcd.</li><li><em>declarative configuration writer / declarative writer</em>: A person or software component
that makes updates to a live object. The live writers referred to in this topic make changes
to object configuration files and run <code>kubectl apply</code> to write the changes.</li></ul><h2 id="how-to-create-objects">How to create objects</h2><p>Use <code>kubectl apply</code> to create all objects, except those that already exist,
defined by configuration files in a specified directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f &lt;directory&gt;
</span></span></code></pre></div><p>This sets the <code>kubectl.kubernetes.io/last-applied-configuration: '{...}'</code>
annotation on each object. The annotation contains the contents of the object
configuration file that was used to create the object.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Add the <code>-R</code> flag to recursively process directories.</div><p>Here's an example of an object configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/simple_deployment.yaml"><code>application/simple_deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/simple_deployment.yaml to clipboard"></div><div class="includecode" id="application-simple-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Run <code>kubectl diff</code> to print the object that will be created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl diff -f https://k8s.io/examples/application/simple_deployment.yaml
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p><code>diff</code> uses <a href="/docs/reference/using-api/api-concepts/#dry-run">server-side dry-run</a>,
which needs to be enabled on <code>kube-apiserver</code>.</p><p>Since <code>diff</code> performs a server-side apply request in dry-run mode,
it requires granting <code>PATCH</code>, <code>CREATE</code>, and <code>UPDATE</code> permissions.
See <a href="/docs/reference/using-api/api-concepts/#dry-run-authorization">Dry-Run Authorization</a>
for details.</p></div><p>Create the object using <code>kubectl apply</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml
</span></span></code></pre></div><p>Print the live configuration using <code>kubectl get</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml
</span></span></code></pre></div><p>The output shows that the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation
was written to the live configuration, and it matches the configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># This is the json representation of simple_deployment.yaml</span><span>
</span></span></span><span><span><span>    </span><span># It was written by kubectl apply when the object was created</span><span>
</span></span></span><span><span><span>    </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>      {"apiVersion":"apps/v1","kind":"Deployment",
</span></span></span><span><span><span>      "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},
</span></span></span><span><span><span>      "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}},
</span></span></span><span><span><span>      "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx",
</span></span></span><span><span><span>      "ports":[{"containerPort":80}]}]}}}}</span><span>      
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span></code></pre></div><h2 id="how-to-update-objects">How to update objects</h2><p>You can also use <code>kubectl apply</code> to update all objects defined in a directory, even
if those objects already exist. This approach accomplishes the following:</p><ol><li>Sets fields that appear in the configuration file in the live configuration.</li><li>Clears fields removed from the configuration file in the live configuration.</li></ol><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl diff -f &lt;directory&gt;
</span></span><span><span>kubectl apply -f &lt;directory&gt;
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Add the <code>-R</code> flag to recursively process directories.</div><p>Here's an example configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/simple_deployment.yaml"><code>application/simple_deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/simple_deployment.yaml to clipboard"></div><div class="includecode" id="application-simple-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Create the object using <code>kubectl apply</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For purposes of illustration, the preceding command refers to a single
configuration file instead of a directory.</div><p>Print the live configuration using <code>kubectl get</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml
</span></span></code></pre></div><p>The output shows that the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation
was written to the live configuration, and it matches the configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># This is the json representation of simple_deployment.yaml</span><span>
</span></span></span><span><span><span>    </span><span># It was written by kubectl apply when the object was created</span><span>
</span></span></span><span><span><span>    </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>      {"apiVersion":"apps/v1","kind":"Deployment",
</span></span></span><span><span><span>      "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},
</span></span></span><span><span><span>      "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}},
</span></span></span><span><span><span>      "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx",
</span></span></span><span><span><span>      "ports":[{"containerPort":80}]}]}}}}</span><span>      
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span></code></pre></div><p>Directly update the <code>replicas</code> field in the live configuration by using <code>kubectl scale</code>.
This does not use <code>kubectl apply</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale deployment/nginx-deployment --replicas<span>=</span><span>2</span>
</span></span></code></pre></div><p>Print the live configuration using <code>kubectl get</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment nginx-deployment -o yaml
</span></span></code></pre></div><p>The output shows that the <code>replicas</code> field has been set to 2, and the <code>last-applied-configuration</code>
annotation does not contain a <code>replicas</code> field:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># note that the annotation does not contain replicas</span><span>
</span></span></span><span><span><span>    </span><span># because it was not updated through apply</span><span>
</span></span></span><span><span><span>    </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>      {"apiVersion":"apps/v1","kind":"Deployment",
</span></span></span><span><span><span>      "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},
</span></span></span><span><span><span>      "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}},
</span></span></span><span><span><span>      "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx",
</span></span></span><span><span><span>      "ports":[{"containerPort":80}]}]}}}}</span><span>      
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># written by scale</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span></code></pre></div><p>Update the <code>simple_deployment.yaml</code> configuration file to change the image from
<code>nginx:1.14.2</code> to <code>nginx:1.16.1</code>, and delete the <code>minReadySeconds</code> field:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/update_deployment.yaml"><code>application/update_deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/update_deployment.yaml to clipboard"></div><div class="includecode" id="application-update-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.16.1<span> </span><span># update the image</span><span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Apply the changes made to the configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl diff -f https://k8s.io/examples/application/update_deployment.yaml
</span></span><span><span>kubectl apply -f https://k8s.io/examples/application/update_deployment.yaml
</span></span></code></pre></div><p>Print the live configuration using <code>kubectl get</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f https://k8s.io/examples/application/update_deployment.yaml -o yaml
</span></span></code></pre></div><p>The output shows the following changes to the live configuration:</p><ul><li>The <code>replicas</code> field retains the value of 2 set by <code>kubectl scale</code>.
This is possible because it is omitted from the configuration file.</li><li>The <code>image</code> field has been updated to <code>nginx:1.16.1</code> from <code>nginx:1.14.2</code>.</li><li>The <code>last-applied-configuration</code> annotation has been updated with the new image.</li><li>The <code>minReadySeconds</code> field has been cleared.</li><li>The <code>last-applied-configuration</code> annotation no longer contains the <code>minReadySeconds</code> field.</li></ul><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># The annotation contains the updated image to nginx 1.16.1,</span><span>
</span></span></span><span><span><span>    </span><span># but does not contain the updated replicas to 2</span><span>
</span></span></span><span><span><span>    </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>      {"apiVersion":"apps/v1","kind":"Deployment",
</span></span></span><span><span><span>      "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},
</span></span></span><span><span><span>      "spec":{"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}},
</span></span></span><span><span><span>      "spec":{"containers":[{"image":"nginx:1.16.1","name":"nginx",
</span></span></span><span><span><span>      "ports":[{"containerPort":80}]}]}}}}</span><span>      
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># Set by `kubectl scale`.  Ignored by `kubectl apply`.</span><span>
</span></span></span><span><span><span>  </span><span># minReadySeconds cleared by `kubectl apply`</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.16.1<span> </span><span># Set by `kubectl apply`</span><span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span></code></pre></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Mixing <code>kubectl apply</code> with the imperative object configuration commands
<code>create</code> and <code>replace</code> is not supported. This is because <code>create</code>
and <code>replace</code> do not retain the <code>kubectl.kubernetes.io/last-applied-configuration</code>
that <code>kubectl apply</code> uses to compute updates.</div><h2 id="how-to-delete-objects">How to delete objects</h2><p>There are two approaches to delete objects managed by <code>kubectl apply</code>.</p><h3 id="recommended-kubectl-delete-f-filename">Recommended: <code>kubectl delete -f &lt;filename&gt;</code></h3><p>Manually deleting objects using the imperative command is the recommended
approach, as it is more explicit about what is being deleted, and less likely
to result in the user deleting something unintentionally:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f &lt;filename&gt;
</span></span></code></pre></div><h3 id="alternative-kubectl-apply-f-directory-prune">Alternative: <code>kubectl apply -f &lt;directory&gt; --prune</code></h3><p>As an alternative to <code>kubectl delete</code>, you can use <code>kubectl apply</code> to identify objects to be deleted after
their manifests have been removed from a directory in the local filesystem.</p><p>In Kubernetes 1.34, there are two pruning modes available in kubectl apply:</p><ul><li>Allowlist-based pruning: This mode has existed since kubectl v1.5 but is still
in alpha due to usability, correctness and performance issues with its design.
The ApplySet-based mode is designed to replace it.</li><li>ApplySet-based pruning: An <em>apply set</em> is a server-side object (by default, a Secret)
that kubectl can use to accurately and efficiently track set membership across <strong>apply</strong>
operations. This mode was introduced in alpha in kubectl v1.27 as a replacement for allowlist-based pruning.</li></ul><ul class="nav nav-tabs" id="kubectl-apply-prune"><li class="nav-item"><a class="nav-link active" href="#kubectl-apply-prune-0">Allow list</a></li><li class="nav-item"><a class="nav-link" href="#kubectl-apply-prune-1">Apply set</a></li></ul><div class="tab-content" id="kubectl-apply-prune"><div id="kubectl-apply-prune-0" class="tab-pane show active"><p><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.5 [alpha]</code></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Take care when using <code>--prune</code> with <code>kubectl apply</code> in allow list mode. Which
objects are pruned depends on the values of the <code>--prune-allowlist</code>, <code>--selector</code>
and <code>--namespace</code> flags, and relies on dynamic discovery of the objects in scope.
Especially if flag values are changed between invocations, this can lead to objects
being unexpectedly deleted or retained.</div><p>To use allowlist-based pruning, add the following flags to your <code>kubectl apply</code> invocation:</p><ul><li><code>--prune</code>: Delete previously applied objects that are not in the set passed to the current invocation.</li><li><code>--prune-allowlist</code>: A list of group-version-kinds (GVKs) to consider for pruning.
This flag is optional but strongly encouraged, as its default value is a partial
list of both namespaced and cluster-scoped types, which can lead to surprising results.</li><li><code>--selector/-l</code>: Use a label selector to constrain the set of objects selected
for pruning. This flag is optional but strongly encouraged.</li><li><code>--all</code>: use instead of <code>--selector/-l</code> to explicitly select all previously
applied objects of the allowlisted types.</li></ul><p>Allowlist-based pruning queries the API server for all objects of the allowlisted GVKs that match the given labels (if any), and attempts to match the returned live object configurations against the object
manifest files. If an object matches the query, and it does not have a
manifest in the directory, and it has a <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation,
it is deleted.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f &lt;directory&gt; --prune -l &lt;labels&gt; --prune-allowlist<span>=</span>&lt;gvk-list&gt;
</span></span></code></pre></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Apply with prune should only be run against the root directory
containing the object manifests. Running against sub-directories
can cause objects to be unintentionally deleted if they were previously applied,
have the labels given (if any), and do not appear in the subdirectory.</div></p></div><div id="kubectl-apply-prune-1" class="tab-pane"><p><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [alpha]</code></div><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><code>kubectl apply --prune --applyset</code> is in alpha, and backwards incompatible
changes might be introduced in subsequent releases.</div><p>To use ApplySet-based pruning, set the <code>KUBECTL_APPLYSET=true</code> environment variable,
and add the following flags to your <code>kubectl apply</code> invocation:</p><ul><li><code>--prune</code>: Delete previously applied objects that are not in the set passed
to the current invocation.</li><li><code>--applyset</code>: The name of an object that kubectl can use to accurately and
efficiently track set membership across <code>apply</code> operations.</li></ul><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>KUBECTL_APPLYSET</span><span>=</span><span>true</span> kubectl apply -f &lt;directory&gt; --prune --applyset<span>=</span>&lt;name&gt;
</span></span></code></pre></div><p>By default, the type of the ApplySet parent object used is a Secret. However,
ConfigMaps can also be used in the format: <code>--applyset=configmaps/&lt;name&gt;</code>.
When using a Secret or ConfigMap, kubectl will create the object if it does not already exist.</p><p>It is also possible to use custom resources as ApplySet parent objects. To enable
this, label the Custom Resource Definition (CRD) that defines the resource you want
to use with the following: <code>applyset.kubernetes.io/is-parent-type: true</code>. Then, create
the object you want to use as an ApplySet parent (kubectl does not do this automatically
for custom resources). Finally, refer to that object in the applyset flag as follows:
<code>--applyset=&lt;resource&gt;.&lt;group&gt;/&lt;name&gt;</code> (for example, <code>widgets.custom.example.com/widget-name</code>).</p><p>With ApplySet-based pruning, kubectl adds the <code>applyset.kubernetes.io/part-of=&lt;parentID&gt;</code>
label to each object in the set before they are sent to the server. For performance reasons,
it also collects the list of resource types and namespaces that the set contains and adds
these in annotations on the live parent object. Finally, at the end of the apply operation,
it queries the API server for objects of those types in those namespaces
(or in the cluster scope, as applicable) that belong to the set, as defined by the
<code>applyset.kubernetes.io/part-of=&lt;parentID&gt;</code> label.</p><p>Caveats and restrictions:</p><ul><li>Each object may be a member of at most one set.</li><li>The <code>--namespace</code> flag is required when using any namespaced parent, including
the default Secret. This means that ApplySets spanning multiple namespaces must
use a cluster-scoped custom resource as the parent object.</li><li>To safely use ApplySet-based pruning with multiple directories,
use a unique ApplySet name for each.</li></ul></p></div></div><h2 id="how-to-view-an-object">How to view an object</h2><p>You can use <code>kubectl get</code> with <code>-o yaml</code> to view the configuration of a live object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f &lt;filename|url&gt; -o yaml
</span></span></code></pre></div><h2 id="how-apply-calculates-differences-and-merges-changes">How apply calculates differences and merges changes</h2><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>A <em>patch</em> is an update operation that is scoped to specific fields of an object
instead of the entire object. This enables updating only a specific set of fields
on an object without reading the object first.</div><p>When <code>kubectl apply</code> updates the live configuration for an object,
it does so by sending a patch request to the API server. The
patch defines updates scoped to specific fields of the live object
configuration. The <code>kubectl apply</code> command calculates this patch request
using the configuration file, the live configuration, and the
<code>last-applied-configuration</code> annotation stored in the live configuration.</p><h3 id="merge-patch-calculation">Merge patch calculation</h3><p>The <code>kubectl apply</code> command writes the contents of the configuration file to the
<code>kubectl.kubernetes.io/last-applied-configuration</code> annotation. This
is used to identify fields that have been removed from the configuration
file and need to be cleared from the live configuration. Here are the steps used
to calculate which fields should be deleted or set:</p><ol><li>Calculate the fields to delete. These are the fields present in
<code>last-applied-configuration</code> and missing from the configuration file.</li><li>Calculate the fields to add or set. These are the fields present in
the configuration file whose values don't match the live configuration.</li></ol><p>Here's an example. Suppose this is the configuration file for a Deployment object:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/update_deployment.yaml"><code>application/update_deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/update_deployment.yaml to clipboard"></div><div class="includecode" id="application-update-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.16.1<span> </span><span># update the image</span><span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Also, suppose this is the live configuration for the same Deployment object:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># note that the annotation does not contain replicas</span><span>
</span></span></span><span><span><span>    </span><span># because it was not updated through apply</span><span>
</span></span></span><span><span><span>    </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>      {"apiVersion":"apps/v1","kind":"Deployment",
</span></span></span><span><span><span>      "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},
</span></span></span><span><span><span>      "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}},
</span></span></span><span><span><span>      "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx",
</span></span></span><span><span><span>      "ports":[{"containerPort":80}]}]}}}}</span><span>      
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># written by scale</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span></code></pre></div><p>Here are the merge calculations that would be performed by <code>kubectl apply</code>:</p><ol><li>Calculate the fields to delete by reading values from
<code>last-applied-configuration</code> and comparing them to values in the
configuration file.
Clear fields explicitly set to null in the local object configuration file
regardless of whether they appear in the <code>last-applied-configuration</code>.
In this example, <code>minReadySeconds</code> appears in the
<code>last-applied-configuration</code> annotation, but does not appear in the configuration file.
<strong>Action:</strong> Clear <code>minReadySeconds</code> from the live configuration.</li><li>Calculate the fields to set by reading values from the configuration
file and comparing them to values in the live configuration. In this example,
the value of <code>image</code> in the configuration file does not match
the value in the live configuration. <strong>Action:</strong> Set the value of <code>image</code> in the live configuration.</li><li>Set the <code>last-applied-configuration</code> annotation to match the value
of the configuration file.</li><li>Merge the results from 1, 2, 3 into a single patch request to the API server.</li></ol><p>Here is the live configuration that is the result of the merge:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># The annotation contains the updated image to nginx 1.16.1,</span><span>
</span></span></span><span><span><span>    </span><span># but does not contain the updated replicas to 2</span><span>
</span></span></span><span><span><span>    </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>      {"apiVersion":"apps/v1","kind":"Deployment",
</span></span></span><span><span><span>      "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"},
</span></span></span><span><span><span>      "spec":{"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}},
</span></span></span><span><span><span>      "spec":{"containers":[{"image":"nginx:1.16.1","name":"nginx",
</span></span></span><span><span><span>      "ports":[{"containerPort":80}]}]}}}}</span><span>      
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># Set by `kubectl scale`.  Ignored by `kubectl apply`.</span><span>
</span></span></span><span><span><span>  </span><span># minReadySeconds cleared by `kubectl apply`</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.16.1<span> </span><span># Set by `kubectl apply`</span><span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span></code></pre></div><h3 id="how-different-types-of-fields-are-merged">How different types of fields are merged</h3><p>How a particular field in a configuration file is merged with
the live configuration depends on the
type of the field. There are several types of fields:</p><ul><li><p><em>primitive</em>: A field of type string, integer, or boolean.
For example, <code>image</code> and <code>replicas</code> are primitive fields. <strong>Action:</strong> Replace.</p></li><li><p><em>map</em>, also called <em>object</em>: A field of type map or a complex type that contains subfields. For example, <code>labels</code>,
<code>annotations</code>,<code>spec</code> and <code>metadata</code> are all maps. <strong>Action:</strong> Merge elements or subfields.</p></li><li><p><em>list</em>: A field containing a list of items that can be either primitive types or maps.
For example, <code>containers</code>, <code>ports</code>, and <code>args</code> are lists. <strong>Action:</strong> Varies.</p></li></ul><p>When <code>kubectl apply</code> updates a map or list field, it typically does
not replace the entire field, but instead updates the individual subelements.
For instance, when merging the <code>spec</code> on a Deployment, the entire <code>spec</code> is
not replaced. Instead the subfields of <code>spec</code>, such as <code>replicas</code>, are compared
and merged.</p><h3 id="merging-changes-to-primitive-fields">Merging changes to primitive fields</h3><p>Primitive fields are replaced or cleared.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>-</code> is used for "not applicable" because the value is not used.</div><table><thead><tr><th>Field in object configuration file</th><th>Field in live object configuration</th><th>Field in last-applied-configuration</th><th>Action</th></tr></thead><tbody><tr><td>Yes</td><td>Yes</td><td>-</td><td>Set live to configuration file value.</td></tr><tr><td>Yes</td><td>No</td><td>-</td><td>Set live to local configuration.</td></tr><tr><td>No</td><td>-</td><td>Yes</td><td>Clear from live configuration.</td></tr><tr><td>No</td><td>-</td><td>No</td><td>Do nothing. Keep live value.</td></tr></tbody></table><h3 id="merging-changes-to-map-fields">Merging changes to map fields</h3><p>Fields that represent maps are merged by comparing each of the subfields or elements of the map:</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>-</code> is used for "not applicable" because the value is not used.</div><table><thead><tr><th>Key in object configuration file</th><th>Key in live object configuration</th><th>Field in last-applied-configuration</th><th>Action</th></tr></thead><tbody><tr><td>Yes</td><td>Yes</td><td>-</td><td>Compare sub fields values.</td></tr><tr><td>Yes</td><td>No</td><td>-</td><td>Set live to local configuration.</td></tr><tr><td>No</td><td>-</td><td>Yes</td><td>Delete from live configuration.</td></tr><tr><td>No</td><td>-</td><td>No</td><td>Do nothing. Keep live value.</td></tr></tbody></table><h3 id="merging-changes-for-fields-of-type-list">Merging changes for fields of type list</h3><p>Merging changes to a list uses one of three strategies:</p><ul><li>Replace the list if all its elements are primitives.</li><li>Merge individual elements in a list of complex elements.</li><li>Merge a list of primitive elements.</li></ul><p>The choice of strategy is made on a per-field basis.</p><h4 id="replace-the-list-if-all-its-elements-are-primitives">Replace the list if all its elements are primitives</h4><p>Treat the list the same as a primitive field. Replace or delete the
entire list. This preserves ordering.</p><p><strong>Example:</strong> Use <code>kubectl apply</code> to update the <code>args</code> field of a Container in a Pod. This sets
the value of <code>args</code> in the live configuration to the value in the configuration file.
Any <code>args</code> elements that had previously been added to the live configuration are lost.
The order of the <code>args</code> elements defined in the configuration file is
retained in the live configuration.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># last-applied-configuration value</span><span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"a"</span>,<span> </span><span>"b"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># configuration file value</span><span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"a"</span>,<span> </span><span>"c"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># live configuration</span><span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"a"</span>,<span> </span><span>"b"</span>,<span> </span><span>"d"</span>]<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># result after merge</span><span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"a"</span>,<span> </span><span>"c"</span>]<span>
</span></span></span></code></pre></div><p><strong>Explanation:</strong> The merge used the configuration file value as the new list value.</p><h4 id="merge-individual-elements-of-a-list-of-complex-elements">Merge individual elements of a list of complex elements:</h4><p>Treat the list as a map, and treat a specific field of each element as a key.
Add, delete, or update individual elements. This does not preserve ordering.</p><p>This merge strategy uses a special tag on each field called a <code>patchMergeKey</code>. The
<code>patchMergeKey</code> is defined for each field in the Kubernetes source code:
<a href="https://github.com/kubernetes/api/blob/d04500c8c3dda9c980b668c57abc2ca61efcf5c4/core/v1/types.go#L2747">types.go</a>
When merging a list of maps, the field specified as the <code>patchMergeKey</code> for a given element
is used like a map key for that element.</p><p><strong>Example:</strong> Use <code>kubectl apply</code> to update the <code>containers</code> field of a PodSpec.
This merges the list as though it was a map where each element is keyed
by <code>name</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># last-applied-configuration value</span><span>
</span></span></span><span><span><span>    </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx:1.16<span>
</span></span></span><span><span><span>    </span>- <span>name: nginx-helper-a # key</span>:<span> </span>nginx-helper-a; will be deleted in result<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>    </span>- <span>name: nginx-helper-b # key</span>:<span> </span>nginx-helper-b; will be retained<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># configuration file value</span><span>
</span></span></span><span><span><span>    </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx:1.16<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx-helper-b<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>    </span>- <span>name: nginx-helper-c # key</span>:<span> </span>nginx-helper-c; will be added in result<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># live configuration</span><span>
</span></span></span><span><span><span>    </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx:1.16<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx-helper-a<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx-helper-b<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span> </span>[<span>"run"</span>]<span> </span><span># Field will be retained</span><span>
</span></span></span><span><span><span>    </span>- <span>name: nginx-helper-d # key</span>:<span> </span>nginx-helper-d; will be retained<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># result after merge</span><span>
</span></span></span><span><span><span>    </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx:1.16<span>
</span></span></span><span><span><span>      </span><span># Element nginx-helper-a was deleted</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx-helper-b<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span> </span>[<span>"run"</span>]<span> </span><span># Field was retained</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx-helper-c<span> </span><span># Element was added</span><span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>nginx-helper-d<span> </span><span># Element was ignored</span><span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>helper:1.3<span>
</span></span></span></code></pre></div><p><strong>Explanation:</strong></p><ul><li>The container named "nginx-helper-a" was deleted because no container
named "nginx-helper-a" appeared in the configuration file.</li><li>The container named "nginx-helper-b" retained the changes to <code>args</code>
in the live configuration. <code>kubectl apply</code> was able to identify
that "nginx-helper-b" in the live configuration was the same
"nginx-helper-b" as in the configuration file, even though their fields
had different values (no <code>args</code> in the configuration file). This is
because the <code>patchMergeKey</code> field value (name) was identical in both.</li><li>The container named "nginx-helper-c" was added because no container
with that name appeared in the live configuration, but one with
that name appeared in the configuration file.</li><li>The container named "nginx-helper-d" was retained because
no element with that name appeared in the last-applied-configuration.</li></ul><h4 id="merge-a-list-of-primitive-elements">Merge a list of primitive elements</h4><p>As of Kubernetes 1.5, merging lists of primitive elements is not supported.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Which of the above strategies is chosen for a given field is controlled by
the <code>patchStrategy</code> tag in <a href="https://github.com/kubernetes/api/blob/d04500c8c3dda9c980b668c57abc2ca61efcf5c4/core/v1/types.go#L2748">types.go</a>
If no <code>patchStrategy</code> is specified for a field of type list, then
the list is replaced.</div><h2 id="default-field-values">Default field values</h2><p>The API server sets certain fields to default values in the live configuration if they are
not specified when the object is created.</p><p>Here's a configuration file for a Deployment. The file does not specify <code>strategy</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/simple_deployment.yaml"><code>application/simple_deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/simple_deployment.yaml to clipboard"></div><div class="includecode" id="application-simple-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Create the object using <code>kubectl apply</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml
</span></span></code></pre></div><p>Print the live configuration using <code>kubectl get</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml
</span></span></code></pre></div><p>The output shows that the API server set several fields to default values in the live
configuration. These fields were not specified in the configuration file.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span># ...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>rollingUpdate</span>:<span> </span><span># defaulted by apiserver - derived from strategy.type</span><span>
</span></span></span><span><span><span>      </span><span>maxSurge</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>RollingUpdate<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>creationTimestamp</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>          </span><span>protocol</span>:<span> </span>TCP<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span> </span>{}<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>        </span><span>terminationMessagePath</span>:<span> </span>/dev/termination-log<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>      </span><span>dnsPolicy</span>:<span> </span>ClusterFirst<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Always<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>      </span><span>securityContext</span>:<span> </span>{}<span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span>      </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>30</span><span> </span><span># defaulted by apiserver</span><span>
</span></span></span><span><span><span></span><span># ...</span><span>
</span></span></span></code></pre></div><p>In a patch request, defaulted fields are not re-defaulted unless they are explicitly cleared
as part of a patch request. This can cause unexpected behavior for
fields that are defaulted based
on the values of other fields. When the other fields are later changed,
the values defaulted from them will not be updated unless they are
explicitly cleared.</p><p>For this reason, it is recommended that certain fields defaulted
by the server are explicitly defined in the configuration file, even
if the desired values match the server defaults. This makes it
easier to recognize conflicting values that will not be re-defaulted
by the server.</p><p><strong>Example:</strong></p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># last-applied-configuration</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># configuration file</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Recreate<span> </span><span># updated value</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># live configuration</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>RollingUpdate<span> </span><span># defaulted value</span><span>
</span></span></span><span><span><span>    </span><span>rollingUpdate</span>:<span> </span><span># defaulted value derived from type</span><span>
</span></span></span><span><span><span>      </span><span>maxSurge </span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># result after merge - ERROR!</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type: Recreate # updated value</span>:<span> </span>incompatible with rollingUpdate<span>
</span></span></span><span><span><span>    </span><span>rollingUpdate: # defaulted value</span>:<span> </span>incompatible with "type: Recreate"<span>
</span></span></span><span><span><span>      </span><span>maxSurge </span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><p><strong>Explanation:</strong></p><ol><li>The user creates a Deployment without defining <code>strategy.type</code>.</li><li>The server defaults <code>strategy.type</code> to <code>RollingUpdate</code> and defaults the
<code>strategy.rollingUpdate</code> values.</li><li>The user changes <code>strategy.type</code> to <code>Recreate</code>. The <code>strategy.rollingUpdate</code>
values remain at their defaulted values, though the server expects them to be cleared.
If the <code>strategy.rollingUpdate</code> values had been defined initially in the configuration file,
it would have been more clear that they needed to be deleted.</li><li>Apply fails because <code>strategy.rollingUpdate</code> is not cleared. The <code>strategy.rollingupdate</code>
field cannot be defined with a <code>strategy.type</code> of <code>Recreate</code>.</li></ol><p>Recommendation: These fields should be explicitly defined in the object configuration file:</p><ul><li>Selectors and PodTemplate labels on workloads, such as Deployment, StatefulSet, Job, DaemonSet,
ReplicaSet, and ReplicationController</li><li>Deployment rollout strategy</li></ul><h3 id="how-to-clear-server-defaulted-fields-or-fields-set-by-other-writers">How to clear server-defaulted fields or fields set by other writers</h3><p>Fields that do not appear in the configuration file can be cleared by
setting their values to <code>null</code> and then applying the configuration file.
For fields defaulted by the server, this triggers re-defaulting
the values.</p><h2 id="how-to-change-ownership-of-a-field-between-the-configuration-file-and-direct-imperative-writers">How to change ownership of a field between the configuration file and direct imperative writers</h2><p>These are the only methods you should use to change an individual object field:</p><ul><li>Use <code>kubectl apply</code>.</li><li>Write directly to the live configuration without modifying the configuration file:
for example, use <code>kubectl scale</code>.</li></ul><h3 id="changing-the-owner-from-a-direct-imperative-writer-to-a-configuration-file">Changing the owner from a direct imperative writer to a configuration file</h3><p>Add the field to the configuration file. For the field, discontinue direct updates to
the live configuration that do not go through <code>kubectl apply</code>.</p><h3 id="changing-the-owner-from-a-configuration-file-to-a-direct-imperative-writer">Changing the owner from a configuration file to a direct imperative writer</h3><p>As of Kubernetes 1.5, changing ownership of a field from a configuration file to
an imperative writer requires manual steps:</p><ul><li>Remove the field from the configuration file.</li><li>Remove the field from the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation on the live object.</li></ul><h2 id="changing-management-methods">Changing management methods</h2><p>Kubernetes objects should be managed using only one method at a time.
Switching from one method to another is possible, but is a manual process.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>It is OK to use imperative deletion with declarative management.</div><h3 id="migrating-from-imperative-command-management-to-declarative-object-configuration">Migrating from imperative command management to declarative object configuration</h3><p>Migrating from imperative command management to declarative object
configuration involves several manual steps:</p><ol><li><p>Export the live object to a local configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get &lt;kind&gt;/&lt;name&gt; -o yaml &gt; &lt;kind&gt;_&lt;name&gt;.yaml
</span></span></code></pre></div></li><li><p>Manually remove the <code>status</code> field from the configuration file.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This step is optional, as <code>kubectl apply</code> does not update the status field
even if it is present in the configuration file.</div></li><li><p>Set the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation on the object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl replace --save-config -f &lt;kind&gt;_&lt;name&gt;.yaml
</span></span></code></pre></div></li><li><p>Change processes to use <code>kubectl apply</code> for managing the object exclusively.</p></li></ol><h3 id="migrating-from-imperative-object-configuration-to-declarative-object-configuration">Migrating from imperative object configuration to declarative object configuration</h3><ol><li><p>Set the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation on the object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl replace --save-config -f &lt;kind&gt;_&lt;name&gt;.yaml
</span></span></code></pre></div></li><li><p>Change processes to use <code>kubectl apply</code> for managing the object exclusively.</p></li></ol><h2 id="defining-controller-selectors-and-podtemplate-labels">Defining controller selectors and PodTemplate labels</h2><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Updating selectors on controllers is strongly discouraged.</div><p>The recommended approach is to define a single, immutable PodTemplate label
used only by the controller selector with no other semantic meaning.</p><p><strong>Example:</strong></p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>selector</span>:<span>
</span></span></span><span><span><span>  </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>controller-selector</span>:<span> </span><span>"apps/v1/deployment/nginx"</span><span>
</span></span></span><span><span><span></span><span>template</span>:<span>
</span></span></span><span><span><span>  </span><span>metadata</span>:<span>
</span></span></span><span><span><span>    </span><span>labels</span>:<span>
</span></span></span><span><span><span>      </span><span>controller-selector</span>:<span> </span><span>"apps/v1/deployment/nginx"</span><span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl Command Reference</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/">Kubernetes API Reference</a></li></ul></div></div><div><div class="td-content"><h1>Declarative Management of Kubernetes Objects Using Kustomize</h1><p><a href="https://github.com/kubernetes-sigs/kustomize">Kustomize</a> is a standalone tool
to customize Kubernetes objects
through a <a href="https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#kustomization">kustomization file</a>.</p><p>Since 1.14, kubectl also
supports the management of Kubernetes objects using a kustomization file.
To view resources found in a directory containing a kustomization file, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl kustomize &lt;kustomization_directory&gt;
</span></span></code></pre></div><p>To apply those resources, run <code>kubectl apply</code> with <code>--kustomize</code> or <code>-k</code> flag:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k &lt;kustomization_directory&gt;
</span></span></code></pre></div><h2 id="before-you-begin">Before you begin</h2><p>Install <a href="/docs/tasks/tools/"><code>kubectl</code></a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="overview-of-kustomize">Overview of Kustomize</h2><p>Kustomize is a tool for customizing Kubernetes configurations. It has the following
features to manage application configuration files:</p><ul><li>generating resources from other sources</li><li>setting cross-cutting fields for resources</li><li>composing and customizing collections of resources</li></ul><h3 id="generating-resources">Generating Resources</h3><p>ConfigMaps and Secrets hold configuration or sensitive data that are used by other Kubernetes
objects, such as Pods. The source of truth of ConfigMaps or Secrets are usually external to
a cluster, such as a <code>.properties</code> file or an SSH keyfile.
Kustomize has <code>secretGenerator</code> and <code>configMapGenerator</code>, which generate Secret and ConfigMap from files or literals.</p><h4 id="configmapgenerator">configMapGenerator</h4><p>To generate a ConfigMap from a file, add an entry to the <code>files</code> list in <code>configMapGenerator</code>.
Here is an example of generating a ConfigMap with a data item from a <code>.properties</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a application.properties file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;application.properties
</span></span></span><span><span><span>FOO=Bar
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>configMapGenerator:
</span></span></span><span><span><span>- name: example-configmap-1
</span></span></span><span><span><span>  files:
</span></span></span><span><span><span>  - application.properties
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The generated ConfigMap can be examined with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl kustomize ./
</span></span></code></pre></div><p>The generated ConfigMap is:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>application.properties</span>:<span> </span>|<span>
</span></span></span><span><span><span>    FOO=Bar</span><span>    
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-configmap-1-8mbdf7882g<span>
</span></span></span></code></pre></div><p>To generate a ConfigMap from an env file, add an entry to the <code>envs</code> list in <code>configMapGenerator</code>.
Here is an example of generating a ConfigMap with a data item from a <code>.env</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a .env file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;.env
</span></span></span><span><span><span>FOO=Bar
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>configMapGenerator:
</span></span></span><span><span><span>- name: example-configmap-1
</span></span></span><span><span><span>  envs:
</span></span></span><span><span><span>  - .env
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The generated ConfigMap can be examined with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl kustomize ./
</span></span></code></pre></div><p>The generated ConfigMap is:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>FOO</span>:<span> </span>Bar<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-configmap-1-42cfbf598f<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Each variable in the <code>.env</code> file becomes a separate key in the ConfigMap that you generate.
This is different from the previous example which embeds a file named <code>application.properties</code>
(and all its entries) as the value for a single key.</div><p>ConfigMaps can also be generated from literal key-value pairs. To generate a ConfigMap from
a literal key-value pair, add an entry to the <code>literals</code> list in configMapGenerator.
Here is an example of generating a ConfigMap with a data item from a key-value pair:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>configMapGenerator:
</span></span></span><span><span><span>- name: example-configmap-2
</span></span></span><span><span><span>  literals:
</span></span></span><span><span><span>  - FOO=Bar
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The generated ConfigMap can be checked by the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl kustomize ./
</span></span></code></pre></div><p>The generated ConfigMap is:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>FOO</span>:<span> </span>Bar<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-configmap-2-g2hdhfc6tk<span>
</span></span></span></code></pre></div><p>To use a generated ConfigMap in a Deployment, reference it by the name of the configMapGenerator.
Kustomize will automatically replace this name with the generated name.</p><p>This is an example deployment that uses a generated ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Create an application.properties file</span><span>
</span></span></span><span><span><span></span>cat &lt;&lt;EOF &gt;application.properties<span>
</span></span></span><span><span><span></span>FOO=Bar<span>
</span></span></span><span><span><span></span>EOF<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span>cat &lt;&lt;EOF &gt;deployment.yaml<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>my-app<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>app<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/config<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>        </span><span>configMap</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>example-configmap-1<span>
</span></span></span><span><span><span></span>EOF<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span>cat &lt;&lt;EOF &gt;./kustomization.yaml<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span></span>- deployment.yaml<span>
</span></span></span><span><span><span></span><span>configMapGenerator</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>example-configmap-1<span>
</span></span></span><span><span><span>  </span><span>files</span>:<span>
</span></span></span><span><span><span>  </span>- application.properties<span>
</span></span></span><span><span><span></span>EOF<span>
</span></span></span></code></pre></div><p>Generate the ConfigMap and Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl kustomize ./
</span></span></code></pre></div><p>The generated Deployment will refer to the generated ConfigMap by name:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>application.properties</span>:<span> </span>|<span>
</span></span></span><span><span><span>    FOO=Bar</span><span>    
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-configmap-1-g4hk9g2ff8<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-app<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>my-app<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>app<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>mountPath</span>:<span> </span>/config<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>configMap</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>example-configmap-1-g4hk9g2ff8<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>config<span>
</span></span></span></code></pre></div><h4 id="secretgenerator">secretGenerator</h4><p>You can generate Secrets from files or literal key-value pairs.
To generate a Secret from a file, add an entry to the <code>files</code> list in <code>secretGenerator</code>.
Here is an example of generating a Secret with a data item from a file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a password.txt file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./password.txt
</span></span></span><span><span><span>username=admin
</span></span></span><span><span><span>password=secret
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>secretGenerator:
</span></span></span><span><span><span>- name: example-secret-1
</span></span></span><span><span><span>  files:
</span></span></span><span><span><span>  - password.txt
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The generated Secret is as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>password.txt</span>:<span> </span>dXNlcm5hbWU9YWRtaW4KcGFzc3dvcmQ9c2VjcmV0Cg==<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-secret-1-t2kt65hgtb<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span></code></pre></div><p>To generate a Secret from a literal key-value pair, add an entry to <code>literals</code> list
in <code>secretGenerator</code>. Here is an example of generating a Secret with a data item from a key-value pair:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>secretGenerator:
</span></span></span><span><span><span>- name: example-secret-2
</span></span></span><span><span><span>  literals:
</span></span></span><span><span><span>  - username=admin
</span></span></span><span><span><span>  - password=secret
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The generated Secret is as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>password</span>:<span> </span>c2VjcmV0<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>YWRtaW4=<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-secret-2-t52t6g96d8<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span></code></pre></div><p>Like ConfigMaps, generated Secrets can be used in Deployments by referring to the name of the secretGenerator:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a password.txt file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./password.txt
</span></span></span><span><span><span>username=admin
</span></span></span><span><span><span>password=secret
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-app
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    app: my-app
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      app: my-app
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        app: my-app
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: app
</span></span></span><span><span><span>        image: my-app
</span></span></span><span><span><span>        volumeMounts:
</span></span></span><span><span><span>        - name: password
</span></span></span><span><span><span>          mountPath: /secrets
</span></span></span><span><span><span>      volumes:
</span></span></span><span><span><span>      - name: password
</span></span></span><span><span><span>        secret:
</span></span></span><span><span><span>          secretName: example-secret-1
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>secretGenerator:
</span></span></span><span><span><span>- name: example-secret-1
</span></span></span><span><span><span>  files:
</span></span></span><span><span><span>  - password.txt
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><h4 id="generatoroptions">generatorOptions</h4><p>The generated ConfigMaps and Secrets have a content hash suffix appended. This ensures that
a new ConfigMap or Secret is generated when the contents are changed. To disable the behavior
of appending a suffix, one can use <code>generatorOptions</code>. Besides that, it is also possible to
specify cross-cutting options for generated ConfigMaps and Secrets.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>configMapGenerator:
</span></span></span><span><span><span>- name: example-configmap-3
</span></span></span><span><span><span>  literals:
</span></span></span><span><span><span>  - FOO=Bar
</span></span></span><span><span><span>generatorOptions:
</span></span></span><span><span><span>  disableNameSuffixHash: true
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    type: generated
</span></span></span><span><span><span>  annotations:
</span></span></span><span><span><span>    note: generated
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run<code>kubectl kustomize ./</code> to view the generated ConfigMap:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>FOO</span>:<span> </span>Bar<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>note</span>:<span> </span>generated<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>generated<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-configmap-3<span>
</span></span></span></code></pre></div><h3 id="setting-cross-cutting-fields">Setting cross-cutting fields</h3><p>It is quite common to set cross-cutting fields for all Kubernetes resources in a project.
Some use cases for setting cross-cutting fields:</p><ul><li>setting the same namespace for all resources</li><li>adding the same name prefix or suffix</li><li>adding the same set of labels</li><li>adding the same set of annotations</li></ul><p>Here is an example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a deployment.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: nginx-deployment
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    app: nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      app: nginx
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        app: nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>namespace: my-namespace
</span></span></span><span><span><span>namePrefix: dev-
</span></span></span><span><span><span>nameSuffix: "-001"
</span></span></span><span><span><span>labels:
</span></span></span><span><span><span>  - pairs:
</span></span></span><span><span><span>      app: bingo
</span></span></span><span><span><span>    includeSelectors: true 
</span></span></span><span><span><span>commonAnnotations:
</span></span></span><span><span><span>  oncallPager: 800-555-1212
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run <code>kubectl kustomize ./</code> to view those fields are all set in the Deployment Resource:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>oncallPager</span>:<span> </span><span>800-555-1212</span><span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>bingo<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-nginx-deployment-001<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>my-namespace<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>bingo<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>annotations</span>:<span>
</span></span></span><span><span><span>        </span><span>oncallPager</span>:<span> </span><span>800-555-1212</span><span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>bingo<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div><h3 id="composing-and-customizing-resources">Composing and Customizing Resources</h3><p>It is common to compose a set of resources in a project and manage them inside the same file or directory.
Kustomize offers composing resources from different files and applying patches or other customization to them.</p><h4 id="composing">Composing</h4><p>Kustomize supports composition of different resources. The <code>resources</code> field, in the <code>kustomization.yaml</code> file,
defines the list of resources to include in a configuration. Set the path to a resource's configuration file in the <code>resources</code> list.
Here is an example of an NGINX application comprised of a Deployment and a Service:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a deployment.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>        ports:
</span></span></span><span><span><span>        - containerPort: 80
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a service.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; service.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Service
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    run: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  ports:
</span></span></span><span><span><span>  - port: 80
</span></span></span><span><span><span>    protocol: TCP
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    run: my-nginx
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a kustomization.yaml composing them</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>- service.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>The resources from <code>kubectl kustomize ./</code> contain both the Deployment and the Service objects.</p><h4 id="customizing">Customizing</h4><p>Patches can be used to apply different customizations to resources. Kustomize supports different patching
mechanisms through <code>StrategicMerge</code> and <code>Json6902</code> using the <code>patches</code> field. <code>patches</code> may be a file or
an inline string, targeting a single or multiple resources.</p><p>The <code>patches</code> field contains a list of patches applied in the order they are specified. The patch target
selects resources by <code>group</code>, <code>version</code>, <code>kind</code>, <code>name</code>, <code>namespace</code>, <code>labelSelector</code> and <code>annotationSelector</code>.</p><p>Small patches that do one thing are recommended. For example, create one patch for increasing the deployment
replica number and another patch for setting the memory limit. The target resource is matched using <code>group</code>, <code>version</code>,
<code>kind</code>, and <code>name</code> fields from the patch file.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a deployment.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>        ports:
</span></span></span><span><span><span>        - containerPort: 80
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a patch increase_replicas.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; increase_replicas.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  replicas: 3
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create another patch set_memory.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; set_memory.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        resources:
</span></span></span><span><span><span>          limits:
</span></span></span><span><span><span>            memory: 512Mi
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>patches:
</span></span></span><span><span><span>  - path: increase_replicas.yaml
</span></span></span><span><span><span>  - path: set_memory.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run <code>kubectl kustomize ./</code> to view the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>512Mi<span>
</span></span></span></code></pre></div><p>Not all resources or fields support <code>strategicMerge</code> patches. To support modifying arbitrary fields in arbitrary resources,
Kustomize offers applying <a href="https://tools.ietf.org/html/rfc6902">JSON patch</a> through <code>Json6902</code>.
To find the correct Resource for a <code>Json6902</code> patch, it is mandatory to specify the <code>target</code> field in <code>kustomization.yaml</code>.</p><p>For example, increasing the replica number of a Deployment object can also be done through <code>Json6902</code> patch. The target resource
is matched using <code>group</code>, <code>version</code>, <code>kind</code>, and <code>name</code> from the <code>target</code> field.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a deployment.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>        ports:
</span></span></span><span><span><span>        - containerPort: 80
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a json patch</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; patch.yaml
</span></span></span><span><span><span>- op: replace
</span></span></span><span><span><span>  path: /spec/replicas
</span></span></span><span><span><span>  value: 3
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a kustomization.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>
</span></span></span><span><span><span>patches:
</span></span></span><span><span><span>- target:
</span></span></span><span><span><span>    group: apps
</span></span></span><span><span><span>    version: v1
</span></span></span><span><span><span>    kind: Deployment
</span></span></span><span><span><span>    name: my-nginx
</span></span></span><span><span><span>  path: patch.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run <code>kubectl kustomize ./</code> to see the <code>replicas</code> field is updated:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><p>In addition to patches, Kustomize also offers customizing container images or injecting field values from other objects into containers
without creating patches. For example, you can change the image used inside containers by specifying the new image in the <code>images</code> field
in <code>kustomization.yaml</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt; deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>        ports:
</span></span></span><span><span><span>        - containerPort: 80
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>images:
</span></span></span><span><span><span>- name: nginx
</span></span></span><span><span><span>  newName: my.image.registry/nginx
</span></span></span><span><span><span>  newTag: "1.4.0"
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run <code>kubectl kustomize ./</code> to see that the image being used is updated:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>my.image.registry/nginx:1.4.0<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><p>Sometimes, the application running in a Pod may need to use configuration values from other objects. For example,
a Pod from a Deployment object need to read the corresponding Service name from Env or as a command argument.
Since the Service name may change as <code>namePrefix</code> or <code>nameSuffix</code> is added in the <code>kustomization.yaml</code> file. It is
not recommended to hard code the Service name in the command argument. For this usage, Kustomize can inject
the Service name into containers through <code>replacements</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a deployment.yaml file (quoting the here doc delimiter)</span>
</span></span><span><span>cat <span>&lt;&lt;'EOF' &gt; deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>        command: ["start", "--host", "MY_SERVICE_NAME_PLACEHOLDER"]
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a service.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; service.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Service
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    run: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  ports:
</span></span></span><span><span><span>  - port: 80
</span></span></span><span><span><span>    protocol: TCP
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    run: my-nginx
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>namePrefix: dev-
</span></span></span><span><span><span>nameSuffix: "-001"
</span></span></span><span><span><span>
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>- service.yaml
</span></span></span><span><span><span>
</span></span></span><span><span><span>replacements:
</span></span></span><span><span><span>- source:
</span></span></span><span><span><span>    kind: Service
</span></span></span><span><span><span>    name: my-nginx
</span></span></span><span><span><span>    fieldPath: metadata.name
</span></span></span><span><span><span>  targets:
</span></span></span><span><span><span>  - select:
</span></span></span><span><span><span>      kind: Deployment
</span></span></span><span><span><span>      name: my-nginx
</span></span></span><span><span><span>    fieldPaths:
</span></span></span><span><span><span>    - spec.template.spec.containers.0.command.2
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run <code>kubectl kustomize ./</code> to see that the Service name injected into containers is <code>dev-my-nginx-001</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-my-nginx-001<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>command</span>:<span>
</span></span></span><span><span><span>        </span>- start<span>
</span></span></span><span><span><span>        </span>- --host<span>
</span></span></span><span><span><span>        </span>- dev-my-nginx-001<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span></code></pre></div><h2 id="bases-and-overlays">Bases and Overlays</h2><p>Kustomize has the concepts of <strong>bases</strong> and <strong>overlays</strong>. A <strong>base</strong> is a directory with a <code>kustomization.yaml</code>, which contains a
set of resources and associated customization. A base could be either a local directory or a directory from a remote repo,
as long as a <code>kustomization.yaml</code> is present inside. An <strong>overlay</strong> is a directory with a <code>kustomization.yaml</code> that refers to other
kustomization directories as its <code>bases</code>. A <strong>base</strong> has no knowledge of an overlay and can be used in multiple overlays.</p><p>The <code>kustomization.yaml</code> in an <strong>overlay</strong> directory may refer to multiple <code>bases</code>, combining all the resources defined
in these bases into a unified configuration. Additionally, it can apply customizations on top of these resources to meet specific
requirements.</p><p>Here is an example of a base:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a directory to hold the base</span>
</span></span><span><span>mkdir base
</span></span><span><span><span># Create a base/deployment.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; base/deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a base/service.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; base/service.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Service
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>  labels:
</span></span></span><span><span><span>    run: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  ports:
</span></span></span><span><span><span>  - port: 80
</span></span></span><span><span><span>    protocol: TCP
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    run: my-nginx
</span></span></span><span><span><span>EOF</span>
</span></span><span><span><span># Create a base/kustomization.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; base/kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>- service.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>This base can be used in multiple overlays. You can add different <code>namePrefix</code> or other cross-cutting fields
in different overlays. Here are two overlays using the same base.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>mkdir dev
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; dev/kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- ../base
</span></span></span><span><span><span>namePrefix: dev-
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>mkdir prod
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; prod/kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- ../base
</span></span></span><span><span><span>namePrefix: prod-
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><h2 id="how-to-apply-view-delete-objects-using-kustomize">How to apply/view/delete objects using Kustomize</h2><p>Use <code>--kustomize</code> or <code>-k</code> in <code>kubectl</code> commands to recognize resources managed by <code>kustomization.yaml</code>.
Note that <code>-k</code> should point to a kustomization directory, such as</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k &lt;kustomization directory&gt;/
</span></span></code></pre></div><p>Given the following <code>kustomization.yaml</code>,</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a deployment.yaml file</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt; deployment.yaml
</span></span></span><span><span><span>apiVersion: apps/v1
</span></span></span><span><span><span>kind: Deployment
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-nginx
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  selector:
</span></span></span><span><span><span>    matchLabels:
</span></span></span><span><span><span>      run: my-nginx
</span></span></span><span><span><span>  replicas: 2
</span></span></span><span><span><span>  template:
</span></span></span><span><span><span>    metadata:
</span></span></span><span><span><span>      labels:
</span></span></span><span><span><span>        run: my-nginx
</span></span></span><span><span><span>    spec:
</span></span></span><span><span><span>      containers:
</span></span></span><span><span><span>      - name: my-nginx
</span></span></span><span><span><span>        image: nginx
</span></span></span><span><span><span>        ports:
</span></span></span><span><span><span>        - containerPort: 80
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span><span># Create a kustomization.yaml</span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span><span><span>namePrefix: dev-
</span></span></span><span><span><span>labels:
</span></span></span><span><span><span>  - pairs:
</span></span></span><span><span><span>      app: my-nginx
</span></span></span><span><span><span>    includeSelectors: true 
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- deployment.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Run the following command to apply the Deployment object <code>dev-my-nginx</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>&gt; kubectl apply -k ./
</span></span><span><span>deployment.apps/dev-my-nginx created
</span></span></code></pre></div><p>Run one of the following commands to view the Deployment object <code>dev-my-nginx</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -k ./
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe -k ./
</span></span></code></pre></div><p>Run the following command to compare the Deployment object <code>dev-my-nginx</code> against the state
that the cluster would be in if the manifest was applied:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl diff -k ./
</span></span></code></pre></div><p>Run the following command to delete the Deployment object <code>dev-my-nginx</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>&gt; kubectl delete -k ./
</span></span><span><span>deployment.apps <span>"dev-my-nginx"</span> deleted
</span></span></code></pre></div><h2 id="kustomize-feature-list">Kustomize Feature List</h2><table><thead><tr><th>Field</th><th>Type</th><th>Explanation</th></tr></thead><tbody><tr><td>bases</td><td>[]string</td><td>Each entry in this list should resolve to a directory containing a kustomization.yaml file</td></tr><tr><td>commonAnnotations</td><td>map[string]string</td><td>annotations to add to all resources</td></tr><tr><td>commonLabels</td><td>map[string]string</td><td>labels to add to all resources and selectors</td></tr><tr><td>configMapGenerator</td><td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/configmapargs.go#L7">ConfigMapArgs</a></td><td>Each entry in this list generates a ConfigMap</td></tr><tr><td>configurations</td><td>[]string</td><td>Each entry in this list should resolve to a file containing <a href="https://github.com/kubernetes-sigs/kustomize/tree/master/examples/transformerconfigs">Kustomize transformer configurations</a></td></tr><tr><td>crds</td><td>[]string</td><td>Each entry in this list should resolve to an OpenAPI definition file for Kubernetes types</td></tr><tr><td>generatorOptions</td><td><a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/generatoroptions.go#L7">GeneratorOptions</a></td><td>Modify behaviors of all ConfigMap and Secret generator</td></tr><tr><td>images</td><td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/image.go#L8">Image</a></td><td>Each entry is to modify the name, tags and/or digest for one image without creating patches</td></tr><tr><td>labels</td><td>map[string]string</td><td>Add labels without automatically injecting corresponding selectors</td></tr><tr><td>namePrefix</td><td>string</td><td>value of this field is prepended to the names of all resources</td></tr><tr><td>nameSuffix</td><td>string</td><td>value of this field is appended to the names of all resources</td></tr><tr><td>patchesJson6902</td><td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/patch.go#L10">Patch</a></td><td>Each entry in this list should resolve to a Kubernetes object and a Json Patch</td></tr><tr><td>patchesStrategicMerge</td><td>[]string</td><td>Each entry in this list should resolve a strategic merge patch of a Kubernetes object</td></tr><tr><td>replacements</td><td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/replacement.go#L15">Replacements</a></td><td>copy the value from a resource's field into any number of specified targets.</td></tr><tr><td>resources</td><td>[]string</td><td>Each entry in this list must resolve to an existing resource configuration file</td></tr><tr><td>secretGenerator</td><td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/secretargs.go#L7">SecretArgs</a></td><td>Each entry in this list generates a Secret</td></tr><tr><td>vars</td><td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/var.go#L19">Var</a></td><td>Each entry is to capture text from one resource's field</td></tr></tbody></table><h2 id="what-s-next">What's next</h2><ul><li><a href="https://github.com/kubernetes-sigs/kustomize">Kustomize</a></li><li><a href="https://kubectl.docs.kubernetes.io">Kubectl Book</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl Command Reference</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/">Kubernetes API Reference</a></li></ul></div></div><div><div class="td-content"><h1>Managing Kubernetes Objects Using Imperative Commands</h1><p>Kubernetes objects can quickly be created, updated, and deleted directly using
imperative commands built into the <code>kubectl</code> command-line tool. This document
explains how those commands are organized and how to use them to manage live objects.</p><h2 id="before-you-begin">Before you begin</h2><p>Install <a href="/docs/tasks/tools/"><code>kubectl</code></a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="trade-offs">Trade-offs</h2><p>The <code>kubectl</code> tool supports three kinds of object management:</p><ul><li>Imperative commands</li><li>Imperative object configuration</li><li>Declarative object configuration</li></ul><p>See <a href="/docs/concepts/overview/working-with-objects/object-management/">Kubernetes Object Management</a>
for a discussion of the advantages and disadvantage of each kind of object management.</p><h2 id="how-to-create-objects">How to create objects</h2><p>The <code>kubectl</code> tool supports verb-driven commands for creating some of the most common
object types. The commands are named to be recognizable to users unfamiliar with
the Kubernetes object types.</p><ul><li><code>run</code>: Create a new Pod to run a Container.</li><li><code>expose</code>: Create a new Service object to load balance traffic across Pods.</li><li><code>autoscale</code>: Create a new Autoscaler object to automatically horizontally scale a controller, such as a Deployment.</li></ul><p>The <code>kubectl</code> tool also supports creation commands driven by object type.
These commands support more object types and are more explicit about
their intent, but require users to know the type of objects they intend
to create.</p><ul><li><code>create &lt;objecttype&gt; [&lt;subtype&gt;] &lt;instancename&gt;</code></li></ul><p>Some objects types have subtypes that you can specify in the <code>create</code> command.
For example, the Service object has several subtypes including ClusterIP,
LoadBalancer, and NodePort. Here's an example that creates a Service with
subtype NodePort:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create service nodeport &lt;myservicename&gt;
</span></span></code></pre></div><p>In the preceding example, the <code>create service nodeport</code> command is called
a subcommand of the <code>create service</code> command.</p><p>You can use the <code>-h</code> flag to find the arguments and flags supported by
a subcommand:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create service nodeport -h
</span></span></code></pre></div><h2 id="how-to-update-objects">How to update objects</h2><p>The <code>kubectl</code> command supports verb-driven commands for some common update operations.
These commands are named to enable users unfamiliar with Kubernetes
objects to perform updates without knowing the specific fields
that must be set:</p><ul><li><code>scale</code>: Horizontally scale a controller to add or remove Pods by updating the replica count of the controller.</li><li><code>annotate</code>: Add or remove an annotation from an object.</li><li><code>label</code>: Add or remove a label from an object.</li></ul><p>The <code>kubectl</code> command also supports update commands driven by an aspect of the object.
Setting this aspect may set different fields for different object types:</p><ul><li><code>set</code> <code>&lt;field&gt;</code>: Set an aspect of an object.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In Kubernetes version 1.5, not every verb-driven command has an associated aspect-driven command.</div><p>The <code>kubectl</code> tool supports these additional ways to update a live object directly,
however they require a better understanding of the Kubernetes object schema.</p><ul><li><code>edit</code>: Directly edit the raw configuration of a live object by opening its configuration in an editor.</li><li><code>patch</code>: Directly modify specific fields of a live object by using a patch string.
For more details on patch strings, see the patch section in
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#patch-operations">API Conventions</a>.</li></ul><h2 id="how-to-delete-objects">How to delete objects</h2><p>You can use the <code>delete</code> command to delete an object from a cluster:</p><ul><li><code>delete &lt;type&gt;/&lt;name&gt;</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can use <code>kubectl delete</code> for both imperative commands and imperative object
configuration. The difference is in the arguments passed to the command. To use
<code>kubectl delete</code> as an imperative command, pass the object to be deleted as
an argument. Here's an example that passes a Deployment object named nginx:</div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment/nginx
</span></span></code></pre></div><h2 id="how-to-view-an-object">How to view an object</h2><p>There are several commands for printing information about an object:</p><ul><li><code>get</code>: Prints basic information about matching objects. Use <code>get -h</code> to see a list of options.</li><li><code>describe</code>: Prints aggregated detailed information about matching objects.</li><li><code>logs</code>: Prints the stdout and stderr for a container running in a Pod.</li></ul><h2 id="using-set-commands-to-modify-objects-before-creation">Using <code>set</code> commands to modify objects before creation</h2><p>There are some object fields that don't have a flag you can use
in a <code>create</code> command. In some of those cases, you can use a combination of
<code>set</code> and <code>create</code> to specify a value for the field before object
creation. This is done by piping the output of the <code>create</code> command to the
<code>set</code> command, and then back to the <code>create</code> command. Here's an example:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create service clusterip my-svc --clusterip<span>=</span><span>"None"</span> -o yaml --dry-run<span>=</span>client | kubectl <span>set</span> selector --local -f - <span>'environment=qa'</span> -o yaml | kubectl create -f -
</span></span></code></pre></div><ol><li>The <code>kubectl create service -o yaml --dry-run=client</code> command creates the configuration for the Service, but prints it to stdout as YAML instead of sending it to the Kubernetes API server.</li><li>The <code>kubectl set selector --local -f - -o yaml</code> command reads the configuration from stdin, and writes the updated configuration to stdout as YAML.</li><li>The <code>kubectl create -f -</code> command creates the object using the configuration provided via stdin.</li></ol><h2 id="using-edit-to-modify-objects-before-creation">Using <code>--edit</code> to modify objects before creation</h2><p>You can use <code>kubectl create --edit</code> to make arbitrary changes to an object
before it is created. Here's an example:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create service clusterip my-svc --clusterip<span>=</span><span>"None"</span> -o yaml --dry-run<span>=</span>client &gt; /tmp/srv.yaml
</span></span><span><span>kubectl create --edit -f /tmp/srv.yaml
</span></span></code></pre></div><ol><li>The <code>kubectl create service</code> command creates the configuration for the Service and saves it to <code>/tmp/srv.yaml</code>.</li><li>The <code>kubectl create --edit</code> command opens the configuration file for editing before it creates the object.</li></ol><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/declarative-config/">Declarative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl Command Reference</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/">Kubernetes API Reference</a></li></ul></div></div><div><div class="td-content"><h1>Imperative Management of Kubernetes Objects Using Configuration Files</h1><p>Kubernetes objects can be created, updated, and deleted by using the <code>kubectl</code>
command-line tool along with an object configuration file written in YAML or JSON.
This document explains how to define and manage objects using configuration files.</p><h2 id="before-you-begin">Before you begin</h2><p>Install <a href="/docs/tasks/tools/"><code>kubectl</code></a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="trade-offs">Trade-offs</h2><p>The <code>kubectl</code> tool supports three kinds of object management:</p><ul><li>Imperative commands</li><li>Imperative object configuration</li><li>Declarative object configuration</li></ul><p>See <a href="/docs/concepts/overview/working-with-objects/object-management/">Kubernetes Object Management</a>
for a discussion of the advantages and disadvantage of each kind of object management.</p><h2 id="how-to-create-objects">How to create objects</h2><p>You can use <code>kubectl create -f</code> to create an object from a configuration file.
Refer to the <a href="/docs/reference/generated/kubernetes-api/v1.34/">kubernetes API reference</a>
for details.</p><ul><li><code>kubectl create -f &lt;filename|url&gt;</code></li></ul><h2 id="how-to-update-objects">How to update objects</h2><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Updating objects with the <code>replace</code> command drops all
parts of the spec not specified in the configuration file. This
should not be used with objects whose specs are partially managed
by the cluster, such as Services of type <code>LoadBalancer</code>, where
the <code>externalIPs</code> field is managed independently from the configuration
file. Independently managed fields must be copied to the configuration
file to prevent <code>replace</code> from dropping them.</div><p>You can use <code>kubectl replace -f</code> to update a live object according to a
configuration file.</p><ul><li><code>kubectl replace -f &lt;filename|url&gt;</code></li></ul><h2 id="how-to-delete-objects">How to delete objects</h2><p>You can use <code>kubectl delete -f</code> to delete an object that is described in a
configuration file.</p><ul><li><code>kubectl delete -f &lt;filename|url&gt;</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If configuration file has specified the <code>generateName</code> field in the <code>metadata</code>
section instead of the <code>name</code> field, you cannot delete the object using
<code>kubectl delete -f &lt;filename|url&gt;</code>.
You will have to use other flags for deleting the object. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete &lt;type&gt; &lt;name&gt;
</span></span><span><span>kubectl delete &lt;type&gt; -l &lt;label&gt;
</span></span></code></pre></div></div><h2 id="how-to-view-an-object">How to view an object</h2><p>You can use <code>kubectl get -f</code> to view information about an object that is
described in a configuration file.</p><ul><li><code>kubectl get -f &lt;filename|url&gt; -o yaml</code></li></ul><p>The <code>-o yaml</code> flag specifies that the full object configuration is printed.
Use <code>kubectl get -h</code> to see a list of options.</p><h2 id="limitations">Limitations</h2><p>The <code>create</code>, <code>replace</code>, and <code>delete</code> commands work well when each object's
configuration is fully defined and recorded in its configuration
file. However when a live object is updated, and the updates are not merged
into its configuration file, the updates will be lost the next time a <code>replace</code>
is executed. This can happen if a controller, such as
a HorizontalPodAutoscaler, makes updates directly to a live object. Here's
an example:</p><ol><li>You create an object from a configuration file.</li><li>Another source updates the object by changing some field.</li><li>You replace the object from the configuration file. Changes made by
the other source in step 2 are lost.</li></ol><p>If you need to support multiple writers to the same object, you can use
<code>kubectl apply</code> to manage the object.</p><h2 id="creating-and-editing-an-object-from-a-url-without-saving-the-configuration">Creating and editing an object from a URL without saving the configuration</h2><p>Suppose you have the URL of an object configuration file. You can use
<code>kubectl create --edit</code> to make changes to the configuration before the
object is created. This is particularly useful for tutorials and tasks
that point to a configuration file that could be modified by the reader.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f &lt;url&gt; --edit
</span></span></code></pre></div><h2 id="migrating-from-imperative-commands-to-imperative-object-configuration">Migrating from imperative commands to imperative object configuration</h2><p>Migrating from imperative commands to imperative object configuration involves
several manual steps.</p><ol><li><p>Export the live object to a local object configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get &lt;kind&gt;/&lt;name&gt; -o yaml &gt; &lt;kind&gt;_&lt;name&gt;.yaml
</span></span></code></pre></div></li><li><p>Manually remove the status field from the object configuration file.</p></li><li><p>For subsequent object management, use <code>replace</code> exclusively.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl replace -f &lt;kind&gt;_&lt;name&gt;.yaml
</span></span></code></pre></div></li></ol><h2 id="defining-controller-selectors-and-podtemplate-labels">Defining controller selectors and PodTemplate labels</h2><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Updating selectors on controllers is strongly discouraged.</div><p>The recommended approach is to define a single, immutable PodTemplate label
used only by the controller selector with no other semantic meaning.</p><p>Example label:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>selector</span>:<span>
</span></span></span><span><span><span>  </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>controller-selector</span>:<span> </span><span>"apps/v1/deployment/nginx"</span><span>
</span></span></span><span><span><span></span><span>template</span>:<span>
</span></span></span><span><span><span>  </span><span>metadata</span>:<span>
</span></span></span><span><span><span>    </span><span>labels</span>:<span>
</span></span></span><span><span><span>      </span><span>controller-selector</span>:<span> </span><span>"apps/v1/deployment/nginx"</span><span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/declarative-config/">Declarative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl Command Reference</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/">Kubernetes API Reference</a></li></ul></div></div><div><div class="td-content"><h1>Update API Objects in Place Using kubectl patch</h1><div class="lead">Use kubectl patch to update Kubernetes API objects in place. Do a strategic merge patch or a JSON merge patch.</div><p>This task shows how to use <code>kubectl patch</code> to update an API object in place. The exercises
in this task demonstrate a strategic merge patch and a JSON merge patch.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="use-a-strategic-merge-patch-to-update-a-deployment">Use a strategic merge patch to update a Deployment</h2><p>Here's the configuration file for a Deployment that has two replicas. Each replica
is a Pod that has one container:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment-patch.yaml"><code>application/deployment-patch.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment-patch.yaml to clipboard"></div><div class="includecode" id="application-deployment-patch-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>patch-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>patch-demo-ctr<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span>- <span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>        </span><span>key</span>:<span> </span>dedicated<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>test-team<span>
</span></span></span></code></pre></div></div></div><p>Create the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment-patch.yaml
</span></span></code></pre></div><p>View the Pods associated with your Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the Deployment has two Pods. The <code>1/1</code> indicates that
each Pod has one container:</p><pre tabindex="0"><code>NAME                        READY     STATUS    RESTARTS   AGE
patch-demo-28633765-670qr   1/1       Running   0          23s
patch-demo-28633765-j5qs3   1/1       Running   0          23s
</code></pre><p>Make a note of the names of the running Pods. Later, you will see that these Pods
get terminated and replaced by new ones.</p><p>At this point, each Pod has one Container that runs the nginx image. Now suppose
you want each Pod to have two containers: one that runs nginx and one that runs redis.</p><p>Create a file named <code>patch-file.yaml</code> that has this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>patch-demo-ctr-2<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>redis<span>
</span></span></span></code></pre></div><p>Patch your Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment patch-demo --patch-file patch-file.yaml
</span></span></code></pre></div><p>View the patched Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment patch-demo --output yaml
</span></span></code></pre></div><p>The output shows that the PodSpec in the Deployment has two Containers:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>containers</span>:<span>
</span></span></span><span><span><span></span>- <span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>  </span><span>imagePullPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>patch-demo-ctr-2<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>imagePullPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>patch-demo-ctr<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>View the Pods associated with your patched Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the running Pods have different names from the Pods that
were running previously. The Deployment terminated the old Pods and created two
new Pods that comply with the updated Deployment spec. The <code>2/2</code> indicates that
each Pod has two Containers:</p><pre tabindex="0"><code>NAME                          READY     STATUS    RESTARTS   AGE
patch-demo-1081991389-2wrn5   2/2       Running   0          1m
patch-demo-1081991389-jmg7b   2/2       Running   0          1m
</code></pre><p>Take a closer look at one of the patch-demo Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod &lt;your-pod-name&gt; --output yaml
</span></span></code></pre></div><p>The output shows that the Pod has two Containers: one running nginx and one running redis:</p><pre tabindex="0"><code>containers:
- image: redis
  ...
- image: nginx
  ...
</code></pre><h3 id="notes-on-the-strategic-merge-patch">Notes on the strategic merge patch</h3><p>The patch you did in the preceding exercise is called a <em>strategic merge patch</em>.
Notice that the patch did not replace the <code>containers</code> list. Instead it added a new
Container to the list. In other words, the list in the patch was merged with the
existing list. This is not always what happens when you use a strategic merge patch on a list.
In some cases, the list is replaced, not merged.</p><p>With a strategic merge patch, a list is either replaced or merged depending on its
patch strategy. The patch strategy is specified by the value of the <code>patchStrategy</code> key
in a field tag in the Kubernetes source code. For example, the <code>Containers</code> field of <code>PodSpec</code>
struct has a <code>patchStrategy</code> of <code>merge</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span><span>type</span> PodSpec <span>struct</span> {
</span></span><span><span>  <span>...</span>
</span></span><span><span>  Containers []Container <span>`json:"containers" patchStrategy:"merge" patchMergeKey:"name" ...`</span>
</span></span><span><span>  <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>You can also see the patch strategy in the
<a href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json">OpenApi spec</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>"io.k8s.api.core.v1.PodSpec": </span>{<span>
</span></span></span><span><span><span>    </span>...,<span>
</span></span></span><span><span><span>    </span><span>"containers": </span>{<span>
</span></span></span><span><span><span>        </span><span>"description": </span><span>"List of containers belonging to the pod.  ...."</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span><span>"x-kubernetes-patch-merge-key": </span><span>"name"</span>,<span>
</span></span></span><span><span><span>    </span><span>"x-kubernetes-patch-strategy": </span><span>"merge"</span><span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>And you can see the patch strategy in the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">Kubernetes API documentation</a>.</p><p>Create a file named <code>patch-file-tolerations.yaml</code> that has this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span>- <span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>        </span><span>key</span>:<span> </span>disktype<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>ssd<span>
</span></span></span></code></pre></div><p>Patch your Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment patch-demo --patch-file patch-file-tolerations.yaml
</span></span></code></pre></div><p>View the patched Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment patch-demo --output yaml
</span></span></code></pre></div><p>The output shows that the PodSpec in the Deployment has only one Toleration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>tolerations</span>:<span>
</span></span></span><span><span><span></span>- <span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>  </span><span>key</span>:<span> </span>disktype<span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span>ssd<span>
</span></span></span></code></pre></div><p>Notice that the <code>tolerations</code> list in the PodSpec was replaced, not merged. This is because
the Tolerations field of PodSpec does not have a <code>patchStrategy</code> key in its field tag. So the
strategic merge patch uses the default patch strategy, which is <code>replace</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span><span>type</span> PodSpec <span>struct</span> {
</span></span><span><span>  <span>...</span>
</span></span><span><span>  Tolerations []Toleration <span>`json:"tolerations,omitempty" protobuf:"bytes,22,opt,name=tolerations"`</span>
</span></span><span><span>  <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><h2 id="use-a-json-merge-patch-to-update-a-deployment">Use a JSON merge patch to update a Deployment</h2><p>A strategic merge patch is different from a
<a href="https://tools.ietf.org/html/rfc7386">JSON merge patch</a>.
With a JSON merge patch, if you
want to update a list, you have to specify the entire new list. And the new list completely
replaces the existing list.</p><p>The <code>kubectl patch</code> command has a <code>type</code> parameter that you can set to one of these values:</p><table><tr><th>Parameter value</th><th>Merge type</th></tr><tr><td>json</td><td><a href="https://tools.ietf.org/html/rfc6902">JSON Patch, RFC 6902</a></td></tr><tr><td>merge</td><td><a href="https://tools.ietf.org/html/rfc7386">JSON Merge Patch, RFC 7386</a></td></tr><tr><td>strategic</td><td>Strategic merge patch</td></tr></table><p>For a comparison of JSON patch and JSON merge patch, see
<a href="https://erosb.github.io/post/json-patch-vs-merge-patch/">JSON Patch and JSON Merge Patch</a>.</p><p>The default value for the <code>type</code> parameter is <code>strategic</code>. So in the preceding exercise, you
did a strategic merge patch.</p><p>Next, do a JSON merge patch on your same Deployment. Create a file named <code>patch-file-2.yaml</code>
that has this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>patch-demo-ctr-3<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span></code></pre></div><p>In your patch command, set <code>type</code> to <code>merge</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment patch-demo --type merge --patch-file patch-file-2.yaml
</span></span></code></pre></div><p>View the patched Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment patch-demo --output yaml
</span></span></code></pre></div><p>The <code>containers</code> list that you specified in the patch has only one Container.
The output shows that your list of one Container replaced the existing <code>containers</code> list.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>patch-demo-ctr-3<span>
</span></span></span></code></pre></div><p>List the running Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>In the output, you can see that the existing Pods were terminated, and new Pods
were created. The <code>1/1</code> indicates that each new Pod is running only one Container.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>NAME                          READY     STATUS    RESTARTS   AGE
</span></span><span><span>patch-demo-1307768864-69308   1/1       Running   <span>0</span>          1m
</span></span><span><span>patch-demo-1307768864-c86dc   1/1       Running   <span>0</span>          1m
</span></span></code></pre></div><h2 id="use-strategic-merge-patch-to-update-a-deployment-using-the-retainkeys-strategy">Use strategic merge patch to update a Deployment using the retainKeys strategy</h2><p>Here's the configuration file for a Deployment that uses the <code>RollingUpdate</code> strategy:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment-retainkeys.yaml"><code>application/deployment-retainkeys.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment-retainkeys.yaml to clipboard"></div><div class="includecode" id="application-deployment-retainkeys-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>retainkeys-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>rollingUpdate</span>:<span>
</span></span></span><span><span><span>      </span><span>maxSurge</span>:<span> </span><span>30</span>%<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>retainkeys-demo-ctr<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div></div></div><p>Create the deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment-retainkeys.yaml
</span></span></code></pre></div><p>At this point, the deployment is created and is using the <code>RollingUpdate</code> strategy.</p><p>Create a file named <code>patch-file-no-retainkeys.yaml</code> that has this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Recreate<span>
</span></span></span></code></pre></div><p>Patch your Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment retainkeys-demo --type strategic --patch-file patch-file-no-retainkeys.yaml
</span></span></code></pre></div><p>In the output, you can see that it is not possible to set <code>type</code> as <code>Recreate</code> when a value is defined for <code>spec.strategy.rollingUpdate</code>:</p><pre tabindex="0"><code>The Deployment "retainkeys-demo" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
</code></pre><p>The way to remove the value for <code>spec.strategy.rollingUpdate</code> when updating the value for <code>type</code> is to use the <code>retainKeys</code> strategy for the strategic merge.</p><p>Create another file named <code>patch-file-retainkeys.yaml</code> that has this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>$retainKeys</span>:<span>
</span></span></span><span><span><span>    </span>- type<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Recreate<span>
</span></span></span></code></pre></div><p>With this patch, we indicate that we want to retain only the <code>type</code> key of the <code>strategy</code> object. Thus, the <code>rollingUpdate</code> will be removed during the patch operation.</p><p>Patch your Deployment again with this new patch:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment retainkeys-demo --type strategic --patch-file patch-file-retainkeys.yaml
</span></span></code></pre></div><p>Examine the content of the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment retainkeys-demo --output yaml
</span></span></code></pre></div><p>The output shows that the strategy object in the Deployment does not contain the <code>rollingUpdate</code> key anymore:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Recreate<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span></code></pre></div><h3 id="notes-on-the-strategic-merge-patch-using-the-retainkeys-strategy">Notes on the strategic merge patch using the retainKeys strategy</h3><p>The patch you did in the preceding exercise is called a <em>strategic merge patch with retainKeys strategy</em>. This method introduces a new directive <code>$retainKeys</code> that has the following strategies:</p><ul><li>It contains a list of strings.</li><li>All fields needing to be preserved must be present in the <code>$retainKeys</code> list.</li><li>The fields that are present will be merged with live object.</li><li>All of the missing fields will be cleared when patching.</li><li>All fields in the <code>$retainKeys</code> list must be a superset or the same as the fields present in the patch.</li></ul><p>The <code>retainKeys</code> strategy does not work for all objects. It only works when the value of the <code>patchStrategy</code> key in a field tag in the Kubernetes source code contains <code>retainKeys</code>. For example, the <code>Strategy</code> field of the <code>DeploymentSpec</code> struct has a <code>patchStrategy</code> of <code>retainKeys</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span><span>type</span> DeploymentSpec <span>struct</span> {
</span></span><span><span>  <span>...</span>
</span></span><span><span>  <span>// +patchStrategy=retainKeys
</span></span></span><span><span><span></span>  Strategy DeploymentStrategy <span>`json:"strategy,omitempty" patchStrategy:"retainKeys" ...`</span>
</span></span><span><span>  <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>You can also see the <code>retainKeys</code> strategy in the <a href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json">OpenApi spec</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>"io.k8s.api.apps.v1.DeploymentSpec": </span>{<span>
</span></span></span><span><span><span>    </span>...,<span>
</span></span></span><span><span><span>    </span><span>"strategy": </span>{<span>
</span></span></span><span><span><span>        </span><span>"$ref": </span><span>"#/definitions/io.k8s.api.apps.v1.DeploymentStrategy"</span>,<span>
</span></span></span><span><span><span>        </span><span>"description": </span><span>"The deployment strategy to use to replace existing pods with new ones."</span>,<span>
</span></span></span><span><span><span>        </span><span>"x-kubernetes-patch-strategy": </span><span>"retainKeys"</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span>....<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>And you can see the <code>retainKeys</code> strategy in the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#deploymentspec-v1-apps">Kubernetes API documentation</a>.</p><h3 id="alternate-forms-of-the-kubectl-patch-command">Alternate forms of the kubectl patch command</h3><p>The <code>kubectl patch</code> command takes YAML or JSON. It can take the patch as a file or
directly on the command line.</p><p>Create a file named <code>patch-file.json</code> that has this content:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>   <span>"spec"</span>: {
</span></span><span><span>      <span>"template"</span>: {
</span></span><span><span>         <span>"spec"</span>: {
</span></span><span><span>            <span>"containers"</span>: [
</span></span><span><span>               {
</span></span><span><span>                  <span>"name"</span>: <span>"patch-demo-ctr-2"</span>,
</span></span><span><span>                  <span>"image"</span>: <span>"redis"</span>
</span></span><span><span>               }
</span></span><span><span>            ]
</span></span><span><span>         }
</span></span><span><span>      }
</span></span><span><span>   }
</span></span><span><span>}
</span></span></code></pre></div><p>The following commands are equivalent:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment patch-demo --patch-file patch-file.yaml
</span></span><span><span>kubectl patch deployment patch-demo --patch <span>'spec:\n template:\n  spec:\n   containers:\n   - name: patch-demo-ctr-2\n     image: redis'</span>
</span></span><span><span>
</span></span><span><span>kubectl patch deployment patch-demo --patch-file patch-file.json
</span></span><span><span>kubectl patch deployment patch-demo --patch <span>'{"spec": {"template": {"spec": {"containers": [{"name": "patch-demo-ctr-2","image": "redis"}]}}}}'</span>
</span></span></code></pre></div><h3 id="scale-kubectl-patch">Update an object's replica count using <code>kubectl patch</code> with <code>--subresource</code></h3><p>The flag <code>--subresource=[subresource-name]</code> is used with kubectl commands like get, patch,
edit, apply and replace to fetch and update <code>status</code>, <code>scale</code> and <code>resize</code> subresource of the
resources you specify. You can specify a subresource for any of the Kubernetes API resources
(built-in and CRs) that have <code>status</code>, <code>scale</code> or <code>resize</code> subresource.</p><p>For example, a Deployment has a <code>status</code> subresource and a <code>scale</code> subresource, so you can
use <code>kubectl</code> to get or modify just the <code>status</code> subresource of a Deployment.</p><p>Here's a manifest for a Deployment that has two replicas:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment.yaml"><code>application/deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment.yaml to clipboard"></div><div class="includecode" id="application-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># tells deployment to run 2 pods matching the template</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Create the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</span></span></code></pre></div><p>View the Pods associated with your Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><p>In the output, you can see that Deployment has two Pods. For example:</p><pre tabindex="0"><code>NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-7fb96c846b-22567   1/1     Running   0          47s
nginx-deployment-7fb96c846b-mlgns   1/1     Running   0          47s
</code></pre><p>Now, patch that Deployment with <code>--subresource=[subresource-name]</code> flag:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment nginx-deployment --subresource<span>=</span><span>'scale'</span> --type<span>=</span><span>'merge'</span> -p <span>'{"spec":{"replicas":3}}'</span>
</span></span></code></pre></div><p>The output is:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>scale.autoscaling/nginx-deployment patched
</span></span></code></pre></div><p>View the Pods associated with your patched Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><p>In the output, you can see one new pod is created, so now you have 3 running pods.</p><pre tabindex="0"><code>NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-7fb96c846b-22567   1/1     Running   0          107s
nginx-deployment-7fb96c846b-lxfr2   1/1     Running   0          14s
nginx-deployment-7fb96c846b-mlgns   1/1     Running   0          107s
</code></pre><p>View the patched Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment nginx-deployment -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>availableReplicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>readyReplicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you run <code>kubectl patch</code> and specify <code>--subresource</code> flag for resource that doesn't support that
particular subresource, the API server returns a 404 Not Found error.</div><h2 id="summary">Summary</h2><p>In this exercise, you used <code>kubectl patch</code> to change the live configuration
of a Deployment object. You did not change the configuration file that you originally used to
create the Deployment object. Other commands for updating API objects include
<a href="/docs/reference/generated/kubectl/kubectl-commands/#annotate">kubectl annotate</a>,
<a href="/docs/reference/generated/kubectl/kubectl-commands/#edit">kubectl edit</a>,
<a href="/docs/reference/generated/kubectl/kubectl-commands/#replace">kubectl replace</a>,
<a href="/docs/reference/generated/kubectl/kubectl-commands/#scale">kubectl scale</a>,
and
<a href="/docs/reference/generated/kubectl/kubectl-commands/#apply">kubectl apply</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Strategic merge patch is not supported for custom resources.</div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/overview/working-with-objects/object-management/">Kubernetes Object Management</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/declarative-config/">Declarative Management of Kubernetes Objects Using Configuration Files</a></li></ul></div></div><div><div class="td-content"><h1>Migrate Kubernetes Objects Using Storage Version Migration</h1><div class="feature-state-notice feature-alpha" title="Feature Gate: StorageVersionMigrator"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [alpha]</code> (enabled by default: false)</div><p>Kubernetes relies on API data being actively re-written, to support some
maintenance activities related to at rest storage. Two prominent examples are
the versioned schema of stored resources (that is, the preferred storage schema
changing from v1 to v2 for a given resource) and encryption at rest
(that is, rewriting stale data based on a change in how the data should be encrypted).</p><h2 id="before-you-begin">Before you begin</h2><p>Install <a href="/docs/tasks/tools/#kubectl"><code>kubectl</code></a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.30.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>Ensure that your cluster has the <code>StorageVersionMigrator</code> and <code>InformerResourceVersion</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>
enabled. You will need control plane administrator access to make that change.</p><p>Enable storage version migration REST api by setting runtime config
<code>storagemigration.k8s.io/v1alpha1</code> to <code>true</code> for the API server. For more information on
how to do that,
read <a href="/docs/tasks/administer-cluster/enable-disable-api/">enable or disable a Kubernetes API</a>.</p><h2 id="re-encrypt-kubernetes-secrets-using-storage-version-migration">Re-encrypt Kubernetes secrets using storage version migration</h2><ul><li><p>To begin with, <a href="/docs/tasks/administer-cluster/kms-provider/">configure KMS provider</a>
to encrypt data at rest in etcd using following encryption configuration.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span></span>- <span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- secrets<span>
</span></span></span><span><span><span>  </span><span>providers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>      </span><span>keys</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>        </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span>
</span></span></span></code></pre></div><p>Make sure to enable automatic reload of encryption
configuration file by setting <code>--encryption-provider-config-automatic-reload</code> to true.</p></li><li><p>Create a Secret using kubectl.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic my-secret --from-literal<span>=</span><span>key1</span><span>=</span>supersecret
</span></span></code></pre></div></li><li><p><a href="/docs/tasks/administer-cluster/kms-provider/#verifying-that-the-data-is-encrypted">Verify</a>
the serialized data for that Secret object is prefixed with <code>k8s:enc:aescbc:v1:key1</code>.</p></li><li><p>Update the encryption configuration file as follows to rotate the encryption key.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>EncryptionConfiguration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>resources</span>:<span>
</span></span></span><span><span><span></span>- <span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- secrets<span>
</span></span></span><span><span><span>  </span><span>providers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>      </span><span>keys</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>key2<span>
</span></span></span><span><span><span>        </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZSwgaXMgaXQ/<span>
</span></span></span><span><span><span>  </span>- <span>aescbc</span>:<span>
</span></span></span><span><span><span>      </span><span>keys</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>key1<span>
</span></span></span><span><span><span>        </span><span>secret</span>:<span> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span>
</span></span></span></code></pre></div></li><li><p>To ensure that previously created secret <code>my-secret</code> is re-encrypted
with new key <code>key2</code>, you will use <em>Storage Version Migration</em>.</p></li><li><p>Create a StorageVersionMigration manifest named <code>migrate-secret.yaml</code> as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>StorageVersionMigration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>storagemigration.k8s.io/v1alpha1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secrets-migration<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resource</span>:<span>
</span></span></span><span><span><span>    </span><span>group</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>version</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span> </span>secrets<span>
</span></span></span></code></pre></div><p>Create the object using <em>kubectl</em> as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f migrate-secret.yaml
</span></span></code></pre></div></li><li><p>Monitor migration of Secrets by checking the <code>.status</code> of the StorageVersionMigration.
A successful migration should have its
<code>Succeeded</code> condition set to true. Get the StorageVersionMigration object as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get storageversionmigration.storagemigration.k8s.io/secrets-migration -o yaml
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>StorageVersionMigration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>storagemigration.k8s.io/v1alpha1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secrets-migration<span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>628f6922-a9cb-4514-b076-12d3c178967c<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"90"</span><span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2024-03-12T20:29:45Z"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resource</span>:<span>
</span></span></span><span><span><span>    </span><span>group</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>version</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span> </span>secrets<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Running<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>    </span><span>lastUpdateTime</span>:<span> </span><span>"2024-03-12T20:29:46Z"</span><span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>StorageVersionMigrationInProgress<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Succeeded<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>lastUpdateTime</span>:<span> </span><span>"2024-03-12T20:29:46Z"</span><span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>StorageVersionMigrationSucceeded<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"84"</span><span>
</span></span></span></code></pre></div></li><li><p><a href="/docs/tasks/administer-cluster/kms-provider/#verifying-that-the-data-is-encrypted">Verify</a>
the stored secret is now prefixed with <code>k8s:enc:aescbc:v1:key2</code>.</p></li></ul><h2 id="update-the-preferred-storage-schema-of-a-crd">Update the preferred storage schema of a CRD</h2><p>Consider a scenario where a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a>
(CRD) is created to serve custom resources (CRs) and is set as the preferred storage schema. When it's time
to introduce v2 of the CRD, it can be added for serving only with a conversion
webhook. This enables a smoother transition where users can create CRs using
either the v1 or v2 schema, with the webhook in place to perform the necessary
schema conversion between them. Before setting v2 as the preferred storage schema
version, it's important to ensure that all existing CRs stored as v1 are migrated to v2.
This migration can be achieved through <em>Storage Version Migration</em> to migrate all CRs from v1 to v2.</p><ul><li><p>Create a manifest for the CRD, named <code>test-crd.yaml</code>, as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>selfierequests.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>SelfieRequests<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>SelfieRequest<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>SelfieRequest<span>
</span></span></span><span><span><span>    </span><span>listKind</span>:<span> </span>SelfieRequestList<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>hostPort</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhook</span>:<span>
</span></span></span><span><span><span>      </span><span>clientConfig</span>:<span>
</span></span></span><span><span><span>        </span><span>url</span>:<span> </span><span>"https://127.0.0.1:9443/crdconvert"</span><span>
</span></span></span><span><span><span>        </span><span>caBundle</span>:<span> </span>&lt;CABundle info&gt;<span>
</span></span></span><span><span><span>    </span><span>conversionReviewVersions</span>:<span>
</span></span></span><span><span><span>    </span>- v1<span>
</span></span></span><span><span><span>    </span>- v2<span>
</span></span></span></code></pre></div><p>Create CRD using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f test-crd.yaml
</span></span></code></pre></div></li><li><p>Create a manifest for an example testcrd. Name the manifest <code>cr1.yaml</code> and use these contents:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>stable.example.com/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>SelfieRequest<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cr1<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span></code></pre></div><p>Create CR using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f cr1.yaml
</span></span></code></pre></div></li><li><p>Verify that CR is written and stored as v1 by getting the object from etcd.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl get /kubernetes.io/stable.example.com/testcrds/default/cr1 <span>[</span>...<span>]</span> | hexdump -C
</span></span></code></pre></div><p>where <code>[...]</code> contains the additional arguments for connecting to the etcd server.</p></li><li><p>Update the CRD <code>test-crd.yaml</code> to include v2 version for serving and storage
and v1 as serving only, as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span></span><span>name</span>:<span> </span>selfierequests.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>SelfieRequests<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>SelfieRequest<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>SelfieRequest<span>
</span></span></span><span><span><span>    </span><span>listKind</span>:<span> </span>SelfieRequestList<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v2<span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>host</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>hostPort</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhook</span>:<span>
</span></span></span><span><span><span>      </span><span>clientConfig</span>:<span>
</span></span></span><span><span><span>        </span><span>url</span>:<span> </span><span>"https://127.0.0.1:9443/crdconvert"</span><span>
</span></span></span><span><span><span>        </span><span>caBundle</span>:<span> </span>&lt;CABundle info&gt;<span>
</span></span></span><span><span><span>      </span><span>conversionReviewVersions</span>:<span>
</span></span></span><span><span><span>        </span>- v1<span>
</span></span></span><span><span><span>        </span>- v2<span>
</span></span></span></code></pre></div><p>Update CRD using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f test-crd.yaml
</span></span></code></pre></div></li><li><p>Create CR resource file with name <code>cr2.yaml</code> as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>stable.example.com/v2<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>SelfieRequest<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cr2<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span></code></pre></div></li><li><p>Create CR using kubectl:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f cr2.yaml
</span></span></code></pre></div></li><li><p>Verify that CR is written and stored as v2 by getting the object from etcd.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl get /kubernetes.io/stable.example.com/testcrds/default/cr2 <span>[</span>...<span>]</span> | hexdump -C
</span></span></code></pre></div><p>where <code>[...]</code> contains the additional arguments for connecting to the etcd server.</p></li><li><p>Create a StorageVersionMigration manifest named <code>migrate-crd.yaml</code>, with the contents as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>StorageVersionMigration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>storagemigration.k8s.io/v1alpha1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crdsvm<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resource</span>:<span>
</span></span></span><span><span><span>    </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>    </span><span>version</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span> </span>SelfieRequest<span>
</span></span></span></code></pre></div><p>Create the object using <em>kubectl</em> as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f migrate-crd.yaml
</span></span></code></pre></div></li><li><p>Monitor migration of secrets using status. Successful migration should have
<code>Succeeded</code> condition set to "True" in the status field. Get the migration resource
as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get storageversionmigration.storagemigration.k8s.io/crdsvm -o yaml
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>StorageVersionMigration<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>storagemigration.k8s.io/v1alpha1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crdsvm<span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>13062fe4-32d7-47cc-9528-5067fa0c6ac8<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"111"</span><span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2024-03-12T22:40:01Z"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resource</span>:<span>
</span></span></span><span><span><span>    </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>    </span><span>version</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span> </span>testcrds<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Running<span>
</span></span></span><span><span><span>      </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>      </span><span>lastUpdateTime</span>:<span> </span><span>"2024-03-12T22:40:03Z"</span><span>
</span></span></span><span><span><span>      </span><span>reason</span>:<span> </span>StorageVersionMigrationInProgress<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Succeeded<span>
</span></span></span><span><span><span>      </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>      </span><span>lastUpdateTime</span>:<span> </span><span>"2024-03-12T22:40:03Z"</span><span>
</span></span></span><span><span><span>      </span><span>reason</span>:<span> </span>StorageVersionMigrationSucceeded<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"106"</span><span>
</span></span></span></code></pre></div></li><li><p>Verify that previously created cr1 is now written and stored as v2 by getting the object from etcd.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>ETCDCTL_API</span><span>=</span><span>3</span> etcdctl get /kubernetes.io/stable.example.com/testcrds/default/cr1 <span>[</span>...<span>]</span> | hexdump -C
</span></span></code></pre></div><p>where <code>[...]</code> contains the additional arguments for connecting to the etcd server.</p></li></ul></div></div><div><div class="td-content"><h1>Managing Secrets</h1><div class="lead">Managing confidential settings data using Secrets.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">Managing Secrets using kubectl</a></h5><p>Creating Secret objects using kubectl command line.</p></div><div class="entry"><h5><a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">Managing Secrets using Configuration File</a></h5><p>Creating Secret objects using resource configuration file.</p></div><div class="entry"><h5><a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">Managing Secrets using Kustomize</a></h5><p>Creating Secret objects using kustomization.yaml file.</p></div></div></div></div><div><div class="td-content"><h1>Managing Secrets using kubectl</h1><div class="lead">Creating Secret objects using kubectl command line.</div><p>This page shows you how to create, edit, manage, and delete Kubernetes
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secrets</a> using the <code>kubectl</code>
command-line tool.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="create-a-secret">Create a Secret</h2><p>A <code>Secret</code> object stores sensitive data such as credentials
used by Pods to access services. For example, you might need a Secret to store
the username and password needed to access a database.</p><p>You can create the Secret by passing the raw data in the command, or by storing
the credentials in files that you pass in the command. The following commands
create a Secret that stores the username <code>admin</code> and the password <code>S!B\*d$zDsb=</code>.</p><h3 id="use-raw-data">Use raw data</h3><p>Run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic db-user-pass <span>\
</span></span></span><span><span><span></span>    --from-literal<span>=</span><span>username</span><span>=</span>admin <span>\
</span></span></span><span><span><span></span>    --from-literal<span>=</span><span>password</span><span>=</span><span>'S!B\*d$zDsb='</span>
</span></span></code></pre></div><p>You must use single quotes <code>''</code> to escape special characters such as <code>$</code>, <code>\</code>,
<code>*</code>, <code>=</code>, and <code>!</code> in your strings. If you don't, your shell will interpret these
characters.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><h3 id="use-source-files">Use source files</h3><ol><li><p>Store the credentials in files:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> -n <span>'admin'</span> &gt; ./username.txt
</span></span><span><span><span>echo</span> -n <span>'S!B\*d$zDsb='</span> &gt; ./password.txt
</span></span></code></pre></div><p>The <code>-n</code> flag ensures that the generated files do not have an extra newline
character at the end of the text. This is important because when <code>kubectl</code>
reads a file and encodes the content into a base64 string, the extra
newline character gets encoded too. You do not need to escape special
characters in strings that you include in a file.</p></li><li><p>Pass the file paths in the <code>kubectl</code> command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic db-user-pass <span>\
</span></span></span><span><span><span></span>    --from-file<span>=</span>./username.txt <span>\
</span></span></span><span><span><span></span>    --from-file<span>=</span>./password.txt
</span></span></code></pre></div><p>The default key name is the file name. You can optionally set the key name
using <code>--from-file=[key=]source</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic db-user-pass <span>\
</span></span></span><span><span><span></span>    --from-file<span>=</span><span>username</span><span>=</span>./username.txt <span>\
</span></span></span><span><span><span></span>    --from-file<span>=</span><span>password</span><span>=</span>./password.txt
</span></span></code></pre></div></li></ol><p>With either method, the output is similar to:</p><pre tabindex="0"><code>secret/db-user-pass created
</code></pre><h3 id="verify-the-secret">Verify the Secret</h3><p>Check that the Secret was created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secrets
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME              TYPE       DATA      AGE
db-user-pass      Opaque     2         51s
</code></pre><p>View the details of the Secret:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe secret db-user-pass
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>Name:            db-user-pass
Namespace:       default
Labels:          &lt;none&gt;
Annotations:     &lt;none&gt;

Type:            Opaque

Data
====
password:    12 bytes
username:    5 bytes
</code></pre><p>The commands <code>kubectl get</code> and <code>kubectl describe</code> avoid showing the contents
of a <code>Secret</code> by default. This is to protect the <code>Secret</code> from being exposed
accidentally, or from being stored in a terminal log.</p><h3 id="decoding-secret">Decode the Secret</h3><ol><li><p>View the contents of the Secret you created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret db-user-pass -o <span>jsonpath</span><span>=</span><span>'{.data}'</span>
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{ <span>"password"</span>: <span>"UyFCXCpkJHpEc2I9"</span>, <span>"username"</span>: <span>"YWRtaW4="</span> }
</span></span></code></pre></div></li><li><p>Decode the <code>password</code> data:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> <span>'UyFCXCpkJHpEc2I9'</span> | base64 --decode
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>S!B\*d$zDsb=
</code></pre><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>This is an example for documentation purposes. In practice,
this method could cause the command with the encoded data to be stored in
your shell history. Anyone with access to your computer could find the
command and decode the secret. A better approach is to combine the view and
decode commands.</div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret db-user-pass -o <span>jsonpath</span><span>=</span><span>'{.data.password}'</span> | base64 --decode
</span></span></code></pre></div></li></ol><h2 id="edit-secret">Edit a Secret</h2><p>You can edit an existing <code>Secret</code> object unless it is
<a href="/docs/concepts/configuration/secret/#secret-immutable">immutable</a>. To edit a
Secret, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit secrets &lt;secret-name&gt;
</span></span></code></pre></div><p>This opens your default editor and allows you to update the base64 encoded
Secret values in the <code>data</code> field, such as in the following example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Please edit the object below. Lines beginning with a '#' will be ignored,</span><span>
</span></span></span><span><span><span></span><span># and an empty file will abort the edit. If an error occurs while saving this file, it will be</span><span>
</span></span></span><span><span><span></span><span># reopened with the relevant failures.</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>password</span>:<span> </span>UyFCXCpkJHpEc2I9<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>YWRtaW4=<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2022-06-28T17:44:13Z"</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>db-user-pass<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"12708504"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>91becd59-78fa-4c85-823f-6d44436242ac<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span></code></pre></div><h2 id="clean-up">Clean up</h2><p>To delete a Secret, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete secret db-user-pass
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read more about the <a href="/docs/concepts/configuration/secret/">Secret concept</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">manage Secrets using config file</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">manage Secrets using kustomize</a></li></ul></div></div><div><div class="td-content"><h1>Managing Secrets using Configuration File</h1><div class="lead">Creating Secret objects using resource configuration file.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="create-the-config-file">Create the Secret</h2><p>You can define the <code>Secret</code> object in a manifest first, in JSON or YAML format,
and then create that object. The
<a href="/docs/reference/generated/kubernetes-api/v1.34/#secret-v1-core">Secret</a>
resource contains two maps: <code>data</code> and <code>stringData</code>.
The <code>data</code> field is used to store arbitrary data, encoded using base64. The
<code>stringData</code> field is provided for convenience, and it allows you to provide
the same data as unencoded strings.
The keys of <code>data</code> and <code>stringData</code> must consist of alphanumeric characters,
<code>-</code>, <code>_</code> or <code>.</code>.</p><p>The following example stores two strings in a Secret using the <code>data</code> field.</p><ol><li><p>Convert the strings to base64:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> -n <span>'admin'</span> | base64
</span></span><span><span><span>echo</span> -n <span>'1f2d1e2e67df'</span> | base64
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The serialized JSON and YAML values of Secret data are encoded as base64 strings. Newlines are not valid within these strings and must be omitted. When using the <code>base64</code> utility on Darwin/macOS, users should avoid using the <code>-b</code> option to split long lines. Conversely, Linux users <em>should</em> add the option <code>-w 0</code> to <code>base64</code> commands or the pipeline <code>base64 | tr -d '\n'</code> if the <code>-w</code> option is not available.</div><p>The output is similar to:</p><pre tabindex="0"><code>YWRtaW4=
MWYyZDFlMmU2N2Rm
</code></pre></li><li><p>Create the manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>YWRtaW4=<span>
</span></span></span><span><span><span>  </span><span>password</span>:<span> </span>MWYyZDFlMmU2N2Rm<span>
</span></span></span></code></pre></div><p>Note that the name of a Secret object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p></li><li><p>Create the Secret using <a href="/docs/reference/generated/kubectl/kubectl-commands#apply"><code>kubectl apply</code></a>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f ./secret.yaml
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>secret/mysecret created
</code></pre></li></ol><p>To verify that the Secret was created and to decode the Secret data, refer to
<a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/#verify-the-secret">Managing Secrets using kubectl</a>.</p><h3 id="specify-unencoded-data-when-creating-a-secret">Specify unencoded data when creating a Secret</h3><p>For certain scenarios, you may wish to use the <code>stringData</code> field instead. This
field allows you to put a non-base64 encoded string directly into the Secret,
and the string will be encoded for you when the Secret is created or updated.</p><p>A practical example of this might be where you are deploying an application
that uses a Secret to store a configuration file, and you want to populate
parts of that configuration file during your deployment process.</p><p>For example, if your application uses the following configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiUrl</span>:<span> </span><span>"https://my.api.com/api/v1"</span><span>
</span></span></span><span><span><span></span><span>username</span>:<span> </span><span>"&lt;user&gt;"</span><span>
</span></span></span><span><span><span></span><span>password</span>:<span> </span><span>"&lt;password&gt;"</span><span>
</span></span></span></code></pre></div><p>You could store this in a Secret using the following definition:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span><span><span><span></span><span>stringData</span>:<span>
</span></span></span><span><span><span>  </span><span>config.yaml</span>:<span> </span>|<span>
</span></span></span><span><span><span>    apiUrl: "https://my.api.com/api/v1"
</span></span></span><span><span><span>    username: &lt;user&gt;
</span></span></span><span><span><span>    password: &lt;password&gt;</span><span>    
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><p>When you retrieve the Secret data, the command returns the encoded values,
and not the plaintext values you provided in <code>stringData</code>.</p><p>For example, if you run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret mysecret -o yaml
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>config.yaml</span>:<span> </span>YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHt7dXNlcm5hbWV9fQpwYXNzd29yZDoge3twYXNzd29yZH19<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2018-11-15T20:40:59Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"7225"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>c280ad2e-e916-11e8-98f2-025000000001<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span></code></pre></div><h3 id="specify-both-data-and-stringdata">Specify both <code>data</code> and <code>stringData</code></h3><p>If you specify a field in both <code>data</code> and <code>stringData</code>, the value from <code>stringData</code> is used.</p><p>For example, if you define the following Secret:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>YWRtaW4=<span>
</span></span></span><span><span><span></span><span>stringData</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>administrator<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><p>The <code>Secret</code> object is created as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>YWRtaW5pc3RyYXRvcg==<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2018-11-15T20:46:46Z<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"7579"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>91460ecb-e917-11e8-98f2-025000000001<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span></code></pre></div><p><code>YWRtaW5pc3RyYXRvcg==</code> decodes to <code>administrator</code>.</p><h2 id="edit-secret">Edit a Secret</h2><p>To edit the data in the Secret you created using a manifest, modify the <code>data</code>
or <code>stringData</code> field in your manifest and apply the file to your
cluster. You can edit an existing <code>Secret</code> object unless it is
<a href="/docs/concepts/configuration/secret/#secret-immutable">immutable</a>.</p><p>For example, if you want to change the password from the previous example to
<code>birdsarentreal</code>, do the following:</p><ol><li><p>Encode the new password string:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> -n <span>'birdsarentreal'</span> | base64
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>YmlyZHNhcmVudHJlYWw=
</code></pre></li><li><p>Update the <code>data</code> field with your new password string:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>Opaque<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>YWRtaW4=<span>
</span></span></span><span><span><span>  </span><span>password</span>:<span> </span>YmlyZHNhcmVudHJlYWw=<span>
</span></span></span></code></pre></div></li><li><p>Apply the manifest to your cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f ./secret.yaml
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>secret/mysecret configured
</code></pre></li></ol><p>Kubernetes updates the existing <code>Secret</code> object. In detail, the <code>kubectl</code> tool
notices that there is an existing <code>Secret</code> object with the same name. <code>kubectl</code>
fetches the existing object, plans changes to it, and submits the changed
<code>Secret</code> object to your cluster control plane.</p><p>If you specified <code>kubectl apply --server-side</code> instead, <code>kubectl</code> uses
<a href="/docs/reference/using-api/server-side-apply/">Server Side Apply</a> instead.</p><h2 id="clean-up">Clean up</h2><p>To delete the Secret you have created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete secret mysecret
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read more about the <a href="/docs/concepts/configuration/secret/">Secret concept</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">manage Secrets using kubectl</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">manage Secrets using kustomize</a></li></ul></div></div><div><div class="td-content"><h1>Managing Secrets using Kustomize</h1><div class="lead">Creating Secret objects using kustomization.yaml file.</div><p><code>kubectl</code> supports using the <a href="/docs/tasks/manage-kubernetes-objects/kustomization/">Kustomize object management tool</a> to manage Secrets
and ConfigMaps. You create a <em>resource generator</em> using Kustomize, which
generates a Secret that you can apply to the API server using <code>kubectl</code>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="create-a-secret">Create a Secret</h2><p>You can generate a Secret by defining a <code>secretGenerator</code> in a
<code>kustomization.yaml</code> file that references other existing files, <code>.env</code> files, or
literal values. For example, the following instructions create a kustomization
file for the username <code>admin</code> and the password <code>1f2d1e2e67df</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><h3 id="create-the-kustomization-file">Create the kustomization file</h3><ul class="nav nav-tabs" id="secret-data"><li class="nav-item"><a class="nav-link active" href="#secret-data-0">Literals</a></li><li class="nav-item"><a class="nav-link" href="#secret-data-1">Files</a></li><li class="nav-item"><a class="nav-link" href="#secret-data-2">.env files</a></li></ul><div class="tab-content" id="secret-data"><div id="secret-data-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>
</span></span></span><span><span><span></span><span>secretGenerator</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>database-creds<span>
</span></span></span><span><span><span>  </span><span>literals</span>:<span>
</span></span></span><span><span><span>  </span>- username=admin<span>
</span></span></span><span><span><span>  </span>- password=1f2d1e2e67df<span>
</span></span></span></code></pre></div></p></div><div id="secret-data-1" class="tab-pane"><p><ol><li><p>Store the credentials in files. The filenames are the keys of the secret:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> -n <span>'admin'</span> &gt; ./username.txt
</span></span><span><span><span>echo</span> -n <span>'1f2d1e2e67df'</span> &gt; ./password.txt
</span></span></code></pre></div><p>The <code>-n</code> flag ensures that there's no newline character at the end of your
files.</p></li><li><p>Create the <code>kustomization.yaml</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>secretGenerator</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>database-creds<span>
</span></span></span><span><span><span>  </span><span>files</span>:<span>
</span></span></span><span><span><span>  </span>- username.txt<span>
</span></span></span><span><span><span>  </span>- password.txt<span>
</span></span></span></code></pre></div></li></ol></p></div><div id="secret-data-2" class="tab-pane"><p><p>You can also define the secretGenerator in the <code>kustomization.yaml</code> file by
providing <code>.env</code> files. For example, the following <code>kustomization.yaml</code> file
pulls in data from an <code>.env.secret</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>secretGenerator</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>db-user-pass<span>
</span></span></span><span><span><span>  </span><span>envs</span>:<span>
</span></span></span><span><span><span>  </span>- .env.secret<span>
</span></span></span></code></pre></div></p></div></div><p>In all cases, you don't need to encode the values in base64. The name of the YAML
file <strong>must</strong> be <code>kustomization.yaml</code> or <code>kustomization.yml</code>.</p><h3 id="apply-the-kustomization-file">Apply the kustomization file</h3><p>To create the Secret, apply the directory that contains the kustomization file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k &lt;directory-path&gt;
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>secret/database-creds-5hdh7hhgfk created
</code></pre><p>When a Secret is generated, the Secret name is created by hashing
the Secret data and appending the hash value to the name. This ensures that
a new Secret is generated each time the data is modified.</p><p>To verify that the Secret was created and to decode the Secret data,</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -k &lt;directory-path&gt; -o <span>jsonpath</span><span>=</span><span>'{.data}'</span> 
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>{ "password": "MWYyZDFlMmU2N2Rm", "username": "YWRtaW4=" }
</code></pre><pre tabindex="0"><code>echo 'MWYyZDFlMmU2N2Rm' | base64 --decode
</code></pre><p>The output is similar to:</p><pre tabindex="0"><code>1f2d1e2e67df
</code></pre><p>For more information, refer to
<a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/#verify-the-secret">Managing Secrets using kubectl</a> and
<a href="/docs/tasks/manage-kubernetes-objects/kustomization/">Declarative Management of Kubernetes Objects Using Kustomize</a>.</p><h2 id="edit-secret">Edit a Secret</h2><ol><li><p>In your <code>kustomization.yaml</code> file, modify the data, such as the <code>password</code>.</p></li><li><p>Apply the directory that contains the kustomization file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -k &lt;directory-path&gt;
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>secret/db-user-pass-6f24b56cc8 created
</code></pre></li></ol><p>The edited Secret is created as a new <code>Secret</code> object, instead of updating the
existing <code>Secret</code> object. You might need to update references to the Secret in
your Pods.</p><h2 id="clean-up">Clean up</h2><p>To delete a Secret, use <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete secret db-user-pass
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read more about the <a href="/docs/concepts/configuration/secret/">Secret concept</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">manage Secrets using kubectl</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">manage Secrets using config file</a></li></ul></div></div><div><div class="td-content"><h1>Inject Data Into Applications</h1><div class="lead">Specify configuration and other data for the Pods that run your workload.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/inject-data-application/define-command-argument-container/">Define a Command and Arguments for a Container</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/define-interdependent-environment-variables/">Define Dependent Environment Variables</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/define-environment-variable-container/">Define Environment Variables for a Container</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/define-environment-variable-via-file/">Define Environment Variable Values Using An Init Container</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">Expose Pod Information to Containers Through Environment Variables</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">Expose Pod Information to Containers Through Files</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/inject-data-application/distribute-credentials-secure/">Distribute Credentials Securely Using Secrets</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Define a Command and Arguments for a Container</h1><p>This page shows how to define commands and arguments when you run a container
in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="define-a-command-and-arguments-when-you-create-a-pod">Define a command and arguments when you create a Pod</h2><p>When you create a Pod, you can define a command and arguments for the
containers that run in the Pod. To define a command, include the <code>command</code>
field in the configuration file. To define arguments for the command, include
the <code>args</code> field in the configuration file. The command and arguments that
you define cannot be changed after the Pod is created.</p><p>The command and arguments that you define in the configuration file
override the default command and arguments provided by the container image.
If you define args, but do not define a command, the default command is used
with your new arguments.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>command</code> field corresponds to <code>ENTRYPOINT</code>, and the <code>args</code> field corresponds to <code>CMD</code> in some container runtimes.</div><p>In this exercise, you create a Pod that runs one container. The configuration
file for the Pod defines a command and two arguments:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/commands.yaml"><code>pods/commands.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/commands.yaml to clipboard"></div><div class="includecode" id="pods-commands-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>command-demo<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>purpose</span>:<span> </span>demonstrate-command<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>command-demo-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"printenv"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"HOSTNAME"</span>,<span> </span><span>"KUBERNETES_PORT"</span>]<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create a Pod based on the YAML configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/commands.yaml
</span></span></code></pre></div></li><li><p>List the running Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the container that ran in the command-demo Pod has
completed.</p></li><li><p>To see the output of the command that ran in the container, view the logs
from the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs command-demo
</span></span></code></pre></div><p>The output shows the values of the HOSTNAME and KUBERNETES_PORT environment
variables:</p><pre tabindex="0"><code>command-demo
tcp://10.3.240.1:443
</code></pre></li></ol><h2 id="use-environment-variables-to-define-arguments">Use environment variables to define arguments</h2><p>In the preceding example, you defined the arguments directly by
providing strings. As an alternative to providing strings directly,
you can define arguments by using environment variables:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>env</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>MESSAGE<span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span><span>"hello world"</span><span>
</span></span></span><span><span><span></span><span>command</span>:<span> </span>[<span>"/bin/echo"</span>]<span>
</span></span></span><span><span><span></span><span>args</span>:<span> </span>[<span>"$(MESSAGE)"</span>]<span>
</span></span></span></code></pre></div><p>This means you can define an argument for a Pod using any of
the techniques available for defining environment variables, including
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a>
and
<a href="/docs/concepts/configuration/secret/">Secrets</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The environment variable appears in parentheses, <code>"$(VAR)"</code>. This is
required for the variable to be expanded in the <code>command</code> or <code>args</code> field.</div><h2 id="run-a-command-in-a-shell">Run a command in a shell</h2><p>In some cases, you need your command to run in a shell. For example, your
command might consist of several commands piped together, or it might be a shell
script. To run your command in a shell, wrap it like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>command: <span>[</span><span>"/bin/sh"</span><span>]</span>
</span></span><span><span>args: <span>[</span><span>"-c"</span>, <span>"while true; do echo hello; sleep 10;done"</span><span>]</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/">configuring pods and containers</a>.</li><li>Learn more about <a href="/docs/tasks/debug/debug-application/get-shell-running-container/">running commands in a container</a>.</li><li>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#container-v1-core">Container</a>.</li></ul></div></div><div><div class="td-content"><h1>Define Dependent Environment Variables</h1><p>This page shows how to define dependent environment variables for a container
in a Kubernetes Pod.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="define-an-environment-dependent-variable-for-a-container">Define an environment dependent variable for a container</h2><p>When you create a Pod, you can set dependent environment variables for the containers that run in the Pod. To set dependent environment variables, you can use $(VAR_NAME) in the <code>value</code> of <code>env</code> in the configuration file.</p><p>In this exercise, you create a Pod that runs one container. The configuration
file for the Pod defines a dependent environment variable with common usage defined. Here is the configuration manifest for the
Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/dependent-envars.yaml"><code>pods/inject/dependent-envars.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/dependent-envars.yaml to clipboard"></div><div class="includecode" id="pods-inject-dependent-envars-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dependent-envars-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>dependent-envars-demo<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span>
</span></span></span><span><span><span>        </span>- while true; do echo -en '\n'; printf UNCHANGED_REFERENCE=$UNCHANGED_REFERENCE'\n'; printf SERVICE_ADDRESS=$SERVICE_ADDRESS'\n';printf ESCAPED_REFERENCE=$ESCAPED_REFERENCE'\n'; sleep 30; done;<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- sh<span>
</span></span></span><span><span><span>        </span>- -c<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SERVICE_PORT<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"80"</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SERVICE_IP<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"172.17.0.1"</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>UNCHANGED_REFERENCE<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)"</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>PROTOCOL<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"https"</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>SERVICE_ADDRESS<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)"</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>ESCAPED_REFERENCE<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"$$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)"</span><span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create a Pod based on that manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/dependent-envars.yaml
</span></span></code></pre></div><pre tabindex="0"><code>pod/dependent-envars-demo created
</code></pre></li><li><p>List the running Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods dependent-envars-demo
</span></span></code></pre></div><pre tabindex="0"><code>NAME                      READY     STATUS    RESTARTS   AGE
dependent-envars-demo     1/1       Running   0          9s
</code></pre></li><li><p>Check the logs for the container running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs pod/dependent-envars-demo
</span></span></code></pre></div><pre tabindex="0"><code>
UNCHANGED_REFERENCE=$(PROTOCOL)://172.17.0.1:80
SERVICE_ADDRESS=https://172.17.0.1:80
ESCAPED_REFERENCE=$(PROTOCOL)://172.17.0.1:80
</code></pre></li></ol><p>As shown above, you have defined the correct dependency reference of <code>SERVICE_ADDRESS</code>, bad dependency reference of <code>UNCHANGED_REFERENCE</code> and skip dependent references of <code>ESCAPED_REFERENCE</code>.</p><p>When an environment variable is already defined when being referenced,
the reference can be correctly resolved, such as in the <code>SERVICE_ADDRESS</code> case.</p><p>Note that order matters in the <code>env</code> list. An environment variable is not considered
"defined" if it is specified further down the list. That is why <code>UNCHANGED_REFERENCE</code>
fails to resolve <code>$(PROTOCOL)</code> in the example above.</p><p>When the environment variable is undefined or only includes some variables, the undefined environment variable is treated as a normal string, such as <code>UNCHANGED_REFERENCE</code>. Note that incorrectly parsed environment variables, in general, will not block the container from starting.</p><p>The <code>$(VAR_NAME)</code> syntax can be escaped with a double <code>$</code>, ie: <code>$$(VAR_NAME)</code>.
Escaped references are never expanded, regardless of whether the referenced variable
is defined or not. This can be seen from the <code>ESCAPED_REFERENCE</code> case above.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a>.</li><li>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#envvarsource-v1-core">EnvVarSource</a>.</li></ul></div></div><div><div class="td-content"><h1>Define Environment Variables for a Container</h1><p>This page shows how to define environment variables for a container
in a Kubernetes Pod.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="define-an-environment-variable-for-a-container">Define an environment variable for a container</h2><p>When you create a Pod, you can set environment variables for the containers
that run in the Pod. To set environment variables, include the <code>env</code> or
<code>envFrom</code> field in the configuration file.</p><p>The <code>env</code> and <code>envFrom</code> fields have different effects.</p><dl><dt><code>env</code></dt><dd>allows you to set environment variables for a container, specifying a value directly for each variable that you name.</dd><dt><code>envFrom</code></dt><dd>allows you to set environment variables for a container by referencing either a ConfigMap or a Secret.
When you use <code>envFrom</code>, all the key-value pairs in the referenced ConfigMap or Secret
are set as environment variables for the container.
You can also specify a common prefix string.</dd></dl><p>You can read more about <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables">ConfigMap</a>
and <a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables">Secret</a>.</p><p>This page explains how to use <code>env</code>.</p><p>In this exercise, you create a Pod that runs one container. The configuration
file for the Pod defines an environment variable with name <code>DEMO_GREETING</code> and
value <code>"Hello from the environment"</code>. Here is the configuration manifest for the
Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/envars.yaml"><code>pods/inject/envars.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/envars.yaml to clipboard"></div><div class="includecode" id="pods-inject-envars-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>envar-demo<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>purpose</span>:<span> </span>demonstrate-envars<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>envar-demo-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>DEMO_GREETING<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"Hello from the environment"</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>DEMO_FAREWELL<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"Such a sweet sorrow"</span><span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create a Pod based on that manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/envars.yaml
</span></span></code></pre></div></li><li><p>List the running Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>purpose</span><span>=</span>demonstrate-envars
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME            READY     STATUS    RESTARTS   AGE
envar-demo      1/1       Running   0          9s
</code></pre></li><li><p>List the Pod's container environment variables:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> envar-demo -- printenv
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NODE_VERSION=4.4.2
EXAMPLE_SERVICE_PORT_8080_TCP_ADDR=10.3.245.237
HOSTNAME=envar-demo
...
DEMO_GREETING=Hello from the environment
DEMO_FAREWELL=Such a sweet sorrow
</code></pre></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The environment variables set using the <code>env</code> or <code>envFrom</code> field
override any environment variables specified in the container image.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Environment variables may reference each other, however ordering is important.
Variables making use of others defined in the same context must come later in
the list. Similarly, avoid circular references.</div><h2 id="using-environment-variables-inside-of-your-config">Using environment variables inside of your config</h2><p>Environment variables that you define in a Pod's configuration under
<code>.spec.containers[*].env[*]</code> can be used elsewhere in the configuration, for
example in commands and arguments that you set for the Pod's containers.
In the example configuration below, the <code>GREETING</code>, <code>HONORIFIC</code>, and
<code>NAME</code> environment variables are set to <code>Warm greetings to</code>, <code>The Most Honorable</code>, and <code>Kubernetes</code>, respectively. The environment variable
<code>MESSAGE</code> combines the set of all these environment variables and then uses it
as a CLI argument passed to the <code>env-print-demo</code> container.</p><p>Environment variable names may consist of any <a href="https://www.ascii-code.com/characters/printable-characters">printable ASCII characters</a> except '='.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>print-greeting<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>env-print-demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>bash<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>GREETING<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"Warm greetings to"</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>HONORIFIC<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"The Most Honorable"</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>NAME<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"Kubernetes"</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>MESSAGE<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"$(GREETING) $(HONORIFIC) $(NAME)"</span><span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"echo"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"$(MESSAGE)"</span>]<span>
</span></span></span></code></pre></div><p>Upon creation, the command <code>echo Warm greetings to The Most Honorable Kubernetes</code> is run on the container.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a>.</li><li>Learn about <a href="/docs/concepts/configuration/secret/#using-secrets-as-environment-variables">using secrets as environment variables</a>.</li><li>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#envvarsource-v1-core">EnvVarSource</a>.</li></ul></div></div><div><div class="td-content"><h1>Define Environment Variable Values Using An Init Container</h1><div class="feature-state-notice feature-alpha" title="Feature Gate: EnvFiles"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>This page show how to configure environment variables for containers in a Pod via file.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your Kubernetes server must be version v1.34.</p><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="how-the-design-works">How the design works</h2><p>In this exercise, you will create a Pod that sources environment variables from files,
projecting these values into the running container.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/envars-file-container.yaml"><code>pods/inject/envars-file-container.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/envars-file-container.yaml to clipboard"></div><div class="includecode" id="pods-inject-envars-file-container-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>envfile-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>setup-envfile<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span>  </span>nginx<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "DB_ADDRESS=address\nREST_ENDPOINT=endpoint" &gt; /data/config.env'</span>]<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/data<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>use-envfile<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"env"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>DB_ADDRESS<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fileKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>path</span>:<span> </span>config.env<span>
</span></span></span><span><span><span>              </span><span>volumeName</span>:<span> </span>config<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>DB_ADDRESS<span>
</span></span></span><span><span><span>              </span><span>optional</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>      </span><span>emptyDir</span>:<span> </span>{}</span></span></code></pre></div></div></div><p>In this manifest, you can see the <code>initContainer</code> mounts an <code>emptyDir</code> volume and writes environment variables to a file within it,
and the regular containers reference both the file and the environment variable key
through the <code>fileKeyRef</code> field without needing to mount the volume.
When <code>optional</code> field is set to false, the specified <code>key</code> in <code>fileKeyRef</code> must exist in the environment variables file.</p><p>The volume will only be mounted to the container that writes to the file
(<code>initContainer</code>), while the consumer container that consumes the environment variable will not have the volume mounted.</p><p>The env file format adheres to the <a href="/docs/tasks/inject-data-application/define-environment-variable-via-file/#env-file-syntax">kubernetes env file standard</a>.</p><p>During container initialization, the kubelet retrieves environment variables
from specified files in the <code>emptyDir</code> volume and exposes them to the container.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>All container types (initContainers, regular containers, sidecars containers,
and ephemeral containers) support environment variable loading from files.</p><p>While these environment variables can store sensitive information,
<code>emptyDir</code> volumes don't provide the same protection mechanisms as
dedicated Secret objects. Therefore, exposing confidential environment variables
to containers through this feature is not considered a security best practice.</p></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/envars-file-container.yaml
</span></span></code></pre></div><p>Verify that the container in the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If the new Pod isn't yet healthy, rerun this command a few times.</span>
</span></span><span><span>kubectl get pods
</span></span></code></pre></div><p>Check container logs for environment variables:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs dapi-test-pod -c use-envfile | grep DB_ADDRESS
</span></span></code></pre></div><p>The output shows the values of selected environment variables:</p><pre tabindex="0"><code>DB_ADDRESS=address
</code></pre><h2 id="env-file-syntax">Env File Syntax</h2><p>The format of Kubernetes env files originates from <code>.env</code> files.</p><p>In a shell environment, <code>.env</code> files are typically loaded using the <code>source .env</code> command.</p><p>For Kubernetes, the defined env file format adheres to stricter syntax rules:</p><ul><li><p>Blank Lines: Blank lines are ignored.</p></li><li><p>Leading Spaces: Leading spaces on all lines are ignored.</p></li><li><p>Variable Declaration: Variables must be declared as <code>VAR=VAL</code>. Spaces surrounding <code>=</code> and trailing spaces are ignored.</p><pre tabindex="0"><code>VAR=VAL &#8594; VAL
</code></pre></li><li><p>Comments: Lines beginning with # are treated as comments and ignored.</p><pre tabindex="0"><code># comment
VAR=VAL &#8594; VAL

VAR=VAL # not a comment &#8594; VAL # not a comment
</code></pre></li><li><p>Line Continuation: A backslash (<code>\</code>) at the end of a variable declaration line indicates the value continues on the next line. The lines are joined with a single space.</p><pre tabindex="0"><code>VAR=VAL \
VAL2
&#8594; VAL VAL2
</code></pre></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a>.</li><li>Read <a href="/docs/tasks/inject-data-application/define-environment-variable-container/">Defining Environment Variables for a Container</a></li><li>Read <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">Expose Pod Information to Containers Through Environment Variables</a></li></ul></div></div><div><div class="td-content"><h1>Expose Pod Information to Containers Through Environment Variables</h1><p>This page shows how a Pod can use environment variables to expose information
about itself to containers running in the Pod, using the <em>downward API</em>.
You can use environment variables to expose Pod fields, container fields, or both.</p><p>In Kubernetes, there are two ways to expose Pod and container fields to a running container:</p><ul><li><em>Environment variables</em>, as explained in this task</li><li><a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">Volume files</a></li></ul><p>Together, these two ways of exposing Pod and container fields are called the
downward API.</p><p>As Services are the primary mode of communication between containerized applications managed by Kubernetes,
it is helpful to be able to discover them at runtime.</p><p>Read more about accessing Services <a href="/docs/tutorials/services/connect-applications-service/#accessing-the-service">here</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="use-pod-fields-as-values-for-environment-variables">Use Pod fields as values for environment variables</h2><p>In this part of exercise, you create a Pod that has one container, and you
project Pod-level fields into the running container as environment variables.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/dapi-envars-pod.yaml"><code>pods/inject/dapi-envars-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/dapi-envars-pod.yaml to clipboard"></div><div class="includecode" id="pods-inject-dapi-envars-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-envars-fieldref<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"sh"</span>,<span> </span><span>"-c"</span>]<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span>
</span></span></span><span><span><span>      </span>- while true; do<span>
</span></span></span><span><span><span>          </span>echo -en '\n';<span>
</span></span></span><span><span><span>          </span>printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;<span>
</span></span></span><span><span><span>          </span>printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;<span>
</span></span></span><span><span><span>          </span>sleep 10;<span>
</span></span></span><span><span><span>        </span>done;<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_NODE_NAME<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>spec.nodeName<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_POD_NAME<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>metadata.name<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_POD_NAMESPACE<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>metadata.namespace<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_POD_IP<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>status.podIP<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_POD_SERVICE_ACCOUNT<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>spec.serviceAccountName<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>In that manifest, you can see five environment variables. The <code>env</code>
field is an array of
environment variable definitions.
The first element in the array specifies that the <code>MY_NODE_NAME</code> environment
variable gets its value from the Pod's <code>spec.nodeName</code> field. Similarly, the
other environment variables get their names from Pod fields.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The fields in this example are Pod fields. They are not fields of the
container in the Pod.</div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-pod.yaml
</span></span></code></pre></div><p>Verify that the container in the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If the new Pod isn't yet healthy, rerun this command a few times.</span>
</span></span><span><span>kubectl get pods
</span></span></code></pre></div><p>View the container's logs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs dapi-envars-fieldref
</span></span></code></pre></div><p>The output shows the values of selected environment variables:</p><pre tabindex="0"><code>minikube
dapi-envars-fieldref
default
172.17.0.4
default
</code></pre><p>To see why these values are in the log, look at the <code>command</code> and <code>args</code> fields
in the configuration file. When the container starts, it writes the values of
five environment variables to stdout. It repeats this every ten seconds.</p><p>Next, get a shell into the container that is running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it dapi-envars-fieldref -- sh
</span></span></code></pre></div><p>In your shell, view the environment variables:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this in a shell inside the container</span>
</span></span><span><span>printenv
</span></span></code></pre></div><p>The output shows that certain environment variables have been assigned the
values of Pod fields:</p><pre tabindex="0"><code>MY_POD_SERVICE_ACCOUNT=default
...
MY_POD_NAMESPACE=default
MY_POD_IP=172.17.0.4
...
MY_NODE_NAME=minikube
...
MY_POD_NAME=dapi-envars-fieldref
</code></pre><h2 id="use-container-fields-as-values-for-environment-variables">Use container fields as values for environment variables</h2><p>In the preceding exercise, you used information from Pod-level fields as the values
for environment variables.
In this next exercise, you are going to pass fields that are part of the Pod
definition, but taken from the specific
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">container</a>
rather than from the Pod overall.</p><p>Here is a manifest for another Pod that again has just one container:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/dapi-envars-container.yaml"><code>pods/inject/dapi-envars-container.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/dapi-envars-container.yaml to clipboard"></div><div class="includecode" id="pods-inject-dapi-envars-container-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dapi-envars-resourcefieldref<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"sh"</span>,<span> </span><span>"-c"</span>]<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span>
</span></span></span><span><span><span>      </span>- while true; do<span>
</span></span></span><span><span><span>          </span>echo -en '\n';<span>
</span></span></span><span><span><span>          </span>printenv MY_CPU_REQUEST MY_CPU_LIMIT;<span>
</span></span></span><span><span><span>          </span>printenv MY_MEM_REQUEST MY_MEM_LIMIT;<span>
</span></span></span><span><span><span>          </span>sleep 10;<span>
</span></span></span><span><span><span>        </span>done;<span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>requests</span>:<span>
</span></span></span><span><span><span>          </span><span>memory</span>:<span> </span><span>"32Mi"</span><span>
</span></span></span><span><span><span>          </span><span>cpu</span>:<span> </span><span>"125m"</span><span>
</span></span></span><span><span><span>        </span><span>limits</span>:<span>
</span></span></span><span><span><span>          </span><span>memory</span>:<span> </span><span>"64Mi"</span><span>
</span></span></span><span><span><span>          </span><span>cpu</span>:<span> </span><span>"250m"</span><span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_CPU_REQUEST<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>requests.cpu<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_CPU_LIMIT<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>limits.cpu<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_MEM_REQUEST<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>requests.memory<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_MEM_LIMIT<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>limits.memory<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>In this manifest, you can see four environment variables. The <code>env</code>
field is an array of
environment variable definitions.
The first element in the array specifies that the <code>MY_CPU_REQUEST</code> environment
variable gets its value from the <code>requests.cpu</code> field of a container named
<code>test-container</code>. Similarly, the other environment variables get their values
from fields that are specific to this container.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-container.yaml
</span></span></code></pre></div><p>Verify that the container in the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># If the new Pod isn't yet healthy, rerun this command a few times.</span>
</span></span><span><span>kubectl get pods
</span></span></code></pre></div><p>View the container's logs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs dapi-envars-resourcefieldref
</span></span></code></pre></div><p>The output shows the values of selected environment variables:</p><pre tabindex="0"><code>1
1
33554432
67108864
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/docs/tasks/inject-data-application/define-environment-variable-container/">Defining Environment Variables for a Container</a></li><li>Read the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec"><code>spec</code></a>
API definition for Pod. This includes the definition of Container (part of Pod).</li><li>Read the list of <a href="/docs/concepts/workloads/pods/downward-api/#available-fields">available fields</a> that you
can expose using the downward API.</li></ul><p>Read about Pods, containers and environment variables in the legacy API reference:</p><ul><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#container-v1-core">Container</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#envvar-v1-core">EnvVar</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#envvarsource-v1-core">EnvVarSource</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#objectfieldselector-v1-core">ObjectFieldSelector</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#resourcefieldselector-v1-core">ResourceFieldSelector</a></li></ul></div></div><div><div class="td-content"><h1>Expose Pod Information to Containers Through Files</h1><p>This page shows how a Pod can use a
<a href="/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code> volume</a>,
to expose information about itself to containers running in the Pod.
A <code>downwardAPI</code> volume can expose Pod fields and container fields.</p><p>In Kubernetes, there are two ways to expose Pod and container fields to a running container:</p><ul><li><a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">Environment variables</a></li><li>Volume files, as explained in this task</li></ul><p>Together, these two ways of exposing Pod and container fields are called the
<em>downward API</em>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="store-pod-fields">Store Pod fields</h2><p>In this part of exercise, you create a Pod that has one container, and you
project Pod-level fields into the running container as files.
Here is the manifest for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/dapi-volume.yaml"><code>pods/inject/dapi-volume.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/dapi-volume.yaml to clipboard"></div><div class="includecode" id="pods-inject-dapi-volume-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kubernetes-downwardapi-volume-example<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>zone</span>:<span> </span>us-est-coast<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>test-cluster1<span>
</span></span></span><span><span><span>    </span><span>rack</span>:<span> </span>rack-22<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>build</span>:<span> </span>two<span>
</span></span></span><span><span><span>    </span><span>builder</span>:<span> </span>john-doe<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>client-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"sh"</span>,<span> </span><span>"-c"</span>]<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span>
</span></span></span><span><span><span>      </span>- while true; do<span>
</span></span></span><span><span><span>          </span>if [[ -e /etc/podinfo/labels ]]; then<span>
</span></span></span><span><span><span>            </span>echo -en '\n\n'; cat /etc/podinfo/labels; fi;<span>
</span></span></span><span><span><span>          </span>if [[ -e /etc/podinfo/annotations ]]; then<span>
</span></span></span><span><span><span>            </span>echo -en '\n\n'; cat /etc/podinfo/annotations; fi;<span>
</span></span></span><span><span><span>          </span>sleep 5;<span>
</span></span></span><span><span><span>        </span>done;<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>podinfo<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/podinfo<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>podinfo<span>
</span></span></span><span><span><span>      </span><span>downwardAPI</span>:<span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"labels"</span><span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>metadata.labels<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"annotations"</span><span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>metadata.annotations<span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>In the manifest, you can see that the Pod has a <code>downwardAPI</code> Volume,
and the container mounts the volume at <code>/etc/podinfo</code>.</p><p>Look at the <code>items</code> array under <code>downwardAPI</code>. Each element of the array
defines a <code>downwardAPI</code> volume.
The first element specifies that the value of the Pod's
<code>metadata.labels</code> field should be stored in a file named <code>labels</code>.
The second element specifies that the value of the Pod's <code>annotations</code>
field should be stored in a file named <code>annotations</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The fields in this example are Pod fields. They are not
fields of the container in the Pod.</div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume.yaml
</span></span></code></pre></div><p>Verify that the container in the Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>View the container's logs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs kubernetes-downwardapi-volume-example
</span></span></code></pre></div><p>The output shows the contents of the <code>labels</code> file and the <code>annotations</code> file:</p><pre tabindex="0"><code>cluster="test-cluster1"
rack="rack-22"
zone="us-est-coast"

build="two"
builder="john-doe"
</code></pre><p>Get a shell into the container that is running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it kubernetes-downwardapi-volume-example -- sh
</span></span></code></pre></div><p>In your shell, view the <code>labels</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/# cat /etc/podinfo/labels
</span></span></code></pre></div><p>The output shows that all of the Pod's labels have been written
to the <code>labels</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>cluster</span><span>=</span><span>"test-cluster1"</span>
</span></span><span><span><span>rack</span><span>=</span><span>"rack-22"</span>
</span></span><span><span><span>zone</span><span>=</span><span>"us-est-coast"</span>
</span></span></code></pre></div><p>Similarly, view the <code>annotations</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/# cat /etc/podinfo/annotations
</span></span></code></pre></div><p>View the files in the <code>/etc/podinfo</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/# ls -laR /etc/podinfo
</span></span></code></pre></div><p>In the output, you can see that the <code>labels</code> and <code>annotations</code> files
are in a temporary subdirectory: in this example,
<code>..2982_06_02_21_47_53.299460680</code>. In the <code>/etc/podinfo</code> directory, <code>..data</code> is
a symbolic link to the temporary subdirectory. Also in the <code>/etc/podinfo</code> directory,
<code>labels</code> and <code>annotations</code> are symbolic links.</p><pre tabindex="0"><code>drwxr-xr-x  ... Feb 6 21:47 ..2982_06_02_21_47_53.299460680
lrwxrwxrwx  ... Feb 6 21:47 ..data -&gt; ..2982_06_02_21_47_53.299460680
lrwxrwxrwx  ... Feb 6 21:47 annotations -&gt; ..data/annotations
lrwxrwxrwx  ... Feb 6 21:47 labels -&gt; ..data/labels

/etc/..2982_06_02_21_47_53.299460680:
total 8
-rw-r--r--  ... Feb  6 21:47 annotations
-rw-r--r--  ... Feb  6 21:47 labels
</code></pre><p>Using symbolic links enables dynamic atomic refresh of the metadata; updates are
written to a new temporary directory, and the <code>..data</code> symlink is updated
atomically using <a href="http://man7.org/linux/man-pages/man2/rename.2.html">rename(2)</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container using Downward API as a
<a href="/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not
receive Downward API updates.</div><p>Exit the shell:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/# <span>exit</span>
</span></span></code></pre></div><h2 id="store-container-fields">Store container fields</h2><p>The preceding exercise, you made Pod-level fields accessible using the
downward API.
In this next exercise, you are going to pass fields that are part of the Pod
definition, but taken from the specific
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">container</a>
rather than from the Pod overall. Here is a manifest for a Pod that again has
just one container:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/dapi-volume-resources.yaml"><code>pods/inject/dapi-volume-resources.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/dapi-volume-resources.yaml to clipboard"></div><div class="includecode" id="pods-inject-dapi-volume-resources-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kubernetes-downwardapi-volume-example-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>client-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox:1.27.2<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"sh"</span>,<span> </span><span>"-c"</span>]<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span>
</span></span></span><span><span><span>      </span>- while true; do<span>
</span></span></span><span><span><span>          </span>echo -en '\n';<span>
</span></span></span><span><span><span>          </span>if [[ -e /etc/podinfo/cpu_limit ]]; then<span>
</span></span></span><span><span><span>            </span>echo -en '\n'; cat /etc/podinfo/cpu_limit; fi;<span>
</span></span></span><span><span><span>          </span>if [[ -e /etc/podinfo/cpu_request ]]; then<span>
</span></span></span><span><span><span>            </span>echo -en '\n'; cat /etc/podinfo/cpu_request; fi;<span>
</span></span></span><span><span><span>          </span>if [[ -e /etc/podinfo/mem_limit ]]; then<span>
</span></span></span><span><span><span>            </span>echo -en '\n'; cat /etc/podinfo/mem_limit; fi;<span>
</span></span></span><span><span><span>          </span>if [[ -e /etc/podinfo/mem_request ]]; then<span>
</span></span></span><span><span><span>            </span>echo -en '\n'; cat /etc/podinfo/mem_request; fi;<span>
</span></span></span><span><span><span>          </span>sleep 5;<span>
</span></span></span><span><span><span>        </span>done;<span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>requests</span>:<span>
</span></span></span><span><span><span>          </span><span>memory</span>:<span> </span><span>"32Mi"</span><span>
</span></span></span><span><span><span>          </span><span>cpu</span>:<span> </span><span>"125m"</span><span>
</span></span></span><span><span><span>        </span><span>limits</span>:<span>
</span></span></span><span><span><span>          </span><span>memory</span>:<span> </span><span>"64Mi"</span><span>
</span></span></span><span><span><span>          </span><span>cpu</span>:<span> </span><span>"250m"</span><span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>podinfo<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/podinfo<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>podinfo<span>
</span></span></span><span><span><span>      </span><span>downwardAPI</span>:<span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"cpu_limit"</span><span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>client-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>limits.cpu<span>
</span></span></span><span><span><span>              </span><span>divisor</span>:<span> </span>1m<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"cpu_request"</span><span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>client-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>requests.cpu<span>
</span></span></span><span><span><span>              </span><span>divisor</span>:<span> </span>1m<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"mem_limit"</span><span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>client-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>limits.memory<span>
</span></span></span><span><span><span>              </span><span>divisor</span>:<span> </span>1Mi<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"mem_request"</span><span>
</span></span></span><span><span><span>            </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>containerName</span>:<span> </span>client-container<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span> </span>requests.memory<span>
</span></span></span><span><span><span>              </span><span>divisor</span>:<span> </span>1Mi<span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>In the manifest, you can see that the Pod has a
<a href="/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code> volume</a>,
and that the single container in that Pod mounts the volume at <code>/etc/podinfo</code>.</p><p>Look at the <code>items</code> array under <code>downwardAPI</code>. Each element of the array
defines a file in the downward API volume.</p><p>The first element specifies that in the container named <code>client-container</code>,
the value of the <code>limits.cpu</code> field in the format specified by <code>1m</code> should be
published as a file named <code>cpu_limit</code>. The <code>divisor</code> field is optional and has the
default value of <code>1</code>. A divisor of 1 means cores for <code>cpu</code> resources, or
bytes for <code>memory</code> resources.</p><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume-resources.yaml
</span></span></code></pre></div><p>Get a shell into the container that is running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it kubernetes-downwardapi-volume-example-2 -- sh
</span></span></code></pre></div><p>In your shell, view the <code>cpu_limit</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this in a shell inside the container</span>
</span></span><span><span>cat /etc/podinfo/cpu_limit
</span></span></code></pre></div><p>You can use similar commands to view the <code>cpu_request</code>, <code>mem_limit</code> and
<code>mem_request</code> files.</p><h2 id="project-keys-to-specific-paths-and-file-permissions">Project keys to specific paths and file permissions</h2><p>You can project keys to specific paths and specific permissions on a per-file
basis. For more information, see
<a href="/docs/concepts/configuration/secret/">Secrets</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec"><code>spec</code></a>
API definition for Pod. This includes the definition of Container (part of Pod).</li><li>Read the list of <a href="/docs/concepts/workloads/pods/downward-api/#available-fields">available fields</a> that you
can expose using the downward API.</li></ul><p>Read about volumes in the legacy API reference:</p><ul><li>Check the <a href="/docs/reference/generated/kubernetes-api/v1.34/#volume-v1-core"><code>Volume</code></a>
API definition which defines a generic volume in a Pod for containers to access.</li><li>Check the <a href="/docs/reference/generated/kubernetes-api/v1.34/#downwardapivolumesource-v1-core"><code>DownwardAPIVolumeSource</code></a>
API definition which defines a volume that contains Downward API information.</li><li>Check the <a href="/docs/reference/generated/kubernetes-api/v1.34/#downwardapivolumefile-v1-core"><code>DownwardAPIVolumeFile</code></a>
API definition which contains references to object or resource fields for
populating a file in the Downward API volume.</li><li>Check the <a href="/docs/reference/generated/kubernetes-api/v1.34/#resourcefieldselector-v1-core"><code>ResourceFieldSelector</code></a>
API definition which specifies the container resources and their output format.</li></ul></div></div><div><div class="td-content"><h1>Distribute Credentials Securely Using Secrets</h1><p>This page shows how to securely inject sensitive data, such as passwords and
encryption keys, into Pods.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h3 id="convert-your-secret-data-to-a-base-64-representation">Convert your secret data to a base-64 representation</h3><p>Suppose you want to have two pieces of secret data: a username <code>my-app</code> and a password
<code>39528$vdg7Jb</code>. First, use a base64 encoding tool to convert your username and password to a base64 representation. Here's an example using the commonly available base64 program:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> -n <span>'my-app'</span> | base64
</span></span><span><span><span>echo</span> -n <span>'39528$vdg7Jb'</span> | base64
</span></span></code></pre></div><p>The output shows that the base-64 representation of your username is <code>bXktYXBw</code>,
and the base-64 representation of your password is <code>Mzk1MjgkdmRnN0pi</code>.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Use a local tool trusted by your OS to decrease the security risks of external tools.</div><h2 id="create-a-secret">Create a Secret</h2><p>Here is a configuration file you can use to create a Secret that holds your
username and password:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/secret.yaml"><code>pods/inject/secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/secret.yaml to clipboard"></div><div class="includecode" id="pods-inject-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-secret<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>bXktYXBw<span>
</span></span></span><span><span><span>  </span><span>password</span>:<span> </span>Mzk1MjgkdmRnN0pi<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the Secret</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/secret.yaml
</span></span></code></pre></div></li><li><p>View information about the Secret:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret test-secret
</span></span></code></pre></div><p>Output:</p><pre tabindex="0"><code>NAME          TYPE      DATA      AGE
test-secret   Opaque    2         1m
</code></pre></li><li><p>View more detailed information about the Secret:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe secret test-secret
</span></span></code></pre></div><p>Output:</p><pre tabindex="0"><code>Name:       test-secret
Namespace:  default
Labels:     &lt;none&gt;
Annotations:    &lt;none&gt;

Type:   Opaque

Data
====
password:   13 bytes
username:   7 bytes
</code></pre></li></ol><h3 id="create-a-secret-directly-with-kubectl">Create a Secret directly with kubectl</h3><p>If you want to skip the Base64 encoding step, you can create the
same Secret using the <code>kubectl create secret</code> command. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic test-secret --from-literal<span>=</span><span>'username=my-app'</span> --from-literal<span>=</span><span>'password=39528$vdg7Jb'</span>
</span></span></code></pre></div><p>This is more convenient. The detailed approach shown earlier runs
through each step explicitly to demonstrate what is happening.</p><h2 id="create-a-pod-that-has-access-to-the-secret-data-through-a-volume">Create a Pod that has access to the secret data through a Volume</h2><p>Here is a configuration file you can use to create a Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/secret-pod.yaml"><code>pods/inject/secret-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/secret-pod.yaml to clipboard"></div><div class="includecode" id="pods-inject-secret-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span><span># name must match the volume name below</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>secret-volume<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/secret-volume<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span># The secret data is exposed to Containers in the Pod through a Volume.</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>secret-volume<span>
</span></span></span><span><span><span>      </span><span>secret</span>:<span>
</span></span></span><span><span><span>        </span><span>secretName</span>:<span> </span>test-secret<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/inject/secret-pod.yaml
</span></span></code></pre></div></li><li><p>Verify that your Pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod secret-test-pod
</span></span></code></pre></div><p>Output:</p><pre tabindex="0"><code>NAME              READY     STATUS    RESTARTS   AGE
secret-test-pod   1/1       Running   0          42m
</code></pre></li><li><p>Get a shell into the Container that is running in your Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t secret-test-pod -- /bin/bash
</span></span></code></pre></div></li><li><p>The secret data is exposed to the Container through a Volume mounted under
<code>/etc/secret-volume</code>.</p><p>In your shell, list the files in the <code>/etc/secret-volume</code> directory:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this in the shell inside the container</span>
</span></span><span><span>ls /etc/secret-volume
</span></span></code></pre></div><p>The output shows two files, one for each piece of secret data:</p><pre tabindex="0"><code>password username
</code></pre></li><li><p>In your shell, display the contents of the <code>username</code> and <code>password</code> files:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this in the shell inside the container</span>
</span></span><span><span><span>echo</span> <span>"</span><span>$(</span> cat /etc/secret-volume/username <span>)</span><span>"</span>
</span></span><span><span><span>echo</span> <span>"</span><span>$(</span> cat /etc/secret-volume/password <span>)</span><span>"</span>
</span></span></code></pre></div><p>The output is your username and password:</p><pre tabindex="0"><code>my-app
39528$vdg7Jb
</code></pre></li></ol><p>Modify your image or command line so that the program looks for files in the
<code>mountPath</code> directory. Each key in the Secret <code>data</code> map becomes a file name
in this directory.</p><h3 id="project-secret-keys-to-specific-file-paths">Project Secret keys to specific file paths</h3><p>You can also control the paths within the volume where Secret keys are projected. Use the
<code>.spec.volumes[].secret.items</code> field to change the target path of each key:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/etc/foo"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>    </span><span>secret</span>:<span>
</span></span></span><span><span><span>      </span><span>secretName</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>username<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>my-group/my-username<span>
</span></span></span></code></pre></div><p>When you deploy this Pod, the following happens:</p><ul><li>The <code>username</code> key from <code>mysecret</code> is available to the container at the path
<code>/etc/foo/my-group/my-username</code> instead of at <code>/etc/foo/username</code>.</li><li>The <code>password</code> key from that Secret object is not projected.</li></ul><p>If you list keys explicitly using <code>.spec.volumes[].secret.items</code>, consider the
following:</p><ul><li>Only keys specified in <code>items</code> are projected.</li><li>To consume all keys from the Secret, all of them must be listed in the
<code>items</code> field.</li><li>All listed keys must exist in the corresponding Secret. Otherwise, the volume
is not created.</li></ul><h3 id="set-posix-permissions-for-secret-keys">Set POSIX permissions for Secret keys</h3><p>You can set the POSIX file access permission bits for a single Secret key.
If you don't specify any permissions, <code>0644</code> is used by default.
You can also set a default POSIX file mode for the entire Secret volume, and
you can override per key if needed.</p><p>For example, you can specify a default mode like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/etc/foo"</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>    </span><span>secret</span>:<span>
</span></span></span><span><span><span>      </span><span>secretName</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>      </span><span>defaultMode</span>:<span> </span><span>0400</span><span>
</span></span></span></code></pre></div><p>The Secret is mounted on <code>/etc/foo</code>; all the files created by the
secret volume mount have permission <code>0400</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you're defining a Pod or a Pod template using JSON, beware that the JSON
specification doesn't support octal literals for numbers because JSON considers
<code>0400</code> to be the <em>decimal</em> value <code>400</code>. In JSON, use decimal values for the
<code>defaultMode</code> instead. If you're writing YAML, you can write the <code>defaultMode</code>
in octal.</div><h2 id="define-container-environment-variables-using-secret-data">Define container environment variables using Secret data</h2><p>You can consume the data in Secrets as environment variables in your
containers.</p><p>If a container already consumes a Secret in an environment variable,
a Secret update will not be seen by the container unless it is
restarted. There are third party solutions for triggering restarts when
secrets change.</p><h3 id="define-a-container-environment-variable-with-data-from-a-single-secret">Define a container environment variable with data from a single Secret</h3><ul><li><p>Define an environment variable as a key-value pair in a Secret:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic backend-user --from-literal<span>=</span>backend-username<span>=</span><span>'backend-admin'</span>
</span></span></code></pre></div></li><li><p>Assign the <code>backend-username</code> value defined in the Secret to the <code>SECRET_USERNAME</code> environment variable in the Pod specification.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/pod-single-secret-env-variable.yaml"><code>pods/inject/pod-single-secret-env-variable.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/pod-single-secret-env-variable.yaml to clipboard"></div><div class="includecode" id="pods-inject-pod-single-secret-env-variable-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>env-single-secret<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>envars-test-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>SECRET_USERNAME<span>
</span></span></span><span><span><span>      </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>        </span><span>secretKeyRef</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>backend-user<span>
</span></span></span><span><span><span>          </span><span>key</span>:<span> </span>backend-username<span>
</span></span></span></code></pre></div></div></div></li><li><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/pods/inject/pod-single-secret-env-variable.yaml
</span></span></code></pre></div></li><li><p>In your shell, display the content of <code>SECRET_USERNAME</code> container environment variable.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t env-single-secret -- /bin/sh -c <span>'echo $SECRET_USERNAME'</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>backend-admin
</code></pre></li></ul><h3 id="define-container-environment-variables-with-data-from-multiple-secrets">Define container environment variables with data from multiple Secrets</h3><ul><li><p>As with the previous example, create the Secrets first.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic backend-user --from-literal<span>=</span>backend-username<span>=</span><span>'backend-admin'</span>
</span></span><span><span>kubectl create secret generic db-user --from-literal<span>=</span>db-username<span>=</span><span>'db-admin'</span>
</span></span></code></pre></div></li><li><p>Define the environment variables in the Pod specification.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/pod-multiple-secret-env-variable.yaml"><code>pods/inject/pod-multiple-secret-env-variable.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/pod-multiple-secret-env-variable.yaml to clipboard"></div><div class="includecode" id="pods-inject-pod-multiple-secret-env-variable-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>envvars-multiple-secrets<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>envars-test-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>BACKEND_USERNAME<span>
</span></span></span><span><span><span>      </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>        </span><span>secretKeyRef</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>backend-user<span>
</span></span></span><span><span><span>          </span><span>key</span>:<span> </span>backend-username<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>DB_USERNAME<span>
</span></span></span><span><span><span>      </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>        </span><span>secretKeyRef</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>db-user<span>
</span></span></span><span><span><span>          </span><span>key</span>:<span> </span>db-username<span>
</span></span></span></code></pre></div></div></div></li><li><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/pods/inject/pod-multiple-secret-env-variable.yaml
</span></span></code></pre></div></li><li><p>In your shell, display the container environment variables.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t envvars-multiple-secrets -- /bin/sh -c <span>'env | grep _USERNAME'</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>DB_USERNAME=db-admin
BACKEND_USERNAME=backend-admin
</code></pre></li></ul><h2 id="configure-all-key-value-pairs-in-a-secret-as-container-environment-variables">Configure all key-value pairs in a Secret as container environment variables</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This functionality is available in Kubernetes v1.6 and later.</div><ul><li><p>Create a Secret containing multiple key-value pairs</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic test-secret --from-literal<span>=</span><span>username</span><span>=</span><span>'my-app'</span> --from-literal<span>=</span><span>password</span><span>=</span><span>'39528$vdg7Jb'</span>
</span></span></code></pre></div></li><li><p>Use envFrom to define all of the Secret's data as container environment variables.
The key from the Secret becomes the environment variable name in the Pod.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/inject/pod-secret-envFrom.yaml"><code>pods/inject/pod-secret-envFrom.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/inject/pod-secret-envFrom.yaml to clipboard"></div><div class="includecode" id="pods-inject-pod-secret-envfrom-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>envfrom-secret<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>envars-test-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>envFrom</span>:<span>
</span></span></span><span><span><span>    </span>- <span>secretRef</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>test-secret<span>
</span></span></span></code></pre></div></div></div></li><li><p>Create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/pods/inject/pod-secret-envFrom.yaml
</span></span></code></pre></div></li><li><p>In your shell, display <code>username</code> and <code>password</code> container environment variables.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -i -t envfrom-secret -- /bin/sh -c <span>'echo "username: $username\npassword: $password\n"'</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>username: my-app
password: 39528$vdg7Jb
</code></pre></li></ul><h2 id="provide-prod-test-creds">Example: Provide prod/test credentials to Pods using Secrets</h2><p>This example illustrates a Pod which consumes a secret containing production credentials and
another Pod which consumes a secret with test environment credentials.</p><ol><li><p>Create a secret for prod environment credentials:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic prod-db-secret --from-literal<span>=</span><span>username</span><span>=</span>produser --from-literal<span>=</span><span>password</span><span>=</span>Y4nys7f11
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>secret "prod-db-secret" created
</code></pre></li><li><p>Create a secret for test environment credentials.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic test-db-secret --from-literal<span>=</span><span>username</span><span>=</span>testuser --from-literal<span>=</span><span>password</span><span>=</span>iluvtests
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>secret "test-db-secret" created
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Special characters such as <code>$</code>, <code>\</code>, <code>*</code>, <code>=</code>, and <code>!</code> will be interpreted by your
<a href="https://en.wikipedia.org/wiki/Shell_(computing)">shell</a> and require escaping.</p><p>In most shells, the easiest way to escape the password is to surround it with single quotes (<code>'</code>).
For example, if your actual password is <code>S!B\*d$zDsb=</code>, you should execute the command as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic dev-db-secret --from-literal<span>=</span><span>username</span><span>=</span>devuser --from-literal<span>=</span><span>password</span><span>=</span><span>'S!B\*d$zDsb='</span>
</span></span></code></pre></div><p>You do not need to escape special characters in passwords from files (<code>--from-file</code>).</p></div></li><li><p>Create the Pod manifests:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: List
</span></span></span><span><span><span>items:
</span></span></span><span><span><span>- kind: Pod
</span></span></span><span><span><span>  apiVersion: v1
</span></span></span><span><span><span>  metadata:
</span></span></span><span><span><span>    name: prod-db-client-pod
</span></span></span><span><span><span>    labels:
</span></span></span><span><span><span>      name: prod-db-client
</span></span></span><span><span><span>  spec:
</span></span></span><span><span><span>    volumes:
</span></span></span><span><span><span>    - name: secret-volume
</span></span></span><span><span><span>      secret:
</span></span></span><span><span><span>        secretName: prod-db-secret
</span></span></span><span><span><span>    containers:
</span></span></span><span><span><span>    - name: db-client-container
</span></span></span><span><span><span>      image: myClientImage
</span></span></span><span><span><span>      volumeMounts:
</span></span></span><span><span><span>      - name: secret-volume
</span></span></span><span><span><span>        readOnly: true
</span></span></span><span><span><span>        mountPath: "/etc/secret-volume"
</span></span></span><span><span><span>- kind: Pod
</span></span></span><span><span><span>  apiVersion: v1
</span></span></span><span><span><span>  metadata:
</span></span></span><span><span><span>    name: test-db-client-pod
</span></span></span><span><span><span>    labels:
</span></span></span><span><span><span>      name: test-db-client
</span></span></span><span><span><span>  spec:
</span></span></span><span><span><span>    volumes:
</span></span></span><span><span><span>    - name: secret-volume
</span></span></span><span><span><span>      secret:
</span></span></span><span><span><span>        secretName: test-db-secret
</span></span></span><span><span><span>    containers:
</span></span></span><span><span><span>    - name: db-client-container
</span></span></span><span><span><span>      image: myClientImage
</span></span></span><span><span><span>      volumeMounts:
</span></span></span><span><span><span>      - name: secret-volume
</span></span></span><span><span><span>        readOnly: true
</span></span></span><span><span><span>        mountPath: "/etc/secret-volume"
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>How the specs for the two Pods differ only in one field; this facilitates creating Pods
with different capabilities from a common Pod template.</div></li><li><p>Apply all those objects on the API server by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f pod.yaml
</span></span></code></pre></div></li></ol><p>Both containers will have the following files present on their filesystems with the values
for each container's environment:</p><pre tabindex="0"><code>/etc/secret-volume/username
/etc/secret-volume/password
</code></pre><p>You could further simplify the base Pod specification by using two service accounts:</p><ol><li><code>prod-user</code> with the <code>prod-db-secret</code></li><li><code>test-user</code> with the <code>test-db-secret</code></li></ol><p>The Pod specification is shortened to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>prod-db-client-pod<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>prod-db-client<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>serviceAccount</span>:<span> </span>prod-db-client<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>db-client-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>myClientImage<span>
</span></span></span></code></pre></div><h3 id="references">References</h3><ul><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#secret-v1-core">Secret</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#volume-v1-core">Volume</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#pod-v1-core">Pod</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/configuration/secret/">Secrets</a>.</li><li>Learn about <a href="/docs/concepts/storage/volumes/">Volumes</a>.</li></ul></div></div><div><div class="td-content"><h1>Run Applications</h1><div class="lead">Run and manage both stateless and stateful applications.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a Stateless Application Using a Deployment</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/run-single-instance-stateful-application/">Run a Single-Instance Stateful Application</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/run-replicated-stateful-application/">Run a Replicated Stateful Application</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/delete-stateful-set/">Delete a StatefulSet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">Force Delete StatefulSet Pods</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaling</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">HorizontalPodAutoscaler Walkthrough</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/configure-pdb/">Specifying a Disruption Budget for your Application</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/run-application/access-api-from-pod/">Accessing the Kubernetes API from a Pod</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Run a Stateless Application Using a Deployment</h1><p>This page shows how to run an application using a Kubernetes Deployment object.</p><h2 id="objectives">Objectives</h2><ul><li>Create an nginx deployment.</li><li>Use kubectl to list information about the deployment.</li><li>Update the deployment.</li></ul><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.9.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="creating-and-exploring-an-nginx-deployment">Creating and exploring an nginx deployment</h2><p>You can run an application by creating a Kubernetes Deployment object, and you
can describe a Deployment in a YAML file. For example, this YAML file describes
a Deployment that runs the nginx:1.14.2 Docker image:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment.yaml"><code>application/deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment.yaml to clipboard"></div><div class="includecode" id="application-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># tells deployment to run 2 pods matching the template</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><ol><li><p>Create a Deployment based on the YAML file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</span></span></code></pre></div></li><li><p>Display information about the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:     nginx-deployment
Namespace:    default
CreationTimestamp:  Tue, 30 Aug 2016 18:11:37 -0700
Labels:     app=nginx
Annotations:    deployment.kubernetes.io/revision=1
Selector:   app=nginx
Replicas:   2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:   RollingUpdate
MinReadySeconds:  0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:       app=nginx
  Containers:
    nginx:
    Image:              nginx:1.14.2
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
OldReplicaSets:   &lt;none&gt;
NewReplicaSet:    nginx-deployment-1771418926 (2/2 replicas created)
No events.
</code></pre></li><li><p>List the Pods created by the deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1771418926-7o5ns   1/1       Running   0          16h
nginx-deployment-1771418926-r18az   1/1       Running   0          16h
</code></pre></li><li><p>Display information about a Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod &lt;pod-name&gt;
</span></span></code></pre></div><p>where <code>&lt;pod-name&gt;</code> is the name of one of your Pods.</p></li></ol><h2 id="updating-the-deployment">Updating the deployment</h2><p>You can update the deployment by applying a new YAML file. This YAML file
specifies that the deployment should be updated to use nginx 1.16.1.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment-update.yaml"><code>application/deployment-update.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment-update.yaml to clipboard"></div><div class="includecode" id="application-deployment-update-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.16.1<span> </span><span># Update the version of nginx from 1.14.2 to 1.16.1</span><span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><ol><li><p>Apply the new YAML file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment-update.yaml
</span></span></code></pre></div></li><li><p>Watch the deployment create pods with new names and delete the old pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div></li></ol><h2 id="scaling-the-application-by-increasing-the-replica-count">Scaling the application by increasing the replica count</h2><p>You can increase the number of Pods in your Deployment by applying a new YAML
file. This YAML file sets <code>replicas</code> to 4, which specifies that the Deployment
should have four Pods:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment-scale.yaml"><code>application/deployment-scale.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment-scale.yaml to clipboard"></div><div class="includecode" id="application-deployment-scale-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>4</span><span> </span><span># Update the replicas from 2 to 4</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.16.1<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><ol><li><p>Apply the new YAML file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment-scale.yaml
</span></span></code></pre></div></li><li><p>Verify that the Deployment has four Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                               READY     STATUS    RESTARTS   AGE
nginx-deployment-148880595-4zdqq   1/1       Running   0          25s
nginx-deployment-148880595-6zgi1   1/1       Running   0          25s
nginx-deployment-148880595-fxcez   1/1       Running   0          2m
nginx-deployment-148880595-rwovn   1/1       Running   0          2m
</code></pre></li></ol><h2 id="deleting-a-deployment">Deleting a deployment</h2><p>Delete the deployment by name:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment nginx-deployment
</span></span></code></pre></div><h2 id="replicationcontrollers-the-old-way">ReplicationControllers -- the Old Way</h2><p>The preferred way to create a replicated application is to use a Deployment,
which in turn uses a ReplicaSet. Before the Deployment and ReplicaSet were
added to Kubernetes, replicated applications were configured using a
<a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/workloads/controllers/deployment/">Deployment objects</a>.</li></ul></div></div><div><div class="td-content"><h1>Run a Single-Instance Stateful Application</h1><p>This page shows you how to run a single-instance stateful application
in Kubernetes using a PersistentVolume and a Deployment. The
application is MySQL.</p><h2 id="objectives">Objectives</h2><ul><li>Create a PersistentVolume referencing a disk in your environment.</li><li>Create a MySQL Deployment.</li><li>Expose MySQL to other pods in the cluster at a known DNS name.</li></ul><h2 id="before-you-begin">Before you begin</h2><ul><li><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p></li><li><p>You need to either have a <a href="/docs/concepts/storage/dynamic-provisioning/">dynamic PersistentVolume provisioner</a> with a default
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>,
or <a href="/docs/concepts/storage/persistent-volumes/#provisioning">statically provision PersistentVolumes</a>
yourself to satisfy the <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a>
used here.</p></li></ul><h2 id="deploy-mysql">Deploy MySQL</h2><p>You can run a stateful application by creating a Kubernetes Deployment
and connecting it to an existing PersistentVolume using a
PersistentVolumeClaim. For example, this YAML file describes a
Deployment that runs MySQL and references the PersistentVolumeClaim. The file
defines a volume mount for /var/lib/mysql, and then creates a
PersistentVolumeClaim that looks for a 20G volume. This claim is
satisfied by any existing volume that meets the requirements,
or by a dynamic provisioner.</p><p>Note: The password is defined in the config yaml, and this is insecure. See
<a href="/docs/concepts/configuration/secret/">Kubernetes Secrets</a>
for a secure solution.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-deployment.yaml"><code>application/mysql/mysql-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/mysql/mysql-deployment.yaml to clipboard"></div><div class="includecode" id="application-mysql-mysql-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>3306</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span>None<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>strategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Recreate<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>image</span>:<span> </span>mysql:9<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span><span>env</span>:<span>
</span></span></span><span><span><span>          </span><span># Use secret in real usage</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MYSQL_ROOT_PASSWORD<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span>password<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>3306</span><span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>mysql-persistent-storage<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/lib/mysql<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>mysql-persistent-storage<span>
</span></span></span><span><span><span>        </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>          </span><span>claimName</span>:<span> </span>mysql-pv-claim<span>
</span></span></span></code></pre></div></div></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-pv.yaml"><code>application/mysql/mysql-pv.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/mysql/mysql-pv.yaml to clipboard"></div><div class="includecode" id="application-mysql-mysql-pv-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql-pv-volume<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>local<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>manual<span>
</span></span></span><span><span><span>  </span><span>capacity</span>:<span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span>20Gi<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span><span>"/mnt/data"</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql-pv-claim<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>manual<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>20Gi<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Deploy the PV and PVC of the YAML file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mysql/mysql-pv.yaml
</span></span></code></pre></div></li><li><p>Deploy the contents of the YAML file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mysql/mysql-deployment.yaml
</span></span></code></pre></div></li><li><p>Display information about the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployment mysql
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                 mysql
Namespace:            default
CreationTimestamp:    Tue, 01 Nov 2016 11:18:45 -0700
Labels:               app=mysql
Annotations:          deployment.kubernetes.io/revision=1
Selector:             app=mysql
Replicas:             1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:         Recreate
MinReadySeconds:      0
Pod Template:
  Labels:       app=mysql
  Containers:
    mysql:
    Image:      mysql:9
    Port:       3306/TCP
    Environment:
      MYSQL_ROOT_PASSWORD:      password
    Mounts:
      /var/lib/mysql from mysql-persistent-storage (rw)
  Volumes:
    mysql-persistent-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mysql-pv-claim
    ReadOnly:   false
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     False   MinimumReplicasUnavailable
  Progressing   True    ReplicaSetUpdated
OldReplicaSets:       &lt;none&gt;
NewReplicaSet:        mysql-63082529 (1/1 replicas created)
Events:
  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----                -------------    --------    ------            -------
  33s          33s         1        {deployment-controller }             Normal      ScalingReplicaSet Scaled up replica set mysql-63082529 to 1
</code></pre></li><li><p>List the pods created by the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>mysql
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                   READY     STATUS    RESTARTS   AGE
mysql-63082529-2z3ki   1/1       Running   0          3m
</code></pre></li><li><p>Inspect the PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pvc mysql-pv-claim
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:         mysql-pv-claim
Namespace:    default
StorageClass:
Status:       Bound
Volume:       mysql-pv-volume
Labels:       &lt;none&gt;
Annotations:    pv.kubernetes.io/bind-completed=yes
                pv.kubernetes.io/bound-by-controller=yes
Capacity:     20Gi
Access Modes: RWO
Events:       &lt;none&gt;
</code></pre></li></ol><h2 id="accessing-the-mysql-instance">Accessing the MySQL instance</h2><p>The preceding YAML file creates a service that
allows other Pods in the cluster to access the database. The Service option
<code>clusterIP: None</code> lets the Service DNS name resolve directly to the
Pod's IP address. This is optimal when you have only one Pod
behind a Service and you don't intend to increase the number of Pods.</p><p>Run a MySQL client to connect to the server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run -it --rm --image<span>=</span>mysql:9 --restart<span>=</span>Never mysql-client -- mysql -h mysql -ppassword
</span></span></code></pre></div><p>This command creates a new Pod in the cluster running a MySQL client
and connects it to the server through the Service. If it connects, you
know your stateful MySQL database is up and running.</p><pre tabindex="0"><code>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false
If you don't see a command prompt, try pressing enter.

mysql&gt;
</code></pre><h2 id="updating">Updating</h2><p>The image or any other part of the Deployment can be updated as usual
with the <code>kubectl apply</code> command. Here are some precautions that are
specific to stateful apps:</p><ul><li>Don't scale the app. This setup is for single-instance apps
only. The underlying PersistentVolume can only be mounted to one
Pod. For clustered stateful apps, see the
<a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet documentation</a>.</li><li>Use <code>strategy:</code> <code>type: Recreate</code> in the Deployment configuration
YAML file. This instructs Kubernetes to <em>not</em> use rolling
updates. Rolling updates will not work, as you cannot have more than
one Pod running at a time. The <code>Recreate</code> strategy will stop the
first pod before creating a new one with the updated configuration.</li></ul><h2 id="deleting-a-deployment">Deleting a deployment</h2><p>Delete the deployed objects by name:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment,svc mysql
</span></span><span><span>kubectl delete pvc mysql-pv-claim
</span></span><span><span>kubectl delete pv mysql-pv-volume
</span></span></code></pre></div><p>If you manually provisioned a PersistentVolume, you also need to manually
delete it, as well as release the underlying resource.
If you used a dynamic provisioner, it automatically deletes the
PersistentVolume when it sees that you deleted the PersistentVolumeClaim.
Some dynamic provisioners (such as those for EBS and PD) also release the
underlying resource upon deleting the PersistentVolume.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Learn more about <a href="/docs/concepts/workloads/controllers/deployment/">Deployment objects</a>.</p></li><li><p>Learn more about <a href="/docs/tasks/run-application/run-stateless-application-deployment/">Deploying applications</a></p></li><li><p><a href="/docs/reference/generated/kubectl/kubectl-commands/#run">kubectl run documentation</a></p></li><li><p><a href="/docs/concepts/storage/volumes/">Volumes</a> and <a href="/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a></p></li></ul></div></div><div><div class="td-content"><h1>Run a Replicated Stateful Application</h1><p>This page shows how to run a replicated stateful application using a
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>.
This application is a replicated MySQL database. The example topology has a
single primary server and multiple replicas, using asynchronous row-based
replication.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><strong>This is not a production configuration</strong>. MySQL settings remain on insecure defaults to keep the focus
on general patterns for running stateful applications in Kubernetes.</div><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>You need to either have a <a href="/docs/concepts/storage/dynamic-provisioning/">dynamic PersistentVolume provisioner</a> with a default
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>,
or <a href="/docs/concepts/storage/persistent-volumes/#provisioning">statically provision PersistentVolumes</a>
yourself to satisfy the <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a>
used here.</p></li><li>This tutorial assumes you are familiar with
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>
and <a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>,
as well as other core concepts like <a href="/docs/concepts/workloads/pods/">Pods</a>,
<a href="/docs/concepts/services-networking/service/">Services</a>, and
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a>.</li><li>Some familiarity with MySQL helps, but this tutorial aims to present
general patterns that should be useful for other systems.</li><li>You are using the default namespace or another namespace that does not contain any conflicting objects.</li><li>You need to have a AMD64-compatible CPU.</li></ul><h2 id="objectives">Objectives</h2><ul><li>Deploy a replicated MySQL topology with a StatefulSet.</li><li>Send MySQL client traffic.</li><li>Observe resistance to downtime.</li><li>Scale the StatefulSet up and down.</li></ul><h2 id="deploy-mysql">Deploy MySQL</h2><p>The example MySQL deployment consists of a ConfigMap, two Services,
and a StatefulSet.</p><h3 id="configmap">Create a ConfigMap</h3><p>Create the ConfigMap from the following YAML configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-configmap.yaml"><code>application/mysql/mysql-configmap.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/mysql/mysql-configmap.yaml to clipboard"></div><div class="includecode" id="application-mysql-mysql-configmap-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>primary.cnf</span>:<span> </span>|<span>
</span></span></span><span><span><span>    # Apply this config only on the primary.
</span></span></span><span><span><span>    [mysqld]
</span></span></span><span><span><span>    log-bin</span><span>    
</span></span></span><span><span><span>  </span><span>replica.cnf</span>:<span> </span>|<span>
</span></span></span><span><span><span>    # Apply this config only on replicas.
</span></span></span><span><span><span>    [mysqld]
</span></span></span><span><span><span>    super-read-only</span><span>    
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml
</span></span></code></pre></div><p>This ConfigMap provides <code>my.cnf</code> overrides that let you independently control
configuration on the primary MySQL server and its replicas.
In this case, you want the primary server to be able to serve replication logs to replicas
and you want replicas to reject any writes that don't come via replication.</p><p>There's nothing special about the ConfigMap itself that causes different
portions to apply to different Pods.
Each Pod decides which portion to look at as it's initializing,
based on information provided by the StatefulSet controller.</p><h3 id="services">Create Services</h3><p>Create the Services from the following YAML configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-services.yaml"><code>application/mysql/mysql-services.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/mysql/mysql-services.yaml to clipboard"></div><div class="includecode" id="application-mysql-mysql-services-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Headless service for stable DNS entries of StatefulSet members.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>3306</span><span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span># Client service for connecting to any MySQL instance for reads.</span><span>
</span></span></span><span><span><span></span><span># For writes, you must instead connect to the primary: mysql-0.mysql.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql-read<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>readonly</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>3306</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>mysql<span>
</span></span></span></code></pre></div></div></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml
</span></span></code></pre></div><p>The headless Service provides a home for the DNS entries that the StatefulSet
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a> creates for each
Pod that's part of the set.
Because the headless Service is named <code>mysql</code>, the Pods are accessible by
resolving <code>&lt;pod-name&gt;.mysql</code> from within any other Pod in the same Kubernetes
cluster and namespace.</p><p>The client Service, called <code>mysql-read</code>, is a normal Service with its own
cluster IP that distributes connections across all MySQL Pods that report
being Ready. The set of potential endpoints includes the primary MySQL server and all
replicas.</p><p>Note that only read queries can use the load-balanced client Service.
Because there is only one primary MySQL server, clients should connect directly to the
primary MySQL Pod (through its DNS entry within the headless Service) to execute
writes.</p><h3 id="statefulset">Create the StatefulSet</h3><p>Finally, create the StatefulSet from the following YAML configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/mysql/mysql-statefulset.yaml"><code>application/mysql/mysql-statefulset.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/mysql/mysql-statefulset.yaml to clipboard"></div><div class="includecode" id="application-mysql-mysql-statefulset-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StatefulSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>      </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>serviceName</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>init-mysql<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>mysql:5.7<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- bash<span>
</span></span></span><span><span><span>        </span>- <span>"-c"</span><span>
</span></span></span><span><span><span>        </span>- |<span>
</span></span></span><span><span><span>          set -ex
</span></span></span><span><span><span>          # Generate mysql server-id from pod ordinal index.
</span></span></span><span><span><span>          [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1
</span></span></span><span><span><span>          ordinal=${BASH_REMATCH[1]}
</span></span></span><span><span><span>          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf
</span></span></span><span><span><span>          # Add an offset to avoid reserved server-id=0 value.
</span></span></span><span><span><span>          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf
</span></span></span><span><span><span>          # Copy appropriate conf.d files from config-map to emptyDir.
</span></span></span><span><span><span>          if [[ $ordinal -eq 0 ]]; then
</span></span></span><span><span><span>            cp /mnt/config-map/primary.cnf /mnt/conf.d/
</span></span></span><span><span><span>          else
</span></span></span><span><span><span>            cp /mnt/config-map/replica.cnf /mnt/conf.d/
</span></span></span><span><span><span>          fi</span><span>          
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>conf<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/mnt/conf.d<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config-map<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/mnt/config-map<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>clone-mysql<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>gcr.io/google-samples/xtrabackup:1.0<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- bash<span>
</span></span></span><span><span><span>        </span>- <span>"-c"</span><span>
</span></span></span><span><span><span>        </span>- |<span>
</span></span></span><span><span><span>          set -ex
</span></span></span><span><span><span>          # Skip the clone if data already exists.
</span></span></span><span><span><span>          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0
</span></span></span><span><span><span>          # Skip the clone on primary (ordinal index 0).
</span></span></span><span><span><span>          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
</span></span></span><span><span><span>          ordinal=${BASH_REMATCH[1]}
</span></span></span><span><span><span>          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0
</span></span></span><span><span><span>          # Clone data from previous peer.
</span></span></span><span><span><span>          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
</span></span></span><span><span><span>          # Prepare the backup.
</span></span></span><span><span><span>          xtrabackup --prepare --target-dir=/var/lib/mysql</span><span>          
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/lib/mysql<span>
</span></span></span><span><span><span>          </span><span>subPath</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>conf<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/mysql/conf.d<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>mysql:5.7<span>
</span></span></span><span><span><span>        </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MYSQL_ALLOW_EMPTY_PASSWORD<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>          </span><span>containerPort</span>:<span> </span><span>3306</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/lib/mysql<span>
</span></span></span><span><span><span>          </span><span>subPath</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>conf<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/mysql/conf.d<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>1Gi<span>
</span></span></span><span><span><span>        </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>          </span><span>exec</span>:<span>
</span></span></span><span><span><span>            </span><span>command</span>:<span> </span>[<span>"mysqladmin"</span>,<span> </span><span>"ping"</span>]<span>
</span></span></span><span><span><span>          </span><span>initialDelaySeconds</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>          </span><span>periodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>          </span><span>timeoutSeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>        </span><span>readinessProbe</span>:<span>
</span></span></span><span><span><span>          </span><span>exec</span>:<span>
</span></span></span><span><span><span>            </span><span># Check we can execute queries over TCP (skip-networking is off).</span><span>
</span></span></span><span><span><span>            </span><span>command</span>:<span> </span>[<span>"mysql"</span>,<span> </span><span>"-h"</span>,<span> </span><span>"127.0.0.1"</span>,<span> </span><span>"-e"</span>,<span> </span><span>"SELECT 1"</span>]<span>
</span></span></span><span><span><span>          </span><span>initialDelaySeconds</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>          </span><span>periodSeconds</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>          </span><span>timeoutSeconds</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>xtrabackup<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>gcr.io/google-samples/xtrabackup:1.0<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>xtrabackup<span>
</span></span></span><span><span><span>          </span><span>containerPort</span>:<span> </span><span>3307</span><span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- bash<span>
</span></span></span><span><span><span>        </span>- <span>"-c"</span><span>
</span></span></span><span><span><span>        </span>- |<span>
</span></span></span><span><span><span>          set -ex
</span></span></span><span><span><span>          cd /var/lib/mysql
</span></span></span><span><span><span>
</span></span></span><span><span><span>          # Determine binlog position of cloned data, if any.
</span></span></span><span><span><span>          if [[ -f xtrabackup_slave_info &amp;&amp; "x$(&lt;xtrabackup_slave_info)" != "x" ]]; then
</span></span></span><span><span><span>            # XtraBackup already generated a partial "CHANGE MASTER TO" query
</span></span></span><span><span><span>            # because we're cloning from an existing replica. (Need to remove the tailing semicolon!)
</span></span></span><span><span><span>            cat xtrabackup_slave_info | sed -E 's/;$//g' &gt; change_master_to.sql.in
</span></span></span><span><span><span>            # Ignore xtrabackup_binlog_info in this case (it's useless).
</span></span></span><span><span><span>            rm -f xtrabackup_slave_info xtrabackup_binlog_info
</span></span></span><span><span><span>          elif [[ -f xtrabackup_binlog_info ]]; then
</span></span></span><span><span><span>            # We're cloning directly from primary. Parse binlog position.
</span></span></span><span><span><span>            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
</span></span></span><span><span><span>            rm -f xtrabackup_binlog_info xtrabackup_slave_info
</span></span></span><span><span><span>            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
</span></span></span><span><span><span>                  MASTER_LOG_POS=${BASH_REMATCH[2]}" &gt; change_master_to.sql.in
</span></span></span><span><span><span>          fi
</span></span></span><span><span><span>
</span></span></span><span><span><span>          # Check if we need to complete a clone by starting replication.
</span></span></span><span><span><span>          if [[ -f change_master_to.sql.in ]]; then
</span></span></span><span><span><span>            echo "Waiting for mysqld to be ready (accepting connections)"
</span></span></span><span><span><span>            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
</span></span></span><span><span><span>
</span></span></span><span><span><span>            echo "Initializing replication from clone position"
</span></span></span><span><span><span>            mysql -h 127.0.0.1 \
</span></span></span><span><span><span>                  -e "$(&lt;change_master_to.sql.in), \
</span></span></span><span><span><span>                          MASTER_HOST='mysql-0.mysql', \
</span></span></span><span><span><span>                          MASTER_USER='root', \
</span></span></span><span><span><span>                          MASTER_PASSWORD='', \
</span></span></span><span><span><span>                          MASTER_CONNECT_RETRY=10; \
</span></span></span><span><span><span>                        START SLAVE;" || exit 1
</span></span></span><span><span><span>            # In case of container restart, attempt this at-most-once.
</span></span></span><span><span><span>            mv change_master_to.sql.in change_master_to.sql.orig
</span></span></span><span><span><span>          fi
</span></span></span><span><span><span>
</span></span></span><span><span><span>          # Start a server to send backups when requested by peers.
</span></span></span><span><span><span>          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
</span></span></span><span><span><span>            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"</span><span>          
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/lib/mysql<span>
</span></span></span><span><span><span>          </span><span>subPath</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>conf<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/mysql/conf.d<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>100m<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>conf<span>
</span></span></span><span><span><span>        </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config-map<span>
</span></span></span><span><span><span>        </span><span>configMap</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>  </span><span>volumeClaimTemplates</span>:<span>
</span></span></span><span><span><span>  </span>- <span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>accessModes</span>:<span> </span>[<span>"ReadWriteOnce"</span>]<span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>requests</span>:<span>
</span></span></span><span><span><span>          </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span></code></pre></div></div></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml
</span></span></code></pre></div><p>You can watch the startup progress by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>mysql --watch
</span></span></code></pre></div><p>After a while, you should see all 3 Pods become <code>Running</code>:</p><pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE
mysql-0   2/2       Running   0          2m
mysql-1   2/2       Running   0          1m
mysql-2   2/2       Running   0          1m
</code></pre><p>Press <strong>Ctrl+C</strong> to cancel the watch.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you don't see any progress, make sure you have a dynamic PersistentVolume
provisioner enabled, as mentioned in the <a href="#before-you-begin">prerequisites</a>.</div><p>This manifest uses a variety of techniques for managing stateful Pods as part of
a StatefulSet. The next section highlights some of these techniques to explain
what happens as the StatefulSet creates Pods.</p><h2 id="understanding-stateful-pod-initialization">Understanding stateful Pod initialization</h2><p>The StatefulSet controller starts Pods one at a time, in order by their
ordinal index.
It waits until each Pod reports being Ready before starting the next one.</p><p>In addition, the controller assigns each Pod a unique, stable name of the form
<code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code>, which results in Pods named <code>mysql-0</code>,
<code>mysql-1</code>, and <code>mysql-2</code>.</p><p>The Pod template in the above StatefulSet manifest takes advantage of these
properties to perform orderly startup of MySQL replication.</p><h3 id="generating-configuration">Generating configuration</h3><p>Before starting any of the containers in the Pod spec, the Pod first runs any
<a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>
in the order defined.</p><p>The first init container, named <code>init-mysql</code>, generates special MySQL config
files based on the ordinal index.</p><p>The script determines its own ordinal index by extracting it from the end of
the Pod name, which is returned by the <code>hostname</code> command.
Then it saves the ordinal (with a numeric offset to avoid reserved values)
into a file called <code>server-id.cnf</code> in the MySQL <code>conf.d</code> directory.
This translates the unique, stable identity provided by the StatefulSet
into the domain of MySQL server IDs, which require the same properties.</p><p>The script in the <code>init-mysql</code> container also applies either <code>primary.cnf</code> or
<code>replica.cnf</code> from the ConfigMap by copying the contents into <code>conf.d</code>.
Because the example topology consists of a single primary MySQL server and any number of
replicas, the script assigns ordinal <code>0</code> to be the primary server, and everyone
else to be replicas.
Combined with the StatefulSet controller's
<a href="/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">deployment order guarantee</a>,
this ensures the primary MySQL server is Ready before creating replicas, so they can begin
replicating.</p><h3 id="cloning-existing-data">Cloning existing data</h3><p>In general, when a new Pod joins the set as a replica, it must assume the primary MySQL
server might already have data on it. It also must assume that the replication
logs might not go all the way back to the beginning of time.
These conservative assumptions are the key to allow a running StatefulSet
to scale up and down over time, rather than being fixed at its initial size.</p><p>The second init container, named <code>clone-mysql</code>, performs a clone operation on
a replica Pod the first time it starts up on an empty PersistentVolume.
That means it copies all existing data from another running Pod,
so its local state is consistent enough to begin replicating from the primary server.</p><p>MySQL itself does not provide a mechanism to do this, so the example uses a
popular open-source tool called Percona XtraBackup.
During the clone, the source MySQL server might suffer reduced performance.
To minimize impact on the primary MySQL server, the script instructs each Pod to clone
from the Pod whose ordinal index is one lower.
This works because the StatefulSet controller always ensures Pod <code>N</code> is
Ready before starting Pod <code>N+1</code>.</p><h3 id="starting-replication">Starting replication</h3><p>After the init containers complete successfully, the regular containers run.
The MySQL Pods consist of a <code>mysql</code> container that runs the actual <code>mysqld</code>
server, and an <code>xtrabackup</code> container that acts as a
<a href="/blog/2015/06/the-distributed-system-toolkit-patterns">sidecar</a>.</p><p>The <code>xtrabackup</code> sidecar looks at the cloned data files and determines if
it's necessary to initialize MySQL replication on the replica.
If so, it waits for <code>mysqld</code> to be ready and then executes the
<code>CHANGE MASTER TO</code> and <code>START SLAVE</code> commands with replication parameters
extracted from the XtraBackup clone files.</p><p>Once a replica begins replication, it remembers its primary MySQL server and
reconnects automatically if the server restarts or the connection dies.
Also, because replicas look for the primary server at its stable DNS name
(<code>mysql-0.mysql</code>), they automatically find the primary server even if it gets a new
Pod IP due to being rescheduled.</p><p>Lastly, after starting replication, the <code>xtrabackup</code> container listens for
connections from other Pods requesting a data clone.
This server remains up indefinitely in case the StatefulSet scales up, or in
case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.</p><h2 id="sending-client-traffic">Sending client traffic</h2><p>You can send test queries to the primary MySQL server (hostname <code>mysql-0.mysql</code>)
by running a temporary container with the <code>mysql:5.7</code> image and running the
<code>mysql</code> client binary.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run mysql-client --image<span>=</span>mysql:5.7 -i --rm --restart<span>=</span>Never --<span>\
</span></span></span><span><span><span></span>  mysql -h mysql-0.mysql <span>&lt;&lt;EOF
</span></span></span><span><span><span>CREATE DATABASE test;
</span></span></span><span><span><span>CREATE TABLE test.messages (message VARCHAR(250));
</span></span></span><span><span><span>INSERT INTO test.messages VALUES ('hello');
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Use the hostname <code>mysql-read</code> to send test queries to any server that reports
being Ready:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run mysql-client --image<span>=</span>mysql:5.7 -i -t --rm --restart<span>=</span>Never --<span>\
</span></span></span><span><span><span></span>  mysql -h mysql-read -e <span>"SELECT * FROM test.messages"</span>
</span></span></code></pre></div><p>You should get output like this:</p><pre tabindex="0"><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
</code></pre><p>To demonstrate that the <code>mysql-read</code> Service distributes connections across
servers, you can run <code>SELECT @@server_id</code> in a loop:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run mysql-client-loop --image<span>=</span>mysql:5.7 -i -t --rm --restart<span>=</span>Never --<span>\
</span></span></span><span><span><span></span>  bash -ic <span>"while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done"</span>
</span></span></code></pre></div><p>You should see the reported <code>@@server_id</code> change randomly, because a different
endpoint might be selected upon each connection attempt:</p><pre tabindex="0"><code>+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2006-01-02 15:04:05 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2006-01-02 15:04:06 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2006-01-02 15:04:07 |
+-------------+---------------------+
</code></pre><p>You can press <strong>Ctrl+C</strong> when you want to stop the loop, but it's useful to keep
it running in another window so you can see the effects of the following steps.</p><h2 id="simulate-pod-and-node-downtime">Simulate Pod and Node failure</h2><p>To demonstrate the increased availability of reading from the pool of replicas
instead of a single server, keep the <code>SELECT @@server_id</code> loop from above
running while you force a Pod out of the Ready state.</p><h3 id="break-the-readiness-probe">Break the Readiness probe</h3><p>The <a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes">readiness probe</a>
for the <code>mysql</code> container runs the command <code>mysql -h 127.0.0.1 -e 'SELECT 1'</code>
to make sure the server is up and able to execute queries.</p><p>One way to force this readiness probe to fail is to break that command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off
</span></span></code></pre></div><p>This reaches into the actual container's filesystem for Pod <code>mysql-2</code> and
renames the <code>mysql</code> command so the readiness probe can't find it.
After a few seconds, the Pod should report one of its containers as not Ready,
which you can check by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod mysql-2
</span></span></code></pre></div><p>Look for <code>1/2</code> in the <code>READY</code> column:</p><pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE
mysql-2   1/2       Running   0          3m
</code></pre><p>At this point, you should see your <code>SELECT @@server_id</code> loop continue to run,
although it never reports <code>102</code> anymore.
Recall that the <code>init-mysql</code> script defined <code>server-id</code> as <code>100 + $ordinal</code>,
so server ID <code>102</code> corresponds to Pod <code>mysql-2</code>.</p><p>Now repair the Pod and it should reappear in the loop output
after a few seconds:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql
</span></span></code></pre></div><h3 id="delete-pods">Delete Pods</h3><p>The StatefulSet also recreates Pods if they're deleted, similar to what a
ReplicaSet does for stateless Pods.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod mysql-2
</span></span></code></pre></div><p>The StatefulSet controller notices that no <code>mysql-2</code> Pod exists anymore,
and creates a new one with the same name and linked to the same
PersistentVolumeClaim.
You should see server ID <code>102</code> disappear from the loop output for a while
and then return on its own.</p><h3 id="drain-a-node">Drain a Node</h3><p>If your Kubernetes cluster has multiple Nodes, you can simulate Node downtime
(such as when Nodes are upgraded) by issuing a
<a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">drain</a>.</p><p>First determine which Node one of the MySQL Pods is on:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod mysql-2 -o wide
</span></span></code></pre></div><p>The Node name should show up in the last column:</p><pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE       IP            NODE
mysql-2   2/2       Running   0          15m       10.244.5.27   kubernetes-node-9l2t
</code></pre><p>Then, drain the Node by running the following command, which cordons it so
no new Pods may schedule there, and then evicts any existing Pods.
Replace <code>&lt;node-name&gt;</code> with the name of the Node you found in the last step.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Draining a Node can impact other workloads and applications
running on the same node. Only perform the following step in a test
cluster.</div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># See above advice about impact on other workloads</span>
</span></span><span><span>kubectl drain &lt;node-name&gt; --force --delete-emptydir-data --ignore-daemonsets
</span></span></code></pre></div><p>Now you can watch as the Pod reschedules on a different Node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod mysql-2 -o wide --watch
</span></span></code></pre></div><p>It should look something like this:</p><pre tabindex="0"><code>NAME      READY   STATUS          RESTARTS   AGE       IP            NODE
mysql-2   2/2     Terminating     0          15m       10.244.1.56   kubernetes-node-9l2t
[...]
mysql-2   0/2     Pending         0          0s        &lt;none&gt;        kubernetes-node-fjlm
mysql-2   0/2     Init:0/2        0          0s        &lt;none&gt;        kubernetes-node-fjlm
mysql-2   0/2     Init:1/2        0          20s       10.244.5.32   kubernetes-node-fjlm
mysql-2   0/2     PodInitializing 0          21s       10.244.5.32   kubernetes-node-fjlm
mysql-2   1/2     Running         0          22s       10.244.5.32   kubernetes-node-fjlm
mysql-2   2/2     Running         0          30s       10.244.5.32   kubernetes-node-fjlm
</code></pre><p>And again, you should see server ID <code>102</code> disappear from the
<code>SELECT @@server_id</code> loop output for a while and then return.</p><p>Now uncordon the Node to return it to a normal state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl uncordon &lt;node-name&gt;
</span></span></code></pre></div><h2 id="scaling-the-number-of-replicas">Scaling the number of replicas</h2><p>When you use MySQL replication, you can scale your read query capacity by
adding replicas.
For a StatefulSet, you can achieve this with a single command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale statefulset mysql  --replicas<span>=</span><span>5</span>
</span></span></code></pre></div><p>Watch the new Pods come up by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>mysql --watch
</span></span></code></pre></div><p>Once they're up, you should see server IDs <code>103</code> and <code>104</code> start appearing in
the <code>SELECT @@server_id</code> loop output.</p><p>You can also verify that these new servers have the data you added before they
existed:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run mysql-client --image<span>=</span>mysql:5.7 -i -t --rm --restart<span>=</span>Never --<span>\
</span></span></span><span><span><span></span>  mysql -h mysql-3.mysql -e <span>"SELECT * FROM test.messages"</span>
</span></span></code></pre></div><pre tabindex="0"><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
</code></pre><p>Scaling back down is also seamless:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale statefulset mysql --replicas<span>=</span><span>3</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Although scaling up creates new PersistentVolumeClaims
automatically, scaling down does not automatically delete these PVCs.</p><p>This gives you the choice to keep those initialized PVCs around to make
scaling back up quicker, or to extract data before deleting them.</p></div><p>You can see this by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pvc -l <span>app</span><span>=</span>mysql
</span></span></code></pre></div><p>Which shows that all 5 PVCs still exist, despite having scaled the
StatefulSet down to 3:</p><pre tabindex="0"><code>NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
data-mysql-0   Bound     pvc-8acbf5dc-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-1   Bound     pvc-8ad39820-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-2   Bound     pvc-8ad69a6d-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-3   Bound     pvc-50043c45-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
data-mysql-4   Bound     pvc-500a9957-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
</code></pre><p>If you don't intend to reuse the extra PVCs, you can delete them:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pvc data-mysql-3
</span></span><span><span>kubectl delete pvc data-mysql-4
</span></span></code></pre></div><h2 id="cleaning-up">Cleaning up</h2><ol><li><p>Cancel the <code>SELECT @@server_id</code> loop by pressing <strong>Ctrl+C</strong> in its terminal,
or running the following from another terminal:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pod mysql-client-loop --now
</span></span></code></pre></div></li><li><p>Delete the StatefulSet. This also begins terminating the Pods.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete statefulset mysql
</span></span></code></pre></div></li><li><p>Verify that the Pods disappear.
They might take some time to finish terminating.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>mysql
</span></span></code></pre></div><p>You'll know the Pods have terminated when the above returns:</p><pre tabindex="0"><code>No resources found.
</code></pre></li><li><p>Delete the ConfigMap, Services, and PersistentVolumeClaims.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete configmap,service,pvc -l <span>app</span><span>=</span>mysql
</span></span></code></pre></div></li><li><p>If you manually provisioned PersistentVolumes, you also need to manually
delete them, as well as release the underlying resources.
If you used a dynamic provisioner, it automatically deletes the
PersistentVolumes when it sees that you deleted the PersistentVolumeClaims.
Some dynamic provisioners (such as those for EBS and PD) also release the
underlying resources upon deleting the PersistentVolumes.</p></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/run-application/scale-stateful-set/">scaling a StatefulSet</a>.</li><li>Learn more about <a href="/docs/tasks/debug/debug-application/debug-statefulset/">debugging a StatefulSet</a>.</li><li>Learn more about <a href="/docs/tasks/run-application/delete-stateful-set/">deleting a StatefulSet</a>.</li><li>Learn more about <a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">force deleting StatefulSet Pods</a>.</li><li>Look in the <a href="https://artifacthub.io/">Helm Charts repository</a>
for other stateful application examples.</li></ul></div></div><div><div class="td-content"><h1>Scale a StatefulSet</h1><p>This task shows how to scale a StatefulSet. Scaling a StatefulSet refers to
increasing or decreasing the number of replicas.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>StatefulSets are only available in Kubernetes version 1.5 or later.
To check your version of Kubernetes, run <code>kubectl version</code>.</p></li><li><p>Not all stateful applications scale nicely. If you are unsure about whether
to scale your StatefulSets, see <a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet concepts</a>
or <a href="/docs/tutorials/stateful-application/basic-stateful-set/">StatefulSet tutorial</a> for further information.</p></li><li><p>You should perform scaling only when you are confident that your stateful application
cluster is completely healthy.</p></li></ul><h2 id="scaling-statefulsets">Scaling StatefulSets</h2><h3 id="use-kubectl-to-scale-statefulsets">Use kubectl to scale StatefulSets</h3><p>First, find the StatefulSet you want to scale.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get statefulsets &lt;stateful-set-name&gt;
</span></span></code></pre></div><p>Change the number of replicas of your StatefulSet:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale statefulsets &lt;stateful-set-name&gt; --replicas<span>=</span>&lt;new-replicas&gt;
</span></span></code></pre></div><h3 id="make-in-place-updates-on-your-statefulsets">Make in-place updates on your StatefulSets</h3><p>Alternatively, you can do
<a href="/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources">in-place updates</a>
on your StatefulSets.</p><p>If your StatefulSet was initially created with <code>kubectl apply</code>,
update <code>.spec.replicas</code> of the StatefulSet manifests, and then do a <code>kubectl apply</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f &lt;stateful-set-file-updated&gt;
</span></span></code></pre></div><p>Otherwise, edit that field with <code>kubectl edit</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit statefulsets &lt;stateful-set-name&gt;
</span></span></code></pre></div><p>Or use <code>kubectl patch</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch statefulsets &lt;stateful-set-name&gt; -p <span>'{"spec":{"replicas":&lt;new-replicas&gt;}}'</span>
</span></span></code></pre></div><h2 id="troubleshooting">Troubleshooting</h2><h3 id="scaling-down-does-not-work-right">Scaling down does not work right</h3><p>You cannot scale down a StatefulSet when any of the stateful Pods it manages is
unhealthy. Scaling down only takes place after those stateful Pods become running and ready.</p><p>If spec.replicas &gt; 1, Kubernetes cannot determine the reason for an unhealthy Pod.
It might be the result of a permanent fault or of a transient fault. A transient
fault can be caused by a restart required by upgrading or maintenance.</p><p>If the Pod is unhealthy due to a permanent fault, scaling
without correcting the fault may lead to a state where the StatefulSet membership
drops below a certain minimum number of replicas that are needed to function
correctly. This may cause your StatefulSet to become unavailable.</p><p>If the Pod is unhealthy due to a transient fault and the Pod might become available again,
the transient error may interfere with your scale-up or scale-down operation. Some distributed
databases have issues when nodes join and leave at the same time. It is better
to reason about scaling operations at the application level in these cases, and
perform scaling only when you are sure that your stateful application cluster is
completely healthy.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/run-application/delete-stateful-set/">deleting a StatefulSet</a>.</li></ul></div></div><div><div class="td-content"><h1>Delete a StatefulSet</h1><p>This task shows you how to delete a <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>This task assumes you have an application running on your cluster represented by a StatefulSet.</li></ul><h2 id="deleting-a-statefulset">Deleting a StatefulSet</h2><p>You can delete a StatefulSet in the same way you delete other resources in Kubernetes:
use the <code>kubectl delete</code> command, and specify the StatefulSet either by file or by name.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f &lt;file.yaml&gt;
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete statefulsets &lt;statefulset-name&gt;
</span></span></code></pre></div><p>You may need to delete the associated headless service separately after the StatefulSet itself is deleted.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete service &lt;service-name&gt;
</span></span></code></pre></div><p>When deleting a StatefulSet through <code>kubectl</code>, the StatefulSet scales down to 0.
All Pods that are part of this workload are also deleted. If you want to delete
only the StatefulSet and not the Pods, use <code>--cascade=orphan</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f &lt;file.yaml&gt; --cascade<span>=</span>orphan
</span></span></code></pre></div><p>By passing <code>--cascade=orphan</code> to <code>kubectl delete</code>, the Pods managed by the StatefulSet
are left behind even after the StatefulSet object itself is deleted. If the pods have
a label <code>app.kubernetes.io/name=MyApp</code>, you can then delete them as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pods -l app.kubernetes.io/name<span>=</span>MyApp
</span></span></code></pre></div><h3 id="persistent-volumes">Persistent Volumes</h3><p>Deleting the Pods in a StatefulSet will not delete the associated volumes.
This is to ensure that you have the chance to copy data off the volume before
deleting it. Deleting the PVC after the pods have terminated might trigger
deletion of the backing Persistent Volumes depending on the storage class
and reclaim policy. You should never assume ability to access a volume
after claim deletion.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Use caution when deleting a PVC, as it may lead to data loss.</div><h3 id="complete-deletion-of-a-statefulset">Complete deletion of a StatefulSet</h3><p>To delete everything in a StatefulSet, including the associated pods,
you can run a series of commands similar to the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>grace</span><span>=</span><span>$(</span>kubectl get pods &lt;stateful-set-pod&gt; --template <span>'{{.spec.terminationGracePeriodSeconds}}'</span><span>)</span>
</span></span><span><span>kubectl delete statefulset -l app.kubernetes.io/name<span>=</span>MyApp
</span></span><span><span>sleep <span>$grace</span>
</span></span><span><span>kubectl delete pvc -l app.kubernetes.io/name<span>=</span>MyApp
</span></span></code></pre></div><p>In the example above, the Pods have the label <code>app.kubernetes.io/name=MyApp</code>;
substitute your own label as appropriate.</p><h3 id="force-deletion-of-statefulset-pods">Force deletion of StatefulSet pods</h3><p>If you find that some pods in your StatefulSet are stuck in the 'Terminating'
or 'Unknown' states for an extended period of time, you may need to manually
intervene to forcefully delete the pods from the apiserver.
This is a potentially dangerous task. Refer to
<a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">Force Delete StatefulSet Pods</a>
for details.</p><h2 id="what-s-next">What's next</h2><p>Learn more about <a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">force deleting StatefulSet Pods</a>.</p></div></div><div><div class="td-content"><h1>Force Delete StatefulSet Pods</h1><p>This page shows how to delete Pods which are part of a
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">stateful set</a>,
and explains the considerations to keep in mind when doing so.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>This is a fairly advanced task and has the potential to violate some of the properties
inherent to StatefulSet.</li><li>Before proceeding, make yourself familiar with the considerations enumerated below.</li></ul><h2 id="statefulset-considerations">StatefulSet considerations</h2><p>In normal operation of a StatefulSet, there is <strong>never</strong> a need to force delete a StatefulSet Pod.
The <a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet controller</a> is responsible for
creating, scaling and deleting members of the StatefulSet. It tries to ensure that the specified
number of Pods from ordinal 0 through N-1 are alive and ready. StatefulSet ensures that, at any time,
there is at most one Pod with a given identity running in a cluster. This is referred to as
<em>at most one</em> semantics provided by a StatefulSet.</p><p>Manual force deletion should be undertaken with caution, as it has the potential to violate the
at most one semantics inherent to StatefulSet. StatefulSets may be used to run distributed and
clustered applications which have a need for a stable network identity and stable storage.
These applications often have configuration which relies on an ensemble of a fixed number of
members with fixed identities. Having multiple members with the same identity can be disastrous
and may lead to data loss (e.g. split brain scenario in quorum-based systems).</p><h2 id="delete-pods">Delete Pods</h2><p>You can perform a graceful pod deletion with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pods &lt;pod&gt;
</span></span></code></pre></div><p>For the above to lead to graceful termination, the Pod <strong>must not</strong> specify a
<code>pod.Spec.TerminationGracePeriodSeconds</code> of 0. The practice of setting a
<code>pod.Spec.TerminationGracePeriodSeconds</code> of 0 seconds is unsafe and strongly discouraged
for StatefulSet Pods. Graceful deletion is safe and will ensure that the Pod
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">shuts down gracefully</a>
before the kubelet deletes the name from the apiserver.</p><p>A Pod is not deleted automatically when a node is unreachable.
The Pods running on an unreachable Node enter the 'Terminating' or 'Unknown' state after a
<a href="/docs/concepts/architecture/nodes/#condition">timeout</a>.
Pods may also enter these states when the user attempts graceful deletion of a Pod
on an unreachable Node.
The only ways in which a Pod in such a state can be removed from the apiserver are as follows:</p><ul><li>The Node object is deleted (either by you, or by the
<a href="/docs/concepts/architecture/nodes/#node-controller">Node Controller</a>).</li><li>The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry
from the apiserver.</li><li>Force deletion of the Pod by the user.</li></ul><p>The recommended best practice is to use the first or second approach. If a Node is confirmed
to be dead (e.g. permanently disconnected from the network, powered down, etc), then delete
the Node object. If the Node is suffering from a network partition, then try to resolve this
or wait for it to resolve. When the partition heals, the kubelet will complete the deletion
of the Pod and free up its name in the apiserver.</p><p>Normally, the system completes the deletion once the Pod is no longer running on a Node, or
the Node is deleted by an administrator. You may override this by force deleting the Pod.</p><h3 id="force-deletion">Force Deletion</h3><p>Force deletions <strong>do not</strong> wait for confirmation from the kubelet that the Pod has been terminated.
Irrespective of whether a force deletion is successful in killing a Pod, it will immediately
free up the name from the apiserver. This would let the StatefulSet controller create a replacement
Pod with that same identity; this can lead to the duplication of a still-running Pod,
and if said Pod can still communicate with the other members of the StatefulSet,
will violate the at most one semantics that StatefulSet is designed to guarantee.</p><p>When you force delete a StatefulSet pod, you are asserting that the Pod in question will never
again make contact with other Pods in the StatefulSet and its name can be safely freed up for a
replacement to be created.</p><p>If you want to delete a Pod forcibly using kubectl version &gt;= 1.5, do the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pods &lt;pod&gt; --grace-period<span>=</span><span>0</span> --force
</span></span></code></pre></div><p>If you're using any version of kubectl &lt;= 1.4, you should omit the <code>--force</code> option and use:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete pods &lt;pod&gt; --grace-period<span>=</span><span>0</span>
</span></span></code></pre></div><p>If even after these commands the pod is stuck on <code>Unknown</code> state, use the following command to
remove the pod from the cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch pod &lt;pod&gt; -p <span>'{"metadata":{"finalizers":null}}'</span>
</span></span></code></pre></div><p>Always perform force deletion of StatefulSet Pods carefully and with complete knowledge of the risks involved.</p><h2 id="what-s-next">What's next</h2><p>Learn more about <a href="/docs/tasks/debug/debug-application/debug-statefulset/">debugging a StatefulSet</a>.</p></div></div><div><div class="td-content"><h1>Horizontal Pod Autoscaling</h1><p>In Kubernetes, a <em>HorizontalPodAutoscaler</em> automatically updates a workload resource (such as
a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> or
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>), with the
aim of automatically scaling the workload to match demand.</p><p>Horizontal scaling means that the response to increased load is to deploy more
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>.
This is different from <em>vertical</em> scaling, which for Kubernetes would mean
assigning more resources (for example: memory or CPU) to the Pods that are already
running for the workload.</p><p>If the load decreases, and the number of Pods is above the configured minimum,
the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet,
or other similar resource) to scale back down.</p><p>Horizontal pod autoscaling does not apply to objects that can't be scaled (for example:
a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a>.)</p><p>The HorizontalPodAutoscaler is implemented as a Kubernetes API resource and a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>.
The resource determines the behavior of the controller.
The horizontal pod autoscaling controller, running within the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>, periodically adjusts the
desired scale of its target (for example, a Deployment) to match observed metrics such as average
CPU utilization, average memory utilization, or any other custom metric you specify.</p><p>There is <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough example</a> of using
horizontal pod autoscaling.</p><h2 id="how-does-a-horizontalpodautoscaler-work">How does a HorizontalPodAutoscaler work?</h2><figure><div class="mermaid">graph BT
hpa[HorizontalPodAutoscaler] --&gt; scale[Scale]
subgraph rc[RC / Deployment]
scale
end
scale -.-&gt; pod1[Pod 1]
scale -.-&gt; pod2[Pod 2]
scale -.-&gt; pod3[Pod N]
classDef hpa fill:#D5A6BD,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef rc fill:#F9CB9C,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef scale fill:#B6D7A8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
classDef pod fill:#9FC5E8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D;
class hpa hpa;
class rc rc;
class scale scale;
class pod1,pod2,pod3 pod</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>Figure 1. HorizontalPodAutoscaler controls the scale of a Deployment and its ReplicaSet</p><p>Kubernetes implements horizontal pod autoscaling as a control loop that runs intermittently
(it is not a continuous process). The interval is set by the
<code>--horizontal-pod-autoscaler-sync-period</code> parameter to the
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a>
(and the default interval is 15 seconds).</p><p>Once during each period, the controller manager queries the resource utilization against the
metrics specified in each HorizontalPodAutoscaler definition. The controller manager
finds the target resource defined by the <code>scaleTargetRef</code>,
then selects the pods based on the target resource's <code>.spec.selector</code> labels,
and obtains the metrics from either the resource metrics API (for per-pod resource metrics),
or the custom metrics API (for all other metrics).</p><ul><li><p>For per-pod resource metrics (like CPU), the controller fetches the metrics
from the resource metrics API for each Pod targeted by the HorizontalPodAutoscaler.
Then, if a target utilization value is set, the controller calculates the utilization
value as a percentage of the equivalent
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">resource request</a>
on the containers in each Pod. If a target raw value is set, the raw metric values are used directly.
The controller then takes the mean of the utilization or the raw value (depending on the type
of target specified) across all targeted Pods, and produces a ratio used to scale
the number of desired replicas.</p><p>Please note that if some of the Pod's containers do not have the relevant resource request set,
CPU utilization for the Pod will not be defined and the autoscaler will
not take any action for that metric. See the <a href="#algorithm-details">algorithm details</a> section below
for more information about how the autoscaling algorithm works.</p></li><li><p>For per-pod custom metrics, the controller functions similarly to per-pod resource metrics,
except that it works with raw values, not utilization values.</p></li><li><p>For object metrics and external metrics, a single metric is fetched, which describes
the object in question. This metric is compared to the target
value, to produce a ratio as above. In the <code>autoscaling/v2</code> API
version, this value can optionally be divided by the number of Pods before the
comparison is made.</p></li></ul><p>The common use for HorizontalPodAutoscaler is to configure it to fetch metrics from
<a class="glossary-tooltip" title="The aggregation layer lets you install additional Kubernetes-style APIs in your cluster." href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/" target="_blank">aggregated APIs</a>
(<code>metrics.k8s.io</code>, <code>custom.metrics.k8s.io</code>, or <code>external.metrics.k8s.io</code>). The <code>metrics.k8s.io</code> API is
usually provided by an add-on named Metrics Server, which needs to be launched separately.
For more information about resource metrics, see
<a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server">Metrics Server</a>.</p><p><a href="#support-for-metrics-apis">Support for metrics APIs</a> explains the stability guarantees and support status for these
different APIs.</p><p>The HorizontalPodAutoscaler controller accesses corresponding workload resources that support scaling (such as Deployments
and StatefulSet). These resources each have a subresource named <code>scale</code>, an interface that allows you to dynamically set the
number of replicas and examine each of their current states.
For general information about subresources in the Kubernetes API, see
<a href="/docs/reference/using-api/api-concepts/">Kubernetes API Concepts</a>.</p><h3 id="algorithm-details">Algorithm details</h3><p>From the most basic perspective, the HorizontalPodAutoscaler controller
operates on the ratio between desired metric value and current metric
value:</p><div class="math">$$\begin{equation*}
desiredReplicas = ceil\left\lceil currentReplicas \times \frac{currentMetricValue}{desiredMetricValue} \right\rceil
\end{equation*}$$</div><p>For example, if the current metric value is <code>200m</code>, and the desired value
is <code>100m</code>, the number of replicas will be doubled, since
\( { 200.0 \div 100.0 } = 2.0 \).<br>If the current value is instead <code>50m</code>, you'll halve the number of
replicas, since \( { 50.0 \div 100.0 } = 0.5 \). The control plane skips any scaling
action if the ratio is sufficiently close to 1.0 (within a
<a href="#tolerance">configurable tolerance</a>, 0.1 by default).</p><p>When a <code>targetAverageValue</code> or <code>targetAverageUtilization</code> is specified,
the <code>currentMetricValue</code> is computed by taking the average of the given
metric across all Pods in the HorizontalPodAutoscaler's scale target.</p><p>Before checking the tolerance and deciding on the final values, the control
plane also considers whether any metrics are missing, and how many Pods
are <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions"><code>Ready</code></a>.
All Pods with a deletion timestamp set (objects with a deletion timestamp are
in the process of being shut down / removed) are ignored, and all failed Pods
are discarded.</p><p>If a particular Pod is missing metrics, it is set aside for later; Pods
with missing metrics will be used to adjust the final scaling amount.</p><p>When scaling on CPU, if any pod has yet to become ready (it's still
initializing, or possibly is unhealthy) <em>or</em> the most recent metric point for
the pod was before it became ready, that pod is set aside as well.</p><p>Due to technical constraints, the HorizontalPodAutoscaler controller
cannot exactly determine the first time a pod becomes ready when
determining whether to set aside certain CPU metrics. Instead, it
considers a Pod "not yet ready" if it's unready and transitioned to
ready within a short, configurable window of time since it started.
This value is configured with the <code>--horizontal-pod-autoscaler-initial-readiness-delay</code>
command line option, and its default is 30 seconds.
Once a pod has become ready, it considers any transition to
ready to be the first if it occurred within a longer, configurable time
since it started. This value is configured with the
<code>--horizontal-pod-autoscaler-cpu-initialization-period</code> command line option,
and its default is 5 minutes.</p><p>The \( currentMetricValue \over desiredMetricValue \) base scale ratio is then
calculated, using the remaining pods not set aside or discarded from above.</p><p>If there were any missing metrics, the control plane recomputes the average more
conservatively, assuming those pods were consuming 100% of the desired
value in case of a scale down, and 0% in case of a scale up. This dampens
the magnitude of any potential scale.</p><p>Furthermore, if any not-yet-ready pods were present, and the workload would have
scaled up without factoring in missing metrics or not-yet-ready pods,
the controller conservatively assumes that the not-yet-ready pods are consuming 0%
of the desired metric, further dampening the magnitude of a scale up.</p><p>After factoring in the not-yet-ready pods and missing metrics, the
controller recalculates the usage ratio. If the new ratio reverses the scale
direction, or is within the tolerance, the controller doesn't take any scaling
action. In other cases, the new ratio is used to decide any change to the
number of Pods.</p><p>Note that the <em>original</em> value for the average utilization is reported
back via the HorizontalPodAutoscaler status, without factoring in the
not-yet-ready pods or missing metrics, even when the new usage ratio is
used.</p><p>If multiple metrics are specified in a HorizontalPodAutoscaler, this
calculation is done for each metric, and then the largest of the desired
replica counts is chosen. If any of these metrics cannot be converted
into a desired replica count (e.g. due to an error fetching the metrics
from the metrics APIs) and a scale down is suggested by the metrics which
can be fetched, scaling is skipped. This means that the HPA is still capable
of scaling up if one or more metrics give a <code>desiredReplicas</code> greater than
the current value.</p><p>Finally, right before HPA scales the target, the scale recommendation is recorded. The
controller considers all recommendations within a configurable window choosing the
highest recommendation from within that window. You can configure this value using the
<code>--horizontal-pod-autoscaler-downscale-stabilization</code> command line option, which defaults to 5 minutes.
This means that scaledowns will occur gradually, smoothing out the impact of rapidly
fluctuating metric values.</p><h2 id="pod-readiness-and-autoscaling-metrics">Pod readiness and autoscaling metrics</h2><p>The HorizontalPodAutoscaler (HPA) controller includes two command line options that influence how CPU metrics are collected from Pods during startup:</p><ol><li><code>--horizontal-pod-autoscaler-cpu-initialization-period</code> (default: 5 minutes)</li></ol><p>This defines the time window after a Pod starts during which its <strong>CPU usage is ignored</strong> unless:
- The Pod is in a <code>Ready</code> state <strong>and</strong>
- The metric sample was taken entirely during the period it was <code>Ready</code>.</p><p>This command line option helps <strong>exclude misleading high CPU usage</strong> from initializing Pods (for example: Java apps warming up) in HPA scaling decisions.</p><ol><li><code>--horizontal-pod-autoscaler-initial-readiness-delay</code> (default: 30 seconds)</li></ol><p>This defines a short delay period after a Pod starts during which the HPA controller treats Pods that are currently <code>Unready</code> as still initializing, <strong>even if they have previously transitioned to <code>Ready</code> briefly</strong>.</p><p>It is designed to:
- Avoid including Pods that rapidly fluctuate between <code>Ready</code> and <code>Unready</code> during startup.
- Ensure stability in the initial readiness signal before HPA considers their metrics valid.</p><p>You can only set these command line options cluster-wide.</p><h3 id="pod-readiness-key-behaviors">Key behaviors for pod readiness</h3><ul><li>If a Pod is <code>Ready</code> and remains <code>Ready</code>, it can be counted as contributing metrics even within the delay.</li><li>If a Pod rapidly toggles between <code>Ready</code> and <code>Unready</code>, metrics are ignored until it&#8217;s considered stably <code>Ready</code>.</li></ul><h3 id="pod-readiness-good-practices">Good practice for pod readiness</h3><ul><li>Configure a <code>startupProbe</code> that doesn't pass until the high CPU usage has passed, or</li><li>Ensure your <code>readinessProbe</code> only reports <code>Ready</code> <strong>after</strong> the CPU spike subsides, using <code>initialDelaySeconds</code>.</li></ul><p>And ideally also set <code>--horizontal-pod-autoscaler-cpu-initialization-period</code> to <strong>cover the startup duration</strong>.</p><h2 id="api-object">API object</h2><p>The HorizontalPodAutoscaler is an API kind in the Kubernetes
<code>autoscaling</code> API group. The current stable version can be found in
the <code>autoscaling/v2</code> API version which includes support for scaling on
memory and custom metrics. The new fields introduced in
<code>autoscaling/v2</code> are preserved as annotations when working with
<code>autoscaling/v1</code>.</p><p>When you create a HorizontalPodAutoscaler API object, make sure the name specified is a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.
More details about the API object can be found at
<a href="/docs/reference/generated/kubernetes-api/v1.34/#horizontalpodautoscaler-v2-autoscaling">HorizontalPodAutoscaler Object</a>.</p><h2 id="flapping">Stability of workload scale</h2><p>When managing the scale of a group of replicas using the HorizontalPodAutoscaler,
it is possible that the number of replicas keeps fluctuating frequently due to the
dynamic nature of the metrics evaluated. This is sometimes referred to as <em>thrashing</em>,
or <em>flapping</em>. It's similar to the concept of <em>hysteresis</em> in cybernetics.</p><h2 id="autoscaling-during-rolling-update">Autoscaling during rolling update</h2><p>Kubernetes lets you perform a rolling update on a Deployment. In that
case, the Deployment manages the underlying ReplicaSets for you.
When you configure autoscaling for a Deployment, you bind a
HorizontalPodAutoscaler to a single Deployment. The HorizontalPodAutoscaler
manages the <code>replicas</code> field of the Deployment. The deployment controller is responsible
for setting the <code>replicas</code> of the underlying ReplicaSets so that they add up to a suitable
number during the rollout and also afterwards.</p><p>If you perform a rolling update of a StatefulSet that has an autoscaled number of
replicas, the StatefulSet directly manages its set of Pods (there is no intermediate resource
similar to ReplicaSet).</p><h2 id="support-for-resource-metrics">Support for resource metrics</h2><p>Any HPA target can be scaled based on the resource usage of the pods in the scaling target.
When defining the pod specification the resource requests like <code>cpu</code> and <code>memory</code> should
be specified. This is used to determine the resource utilization and used by the HPA controller
to scale the target up or down. To use resource utilization based scaling specify a metric source
like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>Resource<span>
</span></span></span><span><span><span></span><span>resource</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>  </span><span>target</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Utilization<span>
</span></span></span><span><span><span>    </span><span>averageUtilization</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p>With this metric the HPA controller will keep the average utilization of the pods in the scaling
target at 60%. Utilization is the ratio between the current usage of resource to the requested
resources of the pod. See <a href="#algorithm-details">Algorithm</a> for more details about how the utilization
is calculated and averaged.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Since the resource usages of all the containers are summed up the total pod utilization may not
accurately represent the individual container resource usage. This could lead to situations where
a single container might be running with high usage and the HPA will not scale out because the overall
pod usage is still within acceptable limits.</div><h3 id="container-resource-metrics">Container resource metrics</h3><div class="feature-state-notice feature-stable" title="Feature Gate: HPAContainerMetrics"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [stable]</code> (enabled by default: true)</div><p>The HorizontalPodAutoscaler API also supports a container metric source where the HPA can track the
resource usage of individual containers across a set of Pods, in order to scale the target resource.
This lets you configure scaling thresholds for the containers that matter most in a particular Pod.
For example, if you have a web application and a sidecar container that provides logging, you can scale based on the resource
use of the web application, ignoring the sidecar container and its resource use.</p><p>If you revise the target resource to have a new Pod specification with a different set of containers,
you should revise the HPA spec if that newly added container should also be used for
scaling. If the specified container in the metric source is not present or only present in a subset
of the pods then those pods are ignored and the recommendation is recalculated. See <a href="#algorithm-details">Algorithm</a>
for more details about the calculation. To use container resources for autoscaling define a metric
source as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>ContainerResource<span>
</span></span></span><span><span><span></span><span>containerResource</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>  </span><span>container</span>:<span> </span>application<span>
</span></span></span><span><span><span>  </span><span>target</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Utilization<span>
</span></span></span><span><span><span>    </span><span>averageUtilization</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p>In the above example the HPA controller scales the target such that the average utilization of the cpu
in the <code>application</code> container of all the pods is 60%.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you change the name of a container that a HorizontalPodAutoscaler is tracking, you can
make that change in a specific order to ensure scaling remains available and effective
whilst the change is being applied. Before you update the resource that defines the container
(such as a Deployment), you should update the associated HPA to track both the new and
old container names. This way, the HPA is able to calculate a scaling recommendation
throughout the update process.</p><p>Once you have rolled out the container name change to the workload resource, tidy up by removing
the old container name from the HPA specification.</p></div><h2 id="scaling-on-custom-metrics">Scaling on custom metrics</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>(the <code>autoscaling/v2beta2</code> API version previously provided this ability as a beta feature)</p><p>Provided that you use the <code>autoscaling/v2</code> API version, you can configure a HorizontalPodAutoscaler
to scale based on a custom metric (that is not built in to Kubernetes or any Kubernetes component).
The HorizontalPodAutoscaler controller then queries for these custom metrics from the Kubernetes
API.</p><p>See <a href="#support-for-metrics-apis">Support for metrics APIs</a> for the requirements.</p><h2 id="scaling-on-multiple-metrics">Scaling on multiple metrics</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>(the <code>autoscaling/v2beta2</code> API version previously provided this ability as a beta feature)</p><p>Provided that you use the <code>autoscaling/v2</code> API version, you can specify multiple metrics for a
HorizontalPodAutoscaler to scale on. Then, the HorizontalPodAutoscaler controller evaluates each metric,
and proposes a new scale based on that metric. The HorizontalPodAutoscaler takes the maximum scale
recommended for each metric and sets the workload to that size (provided that this isn't larger than the
overall maximum that you configured).</p><h2 id="support-for-metrics-apis">Support for metrics APIs</h2><p>By default, the HorizontalPodAutoscaler controller retrieves metrics from a series of APIs.
In order for it to access these APIs, cluster administrators must ensure that:</p><ul><li><p>The <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">API aggregation layer</a> is enabled.</p></li><li><p>The corresponding APIs are registered:</p><ul><li><p>For resource metrics, this is the <code>metrics.k8s.io</code> <a href="/docs/reference/external-api/metrics.v1beta1/">API</a>,
generally provided by <a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>.
It can be launched as a cluster add-on.</p></li><li><p>For custom metrics, this is the <code>custom.metrics.k8s.io</code> <a href="/docs/reference/external-api/custom-metrics.v1beta2/">API</a>.
It's provided by "adapter" API servers provided by metrics solution vendors.
Check with your metrics pipeline to see if there is a Kubernetes metrics adapter available.</p></li><li><p>For external metrics, this is the <code>external.metrics.k8s.io</code> <a href="/docs/reference/external-api/external-metrics.v1beta1/">API</a>.
It may be provided by the custom metrics adapters provided above.</p></li></ul></li></ul><p>For more information on these different metrics paths and how they differ please see the relevant design proposals for
<a href="https://git.k8s.io/design-proposals-archive/autoscaling/hpa-v2.md">the HPA V2</a>,
<a href="https://git.k8s.io/design-proposals-archive/instrumentation/custom-metrics-api.md">custom.metrics.k8s.io</a>
and <a href="https://git.k8s.io/design-proposals-archive/instrumentation/external-metrics-api.md">external.metrics.k8s.io</a>.</p><p>For examples of how to use them see
<a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">the walkthrough for using custom metrics</a>
and <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects">the walkthrough for using external metrics</a>.</p><h2 id="configurable-scaling-behavior">Configurable scaling behavior</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>(the <code>autoscaling/v2beta2</code> API version previously provided this ability as a beta feature)</p><p>If you use the <code>v2</code> HorizontalPodAutoscaler API, you can use the <code>behavior</code> field
(see the <a href="/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec">API reference</a>)
to configure separate scale-up and scale-down behaviors.
You specify these behaviors by setting <code>scaleUp</code> and / or <code>scaleDown</code>
under the <code>behavior</code> field.</p><p>Scaling policies let you control the rate of change of replicas while scaling.
Also two settings can be used to prevent <a href="#flapping">flapping</a>: you can specify a
<em>stabilization window</em> for smoothing replica counts, and a tolerance to ignore
minor metric fluctuations below a specified threshold.</p><h3 id="scaling-policies">Scaling policies</h3><p>One or more scaling policies can be specified in the <code>behavior</code> section of the spec.
When multiple policies are specified the policy which allows the highest amount of
change is the policy which is selected by default. The following example shows this behavior
while scaling down:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>policies</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Pods<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>4</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Percent<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p><code>periodSeconds</code> indicates the length of time in the past for which the policy must hold true.
The maximum value that you can set for <code>periodSeconds</code> is 1800 (half an hour).
The first policy <em>(Pods)</em> allows at most 4 replicas to be scaled down in one minute. The second policy
<em>(Percent)</em> allows at most 10% of the current replicas to be scaled down in one minute.</p><p>Since by default the policy which allows the highest amount of change is selected, the second policy will
only be used when the number of pod replicas is more than 40. With 40 or less replicas, the first policy will be applied.
For instance if there are 80 replicas and the target has to be scaled down to 10 replicas
then during the first step 8 replicas will be reduced. In the next iteration when the number
of replicas is 72, 10% of the pods is 7.2 but the number is rounded up to 8. On each loop of
the autoscaler controller the number of pods to be change is re-calculated based on the number
of current replicas. When the number of replicas falls below 40 the first policy <em>(Pods)</em> is applied
and 4 replicas will be reduced at a time.</p><p>The policy selection can be changed by specifying the <code>selectPolicy</code> field for a scaling
direction. By setting the value to <code>Min</code> which would select the policy which allows the
smallest change in the replica count. Setting the value to <code>Disabled</code> completely disables
scaling in that direction.</p><h3 id="stabilization-window">Stabilization window</h3><p>The stabilization window is used to restrict the <a href="#flapping">flapping</a> of
replica count when the metrics used for scaling keep fluctuating. The autoscaling algorithm
uses this window to infer a previous desired state and avoid unwanted changes to workload
scale.</p><p>For example, in the following example snippet, a stabilization window is specified for <code>scaleDown</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>stabilizationWindowSeconds</span>:<span> </span><span>300</span><span>
</span></span></span></code></pre></div><p>When the metrics indicate that the target should be scaled down the algorithm looks
into previously computed desired states, and uses the highest value from the specified
interval. In the above example, all desired states from the past 5 minutes will be considered.</p><p>This approximates a rolling maximum, and avoids having the scaling algorithm frequently
remove Pods only to trigger recreating an equivalent Pod just moments later.</p><h3 id="tolerance">Tolerance</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: HPAConfigurableTolerance"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>The <code>tolerance</code> field configures a threshold for metric variations, preventing the
autoscaler from scaling for changes below that value.</p><p>This tolerance is defined as the amount of variation around the desired metric value under
which no scaling will occur. For example, consider a HorizontalPodAutoscaler configured
with a target memory consumption of 100MiB and a scale-up tolerance of 5%:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleUp</span>:<span>
</span></span></span><span><span><span>    </span><span>tolerance</span>:<span> </span><span>0.05</span><span> </span><span># 5% tolerance for scale up</span><span>
</span></span></span></code></pre></div><p>With this configuration, the HPA algorithm will only consider scaling up if the memory
consumption is higher than 105MiB (that is: 5% above the target).</p><p>If you don't set this field, the HPA applies the default cluster-wide tolerance of 10%. This
default can be updated for both scale-up and scale-down using the
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a>
<code>--horizontal-pod-autoscaler-tolerance</code> command line argument. (You can't use the Kubernetes API
to configure this default value.)</p><h3 id="default-behavior">Default behavior</h3><p>To use the custom scaling not all fields have to be specified. Only values which need to be
customized can be specified. These custom values are merged with default values. The default values
match the existing behavior in the HPA algorithm.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>stabilizationWindowSeconds</span>:<span> </span><span>300</span><span>
</span></span></span><span><span><span>    </span><span>policies</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Percent<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>  </span><span>scaleUp</span>:<span>
</span></span></span><span><span><span>    </span><span>stabilizationWindowSeconds</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>    </span><span>policies</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Percent<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Pods<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>4</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>    </span><span>selectPolicy</span>:<span> </span>Max<span>
</span></span></span></code></pre></div><p>For scaling down the stabilization window is <em>300</em> seconds (or the value of the
<code>--horizontal-pod-autoscaler-downscale-stabilization</code> command line option, if provided). There is only a single policy
for scaling down which allows a 100% of the currently running replicas to be removed which
means the scaling target can be scaled down to the minimum allowed replicas.
For scaling up there is no stabilization window. When the metrics indicate that the target should be
scaled up the target is scaled up immediately. There are 2 policies where 4 pods or a 100% of the currently
running replicas may at most be added every 15 seconds till the HPA reaches its steady state.</p><h3 id="example-change-downscale-stabilization-window">Example: change downscale stabilization window</h3><p>To provide a custom downscale stabilization window of 1 minute, the following
behavior would be added to the HPA:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>stabilizationWindowSeconds</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><h3 id="example-limit-scale-down-rate">Example: limit scale down rate</h3><p>To limit the rate at which pods are removed by the HPA to 10% per minute, the
following behavior would be added to the HPA:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>policies</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Percent<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p>To ensure that no more than 5 Pods are removed per minute, you can add a second scale-down
policy with a fixed size of 5, and set <code>selectPolicy</code> to minimum. Setting <code>selectPolicy</code> to <code>Min</code> means
that the autoscaler chooses the policy that affects the smallest number of Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>policies</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Percent<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Pods<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>      </span><span>periodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span><span><span><span>    </span><span>selectPolicy</span>:<span> </span>Min<span>
</span></span></span></code></pre></div><h3 id="example-disable-scale-down">Example: disable scale down</h3><p>The <code>selectPolicy</code> value of <code>Disabled</code> turns off scaling the given direction.
So to prevent downscaling the following policy would be used:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>behavior</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleDown</span>:<span>
</span></span></span><span><span><span>    </span><span>selectPolicy</span>:<span> </span>Disabled<span>
</span></span></span></code></pre></div><h2 id="support-for-horizontalpodautoscaler-in-kubectl">Support for HorizontalPodAutoscaler in kubectl</h2><p>HorizontalPodAutoscaler, like every API resource, is supported in a standard way by <code>kubectl</code>.
You can create a new autoscaler using <code>kubectl create</code> command.
You can list autoscalers by <code>kubectl get hpa</code> or get detailed description by <code>kubectl describe hpa</code>.
Finally, you can delete an autoscaler using <code>kubectl delete hpa</code>.</p><p>In addition, there is a special <code>kubectl autoscale</code> command for creating a HorizontalPodAutoscaler object.
For instance, executing <code>kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80</code>
will create an autoscaler for ReplicaSet <em>foo</em>, with target CPU utilization set to <code>80%</code>
and the number of replicas between 2 and 5.</p><h2 id="implicit-maintenance-mode-deactivation">Implicit maintenance-mode deactivation</h2><p>You can implicitly deactivate the HPA for a target without the
need to change the HPA configuration itself. If the target's desired replica count
is set to 0, and the HPA's minimum replica count is greater than 0, the HPA
stops adjusting the target (and sets the <code>ScalingActive</code> Condition on itself
to <code>false</code>) until you reactivate it by manually adjusting the target's desired
replica count or HPA's minimum replica count.</p><h3 id="migrating-deployments-and-statefulsets-to-horizontal-autoscaling">Migrating Deployments and StatefulSets to horizontal autoscaling</h3><p>When an HPA is enabled, it is recommended that the value of <code>spec.replicas</code> of
the Deployment and / or StatefulSet be removed from their
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." href="/docs/reference/glossary/?all=true#term-manifest" target="_blank">manifest(s)</a>. If this isn't done, any time
a change to that object is applied, for example via <code>kubectl apply -f deployment.yaml</code>, this will instruct Kubernetes to scale the current number of Pods
to the value of the <code>spec.replicas</code> key. This may not be
desired and could be troublesome when an HPA is active, resulting in thrashing or flapping behavior.</p><p>Keep in mind that the removal of <code>spec.replicas</code> may incur a one-time
degradation of Pod counts as the default value of this key is 1 (reference
<a href="/docs/concepts/workloads/controllers/deployment/#replicas">Deployment Replicas</a>).
Upon the update, all Pods except 1 will begin their termination procedures. Any
deployment application afterwards will behave as normal and respect a rolling
update configuration as desired. You can avoid this degradation by choosing one of the following two
methods based on how you are modifying your deployments:</p><ul class="nav nav-tabs" id="fix-replicas-instructions"><li class="nav-item"><a class="nav-link active" href="#fix-replicas-instructions-0">Client Side Apply (this is the default)</a></li><li class="nav-item"><a class="nav-link" href="#fix-replicas-instructions-1">Server Side Apply</a></li></ul><div class="tab-content" id="fix-replicas-instructions"><div id="fix-replicas-instructions-0" class="tab-pane show active"><p><ol><li><code>kubectl apply edit-last-applied deployment/&lt;deployment_name&gt;</code></li><li>In the editor, remove <code>spec.replicas</code>. When you save and exit the editor, <code>kubectl</code>
applies the update. No changes to Pod counts happen at this step.</li><li>You can now remove <code>spec.replicas</code> from the manifest. If you use source code management,
also commit your changes or take whatever other steps for revising the source code
are appropriate for how you track updates.</li><li>From here on out you can run <code>kubectl apply -f deployment.yaml</code></li></ol></p></div><div id="fix-replicas-instructions-1" class="tab-pane"><p><p>When using the <a href="/docs/reference/using-api/server-side-apply/">Server-Side Apply</a>
you can follow the <a href="/docs/reference/using-api/server-side-apply/#transferring-ownership">transferring ownership</a>
guidelines, which cover this exact use case.</p></p></div></div><h2 id="what-s-next">What's next</h2><p>If you configure autoscaling in your cluster, you may also want to consider using
<a href="/docs/concepts/cluster-administration/node-autoscaling/">node autoscaling</a>
to ensure you are running the right number of nodes.</p><p>For more information on HorizontalPodAutoscaler:</p><ul><li>Read a <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough example</a> for horizontal pod autoscaling.</li><li>Read documentation for <a href="/docs/reference/generated/kubectl/kubectl-commands/#autoscale"><code>kubectl autoscale</code></a>.</li><li>If you would like to write your own custom metrics adapter, check out the
<a href="https://github.com/kubernetes-sigs/custom-metrics-apiserver">boilerplate</a> to get started.</li><li>Read the <a href="/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/">API reference</a> for HorizontalPodAutoscaler.</li></ul></div></div><div><div class="td-content"><h1>HorizontalPodAutoscaler Walkthrough</h1><p>A <a href="/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a>
(HPA for short)
automatically updates a workload resource (such as
a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> or
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>), with the
aim of automatically scaling the workload to match demand.</p><p>Horizontal scaling means that the response to increased load is to deploy more
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>.
This is different from <em>vertical</em> scaling, which for Kubernetes would mean
assigning more resources (for example: memory or CPU) to the Pods that are already
running for the workload.</p><p>If the load decreases, and the number of Pods is above the configured minimum,
the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet,
or other similar resource) to scale back down.</p><p>This document walks you through an example of enabling HorizontalPodAutoscaler to
automatically manage scale for an example web app. This example workload is Apache
httpd running some PHP code.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.23.<p>To check the version, enter <code>kubectl version</code>.</p>If you're running an older
release of Kubernetes, refer to the version of the documentation for that release (see
<a href="/docs/home/supported-doc-versions/">available documentation versions</a>).</p><p>To follow this walkthrough, you also need to use a cluster that has a
<a href="https://github.com/kubernetes-sigs/metrics-server#readme">Metrics Server</a> deployed and configured.
The Kubernetes Metrics Server collects resource metrics from
the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelets</a> in your cluster, and exposes those metrics
through the <a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a>,
using an <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">APIService</a> to add
new kinds of resource that represent metric readings.</p><p>To learn how to deploy the Metrics Server, see the
<a href="https://github.com/kubernetes-sigs/metrics-server#deployment">metrics-server documentation</a>.</p><p>If you are running <a class="glossary-tooltip" title="A tool for running Kubernetes locally." href="/docs/tasks/tools/#minikube" target="_blank">Minikube</a>, run the following command to enable metrics-server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube addons <span>enable</span> metrics-server
</span></span></code></pre></div><h2 id="run-and-expose-php-apache-server">Run and expose php-apache server</h2><p>To demonstrate a HorizontalPodAutoscaler, you will first start a Deployment that runs a container using the
<code>hpa-example</code> image, and expose it as a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>
using the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/php-apache.yaml"><code>application/php-apache.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/php-apache.yaml to clipboard"></div><div class="includecode" id="application-php-apache-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/hpa-example<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>200m<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>run</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>run</span>:<span> </span>php-apache<span>
</span></span></span></code></pre></div></div></div><p>To do so, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml
</span></span></code></pre></div><pre tabindex="0"><code>deployment.apps/php-apache created
service/php-apache created
</code></pre><h2 id="create-horizontal-pod-autoscaler">Create the HorizontalPodAutoscaler</h2><p>Now that the server is running, create the autoscaler using <code>kubectl</code>. The
<a href="/docs/reference/generated/kubectl/kubectl-commands#autoscale"><code>kubectl autoscale</code></a> subcommand,
part of <code>kubectl</code>, helps you do this.</p><p>You will shortly run a command that creates a HorizontalPodAutoscaler that maintains
between 1 and 10 replicas of the Pods controlled by the php-apache Deployment that
you created in the first step of these instructions.</p><p>Roughly speaking, the HPA <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> will increase and decrease
the number of replicas (by updating the Deployment) to maintain an average CPU utilization across all Pods of 50%.
The Deployment then updates the ReplicaSet - this is part of how all Deployments work in Kubernetes -
and then the ReplicaSet either adds or removes Pods based on the change to its <code>.spec</code>.</p><p>Since each pod requests 200 milli-cores by <code>kubectl run</code>, this means an average CPU usage of 100 milli-cores.
See <a href="/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details">Algorithm details</a> for more details
on the algorithm.</p><p>Create the HorizontalPodAutoscaler:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl autoscale deployment php-apache --cpu-percent<span>=</span><span>50</span> --min<span>=</span><span>1</span> --max<span>=</span><span>10</span>
</span></span></code></pre></div><pre tabindex="0"><code>horizontalpodautoscaler.autoscaling/php-apache autoscaled
</code></pre><p>You can check the current status of the newly-made HorizontalPodAutoscaler, by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># You can use "hpa" or "horizontalpodautoscaler"; either name works OK.</span>
</span></span><span><span>kubectl get hpa
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME         REFERENCE                     TARGET    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%  1         10        1          18s
</code></pre><p>(if you see other HorizontalPodAutoscalers with different names, that means they already existed,
and isn't usually a problem).</p><p>Please note that the current CPU consumption is 0% as there are no clients sending requests to the server
(the <code>TARGET</code> column shows the average across all the Pods controlled by the corresponding deployment).</p><h2 id="increase-load">Increase the load</h2><p>Next, see how the autoscaler reacts to increased load.
To do this, you'll start a different Pod to act as a client. The container within the client Pod
runs in an infinite loop, sending queries to the php-apache service.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this in a separate terminal</span>
</span></span><span><span><span># so that the load generation continues and you can carry on with the rest of the steps</span>
</span></span><span><span>kubectl run -i --tty load-generator --rm --image<span>=</span>busybox:1.28 --restart<span>=</span>Never -- /bin/sh -c <span>"while sleep 0.01; do wget -q -O- http://php-apache; done"</span>
</span></span></code></pre></div><p>Now run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># type Ctrl+C to end the watch when you're ready</span>
</span></span><span><span>kubectl get hpa php-apache --watch
</span></span></code></pre></div><p>Within a minute or so, you should see the higher CPU load; for example:</p><pre tabindex="0"><code>NAME         REFERENCE                     TARGET      MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   305% / 50%  1         10        1          3m
</code></pre><p>and then, more replicas. For example:</p><pre tabindex="0"><code>NAME         REFERENCE                     TARGET      MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   305% / 50%  1         10        7          3m
</code></pre><p>Here, CPU consumption has increased to 305% of the request.
As a result, the Deployment was resized to 7 replicas:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment php-apache
</span></span></code></pre></div><p>You should see the replica count matching the figure from the HorizontalPodAutoscaler</p><pre tabindex="0"><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   7/7      7           7           19m
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>It may take a few minutes to stabilize the number of replicas. Since the amount
of load is not controlled in any way it may happen that the final number of replicas
will differ from this example.</div><h2 id="stop-load">Stop generating load</h2><p>To finish the example, stop sending the load.</p><p>In the terminal where you created the Pod that runs a <code>busybox</code> image, terminate
the load generation by typing <code>&lt;Ctrl&gt; + C</code>.</p><p>Then verify the result state (after a minute or so):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># type Ctrl+C to end the watch when you're ready</span>
</span></span><span><span>kubectl get hpa php-apache --watch
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME         REFERENCE                     TARGET       MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%     1         10        1          11m
</code></pre><p>and the Deployment also shows that it has scaled down:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment php-apache
</span></span></code></pre></div><pre tabindex="0"><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   1/1     1            1           27m
</code></pre><p>Once CPU utilization dropped to 0, the HPA automatically scaled the number of replicas back down to 1.</p><p>Autoscaling the replicas may take a few minutes.</p><h2 id="autoscaling-on-multiple-metrics-and-custom-metrics">Autoscaling on multiple metrics and custom metrics</h2><p>You can introduce additional metrics to use when autoscaling the <code>php-apache</code> Deployment
by making use of the <code>autoscaling/v2</code> API version.</p><p>First, get the YAML of your HorizontalPodAutoscaler in the <code>autoscaling/v2</code> form:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get hpa php-apache -o yaml &gt; /tmp/hpa-v2.yaml
</span></span></code></pre></div><p>Open the <code>/tmp/hpa-v2.yaml</code> file in an editor, and you should see YAML which looks like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>autoscaling/v2<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>HorizontalPodAutoscaler<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleTargetRef</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>  </span><span>minReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>maxReplicas</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>metrics</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Resource<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>      </span><span>target</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>Utilization<span>
</span></span></span><span><span><span>        </span><span>averageUtilization</span>:<span> </span><span>50</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>observedGeneration</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>lastScaleTime</span>:<span> </span>&lt;some-time&gt;<span>
</span></span></span><span><span><span>  </span><span>currentReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>desiredReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>currentMetrics</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Resource<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>      </span><span>current</span>:<span>
</span></span></span><span><span><span>        </span><span>averageUtilization</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>        </span><span>averageValue</span>:<span> </span><span>0</span><span>
</span></span></span></code></pre></div><p>Notice that the <code>targetCPUUtilizationPercentage</code> field has been replaced with an array called <code>metrics</code>.
The CPU utilization metric is a <em>resource metric</em>, since it is represented as a percentage of a resource
specified on pod containers. Notice that you can specify other resource metrics besides CPU. By default,
the only other supported resource metric is <code>memory</code>. These resources do not change names from cluster
to cluster, and should always be available, as long as the <code>metrics.k8s.io</code> API is available.</p><p>You can also specify resource metrics in terms of direct values, instead of as percentages of the
requested value, by using a <code>target.type</code> of <code>AverageValue</code> instead of <code>Utilization</code>, and
setting the corresponding <code>target.averageValue</code> field instead of the <code>target.averageUtilization</code>.</p><pre tabindex="0"><code>  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 500Mi
</code></pre><p>There are two other types of metrics, both of which are considered <em>custom metrics</em>: pod metrics and
object metrics. These metrics may have names which are cluster specific, and require a more
advanced cluster monitoring setup.</p><p>The first of these alternative metric types is <em>pod metrics</em>. These metrics describe Pods, and
are averaged together across Pods and compared with a target value to determine the replica count.
They work much like resource metrics, except that they <em>only</em> support a <code>target</code> type of <code>AverageValue</code>.</p><p>Pod metrics are specified using a metric block like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>Pods<span>
</span></span></span><span><span><span></span><span>pods</span>:<span>
</span></span></span><span><span><span>  </span><span>metric</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>packets-per-second<span>
</span></span></span><span><span><span>  </span><span>target</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>AverageValue<span>
</span></span></span><span><span><span>    </span><span>averageValue</span>:<span> </span>1k<span>
</span></span></span></code></pre></div><p>The second alternative metric type is <em>object metrics</em>. These metrics describe a different
object in the same namespace, instead of describing Pods. The metrics are not necessarily
fetched from the object; they only describe it. Object metrics support <code>target</code> types of
both <code>Value</code> and <code>AverageValue</code>. With <code>Value</code>, the target is compared directly to the returned
metric from the API. With <code>AverageValue</code>, the value returned from the custom metrics API is divided
by the number of Pods before being compared to the target. The following example is the YAML
representation of the <code>requests-per-second</code> metric.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>Object<span>
</span></span></span><span><span><span></span><span>object</span>:<span>
</span></span></span><span><span><span>  </span><span>metric</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>requests-per-second<span>
</span></span></span><span><span><span>  </span><span>describedObject</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>main-route<span>
</span></span></span><span><span><span>  </span><span>target</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Value<span>
</span></span></span><span><span><span>    </span><span>value</span>:<span> </span>2k<span>
</span></span></span></code></pre></div><p>If you provide multiple such metric blocks, the HorizontalPodAutoscaler will consider each metric in turn.
The HorizontalPodAutoscaler will calculate proposed replica counts for each metric, and then choose the
one with the highest replica count.</p><p>For example, if you had your monitoring system collecting metrics about network traffic,
you could update the definition above using <code>kubectl edit</code> to look like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>autoscaling/v2<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>HorizontalPodAutoscaler<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleTargetRef</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>  </span><span>minReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>maxReplicas</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>metrics</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Resource<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>      </span><span>target</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>Utilization<span>
</span></span></span><span><span><span>        </span><span>averageUtilization</span>:<span> </span><span>50</span><span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Pods<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span>
</span></span></span><span><span><span>      </span><span>metric</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>packets-per-second<span>
</span></span></span><span><span><span>      </span><span>target</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>AverageValue<span>
</span></span></span><span><span><span>        </span><span>averageValue</span>:<span> </span>1k<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Object<span>
</span></span></span><span><span><span>    </span><span>object</span>:<span>
</span></span></span><span><span><span>      </span><span>metric</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>requests-per-second<span>
</span></span></span><span><span><span>      </span><span>describedObject</span>:<span>
</span></span></span><span><span><span>        </span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span>        </span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>main-route<span>
</span></span></span><span><span><span>      </span><span>target</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>Value<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>10k<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>observedGeneration</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>lastScaleTime</span>:<span> </span>&lt;some-time&gt;<span>
</span></span></span><span><span><span>  </span><span>currentReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>desiredReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>currentMetrics</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Resource<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>    </span><span>current</span>:<span>
</span></span></span><span><span><span>      </span><span>averageUtilization</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>      </span><span>averageValue</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Object<span>
</span></span></span><span><span><span>    </span><span>object</span>:<span>
</span></span></span><span><span><span>      </span><span>metric</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>requests-per-second<span>
</span></span></span><span><span><span>      </span><span>describedObject</span>:<span>
</span></span></span><span><span><span>        </span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span>        </span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>main-route<span>
</span></span></span><span><span><span>      </span><span>current</span>:<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>10k<span>
</span></span></span></code></pre></div><p>Then, your HorizontalPodAutoscaler would attempt to ensure that each pod was consuming roughly
50% of its requested CPU, serving 1000 packets per second, and that all pods behind the main-route
Ingress were serving a total of 10000 requests per second.</p><h3 id="autoscaling-on-more-specific-metrics">Autoscaling on more specific metrics</h3><p>Many metrics pipelines allow you to describe metrics either by name or by a set of additional
descriptors called <em>labels</em>. For all non-resource metric types (pod, object, and external,
described below), you can specify an additional label selector which is passed to your metric
pipeline. For instance, if you collect a metric <code>http_requests</code> with the <code>verb</code>
label, you can specify the following metric block to scale only on GET requests:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>Object<span>
</span></span></span><span><span><span></span><span>object</span>:<span>
</span></span></span><span><span><span>  </span><span>metric</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>http_requests<span>
</span></span></span><span><span><span>    </span><span>selector</span>:<span> </span>{<span>matchLabels</span>:<span> </span>{<span>verb</span>:<span> </span>GET}}<span>
</span></span></span></code></pre></div><p>This selector uses the same syntax as the full Kubernetes label selectors. The monitoring pipeline
determines how to collapse multiple series into a single value, if the name and selector
match multiple series. The selector is additive, and cannot select metrics
that describe objects that are <strong>not</strong> the target object (the target pods in the case of the <code>Pods</code>
type, and the described object in the case of the <code>Object</code> type).</p><h3 id="autoscaling-on-metrics-not-related-to-kubernetes-objects">Autoscaling on metrics not related to Kubernetes objects</h3><p>Applications running on Kubernetes may need to autoscale based on metrics that don't have an obvious
relationship to any object in the Kubernetes cluster, such as metrics describing a hosted service with
no direct correlation to Kubernetes namespaces. In Kubernetes 1.10 and later, you can address this use case
with <em>external metrics</em>.</p><p>Using external metrics requires knowledge of your monitoring system; the setup is
similar to that required when using custom metrics. External metrics allow you to autoscale your cluster
based on any metric available in your monitoring system. Provide a <code>metric</code> block with a
<code>name</code> and <code>selector</code>, as above, and use the <code>External</code> metric type instead of <code>Object</code>.
If multiple time series are matched by the <code>metricSelector</code>,
the sum of their values is used by the HorizontalPodAutoscaler.
External metrics support both the <code>Value</code> and <code>AverageValue</code> target types, which function exactly the same
as when you use the <code>Object</code> type.</p><p>For example if your application processes tasks from a hosted queue service, you could add the following
section to your HorizontalPodAutoscaler manifest to specify that you need one worker per 30 outstanding tasks.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>- <span>type</span>:<span> </span>External<span>
</span></span></span><span><span><span>  </span><span>external</span>:<span>
</span></span></span><span><span><span>    </span><span>metric</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>queue_messages_ready<span>
</span></span></span><span><span><span>      </span><span>selector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>queue</span>:<span> </span><span>"worker_tasks"</span><span>
</span></span></span><span><span><span>    </span><span>target</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>AverageValue<span>
</span></span></span><span><span><span>      </span><span>averageValue</span>:<span> </span><span>30</span><span>
</span></span></span></code></pre></div><p>When possible, it's preferable to use the custom metric target types instead of external metrics, since it's
easier for cluster administrators to secure the custom metrics API. The external metrics API potentially allows
access to any metric, so cluster administrators should take care when exposing it.</p><h2 id="appendix-horizontal-pod-autoscaler-status-conditions">Appendix: Horizontal Pod Autoscaler Status Conditions</h2><p>When using the <code>autoscaling/v2</code> form of the HorizontalPodAutoscaler, you will be able to see
<em>status conditions</em> set by Kubernetes on the HorizontalPodAutoscaler. These status conditions indicate
whether or not the HorizontalPodAutoscaler is able to scale, and whether or not it is currently restricted
in any way.</p><p>The conditions appear in the <code>status.conditions</code> field. To see the conditions affecting a HorizontalPodAutoscaler,
we can use <code>kubectl describe hpa</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe hpa cm-test
</span></span></code></pre></div><pre tabindex="0"><code>Name:                           cm-test
Namespace:                      prom
Labels:                         &lt;none&gt;
Annotations:                    &lt;none&gt;
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        ( current / target )
  "http_requests" on pods:      66m / 500m
Min replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions:
  Type                  Status  Reason                  Message
  ----                  ------  ------                  -------
  AbleToScale           True    ReadyForNewScale        the last scale time was sufficiently old as to warrant a new scale
  ScalingActive         True    ValidMetricFound        the HPA was able to successfully calculate a replica count from pods metric http_requests
  ScalingLimited        False   DesiredWithinRange      the desired replica count is within the acceptable range
Events:
</code></pre><p>For this HorizontalPodAutoscaler, you can see several conditions in a healthy state. The first,
<code>AbleToScale</code>, indicates whether or not the HPA is able to fetch and update scales, as well as
whether or not any backoff-related conditions would prevent scaling. The second, <code>ScalingActive</code>,
indicates whether or not the HPA is enabled (i.e. the replica count of the target is not zero) and
is able to calculate desired scales. When it is <code>False</code>, it generally indicates problems with
fetching metrics. Finally, the last condition, <code>ScalingLimited</code>, indicates that the desired scale
was capped by the maximum or minimum of the HorizontalPodAutoscaler. This is an indication that
you may wish to raise or lower the minimum or maximum replica count constraints on your
HorizontalPodAutoscaler.</p><h2 id="quantities">Quantities</h2><p>All metrics in the HorizontalPodAutoscaler and metrics APIs are specified using
a special whole-number notation known in Kubernetes as a
<a class="glossary-tooltip" title="A whole-number representation of small or large numbers using SI suffixes." href="/docs/reference/glossary/?all=true#term-quantity" target="_blank">quantity</a>. For example,
the quantity <code>10500m</code> would be written as <code>10.5</code> in decimal notation. The metrics APIs
will return whole numbers without a suffix when possible, and will generally return
quantities in milli-units otherwise. This means you might see your metric value fluctuate
between <code>1</code> and <code>1500m</code>, or <code>1</code> and <code>1.5</code> when written in decimal notation.</p><h2 id="other-possible-scenarios">Other possible scenarios</h2><h3 id="creating-the-autoscaler-declaratively">Creating the autoscaler declaratively</h3><p>Instead of using <code>kubectl autoscale</code> command to create a HorizontalPodAutoscaler imperatively we
can use the following manifest to create it declaratively:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/hpa/php-apache.yaml"><code>application/hpa/php-apache.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/hpa/php-apache.yaml to clipboard"></div><div class="includecode" id="application-hpa-php-apache-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>autoscaling/v2<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>HorizontalPodAutoscaler<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleTargetRef</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>php-apache<span>
</span></span></span><span><span><span>  </span><span>minReplicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>maxReplicas</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>metrics</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>Resource<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>      </span><span>target</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>Utilization<span>
</span></span></span><span><span><span>        </span><span>averageUtilization</span>:<span> </span><span>50</span><span>
</span></span></span></code></pre></div></div></div><p>Then, create the autoscaler by executing the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/application/hpa/php-apache.yaml
</span></span></code></pre></div><pre tabindex="0"><code>horizontalpodautoscaler.autoscaling/php-apache created
</code></pre></div></div><div><div class="td-content"><h1>Specifying a Disruption Budget for your Application</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>This page shows how to limit the number of concurrent disruptions
that your application experiences, allowing for higher availability
while permitting the cluster administrator to manage the clusters
nodes.</p><h2 id="before-you-begin">Before you begin</h2>Your Kubernetes server must be at or later than version v1.21.<p>To check the version, enter <code>kubectl version</code>.</p><ul><li>You are the owner of an application running on a Kubernetes cluster that requires
high availability.</li><li>You should know how to deploy <a href="/docs/tasks/run-application/run-stateless-application-deployment/">Replicated Stateless Applications</a>
and/or <a href="/docs/tasks/run-application/run-replicated-stateful-application/">Replicated Stateful Applications</a>.</li><li>You should have read about <a href="/docs/concepts/workloads/pods/disruptions/">Pod Disruptions</a>.</li><li>You should confirm with your cluster owner or service provider that they respect
Pod Disruption Budgets.</li></ul><h2 id="protecting-an-application-with-a-poddisruptionbudget">Protecting an Application with a PodDisruptionBudget</h2><ol><li>Identify what application you want to protect with a PodDisruptionBudget (PDB).</li><li>Think about how your application reacts to disruptions.</li><li>Create a PDB definition as a YAML file.</li><li>Create the PDB object from the YAML file.</li></ol><h2 id="identify-an-application-to-protect">Identify an Application to Protect</h2><p>The most common use case when you want to protect an application
specified by one of the built-in Kubernetes controllers:</p><ul><li>Deployment</li><li>ReplicationController</li><li>ReplicaSet</li><li>StatefulSet</li></ul><p>In this case, make a note of the controller's <code>.spec.selector</code>; the same
selector goes into the PDBs <code>.spec.selector</code>.</p><p>From version 1.15 PDBs support custom controllers where the
<a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource">scale subresource</a>
is enabled.</p><p>You can also use PDBs with pods which are not controlled by one of the above
controllers, or arbitrary groups of pods, but there are some restrictions,
described in <a href="#arbitrary-controllers-and-selectors">Arbitrary workloads and arbitrary selectors</a>.</p><h2 id="think-about-how-your-application-reacts-to-disruptions">Think about how your application reacts to disruptions</h2><p>Decide how many instances can be down at the same time for a short period
due to a voluntary disruption.</p><ul><li>Stateless frontends:<ul><li>Concern: don't reduce serving capacity by more than 10%.<ul><li>Solution: use PDB with minAvailable 90% for example.</li></ul></li></ul></li><li>Single-instance Stateful Application:<ul><li>Concern: do not terminate this application without talking to me.<ul><li>Possible Solution 1: Do not use a PDB and tolerate occasional downtime.</li><li>Possible Solution 2: Set PDB with maxUnavailable=0. Have an understanding
(outside of Kubernetes) that the cluster operator needs to consult you before
termination. When the cluster operator contacts you, prepare for downtime,
and then delete the PDB to indicate readiness for disruption. Recreate afterwards.</li></ul></li></ul></li><li>Multiple-instance Stateful application such as Consul, ZooKeeper, or etcd:<ul><li>Concern: Do not reduce number of instances below quorum, otherwise writes fail.<ul><li>Possible Solution 1: set maxUnavailable to 1 (works with varying scale of application).</li><li>Possible Solution 2: set minAvailable to quorum-size (e.g. 3 when scale is 5).
(Allows more disruptions at once).</li></ul></li></ul></li><li>Restartable Batch Job:<ul><li>Concern: Job needs to complete in case of voluntary disruption.<ul><li>Possible solution: Do not create a PDB. The Job controller will create a replacement pod.</li></ul></li></ul></li></ul><h3 id="rounding-logic-when-specifying-percentages">Rounding logic when specifying percentages</h3><p>Values for <code>minAvailable</code> or <code>maxUnavailable</code> can be expressed as integers or as a percentage.</p><ul><li>When you specify an integer, it represents a number of Pods. For instance, if you set
<code>minAvailable</code> to 10, then 10 Pods must always be available, even during a disruption.</li><li>When you specify a percentage by setting the value to a string representation of a
percentage (eg. <code>"50%"</code>), it represents a percentage of total Pods. For instance, if
you set <code>minAvailable</code> to <code>"50%"</code>, then at least 50% of the Pods remain available
during a disruption.</li></ul><p>When you specify the value as a percentage, it may not map to an exact number of Pods.
For example, if you have 7 Pods and you set <code>minAvailable</code> to <code>"50%"</code>, it's not
immediately obvious whether that means 3 Pods or 4 Pods must be available. Kubernetes
rounds up to the nearest integer, so in this case, 4 Pods must be available. When you
specify the value <code>maxUnavailable</code> as a percentage, Kubernetes rounds up the number of
Pods that may be disrupted. Thereby a disruption can exceed your defined
<code>maxUnavailable</code> percentage. You can examine the
<a href="https://github.com/kubernetes/kubernetes/blob/23be9587a0f8677eb8091464098881df939c44a9/pkg/controller/disruption/disruption.go#L539">code</a>
that controls this behavior.</p><h2 id="specifying-a-poddisruptionbudget">Specifying a PodDisruptionBudget</h2><p>A <code>PodDisruptionBudget</code> has three fields:</p><ul><li>A label selector <code>.spec.selector</code> to specify the set of
pods to which it applies. This field is required.</li><li><code>.spec.minAvailable</code> which is a description of the number of pods from that
set that must still be available after the eviction, even in the absence
of the evicted pod. <code>minAvailable</code> can be either an absolute number or a percentage.</li><li><code>.spec.maxUnavailable</code> (available in Kubernetes 1.7 and higher) which is a description
of the number of pods from that set that can be unavailable after the eviction.
It can be either an absolute number or a percentage.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The behavior for an empty selector differs between the policy/v1beta1 and policy/v1 APIs for
PodDisruptionBudgets. For policy/v1beta1 an empty selector matches zero pods, while
for policy/v1 an empty selector matches every pod in the namespace.</div><p>You can specify only one of <code>maxUnavailable</code> and <code>minAvailable</code> in a single <code>PodDisruptionBudget</code>.
<code>maxUnavailable</code> can only be used to control the eviction of pods
that all have the same associated controller managing them. In the examples below, "desired replicas"
is the <code>scale</code> of the controller managing the pods being selected by the
<code>PodDisruptionBudget</code>.</p><p>Example 1: With a <code>minAvailable</code> of 5, evictions are allowed as long as they leave behind
5 or more <a href="#healthiness-of-a-pod">healthy</a> pods among those selected by the PodDisruptionBudget's <code>selector</code>.</p><p>Example 2: With a <code>minAvailable</code> of 30%, evictions are allowed as long as at least 30%
of the number of desired replicas are healthy.</p><p>Example 3: With a <code>maxUnavailable</code> of 5, evictions are allowed as long as there are at most 5
unhealthy replicas among the total number of desired replicas.</p><p>Example 4: With a <code>maxUnavailable</code> of 30%, evictions are allowed as long as the number of
unhealthy replicas does not exceed 30% of the total number of desired replica rounded up to
the nearest integer. If the total number of desired replicas is just one, that single replica
is still allowed for disruption, leading to an effective unavailability of 100%.</p><p>In typical usage, a single budget would be used for a collection of pods managed by
a controller&#8212;for example, the pods in a single ReplicaSet or StatefulSet.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A disruption budget does not truly guarantee that the specified
number/percentage of pods will always be up. For example, a node that hosts a
pod from the collection may fail when the collection is at the minimum size
specified in the budget, thus bringing the number of available pods from the
collection below the specified size. The budget can only protect against
voluntary evictions, not all causes of unavailability.</div><p>If you set <code>maxUnavailable</code> to 0% or 0, or you set <code>minAvailable</code> to 100% or the number of replicas,
you are requiring zero voluntary evictions. When you set zero voluntary evictions for a workload
object such as ReplicaSet, then you cannot successfully drain a Node running one of those Pods.
If you try to drain a Node where an unevictable Pod is running, the drain never completes.
This is permitted as per the semantics of <code>PodDisruptionBudget</code>.</p><p>You can find examples of pod disruption budgets defined below. They match pods with the label
<code>app: zookeeper</code>.</p><p>Example PDB Using minAvailable:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/zookeeper-pod-disruption-budget-minavailable.yaml"><code>policy/zookeeper-pod-disruption-budget-minavailable.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy policy/zookeeper-pod-disruption-budget-minavailable.yaml to clipboard"></div><div class="includecode" id="policy-zookeeper-pod-disruption-budget-minavailable-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>policy/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PodDisruptionBudget<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>zk-pdb<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>minAvailable</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>zookeeper<span>
</span></span></span></code></pre></div></div></div><p>Example PDB Using maxUnavailable:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/zookeeper-pod-disruption-budget-maxunavailable.yaml"><code>policy/zookeeper-pod-disruption-budget-maxunavailable.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy policy/zookeeper-pod-disruption-budget-maxunavailable.yaml to clipboard"></div><div class="includecode" id="policy-zookeeper-pod-disruption-budget-maxunavailable-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>policy/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PodDisruptionBudget<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>zk-pdb<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>zookeeper<span>
</span></span></span></code></pre></div></div></div><p>For example, if the above <code>zk-pdb</code> object selects the pods of a StatefulSet of size 3, both
specifications have the exact same meaning. The use of <code>maxUnavailable</code> is recommended as it
automatically responds to changes in the number of replicas of the corresponding controller.</p><h2 id="create-the-pdb-object">Create the PDB object</h2><p>You can create or update the PDB object using kubectl.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f mypdb.yaml
</span></span></code></pre></div><h2 id="check-the-status-of-the-pdb">Check the status of the PDB</h2><p>Use kubectl to check that your PDB is created.</p><p>Assuming you don't actually have pods matching <code>app: zookeeper</code> in your namespace,
then you'll see something like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get poddisruptionbudgets
</span></span></code></pre></div><pre tabindex="0"><code>NAME     MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
zk-pdb   2               N/A               0                     7s
</code></pre><p>If there are matching pods (say, 3), then you would see something like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get poddisruptionbudgets
</span></span></code></pre></div><pre tabindex="0"><code>NAME     MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
zk-pdb   2               N/A               1                     7s
</code></pre><p>The non-zero value for <code>ALLOWED DISRUPTIONS</code> means that the disruption controller has seen the pods,
counted the matching pods, and updated the status of the PDB.</p><p>You can get more information about the status of a PDB with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get poddisruptionbudgets zk-pdb -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>policy/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PodDisruptionBudget<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span></span>&#8230;<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2020-03-04T04:22:56Z"</span><span>
</span></span></span><span><span><span>  </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>zk-pdb<span>
</span></span></span><span><span><span></span>&#8230;<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>currentHealthy</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>desiredHealthy</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>disruptionsAllowed</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>expectedPods</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>observedGeneration</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div><h3 id="healthiness-of-a-pod">Healthiness of a Pod</h3><p>The current implementation considers healthy pods, as pods that have <code>.status.conditions</code>
item with <code>type="Ready"</code> and <code>status="True"</code>.
These pods are tracked via <code>.status.currentHealthy</code> field in the PDB status.</p><h2 id="unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</h2><div class="feature-state-notice feature-stable" title="Feature Gate: PDBUnhealthyPodEvictionPolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>PodDisruptionBudget guarding an application ensures that <code>.status.currentHealthy</code> number of pods
does not fall below the number specified in <code>.status.desiredHealthy</code> by disallowing eviction of healthy pods.
By using <code>.spec.unhealthyPodEvictionPolicy</code>, you can also define the criteria when unhealthy pods
should be considered for eviction. The default behavior when no policy is specified corresponds
to the <code>IfHealthyBudget</code> policy.</p><p>Policies:</p><dl><dt><code>IfHealthyBudget</code></dt><dd>Running pods (<code>.status.phase="Running"</code>), but not yet healthy can be evicted only
if the guarded application is not disrupted (<code>.status.currentHealthy</code> is at least
equal to <code>.status.desiredHealthy</code>).</dd><dd><p>This policy ensures that running pods of an already disrupted application have
the best chance to become healthy. This has negative implications for draining
nodes, which can be blocked by misbehaving applications that are guarded by a PDB.
More specifically applications with pods in <code>CrashLoopBackOff</code> state
(due to a bug or misconfiguration), or pods that are just failing to report the
<code>Ready</code> condition.</p></dd><dt><code>AlwaysAllow</code></dt><dd>Running pods (<code>.status.phase="Running"</code>), but not yet healthy are considered
disrupted and can be evicted regardless of whether the criteria in a PDB is met.</dd><dd><p>This means prospective running pods of a disrupted application might not get a
chance to become healthy. By using this policy, cluster managers can easily evict
misbehaving applications that are guarded by a PDB. More specifically applications
with pods in <code>CrashLoopBackOff</code> state (due to a bug or misconfiguration), or pods
that are just failing to report the <code>Ready</code> condition.</p></dd></dl><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Pods in <code>Pending</code>, <code>Succeeded</code> or <code>Failed</code> phase are always considered for eviction.</div><h2 id="arbitrary-controllers-and-selectors">Arbitrary workloads and arbitrary selectors</h2><p>You can skip this section if you only use PDBs with the built-in
workload resources (Deployment, ReplicaSet, StatefulSet and ReplicationController)
or with <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">custom resources</a>
that implement a <code>scale</code> <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/#advanced-features-and-flexibility">subresource</a>,
and where the PDB selector exactly matches the selector of the Pod's owning resource.</p><p>You can use a PDB with pods controlled by another resource, by an
"operator", or bare pods, but with these restrictions:</p><ul><li>only <code>.spec.minAvailable</code> can be used, not <code>.spec.maxUnavailable</code>.</li><li>only an integer value can be used with <code>.spec.minAvailable</code>, not a percentage.</li></ul><p>It is not possible to use other availability configurations,
because Kubernetes cannot derive a total number of pods without a supported owning resource.</p><p>You can use a selector which selects a subset or superset of the pods belonging to a
workload resource. The eviction API will disallow eviction of any pod covered by multiple PDBs,
so most users will want to avoid overlapping selectors. One reasonable use of overlapping
PDBs is when pods are being transitioned from one PDB to another.</p></div></div><div><div class="td-content"><h1>Accessing the Kubernetes API from a Pod</h1><p>This guide demonstrates how to access the Kubernetes API from within a pod.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="accessing-the-api-from-within-a-pod">Accessing the API from within a Pod</h2><p>When accessing the API from within a Pod, locating and authenticating
to the API server are slightly different to the external client case.</p><p>The easiest way to use the Kubernetes API from a Pod is to use
one of the official <a href="/docs/reference/using-api/client-libraries/">client libraries</a>. These
libraries can automatically discover the API server and authenticate.</p><h3 id="using-official-client-libraries">Using Official Client Libraries</h3><p>From within a Pod, the recommended ways to connect to the Kubernetes API are:</p><ul><li><p>For a Go client, use the official
<a href="https://github.com/kubernetes/client-go/">Go client library</a>.
The <code>rest.InClusterConfig()</code> function handles API host discovery and authentication automatically.
See <a href="https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go">an example here</a>.</p></li><li><p>For a Python client, use the official
<a href="https://github.com/kubernetes-client/python/">Python client library</a>.
The <code>config.load_incluster_config()</code> function handles API host discovery and authentication automatically.
See <a href="https://github.com/kubernetes-client/python/blob/master/examples/in_cluster_config.py">an example here</a>.</p></li><li><p>There are a number of other libraries available, please refer to the
<a href="/docs/reference/using-api/client-libraries/">Client Libraries</a> page.</p></li></ul><p>In each case, the service account credentials of the Pod are used to communicate
securely with the API server.</p><h3 id="directly-accessing-the-rest-api">Directly accessing the REST API</h3><p>While running in a Pod, your container can create an HTTPS URL for the Kubernetes API
server by fetching the <code>KUBERNETES_SERVICE_HOST</code> and <code>KUBERNETES_SERVICE_PORT_HTTPS</code>
environment variables. The API server's in-cluster address is also published to a
Service named <code>kubernetes</code> in the <code>default</code> namespace so that pods may reference
<code>kubernetes.default.svc</code> as a DNS name for the local API server.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes does not guarantee that the API server has a valid certificate for
the hostname <code>kubernetes.default.svc</code>;
however, the control plane <strong>is</strong> expected to present a valid certificate for the
hostname or IP address that <code>$KUBERNETES_SERVICE_HOST</code> represents.</div><p>The recommended way to authenticate to the API server is with a
<a href="/docs/tasks/configure-pod-container/configure-service-account/">service account</a>
credential. By default, a Pod
is associated with a service account, and a credential (token) for that
service account is placed into the filesystem tree of each container in that Pod,
at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>.</p><p>If available, a certificate bundle is placed into the filesystem tree of each
container at <code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code>, and should be
used to verify the serving certificate of the API server.</p><p>Finally, the default namespace to be used for namespaced API operations is placed in a file
at <code>/var/run/secrets/kubernetes.io/serviceaccount/namespace</code> in each container.</p><h3 id="using-kubectl-proxy">Using kubectl proxy</h3><p>If you would like to query the API without an official client library, you can run <code>kubectl proxy</code>
as the <a href="/docs/tasks/inject-data-application/define-command-argument-container/">command</a>
of a new sidecar container in the Pod. This way, <code>kubectl proxy</code> will authenticate
to the API and expose it on the <code>localhost</code> interface of the Pod, so that other containers
in the Pod can use it directly.</p><h3 id="without-using-a-proxy">Without using a proxy</h3><p>It is possible to avoid using the kubectl proxy by passing the authentication token
directly to the API server. The internal certificate secures the connection.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Point to the internal API server hostname</span>
</span></span><span><span><span>APISERVER</span><span>=</span>https://kubernetes.default.svc
</span></span><span><span>
</span></span><span><span><span># Path to ServiceAccount token</span>
</span></span><span><span><span>SERVICEACCOUNT</span><span>=</span>/var/run/secrets/kubernetes.io/serviceaccount
</span></span><span><span>
</span></span><span><span><span># Read this Pod's namespace</span>
</span></span><span><span><span>NAMESPACE</span><span>=</span><span>$(</span>cat <span>${</span><span>SERVICEACCOUNT</span><span>}</span>/namespace<span>)</span>
</span></span><span><span>
</span></span><span><span><span># Read the ServiceAccount bearer token</span>
</span></span><span><span><span>TOKEN</span><span>=</span><span>$(</span>cat <span>${</span><span>SERVICEACCOUNT</span><span>}</span>/token<span>)</span>
</span></span><span><span>
</span></span><span><span><span># Reference the internal certificate authority (CA)</span>
</span></span><span><span><span>CACERT</span><span>=</span><span>${</span><span>SERVICEACCOUNT</span><span>}</span>/ca.crt
</span></span><span><span>
</span></span><span><span><span># Explore the API with TOKEN</span>
</span></span><span><span>curl --cacert <span>${</span><span>CACERT</span><span>}</span> --header <span>"Authorization: Bearer </span><span>${</span><span>TOKEN</span><span>}</span><span>"</span> -X GET <span>${</span><span>APISERVER</span><span>}</span>/api
</span></span></code></pre></div><p>The output will be similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"APIVersions"</span>,
</span></span><span><span>  <span>"versions"</span>: [<span>"v1"</span>],
</span></span><span><span>  <span>"serverAddressByClientCIDRs"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"clientCIDR"</span>: <span>"0.0.0.0/0"</span>,
</span></span><span><span>      <span>"serverAddress"</span>: <span>"10.0.1.149:443"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div></div></div><div><div class="td-content"><h1>Run Jobs</h1><div class="lead">Run Jobs using parallel processing.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Running Automated Tasks with a CronJob</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Coarse Parallel Processing Using a Work Queue</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Fine Parallel Processing Using a Work Queue</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job for Parallel Processing with Static Work Assignment</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/job/parallel-processing-expansion/">Parallel Processing using Expansions</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/job/pod-failure-policy/">Handling retriable and non-retriable pod failures with Pod failure policy</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Running Automated Tasks with a CronJob</h1><p>This page shows how to run automated tasks using Kubernetes <a class="glossary-tooltip" title="A repeating task (a Job) that runs on a regular schedule." href="/docs/concepts/workloads/controllers/cron-jobs/" target="_blank">CronJob</a> object.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li></ul><h2 id="creating-a-cron-job">Creating a CronJob</h2><p>Cron jobs require a config file.
Here is a manifest for a CronJob that runs a simple demonstration task every minute:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml"><code>application/job/cronjob.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/cronjob.yaml to clipboard"></div><div class="includecode" id="application-job-cronjob-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronJob<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>schedule</span>:<span> </span><span>"* * * * *"</span><span>
</span></span></span><span><span><span>  </span><span>jobTemplate</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>template</span>:<span>
</span></span></span><span><span><span>        </span><span>spec</span>:<span>
</span></span></span><span><span><span>          </span><span>containers</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span>            </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>            </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>            </span><span>command</span>:<span>
</span></span></span><span><span><span>            </span>- /bin/sh<span>
</span></span></span><span><span><span>            </span>- -c<span>
</span></span></span><span><span><span>            </span>- date; echo Hello from the Kubernetes cluster<span>
</span></span></span><span><span><span>          </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span></code></pre></div></div></div><p>Run the example CronJob by using this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/application/job/cronjob.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>cronjob.batch/hello created
</code></pre><p>After creating the cron job, get its status using this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get cronjob hello
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        &lt;none&gt;          10s
</code></pre><p>As you can see from the results of the command, the cron job has not scheduled or run any jobs yet.
<a class="glossary-tooltip" title="A verb that is used to track changes to an object in Kubernetes as a stream." href="/docs/reference/using-api/api-concepts/#api-verbs" target="_blank">Watch</a> for the job to be created in around one minute:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get <span>jobs</span> --watch
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               COMPLETIONS   DURATION   AGE
hello-4111706356   0/1                      0s
hello-4111706356   0/1           0s         0s
hello-4111706356   1/1           5s         5s
</code></pre><p>Now you've seen one running job scheduled by the "hello" cron job.
You can stop watching the job and view the cron job again to see that it scheduled the job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get cronjob hello
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        50s             75s
</code></pre><p>You should see that the cron job <code>hello</code> successfully scheduled a job at the time specified in
<code>LAST SCHEDULE</code>. There are currently 0 active jobs, meaning that the job has completed or failed.</p><p>Now, find the pods that the last scheduled job created and view the standard output of one of the pods.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The job name is different from the pod name.</div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Replace "hello-4111706356" with the job name in your system</span>
</span></span><span><span><span>pods</span><span>=</span><span>$(</span>kubectl get pods --selector<span>=</span>job-name<span>=</span>hello-4111706356 --output<span>=</span><span>jsonpath</span><span>={</span>.items<span>[</span>*<span>]</span>.metadata.name<span>}</span><span>)</span>
</span></span></code></pre></div><p>Show the pod log:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs <span>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Fri Feb 22 11:02:09 UTC 2019
Hello from the Kubernetes cluster
</code></pre><h2 id="deleting-a-cron-job">Deleting a CronJob</h2><p>When you don't need a cron job any more, delete it with <code>kubectl delete cronjob &lt;cronjob name&gt;</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete cronjob hello
</span></span></code></pre></div><p>Deleting the cron job removes all the jobs and pods it created and stops it from creating additional jobs.
You can read more about removing jobs in <a href="/docs/concepts/architecture/garbage-collection/">garbage collection</a>.</p></div></div><div><div class="td-content"><h1>Coarse Parallel Processing Using a Work Queue</h1><p>In this example, you will run a Kubernetes Job with multiple parallel
worker processes.</p><p>In this example, as each pod is created, it picks up one unit of work
from a task queue, completes it, deletes it from the queue, and exits.</p><p>Here is an overview of the steps in this example:</p><ol><li><strong>Start a message queue service.</strong> In this example, you use RabbitMQ, but you could use another
one. In practice you would set up a message queue service once and reuse it for many jobs.</li><li><strong>Create a queue, and fill it with messages.</strong> Each message represents one task to be done. In
this example, a message is an integer that we will do a lengthy computation on.</li><li><strong>Start a Job that works on tasks from the queue</strong>. The Job starts several pods. Each pod takes
one task from the message queue, processes it, and exits.</li></ol><h2 id="before-you-begin">Before you begin</h2><p>You should already be familiar with the basic,
non-parallel, use of <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You will need a container image registry where you can upload images to run in your cluster.</p><p>This task example also assumes that you have Docker installed locally.</p><h2 id="starting-a-message-queue-service">Starting a message queue service</h2><p>This example uses RabbitMQ, however, you can adapt the example to use another AMQP-type message service.</p><p>In practice you could set up a message queue service once in a
cluster and reuse it for many jobs, as well as for long-running services.</p><p>Start RabbitMQ as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># make a Service for the StatefulSet to use</span>
</span></span><span><span>kubectl create -f https://kubernetes.io/examples/application/job/rabbitmq/rabbitmq-service.yaml
</span></span></code></pre></div><pre tabindex="0"><code>service "rabbitmq-service" created
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://kubernetes.io/examples/application/job/rabbitmq/rabbitmq-statefulset.yaml
</span></span></code></pre></div><pre tabindex="0"><code>statefulset "rabbitmq" created
</code></pre><h2 id="testing-the-message-queue-service">Testing the message queue service</h2><p>Now, we can experiment with accessing the message queue. We will
create a temporary interactive pod, install some tools on it,
and experiment with queues.</p><p>First create a temporary interactive Pod.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a temporary interactive container</span>
</span></span><span><span>kubectl run -i --tty temp --image ubuntu:22.04
</span></span></code></pre></div><pre tabindex="0"><code>Waiting for pod default/temp-loe07 to be running, status is Pending, pod ready: false
... [ previous line repeats several times .. hit return when it stops ] ...
</code></pre><p>Note that your pod name and command prompt will be different.</p><p>Next install the <code>amqp-tools</code> so you can work with message queues.
The next commands show what you need to run inside the interactive shell in that Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>apt-get update <span>&amp;&amp;</span> apt-get install -y curl ca-certificates amqp-tools python3 dnsutils
</span></span></code></pre></div><p>Later, you will make a container image that includes these packages.</p><p>Next, you will check that you can discover the Service for RabbitMQ:</p><pre tabindex="0"><code># Run these commands inside the Pod
# Note the rabbitmq-service has a DNS name, provided by Kubernetes:
nslookup rabbitmq-service
</code></pre><pre tabindex="0"><code>Server:        10.0.0.10
Address:    10.0.0.10#53

Name:    rabbitmq-service.default.svc.cluster.local
Address: 10.0.147.152
</code></pre><p>(the IP addresses will vary)</p><p>If the kube-dns addon is not set up correctly, the previous step may not work for you.
You can also find the IP address for that Service in an environment variable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># run this check inside the Pod</span>
</span></span><span><span>env | grep RABBITMQ_SERVICE | grep HOST
</span></span></code></pre></div><pre tabindex="0"><code>RABBITMQ_SERVICE_SERVICE_HOST=10.0.147.152
</code></pre><p>(the IP address will vary)</p><p>Next you will verify that you can create a queue, and publish and consume messages.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run these commands inside the Pod</span>
</span></span><span><span><span># In the next line, rabbitmq-service is the hostname where the rabbitmq-service</span>
</span></span><span><span><span># can be reached.  5672 is the standard port for rabbitmq.</span>
</span></span><span><span><span>export</span> <span>BROKER_URL</span><span>=</span>amqp://guest:guest@rabbitmq-service:5672
</span></span><span><span><span># If you could not resolve "rabbitmq-service" in the previous step,</span>
</span></span><span><span><span># then use this command instead:</span>
</span></span><span><span><span>BROKER_URL</span><span>=</span>amqp://guest:guest@<span>$RABBITMQ_SERVICE_SERVICE_HOST</span>:5672
</span></span><span><span>
</span></span><span><span><span># Now create a queue:</span>
</span></span><span><span>
</span></span><span><span>/usr/bin/amqp-declare-queue --url<span>=</span><span>$BROKER_URL</span> -q foo -d
</span></span></code></pre></div><pre tabindex="0"><code>foo
</code></pre><p>Publish one message to the queue:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>/usr/bin/amqp-publish --url<span>=</span><span>$BROKER_URL</span> -r foo -p -b Hello
</span></span><span><span>
</span></span><span><span><span># And get it back.</span>
</span></span><span><span>
</span></span><span><span>/usr/bin/amqp-consume --url<span>=</span><span>$BROKER_URL</span> -q foo -c <span>1</span> cat <span>&amp;&amp;</span> <span>echo</span> 1&gt;&amp;<span>2</span>
</span></span></code></pre></div><pre tabindex="0"><code>Hello
</code></pre><p>In the last command, the <code>amqp-consume</code> tool took one message (<code>-c 1</code>)
from the queue, and passes that message to the standard input of an arbitrary command.
In this case, the program <code>cat</code> prints out the characters read from standard input, and
the echo adds a carriage return so the example is readable.</p><h2 id="fill-the-queue-with-tasks">Fill the queue with tasks</h2><p>Now, fill the queue with some simulated tasks. In this example, the tasks are strings to be
printed.</p><p>In a practice, the content of the messages might be:</p><ul><li>names of files to that need to be processed</li><li>extra flags to the program</li><li>ranges of keys in a database table</li><li>configuration parameters to a simulation</li><li>frame numbers of a scene to be rendered</li></ul><p>If there is large data that is needed in a read-only mode by all pods
of the Job, you typically put that in a shared file system like NFS and mount
that readonly on all the pods, or write the program in the pod so that it can natively read
data from a cluster file system (for example: HDFS).</p><p>For this example, you will create the queue and fill it using the AMQP command line tools.
In practice, you might write a program to fill the queue using an AMQP client library.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Run this on your computer, not in the Pod</span>
</span></span><span><span>/usr/bin/amqp-declare-queue --url<span>=</span><span>$BROKER_URL</span> -q job1  -d
</span></span></code></pre></div><pre tabindex="0"><code>job1
</code></pre><p>Add items to the queue:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> f in apple banana cherry date fig grape lemon melon
</span></span><span><span><span>do</span>
</span></span><span><span>  /usr/bin/amqp-publish --url<span>=</span><span>$BROKER_URL</span> -r job1 -p -b <span>$f</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>You added 8 messages to the queue.</p><h2 id="create-a-container-image">Create a container image</h2><p>Now you are ready to create an image that you will run as a Job.</p><p>The job will use the <code>amqp-consume</code> utility to read the message
from the queue and run the actual work. Here is a very simple
example program:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/rabbitmq/worker.py"><code>application/job/rabbitmq/worker.py</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/rabbitmq/worker.py to clipboard"></div><div class="includecode" id="application-job-rabbitmq-worker-py"><div class="highlight"><pre tabindex="0"><code class="language-python"><span><span><span>#!/usr/bin/env python</span>
</span></span><span><span>
</span></span><span><span><span># Just prints standard out and sleeps for 10 seconds.</span>
</span></span><span><span><span>import</span> <span>sys</span>
</span></span><span><span><span>import</span> <span>time</span>
</span></span><span><span><span>print</span>(<span>"Processing "</span> <span>+</span> sys<span>.</span>stdin<span>.</span>readlines()[<span>0</span>])
</span></span><span><span>time<span>.</span>sleep(<span>10</span>)
</span></span></code></pre></div></div></div><p>Give the script execution permission:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>chmod +x worker.py
</span></span></code></pre></div><p>Now, build an image. Make a temporary directory, change to it,
download the <a href="/examples/application/job/rabbitmq/Dockerfile">Dockerfile</a>,
and <a href="/examples/application/job/rabbitmq/worker.py">worker.py</a>. In either case,
build the image with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>docker build -t job-wq-1 .
</span></span></code></pre></div><p>For the <a href="https://hub.docker.com/">Docker Hub</a>, tag your app image with
your username and push to the Hub with the below commands. Replace
<code>&lt;username&gt;</code> with your Hub username.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>docker tag job-wq-1 &lt;username&gt;/job-wq-1
</span></span><span><span>docker push &lt;username&gt;/job-wq-1
</span></span></code></pre></div><p>If you are using an alternative container image registry, tag the
image and push it there instead.</p><h2 id="defining-a-job">Defining a Job</h2><p>Here is a manifest for a Job. You'll need to make a copy of the Job manifest
(call it <code>./job.yaml</code>),
and edit the name of the container image to match the name you used.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/rabbitmq/job.yaml"><code>application/job/rabbitmq/job.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/rabbitmq/job.yaml to clipboard"></div><div class="includecode" id="application-job-rabbitmq-job-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-wq-1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>8</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>job-wq-1<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>c<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>gcr.io/&lt;project&gt;/job-wq-1<span>
</span></span></span><span><span><span>        </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>BROKER_URL<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span>amqp://guest:guest@rabbitmq-service:5672<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>QUEUE<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span>job1<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span></code></pre></div></div></div><p>In this example, each pod works on one item from the queue and then exits.
So, the completion count of the Job corresponds to the number of work items
done. That is why the example manifest has <code>.spec.completions</code> set to <code>8</code>.</p><h2 id="running-the-job">Running the Job</h2><p>Now, run the Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># this assumes you downloaded and then edited the manifest already</span>
</span></span><span><span>kubectl apply -f ./job.yaml
</span></span></code></pre></div><p>You can wait for the Job to succeed, with a timeout:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># The check for condition name is case insensitive</span>
</span></span><span><span>kubectl <span>wait</span> --for<span>=</span><span>condition</span><span>=</span><span>complete</span> --timeout<span>=</span>300s job/job-wq-1
</span></span></code></pre></div><p>Next, check on the Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe jobs/job-wq-1
</span></span></code></pre></div><pre tabindex="0"><code>Name:             job-wq-1
Namespace:        default
Selector:         controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f
Labels:           controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f
                  job-name=job-wq-1
Annotations:      &lt;none&gt;
Parallelism:      2
Completions:      8
Start Time:       Wed, 06 Sep 2022 16:42:02 +0000
Pods Statuses:    0 Running / 8 Succeeded / 0 Failed
Pod Template:
  Labels:       controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f
                job-name=job-wq-1
  Containers:
   c:
    Image:      container-registry.example/causal-jigsaw-637/job-wq-1
    Port:
    Environment:
      BROKER_URL:       amqp://guest:guest@rabbitmq-service:5672
      QUEUE:            job1
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen  LastSeen   Count    From    SubobjectPath    Type      Reason              Message
  &#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;  &#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;   &#9472;&#9472;&#9472;&#9472;&#9472;    &#9472;&#9472;&#9472;&#9472;    &#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;    &#9472;&#9472;&#9472;&#9472;&#9472;&#9472;    &#9472;&#9472;&#9472;&#9472;&#9472;&#9472;              &#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-hcobb
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-weytj
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-qaam5
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-b67sr
  26s        26s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-xe5hj
  15s        15s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-w2zqe
  14s        14s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-d6ppa
  14s        14s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-p17e0
</code></pre><p>All the pods for that Job succeeded! You're done.</p><h2 id="alternatives">Alternatives</h2><p>This approach has the advantage that you do not need to modify your "worker" program to be
aware that there is a work queue. You can include the worker program unmodified in your container
image.</p><p>Using this approach does require that you run a message queue service.
If running a queue service is inconvenient, you may
want to consider one of the other <a href="/docs/concepts/workloads/controllers/job/#job-patterns">job patterns</a>.</p><p>This approach creates a pod for every work item. If your work items only take a few seconds,
though, creating a Pod for every work item may add a lot of overhead. Consider another
design, such as in the <a href="/docs/tasks/job/fine-parallel-processing-work-queue/">fine parallel work queue example</a>,
that executes multiple work items per Pod.</p><p>In this example, you used the <code>amqp-consume</code> utility to read the message
from the queue and run the actual program. This has the advantage that you
do not need to modify your program to be aware of the queue.
The <a href="/docs/tasks/job/fine-parallel-processing-work-queue/">fine parallel work queue example</a>
shows how to communicate with the work queue using a client library.</p><h2 id="caveats">Caveats</h2><p>If the number of completions is set to less than the number of items in the queue, then
not all items will be processed.</p><p>If the number of completions is set to more than the number of items in the queue,
then the Job will not appear to be completed, even though all items in the queue
have been processed. It will start additional pods which will block waiting
for a message.
You would need to make your own mechanism to spot when there is work
to do and measure the size of the queue, setting the number of completions to match.</p><p>There is an unlikely race with this pattern. If the container is killed in between the time
that the message is acknowledged by the <code>amqp-consume</code> command and the time that the container
exits with success, or if the node crashes before the kubelet is able to post the success of the pod
back to the API server, then the Job will not appear to be complete, even though all items
in the queue have been processed.</p></div></div><div><div class="td-content"><h1>Fine Parallel Processing Using a Work Queue</h1><p>In this example, you will run a Kubernetes Job that runs multiple parallel
tasks as worker processes, each running as a separate Pod.</p><p>In this example, as each pod is created, it picks up one unit of work
from a task queue, processes it, and repeats until the end of the queue is reached.</p><p>Here is an overview of the steps in this example:</p><ol><li><strong>Start a storage service to hold the work queue.</strong> In this example, you will use Redis to store
work items. In the <a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">previous example</a>,
you used RabbitMQ. In this example, you will use Redis and a custom work-queue client library;
this is because AMQP does not provide a good way for clients to
detect when a finite-length work queue is empty. In practice you would set up a store such
as Redis once and reuse it for the work queues of many jobs, and other things.</li><li><strong>Create a queue, and fill it with messages.</strong> Each message represents one task to be done. In
this example, a message is an integer that we will do a lengthy computation on.</li><li><strong>Start a Job that works on tasks from the queue</strong>. The Job starts several pods. Each pod takes
one task from the message queue, processes it, and repeats until the end of the queue is reached.</li></ol><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You will need a container image registry where you can upload images to run in your cluster.
The example uses <a href="https://hub.docker.com/">Docker Hub</a>, but you could adapt it to a different
container image registry.</p><p>This task example also assumes that you have Docker installed locally. You use Docker to
build container images.</p><p>Be familiar with the basic,
non-parallel, use of <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><h2 id="starting-redis">Starting Redis</h2><p>For this example, for simplicity, you will start a single instance of Redis.
See the <a href="https://github.com/kubernetes/examples/tree/master/web/guestbook/">Redis Example</a> for an example
of deploying Redis scalably and redundantly.</p><p>You could also download the following files directly:</p><ul><li><a href="/examples/application/job/redis/redis-pod.yaml"><code>redis-pod.yaml</code></a></li><li><a href="/examples/application/job/redis/redis-service.yaml"><code>redis-service.yaml</code></a></li><li><a href="/examples/application/job/redis/Dockerfile"><code>Dockerfile</code></a></li><li><a href="/examples/application/job/redis/job.yaml"><code>job.yaml</code></a></li><li><a href="/examples/application/job/redis/rediswq.py"><code>rediswq.py</code></a></li><li><a href="/examples/application/job/redis/worker.py"><code>worker.py</code></a></li></ul><p>To start a single instance of Redis, you need to create the redis pod and redis service:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/job/redis/redis-pod.yaml
</span></span><span><span>kubectl apply -f https://k8s.io/examples/application/job/redis/redis-service.yaml
</span></span></code></pre></div><h2 id="filling-the-queue-with-tasks">Filling the queue with tasks</h2><p>Now let's fill the queue with some "tasks". In this example, the tasks are strings to be
printed.</p><p>Start a temporary interactive pod for running the Redis CLI.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run -i --tty temp --image redis --command <span>"/bin/sh"</span>
</span></span></code></pre></div><pre tabindex="0"><code>Waiting for pod default/redis2-c7h78 to be running, status is Pending, pod ready: false
Hit enter for command prompt
</code></pre><p>Now hit enter, start the Redis CLI, and create a list with some work items in it.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>redis-cli -h redis
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>redis:6379&gt; rpush job2 "apple"
</span></span></span><span><span><span>(integer) 1
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "banana"
</span></span></span><span><span><span>(integer) 2
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "cherry"
</span></span></span><span><span><span>(integer) 3
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "date"
</span></span></span><span><span><span>(integer) 4
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "fig"
</span></span></span><span><span><span>(integer) 5
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "grape"
</span></span></span><span><span><span>(integer) 6
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "lemon"
</span></span></span><span><span><span>(integer) 7
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "melon"
</span></span></span><span><span><span>(integer) 8
</span></span></span><span><span><span>redis:6379&gt; rpush job2 "orange"
</span></span></span><span><span><span>(integer) 9
</span></span></span><span><span><span>redis:6379&gt; lrange job2 0 -1
</span></span></span><span><span><span>1) "apple"
</span></span></span><span><span><span>2) "banana"
</span></span></span><span><span><span>3) "cherry"
</span></span></span><span><span><span>4) "date"
</span></span></span><span><span><span>5) "fig"
</span></span></span><span><span><span>6) "grape"
</span></span></span><span><span><span>7) "lemon"
</span></span></span><span><span><span>8) "melon"
</span></span></span><span><span><span>9) "orange"
</span></span></span></code></pre></div><p>So, the list with key <code>job2</code> will be the work queue.</p><p>Note: if you do not have Kube DNS setup correctly, you may need to change
the first step of the above block to <code>redis-cli -h $REDIS_SERVICE_HOST</code>.</p><h2 id="create-an-image">Create a container image</h2><p>Now you are ready to create an image that will process the work in that queue.</p><p>You're going to use a Python worker program with a Redis client to read
the messages from the message queue.</p><p>A simple Redis work queue client library is provided,
called <code>rediswq.py</code> (<a href="/examples/application/job/redis/rediswq.py">Download</a>).</p><p>The "worker" program in each Pod of the Job uses the work queue
client library to get work. Here it is:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/redis/worker.py"><code>application/job/redis/worker.py</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/redis/worker.py to clipboard"></div><div class="includecode" id="application-job-redis-worker-py"><div class="highlight"><pre tabindex="0"><code class="language-python"><span><span><span>#!/usr/bin/env python</span>
</span></span><span><span>
</span></span><span><span><span>import</span> <span>time</span>
</span></span><span><span><span>import</span> <span>rediswq</span>
</span></span><span><span>
</span></span><span><span>host<span>=</span><span>"redis"</span>
</span></span><span><span><span># Uncomment next two lines if you do not have Kube-DNS working.</span>
</span></span><span><span><span># import os</span>
</span></span><span><span><span># host = os.getenv("REDIS_SERVICE_HOST")</span>
</span></span><span><span>
</span></span><span><span>q <span>=</span> rediswq<span>.</span>RedisWQ(name<span>=</span><span>"job2"</span>, host<span>=</span>host)
</span></span><span><span><span>print</span>(<span>"Worker with sessionID: "</span> <span>+</span>  q<span>.</span>sessionID())
</span></span><span><span><span>print</span>(<span>"Initial queue state: empty="</span> <span>+</span> <span>str</span>(q<span>.</span>empty()))
</span></span><span><span><span>while</span> <span>not</span> q<span>.</span>empty():
</span></span><span><span>  item <span>=</span> q<span>.</span>lease(lease_secs<span>=</span><span>10</span>, block<span>=</span><span>True</span>, timeout<span>=</span><span>2</span>) 
</span></span><span><span>  <span>if</span> item <span>is</span> <span>not</span> <span>None</span>:
</span></span><span><span>    itemstr <span>=</span> item<span>.</span>decode(<span>"utf-8"</span>)
</span></span><span><span>    <span>print</span>(<span>"Working on "</span> <span>+</span> itemstr)
</span></span><span><span>    time<span>.</span>sleep(<span>10</span>) <span># Put your actual work here instead of sleep.</span>
</span></span><span><span>    q<span>.</span>complete(item)
</span></span><span><span>  <span>else</span>:
</span></span><span><span>    <span>print</span>(<span>"Waiting for work"</span>)
</span></span><span><span><span>print</span>(<span>"Queue empty, exiting"</span>)
</span></span></code></pre></div></div></div><p>You could also download <a href="/examples/application/job/redis/worker.py"><code>worker.py</code></a>,
<a href="/examples/application/job/redis/rediswq.py"><code>rediswq.py</code></a>, and
<a href="/examples/application/job/redis/Dockerfile"><code>Dockerfile</code></a> files, then build
the container image. Here's an example using Docker to do the image build:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>docker build -t job-wq-2 .
</span></span></code></pre></div><h3 id="push-the-image">Push the image</h3><p>For the <a href="https://hub.docker.com/">Docker Hub</a>, tag your app image with
your username and push to the Hub with the below commands. Replace
<code>&lt;username&gt;</code> with your Hub username.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>docker tag job-wq-2 &lt;username&gt;/job-wq-2
</span></span><span><span>docker push &lt;username&gt;/job-wq-2
</span></span></code></pre></div><p>You need to push to a public repository or <a href="/docs/concepts/containers/images/">configure your cluster to be able to access
your private repository</a>.</p><h2 id="defining-a-job">Defining a Job</h2><p>Here is a manifest for the Job you will create:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/redis/job.yaml"><code>application/job/redis/job.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/redis/job.yaml to clipboard"></div><div class="includecode" id="application-job-redis-job-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-wq-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>job-wq-2<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>c<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>gcr.io/myproject/job-wq-2<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Be sure to edit the manifest to
change <code>gcr.io/myproject</code> to your own path.</div><p>In this example, each pod works on several items from the queue and then exits when there are no more items.
Since the workers themselves detect when the workqueue is empty, and the Job controller does not
know about the workqueue, it relies on the workers to signal when they are done working.
The workers signal that the queue is empty by exiting with success. So, as soon as <strong>any</strong> worker
exits with success, the controller knows the work is done, and that the Pods will exit soon.
So, you need to leave the completion count of the Job unset. The job controller will wait for
the other pods to complete too.</p><h2 id="running-the-job">Running the Job</h2><p>So, now run the Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># this assumes you downloaded and then edited the manifest already</span>
</span></span><span><span>kubectl apply -f ./job.yaml
</span></span></code></pre></div><p>Now wait a bit, then check on the Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe jobs/job-wq-2
</span></span></code></pre></div><pre tabindex="0"><code>Name:             job-wq-2
Namespace:        default
Selector:         controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f
Labels:           controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f
                  job-name=job-wq-2
Annotations:      &lt;none&gt;
Parallelism:      2
Completions:      &lt;unset&gt;
Start Time:       Mon, 11 Jan 2022 17:07:59 +0000
Pods Statuses:    1 Running / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f
                job-name=job-wq-2
  Containers:
   c:
    Image:              container-registry.example/exampleproject/job-wq-2
    Port:
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen    LastSeen    Count    From            SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----            -------------    --------    ------            -------
  33s          33s         1        {job-controller }                Normal      SuccessfulCreate  Created pod: job-wq-2-lglf8
</code></pre><p>You can wait for the Job to succeed, with a timeout:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># The check for condition name is case insensitive</span>
</span></span><span><span>kubectl <span>wait</span> --for<span>=</span><span>condition</span><span>=</span><span>complete</span> --timeout<span>=</span>300s job/job-wq-2
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs pods/job-wq-2-7r7b2
</span></span></code></pre></div><pre tabindex="0"><code>Worker with sessionID: bbd72d0a-9e5c-4dd6-abf6-416cc267991f
Initial queue state: empty=False
Working on banana
Working on date
Working on lemon
</code></pre><p>As you can see, one of the pods for this Job worked on several work units.</p><h2 id="alternatives">Alternatives</h2><p>If running a queue service or modifying your containers to use a work queue is inconvenient, you may
want to consider one of the other
<a href="/docs/concepts/workloads/controllers/job/#job-patterns">job patterns</a>.</p><p>If you have a continuous stream of background processing work to run, then
consider running your background workers with a ReplicaSet instead,
and consider running a background processing library such as
<a href="https://github.com/resque/resque">https://github.com/resque/resque</a>.</p></div></div><div><div class="td-content"><h1>Indexed Job for Parallel Processing with Static Work Assignment</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>In this example, you will run a Kubernetes Job that uses multiple parallel
worker processes.
Each worker is a different container running in its own Pod. The Pods have an
<em>index number</em> that the control plane sets automatically, which allows each Pod
to identify which part of the overall task to work on.</p><p>The pod index is available in the <a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." href="/docs/concepts/overview/working-with-objects/annotations" target="_blank">annotation</a>
<code>batch.kubernetes.io/job-completion-index</code> as a string representing its
decimal value. In order for the containerized task process to obtain this index,
you can publish the value of the annotation using the <a href="/docs/concepts/workloads/pods/downward-api/">downward API</a>
mechanism.
For convenience, the control plane automatically sets the downward API to
expose the index in the <code>JOB_COMPLETION_INDEX</code> environment variable.</p><p>Here is an overview of the steps in this example:</p><ol><li><strong>Define a Job manifest using indexed completion</strong>.
The downward API allows you to pass the pod index annotation as an
environment variable or file to the container.</li><li><strong>Start an <code>Indexed</code> Job based on that manifest</strong>.</li></ol><h2 id="before-you-begin">Before you begin</h2><p>You should already be familiar with the basic,
non-parallel, use of <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.21.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="choose-an-approach">Choose an approach</h2><p>To access the work item from the worker program, you have a few options:</p><ol><li>Read the <code>JOB_COMPLETION_INDEX</code> environment variable. The Job
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>
automatically links this variable to the annotation containing the completion
index.</li><li>Read a file that contains the completion index.</li><li>Assuming that you can't modify the program, you can wrap it with a script
that reads the index using any of the methods above and converts it into
something that the program can use as input.</li></ol><p>For this example, imagine that you chose option 3 and you want to run the
<a href="https://man7.org/linux/man-pages/man1/rev.1.html">rev</a> utility. This
program accepts a file as an argument and prints its content reversed.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>rev data.txt
</span></span></code></pre></div><p>You'll use the <code>rev</code> tool from the
<a href="https://hub.docker.com/_/busybox"><code>busybox</code></a> container image.</p><p>As this is only an example, each Pod only does a tiny piece of work (reversing a short
string). In a real workload you might, for example, create a Job that represents
the
task of producing 60 seconds of video based on scene data.
Each work item in the video rendering Job would be to render a particular
frame of that video clip. Indexed completion would mean that each Pod in
the Job knows which frame to render and publish, by counting frames from
the start of the clip.</p><h2 id="define-an-indexed-job">Define an Indexed Job</h2><p>Here is a sample Job manifest that uses <code>Indexed</code> completion mode:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/indexed-job.yaml"><code>application/job/indexed-job.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/indexed-job.yaml to clipboard"></div><div class="includecode" id="application-job-indexed-job-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>'indexed-job'</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>completionMode</span>:<span> </span>Indexed<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span><span>'input'</span><span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span><span>'docker.io/library/bash'</span><span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- <span>"bash"</span><span>
</span></span></span><span><span><span>        </span>- <span>"-c"</span><span>
</span></span></span><span><span><span>        </span>- |<span>
</span></span></span><span><span><span>          items=(foo bar baz qux xyz)
</span></span></span><span><span><span>          echo ${items[$JOB_COMPLETION_INDEX]} &gt; /input/data.txt</span><span>          
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>mountPath</span>:<span> </span>/input<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>input<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span><span>'worker'</span><span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span><span>'docker.io/library/busybox'</span><span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- <span>"rev"</span><span>
</span></span></span><span><span><span>        </span>- <span>"/input/data.txt"</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>mountPath</span>:<span> </span>/input<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>input<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>input<span>
</span></span></span><span><span><span>        </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span></code></pre></div></div></div><p>In the example above, you use the builtin <code>JOB_COMPLETION_INDEX</code> environment
variable set by the Job controller for all containers. An <a href="/docs/concepts/workloads/pods/init-containers/">init container</a>
maps the index to a static value and writes it to a file that is shared with the
container running the worker through an <a href="/docs/concepts/storage/volumes/#emptydir">emptyDir volume</a>.
Optionally, you can <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">define your own environment variable through the downward
API</a>
to publish the index to containers. You can also choose to load a list of values
from a <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap as an environment variable or file</a>.</p><p>Alternatively, you can directly <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#store-pod-fields">use the downward API to pass the annotation
value as a volume file</a>,
like shown in the following example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/indexed-job-vol.yaml"><code>application/job/indexed-job-vol.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/indexed-job-vol.yaml to clipboard"></div><div class="includecode" id="application-job-indexed-job-vol-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>'indexed-job'</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>completionMode</span>:<span> </span>Indexed<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span><span>'worker'</span><span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span><span>'docker.io/library/busybox'</span><span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- <span>"rev"</span><span>
</span></span></span><span><span><span>        </span>- <span>"/input/data.txt"</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>mountPath</span>:<span> </span>/input<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>input<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>input<span>
</span></span></span><span><span><span>        </span><span>downwardAPI</span>:<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span><span>"data.txt"</span><span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>metadata.annotations['batch.kubernetes.io/job-completion-index']</span></span></code></pre></div></div></div><h2 id="running-the-job">Running the Job</h2><p>Now run the Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This uses the first approach (relying on $JOB_COMPLETION_INDEX)</span>
</span></span><span><span>kubectl apply -f https://kubernetes.io/examples/application/job/indexed-job.yaml
</span></span></code></pre></div><p>When you create this Job, the control plane creates a series of Pods, one for each index you specified. The value of <code>.spec.parallelism</code> determines how many can run at once whereas <code>.spec.completions</code> determines how many Pods the Job creates in total.</p><p>Because <code>.spec.parallelism</code> is less than <code>.spec.completions</code>, the control plane waits for some of the first Pods to complete before starting more of them.</p><p>You can wait for the Job to succeed, with a timeout:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># The check for condition name is case insensitive</span>
</span></span><span><span>kubectl <span>wait</span> --for<span>=</span><span>condition</span><span>=</span><span>complete</span> --timeout<span>=</span>300s job/indexed-job
</span></span></code></pre></div><p>Now, describe the Job and check that it was successful.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe jobs/indexed-job
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>Name:              indexed-job
Namespace:         default
Selector:          controller-uid=bf865e04-0b67-483b-9a90-74cfc4c3e756
Labels:            controller-uid=bf865e04-0b67-483b-9a90-74cfc4c3e756
                   job-name=indexed-job
Annotations:       &lt;none&gt;
Parallelism:       3
Completions:       5
Start Time:        Thu, 11 Mar 2021 15:47:34 +0000
Pods Statuses:     2 Running / 3 Succeeded / 0 Failed
Completed Indexes: 0-2
Pod Template:
  Labels:  controller-uid=bf865e04-0b67-483b-9a90-74cfc4c3e756
           job-name=indexed-job
  Init Containers:
   input:
    Image:      docker.io/library/bash
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      bash
      -c
      items=(foo bar baz qux xyz)
      echo ${items[$JOB_COMPLETION_INDEX]} &gt; /input/data.txt

    Environment:  &lt;none&gt;
    Mounts:
      /input from input (rw)
  Containers:
   worker:
    Image:      docker.io/library/busybox
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      rev
      /input/data.txt
    Environment:  &lt;none&gt;
    Mounts:
      /input from input (rw)
  Volumes:
   input:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  4s    job-controller  Created pod: indexed-job-njkjj
  Normal  SuccessfulCreate  4s    job-controller  Created pod: indexed-job-9kd4h
  Normal  SuccessfulCreate  4s    job-controller  Created pod: indexed-job-qjwsz
  Normal  SuccessfulCreate  1s    job-controller  Created pod: indexed-job-fdhq5
  Normal  SuccessfulCreate  1s    job-controller  Created pod: indexed-job-ncslj
</code></pre><p>In this example, you run the Job with custom values for each index. You can
inspect the output of one of the pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs indexed-job-fdhq5 <span># Change this to match the name of a Pod from that Job</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>xuq
</code></pre></div></div><div><div class="td-content"><h1>Job with Pod-to-Pod Communication</h1><p>In this example, you will run a Job in <a href="/blog/2021/04/19/introducing-indexed-jobs/">Indexed completion mode</a>
configured such that the pods created by the Job can communicate with each other using pod hostnames rather
than pod IP addresses.</p><p>Pods within a Job might need to communicate among themselves. The user workload running in each pod
could query the Kubernetes API server to learn the IPs of the other Pods, but it's much simpler to
rely on Kubernetes' built-in DNS resolution.</p><p>Jobs in Indexed completion mode automatically set the pods' hostname to be in the format of
<code>${jobName}-${completionIndex}</code>. You can use this format to deterministically build
pod hostnames and enable pod communication <em>without</em> needing to create a client connection to
the Kubernetes control plane to obtain pod hostnames/IPs via API requests.</p><p>This configuration is useful for use cases where pod networking is required but you don't want
to depend on a network connection with the Kubernetes API server.</p><h2 id="before-you-begin">Before you begin</h2><p>You should already be familiar with the basic use of <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.21.<p>To check the version, enter <code>kubectl version</code>.</p></p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you are using minikube or a similar tool, you may need to take
<a href="https://minikube.sigs.k8s.io/docs/handbook/addons/ingress-dns/">extra steps</a>
to ensure you have DNS.</div><h2 id="starting-a-job-with-pod-to-pod-communication">Starting a Job with pod-to-pod communication</h2><p>To enable pod-to-pod communication using pod hostnames in a Job, you must do the following:</p><ol><li><p>Set up a <a href="/docs/concepts/services-networking/service/#headless-services">headless Service</a>
with a valid label selector for the pods created by your Job. The headless service must be
in the same namespace as the Job. One easy way to do this is to use the
<code>job-name: &lt;your-job-name&gt;</code> selector, since the <code>job-name</code> label will be automatically added
by Kubernetes. This configuration will trigger the DNS system to create records of the hostnames
of the pods running your Job.</p></li><li><p>Configure the headless service as subdomain service for the Job pods by including the following
value in your Job template spec:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>subdomain</span>:<span> </span>&lt;headless-svc-name&gt;<span>
</span></span></span></code></pre></div></li></ol><h3 id="example">Example</h3><p>Below is a working example of a Job with pod-to-pod communication via pod hostnames enabled.
The Job is completed only after all pods successfully ping each other using hostnames.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the Bash script executed on each pod in the example below, the pod hostnames can be prefixed
by the namespace as well if the pod needs to be reached from outside the namespace.</div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>headless-svc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span>None<span> </span><span># clusterIP must be None to create a headless service</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>job-name</span>:<span> </span>example-job<span> </span><span># must match Job name</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-job<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>completionMode</span>:<span> </span>Indexed<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>subdomain</span>:<span> </span>headless-svc<span> </span><span># has to match Service name</span><span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>example-workload<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>bash:latest<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- bash<span>
</span></span></span><span><span><span>        </span>- -c<span>
</span></span></span><span><span><span>        </span>- |<span>
</span></span></span><span><span><span>          for i in 0 1 2
</span></span></span><span><span><span>          do
</span></span></span><span><span><span>            gotStatus="-1"
</span></span></span><span><span><span>            wantStatus="0"             
</span></span></span><span><span><span>            while [ $gotStatus -ne $wantStatus ]
</span></span></span><span><span><span>            do                                       
</span></span></span><span><span><span>              ping -c 1 example-job-${i}.headless-svc &gt; /dev/null 2&gt;&amp;1
</span></span></span><span><span><span>              gotStatus=$?                
</span></span></span><span><span><span>              if [ $gotStatus -ne $wantStatus ]; then
</span></span></span><span><span><span>                echo "Failed to ping pod example-job-${i}.headless-svc, retrying in 1 second..."
</span></span></span><span><span><span>                sleep 1
</span></span></span><span><span><span>              fi
</span></span></span><span><span><span>            done                                                         
</span></span></span><span><span><span>            echo "Successfully pinged pod: example-job-${i}.headless-svc"
</span></span></span><span><span><span>          done</span><span>          
</span></span></span></code></pre></div><p>After applying the example above, reach each other over the network
using: <code>&lt;pod-hostname&gt;.&lt;headless-service-name&gt;</code>. You should see output similar to the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs example-job-0-qws42
</span></span></code></pre></div><pre tabindex="0"><code>Failed to ping pod example-job-0.headless-svc, retrying in 1 second...
Successfully pinged pod: example-job-0.headless-svc
Successfully pinged pod: example-job-1.headless-svc
Successfully pinged pod: example-job-2.headless-svc
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Keep in mind that the <code>&lt;pod-hostname&gt;.&lt;headless-service-name&gt;</code> name format used
in this example would not work with DNS policy set to <code>None</code> or <code>Default</code>.
Refer to <a href="/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy">Pod's DNS Policy</a>.</div></div></div><div><div class="td-content"><h1>Parallel Processing using Expansions</h1><p>This task demonstrates running multiple <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Jobs</a>
based on a common template. You can use this approach to process batches of work in
parallel.</p><p>For this example there are only three items: <em>apple</em>, <em>banana</em>, and <em>cherry</em>.
The sample Jobs process each item by printing a string then pausing.</p><p>See <a href="#using-jobs-in-real-workloads">using Jobs in real workloads</a> to learn about how
this pattern fits more realistic use cases.</p><h2 id="before-you-begin">Before you begin</h2><p>You should be familiar with the basic,
non-parallel, use of <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>For basic templating you need the command-line utility <code>sed</code>.</p><p>To follow the advanced templating example, you need a working installation of
<a href="https://www.python.org/">Python</a>, and the Jinja2 template
library for Python.</p><p>Once you have Python set up, you can install Jinja2 by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>pip install --user jinja2
</span></span></code></pre></div><h2 id="create-jobs-based-on-a-template">Create Jobs based on a template</h2><p>First, download the following template of a Job to a file called <code>job-tmpl.yaml</code>.
Here's what you'll download:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/job-tmpl.yaml"><code>application/job/job-tmpl.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/job-tmpl.yaml to clipboard"></div><div class="includecode" id="application-job-job-tmpl-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>process-item-$ITEM<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>jobgroup</span>:<span> </span>jobexample<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>jobexample<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>jobgroup</span>:<span> </span>jobexample<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>c<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"echo Processing item $ITEM &amp;&amp; sleep 5"</span>]<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Use curl to download job-tmpl.yaml</span>
</span></span><span><span>curl -L -s -O https://k8s.io/examples/application/job/job-tmpl.yaml
</span></span></code></pre></div><p>The file you downloaded is not yet a valid Kubernetes
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." href="/docs/reference/glossary/?all=true#term-manifest" target="_blank">manifest</a>.
Instead that template is a YAML representation of a Job object with some placeholders
that need to be filled in before it can be used. The <code>$ITEM</code> syntax is not meaningful to Kubernetes.</p><h3 id="create-manifests-from-the-template">Create manifests from the template</h3><p>The following shell snippet uses <code>sed</code> to replace the string <code>$ITEM</code> with the loop
variable, writing into a temporary directory named <code>jobs</code>. Run this now:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Expand the template into multiple files, one for each item to be processed.</span>
</span></span><span><span>mkdir ./jobs
</span></span><span><span><span>for</span> i in apple banana cherry
</span></span><span><span><span>do</span>
</span></span><span><span>  cat job-tmpl.yaml | sed <span>"s/\$ITEM/</span><span>$i</span><span>/"</span> &gt; ./jobs/job-<span>$i</span>.yaml
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>Check if it worked:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>ls jobs/
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>job-apple.yaml
job-banana.yaml
job-cherry.yaml
</code></pre><p>You could use any type of template language (for example: Jinja2; ERB), or
write a program to generate the Job manifests.</p><h3 id="create-jobs-from-the-manifests">Create Jobs from the manifests</h3><p>Next, create all the Jobs with one kubectl command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f ./jobs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>job.batch/process-item-apple created
job.batch/process-item-banana created
job.batch/process-item-cherry created
</code></pre><p>Now, check on the jobs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get <span>jobs</span> -l <span>jobgroup</span><span>=</span>jobexample
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                  COMPLETIONS   DURATION   AGE
process-item-apple    1/1           14s        22s
process-item-banana   1/1           12s        21s
process-item-cherry   1/1           12s        20s
</code></pre><p>Using the <code>-l</code> option to kubectl selects only the Jobs that are part
of this group of jobs (there might be other unrelated jobs in the system).</p><p>You can check on the Pods as well using the same
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">label selector</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>jobgroup</span><span>=</span>jobexample
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME                        READY     STATUS      RESTARTS   AGE
process-item-apple-kixwv    0/1       Completed   0          4m
process-item-banana-wrsf7   0/1       Completed   0          4m
process-item-cherry-dnfu9   0/1       Completed   0          4m
</code></pre><p>We can use this single command to check on the output of all jobs at once:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs -f -l <span>jobgroup</span><span>=</span>jobexample
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code>Processing item apple
Processing item banana
Processing item cherry
</code></pre><h3 id="cleanup-1">Clean up</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Remove the Jobs you created</span>
</span></span><span><span><span># Your cluster automatically cleans up their Pods</span>
</span></span><span><span>kubectl delete job -l <span>jobgroup</span><span>=</span>jobexample
</span></span></code></pre></div><h2 id="use-advanced-template-parameters">Use advanced template parameters</h2><p>In the <a href="#create-jobs-based-on-a-template">first example</a>, each instance of the template had one
parameter, and that parameter was also used in the Job's name. However,
<a href="/docs/concepts/overview/working-with-objects/names/#names">names</a> are restricted
to contain only certain characters.</p><p>This slightly more complex example uses the
<a href="https://palletsprojects.com/p/jinja/">Jinja template language</a> to generate manifests
and then objects from those manifests, with a multiple parameters for each Job.</p><p>For this part of the task, you are going to use a one-line Python script to
convert the template to a set of manifests.</p><p>First, copy and paste the following template of a Job object, into a file called <code>job.yaml.jinja2</code>:</p><pre tabindex="0"><code class="language-liquid">{% set params = [{ "name": "apple", "url": "http://dbpedia.org/resource/Apple", },
                  { "name": "banana", "url": "http://dbpedia.org/resource/Banana", },
                  { "name": "cherry", "url": "http://dbpedia.org/resource/Cherry" }]
%}
{% for p in params %}
{% set name = p["name"] %}
{% set url = p["url"] %}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: jobexample-{{ name }}
  labels:
    jobgroup: jobexample
spec:
  template:
    metadata:
      name: jobexample
      labels:
        jobgroup: jobexample
    spec:
      containers:
      - name: c
        image: busybox:1.28
        command: ["sh", "-c", "echo Processing URL {{ url }} &amp;&amp; sleep 5"]
      restartPolicy: Never
{% endfor %}
</code></pre><p>The above template defines two parameters for each Job object using a list of
python dicts (lines 1-4). A <code>for</code> loop emits one Job manifest for each
set of parameters (remaining lines).</p><p>This example relies on a feature of YAML. One YAML file can contain multiple
documents (Kubernetes manifests, in this case), separated by <code>---</code> on a line
by itself.
You can pipe the output directly to <code>kubectl</code> to create the Jobs.</p><p>Next, use this one-line Python program to expand the template:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>alias</span> <span>render_template</span><span>=</span><span>'python -c "from jinja2 import Template; import sys; print(Template(sys.stdin.read()).render());"'</span>
</span></span></code></pre></div><p>Use <code>render_template</code> to convert the parameters and template into a single
YAML file containing Kubernetes manifests:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This requires the alias you defined earlier</span>
</span></span><span><span>cat job.yaml.jinja2 | render_template &gt; jobs.yaml
</span></span></code></pre></div><p>You can view <code>jobs.yaml</code> to verify that the <code>render_template</code> script worked
correctly.</p><p>Once you are happy that <code>render_template</code> is working how you intend,
you can pipe its output into <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat job.yaml.jinja2 | render_template | kubectl apply -f -
</span></span></code></pre></div><p>Kubernetes accepts and runs the Jobs you created.</p><h3 id="cleanup-2">Clean up</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Remove the Jobs you created</span>
</span></span><span><span><span># Your cluster automatically cleans up their Pods</span>
</span></span><span><span>kubectl delete job -l <span>jobgroup</span><span>=</span>jobexample
</span></span></code></pre></div><h2 id="using-jobs-in-real-workloads">Using Jobs in real workloads</h2><p>In a real use case, each Job performs some substantial computation, such as rendering a frame
of a movie, or processing a range of rows in a database. If you were rendering a movie
you would set <code>$ITEM</code> to the frame number. If you were processing rows from a database
table, you would set <code>$ITEM</code> to represent the range of database rows to process.</p><p>In the task, you ran a command to collect the output from Pods by fetching
their logs. In a real use case, each Pod for a Job writes its output to
durable storage before completing. You can use a PersistentVolume for each Job,
or an external storage service. For example, if you are rendering frames for a movie,
use HTTP to <code>PUT</code> the rendered frame data to a URL, using a different URL for each
frame.</p><h2 id="labels-on-jobs-and-pods">Labels on Jobs and Pods</h2><p>After you create a Job, Kubernetes automatically adds additional
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a> that
distinguish one Job's pods from another Job's pods.</p><p>In this example, each Job and its Pod template have a label:
<code>jobgroup=jobexample</code>.</p><p>Kubernetes itself pays no attention to labels named <code>jobgroup</code>. Setting a label
for all the Jobs you create from a template makes it convenient to operate on all
those Jobs at once.
In the <a href="#create-jobs-based-on-a-template">first example</a> you used a template to
create several Jobs. The template ensures that each Pod also gets the same label, so
you can check on all Pods for these templated Jobs with a single command.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The label key <code>jobgroup</code> is not special or reserved.
You can pick your own labelling scheme.
There are <a href="/docs/concepts/overview/working-with-objects/common-labels/#labels">recommended labels</a>
that you can use if you wish.</div><h2 id="alternatives">Alternatives</h2><p>If you plan to create a large number of Job objects, you may find that:</p><ul><li>Even using labels, managing so many Jobs is cumbersome.</li><li>If you create many Jobs in a batch, you might place high load
on the Kubernetes control plane. Alternatively, the Kubernetes API
server could rate limit you, temporarily rejecting your requests with a 429 status.</li><li>You are limited by a <a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." href="/docs/concepts/policy/resource-quotas/" target="_blank">resource quota</a>
on Jobs: the API server permanently rejects some of your requests
when you create a great deal of work in one batch.</li></ul><p>There are other <a href="/docs/concepts/workloads/controllers/job/#job-patterns">job patterns</a>
that you can use to process large amounts of work without creating very many Job
objects.</p><p>You could also consider writing your own <a href="/docs/concepts/architecture/controller/">controller</a>
to manage Job objects automatically.</p></div></div><div><div class="td-content"><h1>Handling retriable and non-retriable pod failures with Pod failure policy</h1><div class="feature-state-notice feature-stable" title="Feature Gate: JobPodFailurePolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>This document shows you how to use the
<a href="/docs/concepts/workloads/controllers/job/#pod-failure-policy">Pod failure policy</a>,
in combination with the default
<a href="/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy">Pod backoff failure policy</a>,
to improve the control over the handling of container- or Pod-level failure
within a <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Job</a>.</p><p>The definition of Pod failure policy may help you to:</p><ul><li>better utilize the computational resources by avoiding unnecessary Pod retries.</li><li>avoid Job failures due to Pod disruptions (such <a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank">preemption</a>,
<a class="glossary-tooltip" title="API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination." href="/docs/concepts/scheduling-eviction/api-eviction/" target="_blank">API-initiated eviction</a>
or <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taint</a>-based eviction).</li></ul><h2 id="before-you-begin">Before you begin</h2><p>You should already be familiar with the basic use of <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.25.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="usage-scenarios">Usage scenarios</h2><p>Consider the following usage scenarios for Jobs that define a Pod failure policy :</p><ul><li><a href="#pod-failure-policy-failjob">Avoiding unnecessary Pod retries</a></li><li><a href="#pod-failure-policy-ignore">Ignoring Pod disruptions</a></li><li><a href="#pod-failure-policy-config-issue">Avoiding unnecessary Pod retries based on custom Pod Conditions</a></li><li><a href="#backoff-limit-per-index-failindex">Avoiding unnecessary Pod retries per index</a></li></ul><h3 id="pod-failure-policy-failjob">Using Pod failure policy to avoid unnecessary Pod retries</h3><p>With the following example, you can learn how to use Pod failure policy to
avoid unnecessary Pod restarts when a Pod failure indicates a non-retriable
software bug.</p><ol><li><p>Examine the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-failjob.yaml"><code>/controllers/job-pod-failure-policy-failjob.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-pod-failure-policy-failjob.yaml to clipboard"></div><div class="includecode" id="controllers-job-pod-failure-policy-failjob-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-pod-failure-policy-failjob<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>8</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>docker.io/library/bash:5<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"bash"</span>]<span>
</span></span></span><span><span><span>        </span><span>args</span>:<span>
</span></span></span><span><span><span>        </span>- -c<span>
</span></span></span><span><span><span>        </span>- echo "Hello world! I'm going to exit with 42 to simulate a software bug." &amp;&amp; sleep 30 &amp;&amp; exit 42<span>
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>6</span><span>
</span></span></span><span><span><span>  </span><span>podFailurePolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>action</span>:<span> </span>FailJob<span>
</span></span></span><span><span><span>      </span><span>onExitCodes</span>:<span>
</span></span></span><span><span><span>        </span><span>containerName</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span> </span>[<span>42</span>]<span>
</span></span></span></code></pre></div></div></div></li><li><p>Apply the manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create -f https://k8s.io/examples/controllers/job-pod-failure-policy-failjob.yaml
</span></span></code></pre></div></li><li><p>After around 30 seconds the entire Job should be terminated. Inspect the status of the Job by running:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get <span>jobs</span> -l job-name<span>=</span>job-pod-failure-policy-failjob -o yaml
</span></span></code></pre></div><p>In the Job status, the following conditions display:</p><ul><li><code>FailureTarget</code> condition: has a <code>reason</code> field set to <code>PodFailurePolicy</code> and
a <code>message</code> field with more information about the termination, like
<code>Container main for pod default/job-pod-failure-policy-failjob-8ckj8 failed with exit code 42 matching FailJob rule at index 0</code>.
The Job controller adds this condition as soon as the Job is considered a failure.
For details, see <a href="/docs/concepts/workloads/controllers/job/#termination-of-job-pods">Termination of Job Pods</a>.</li><li><code>Failed</code> condition: same <code>reason</code> and <code>message</code> as the <code>FailureTarget</code>
condition. The Job controller adds this condition after all of the Job's Pods
are terminated.</li></ul><p>For comparison, if the Pod failure policy was disabled it would take 6 retries
of the Pod, taking at least 2 minutes.</p></li></ol><h4 id="clean-up">Clean up</h4><p>Delete the Job you created:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete jobs/job-pod-failure-policy-failjob
</span></span></code></pre></div><p>The cluster automatically cleans up the Pods.</p><h3 id="pod-failure-policy-ignore">Using Pod failure policy to ignore Pod disruptions</h3><p>With the following example, you can learn how to use Pod failure policy to
ignore Pod disruptions from incrementing the Pod retry counter towards the
<code>.spec.backoffLimit</code> limit.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Timing is important for this example, so you may want to read the steps before
execution. In order to trigger a Pod disruption it is important to drain the
node while the Pod is running on it (within 90s since the Pod is scheduled).</div><ol><li><p>Examine the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-ignore.yaml"><code>/controllers/job-pod-failure-policy-ignore.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-pod-failure-policy-ignore.yaml to clipboard"></div><div class="includecode" id="controllers-job-pod-failure-policy-ignore-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-pod-failure-policy-ignore<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>4</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>docker.io/library/bash:5<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"bash"</span>]<span>
</span></span></span><span><span><span>        </span><span>args</span>:<span>
</span></span></span><span><span><span>        </span>- -c<span>
</span></span></span><span><span><span>        </span>- echo "Hello world! I'm going to exit with 0 (success)." &amp;&amp; sleep 90 &amp;&amp; exit 0<span>
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>  </span><span>podFailurePolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>action</span>:<span> </span>Ignore<span>
</span></span></span><span><span><span>      </span><span>onPodConditions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>type</span>:<span> </span>DisruptionTarget<span>
</span></span></span></code></pre></div></div></div></li><li><p>Apply the manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create -f https://k8s.io/examples/controllers/job-pod-failure-policy-ignore.yaml
</span></span></code></pre></div></li><li><p>Run this command to check the <code>nodeName</code> the Pod is scheduled to:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span>nodeName</span><span>=</span><span>$(</span>kubectl get pods -l job-name<span>=</span>job-pod-failure-policy-ignore -o <span>jsonpath</span><span>=</span><span>'{.items[0].spec.nodeName}'</span><span>)</span>
</span></span></code></pre></div></li><li><p>Drain the node to evict the Pod before it completes (within 90s):</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl drain nodes/<span>$nodeName</span> --ignore-daemonsets --grace-period<span>=</span><span>0</span>
</span></span></code></pre></div></li><li><p>Inspect the <code>.status.failed</code> to check the counter for the Job is not incremented:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get <span>jobs</span> -l job-name<span>=</span>job-pod-failure-policy-ignore -o yaml
</span></span></code></pre></div></li><li><p>Uncordon the node:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl uncordon nodes/<span>$nodeName</span>
</span></span></code></pre></div></li></ol><p>The Job resumes and succeeds.</p><p>For comparison, if the Pod failure policy was disabled the Pod disruption would
result in terminating the entire Job (as the <code>.spec.backoffLimit</code> is set to 0).</p><h4 id="cleaning-up">Cleaning up</h4><p>Delete the Job you created:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete jobs/job-pod-failure-policy-ignore
</span></span></code></pre></div><p>The cluster automatically cleans up the Pods.</p><h3 id="pod-failure-policy-config-issue">Using Pod failure policy to avoid unnecessary Pod retries based on custom Pod Conditions</h3><p>With the following example, you can learn how to use Pod failure policy to
avoid unnecessary Pod restarts based on custom Pod Conditions.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The example below works since version 1.27 as it relies on transitioning of
deleted pods, in the <code>Pending</code> phase, to a terminal phase
(see: <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">Pod Phase</a>).</div><ol><li><p>Examine the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-config-issue.yaml"><code>/controllers/job-pod-failure-policy-config-issue.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-pod-failure-policy-config-issue.yaml to clipboard"></div><div class="includecode" id="controllers-job-pod-failure-policy-config-issue-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-pod-failure-policy-config-issue<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>8</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span><span>"non-existing-repo/non-existing-image:example"</span><span>
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>6</span><span>
</span></span></span><span><span><span>  </span><span>podFailurePolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>action</span>:<span> </span>FailJob<span>
</span></span></span><span><span><span>      </span><span>onPodConditions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>type</span>:<span> </span>ConfigIssue<span>
</span></span></span></code></pre></div></div></div></li><li><p>Apply the manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create -f https://k8s.io/examples/controllers/job-pod-failure-policy-config-issue.yaml
</span></span></code></pre></div><p>Note that, the image is misconfigured, as it does not exist.</p></li><li><p>Inspect the status of the job's Pods by running:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get pods -l job-name<span>=</span>job-pod-failure-policy-config-issue -o yaml
</span></span></code></pre></div><p>You will see output similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>containerStatuses</span>:<span>
</span></span></span><span><span><span></span>- <span>image</span>:<span> </span>non-existing-repo/non-existing-image:example<span>
</span></span></span><span><span><span>   </span>...<span>
</span></span></span><span><span><span>   </span><span>state</span>:<span>
</span></span></span><span><span><span>   </span><span>waiting</span>:<span>
</span></span></span><span><span><span>      </span><span>message</span>:<span> </span>Back-off pulling image "non-existing-repo/non-existing-image:example"<span>
</span></span></span><span><span><span>      </span><span>reason</span>:<span> </span>ImagePullBackOff<span>
</span></span></span><span><span><span>      </span>...<span>
</span></span></span><span><span><span></span><span>phase</span>:<span> </span>Pending<span>
</span></span></span></code></pre></div><p>Note that the pod remains in the <code>Pending</code> phase as it fails to pull the
misconfigured image. This, in principle, could be a transient issue and the
image could get pulled. However, in this case, the image does not exist so
we indicate this fact by a custom condition.</p></li><li><p>Add the custom condition. First prepare the patch by running:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>cat <span>&lt;&lt;EOF &gt; patch.yaml
</span></span></span><span><span><span>status:
</span></span></span><span><span><span>  conditions:
</span></span></span><span><span><span>  - type: ConfigIssue
</span></span></span><span><span><span>    status: "True"
</span></span></span><span><span><span>    reason: "NonExistingImage"
</span></span></span><span><span><span>    lastTransitionTime: "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Second, select one of the pods created by the job by running:</p><pre tabindex="0"><code>podName=$(kubectl get pods -l job-name=job-pod-failure-policy-config-issue -o jsonpath='{.items[0].metadata.name}')
</code></pre><p>Then, apply the patch on one of the pods by running the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl patch pod <span>$podName</span> --subresource<span>=</span>status --patch-file<span>=</span>patch.yaml
</span></span></code></pre></div><p>If applied successfully, you will get a notification like this:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>pod/job-pod-failure-policy-config-issue-k6pvp patched
</span></span></code></pre></div></li><li><p>Delete the pod to transition it to <code>Failed</code> phase, by running the command:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete pods/<span>$podName</span>
</span></span></code></pre></div></li><li><p>Inspect the status of the Job by running:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get <span>jobs</span> -l job-name<span>=</span>job-pod-failure-policy-config-issue -o yaml
</span></span></code></pre></div><p>In the Job status, see a job <code>Failed</code> condition with the field <code>reason</code>
equal <code>PodFailurePolicy</code>. Additionally, the <code>message</code> field contains a
more detailed information about the Job termination, such as:
<code>Pod default/job-pod-failure-policy-config-issue-k6pvp has condition ConfigIssue matching FailJob rule at index 0</code>.</p></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In a production environment, the steps 3 and 4 should be automated by a
user-provided controller.</div><h4 id="cleaning-up-1">Cleaning up</h4><p>Delete the Job you created:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete jobs/job-pod-failure-policy-config-issue
</span></span></code></pre></div><p>The cluster automatically cleans up the Pods.</p><h3 id="backoff-limit-per-index-failindex">Using Pod Failure Policy to avoid unnecessary Pod retries per index</h3><p>To avoid unnecessary Pod restarts per index, you can use the <em>Pod failure policy</em> and
<em>backoff limit per index</em> features. This section of the page shows how to use these features
together.</p><ol><li><p>Examine the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-backoff-limit-per-index-failindex.yaml"><code>/controllers/job-backoff-limit-per-index-failindex.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-backoff-limit-per-index-failindex.yaml to clipboard"></div><div class="includecode" id="controllers-job-backoff-limit-per-index-failindex-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-backoff-limit-per-index-failindex<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>4</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>completionMode</span>:<span> </span>Indexed<span>
</span></span></span><span><span><span>  </span><span>backoffLimitPerIndex</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>docker.io/library/python:3<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>          </span><span># The script:</span><span>
</span></span></span><span><span><span>          </span><span># - fails the Pod with index 0 with exit code 1, which results in one retry;</span><span>
</span></span></span><span><span><span>          </span><span># - fails the Pod with index 1 with exit code 42 which results</span><span>
</span></span></span><span><span><span>          </span><span>#   in failing the index without retry.</span><span>
</span></span></span><span><span><span>          </span><span># - succeeds Pods with any other index.</span><span>
</span></span></span><span><span><span>          </span>- python3<span>
</span></span></span><span><span><span>          </span>- -c<span>
</span></span></span><span><span><span>          </span>- |<span>
</span></span></span><span><span><span>            import os, sys
</span></span></span><span><span><span>            index = int(os.environ.get("JOB_COMPLETION_INDEX"))
</span></span></span><span><span><span>            if index == 0:
</span></span></span><span><span><span>              sys.exit(1)
</span></span></span><span><span><span>            elif index == 1:
</span></span></span><span><span><span>              sys.exit(42)
</span></span></span><span><span><span>            else:
</span></span></span><span><span><span>              sys.exit(0)</span><span>            
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>6</span><span>
</span></span></span><span><span><span>  </span><span>podFailurePolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>action</span>:<span> </span>FailIndex<span>
</span></span></span><span><span><span>      </span><span>onExitCodes</span>:<span>
</span></span></span><span><span><span>        </span><span>containerName</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span> </span>[<span>42</span>]<span>
</span></span></span></code></pre></div></div></div></li><li><p>Apply the manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create -f https://k8s.io/examples/controllers/job-backoff-limit-per-index-failindex.yaml
</span></span></code></pre></div></li><li><p>After around 15 seconds, inspect the status of the Pods for the Job. You can do that by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l job-name<span>=</span>job-backoff-limit-per-index-failindex -o yaml
</span></span></code></pre></div><p>You will see output similar to this:</p><pre tabindex="0"><code class="language-none">NAME                                            READY   STATUS      RESTARTS   AGE
job-backoff-limit-per-index-failindex-0-4g4cm   0/1     Error       0          4s
job-backoff-limit-per-index-failindex-0-fkdzq   0/1     Error       0          15s
job-backoff-limit-per-index-failindex-1-2bgdj   0/1     Error       0          15s
job-backoff-limit-per-index-failindex-2-vs6lt   0/1     Completed   0          11s
job-backoff-limit-per-index-failindex-3-s7s47   0/1     Completed   0          6s
</code></pre><p>Note that the output shows the following:</p><ul><li>Two Pods have index 0, because of the backoff limit allowed for one retry
of the index.</li><li>Only one Pod has index 1, because the exit code of the failed Pod matched
the Pod failure policy with the <code>FailIndex</code> action.</li></ul></li><li><p>Inspect the status of the Job by running:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get <span>jobs</span> -l job-name<span>=</span>job-backoff-limit-per-index-failindex -o yaml
</span></span></code></pre></div><p>In the Job status, see that the <code>failedIndexes</code> field shows "0,1", because
both indexes failed. Because the index 1 was not retried the number of failed
Pods, indicated by the status field "failed" equals 3.</p></li></ol><h4 id="cleaning-up-2">Cleaning up</h4><p>Delete the Job you created:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete jobs/job-backoff-limit-per-index-failindex
</span></span></code></pre></div><p>The cluster automatically cleans up the Pods.</p><h2 id="alternatives">Alternatives</h2><p>You could rely solely on the
<a href="/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy">Pod backoff failure policy</a>,
by specifying the Job's <code>.spec.backoffLimit</code> field. However, in many situations
it is problematic to find a balance between setting a low value for <code>.spec.backoffLimit</code>
to avoid unnecessary Pod retries, yet high enough to make sure the Job would
not be terminated by Pod disruptions.</p></div></div><div><div class="td-content"><h1>Access Applications in a Cluster</h1><div class="lead">Configure load balancing, port forwarding, or setup firewall or DNS configurations to access applications in a cluster.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/web-ui-dashboard/">Deploy and Access the Kubernetes Dashboard</a></h5><p>Deploy the web UI (Kubernetes Dashboard) and access it.</p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/access-cluster/">Accessing Clusters</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Use Port Forwarding to Access Applications in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/connecting-frontend-backend/">Connect a Frontend to a Backend Using Services</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/create-external-load-balancer/">Create an External Load Balancer</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/list-all-running-container-images/">List All Container Images Running in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/ingress-minikube/">Set up Ingress on Minikube with the NGINX Ingress Controller</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">Communicate Between Containers in the Same Pod Using a Shared Volume</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/configure-dns-cluster/">Configure DNS for a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/access-cluster-services/">Access Services Running on Clusters</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Deploy and Access the Kubernetes Dashboard</h1><div class="lead">Deploy the web UI (Kubernetes Dashboard) and access it.</div><p>Dashboard is a web-based Kubernetes user interface.
You can use Dashboard to deploy containerized applications to a Kubernetes cluster,
troubleshoot your containerized application, and manage the cluster resources.
You can use Dashboard to get an overview of applications running on your cluster,
as well as for creating or modifying individual Kubernetes resources
(such as Deployments, Jobs, DaemonSets, etc).
For example, you can scale a Deployment, initiate a rolling update, restart a pod
or deploy new applications using a deploy wizard.</p><p>Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.</p><p><img alt="Kubernetes Dashboard UI" src="/images/docs/ui-dashboard.png"></p><h2 id="deploying-the-dashboard-ui">Deploying the Dashboard UI</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes Dashboard supports only Helm-based installation currently as it is faster
and gives us better control over all dependencies required by Dashboard to run.</div><p>The Dashboard UI is not deployed by default. To deploy it, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Add kubernetes-dashboard repository</span>
</span></span><span><span>helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
</span></span><span><span><span># Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart</span>
</span></span><span><span>helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
</span></span></code></pre></div><h2 id="accessing-the-dashboard-ui">Accessing the Dashboard UI</h2><p>To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default.
Currently, Dashboard only supports logging in with a Bearer Token.
To create a token for this demo, you can follow our guide on
<a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">creating a sample user</a>.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>The sample user created in the tutorial will have administrative privileges and is for educational purposes only.</div><h3 id="command-line-proxy">Command line proxy</h3><p>You can enable access to the Dashboard using the <code>kubectl</code> command-line tool,
by running the following command:</p><pre tabindex="0"><code>kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
</code></pre><p>Kubectl will make Dashboard available at <a href="https://localhost:8443">https://localhost:8443</a>.</p><p>The UI can <em>only</em> be accessed from the machine where the command is executed. See <code>kubectl port-forward --help</code> for more options.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubeconfig authentication method does <strong>not</strong> support external identity providers
or X.509 certificate-based authentication.</div><h2 id="welcome-view">Welcome view</h2><p>When you access Dashboard on an empty cluster, you'll see the welcome page.
This page contains a link to this document as well as a button to deploy your first application.
In addition, you can view which system applications are running by default in the <code>kube-system</code>
<a href="/docs/tasks/administer-cluster/namespaces/">namespace</a> of your cluster, for example the Dashboard itself.</p><p><img alt="Kubernetes Dashboard welcome page" src="/images/docs/ui-dashboard-zerostate.png"></p><h2 id="deploying-containerized-applications">Deploying containerized applications</h2><p>Dashboard lets you create and deploy a containerized application as a Deployment and optional Service with a simple wizard.
You can either manually specify application details, or upload a YAML or JSON <em>manifest</em> file containing application configuration.</p><p>Click the <strong>CREATE</strong> button in the upper right corner of any page to begin.</p><h3 id="specifying-application-details">Specifying application details</h3><p>The deploy wizard expects that you provide the following information:</p><ul><li><p><strong>App name</strong> (mandatory): Name for your application.
A <a href="/docs/concepts/overview/working-with-objects/labels/">label</a> with the name will be
added to the Deployment and Service, if any, that will be deployed.</p><p>The application name must be unique within the selected Kubernetes <a href="/docs/tasks/administer-cluster/namespaces/">namespace</a>.
It must start with a lowercase character, and end with a lowercase character or a number,
and contain only lowercase letters, numbers and dashes (-). It is limited to 24 characters.
Leading and trailing spaces are ignored.</p></li><li><p><strong>Container image</strong> (mandatory):
The URL of a public Docker <a href="/docs/concepts/containers/images/">container image</a> on any registry,
or a private image (commonly hosted on the Google Container Registry or Docker Hub).
The container image specification must end with a colon.</p></li><li><p><strong>Number of pods</strong> (mandatory): The target number of Pods you want your application to be deployed in.
The value must be a positive integer.</p><p>A <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> will be created to
maintain the desired number of Pods across your cluster.</p></li><li><p><strong>Service</strong> (optional): For some parts of your application (e.g. frontends) you may want to expose a
<a href="/docs/concepts/services-networking/service/">Service</a> onto an external,
maybe public IP address outside of your cluster (external Service).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For external Services, you may need to open up one or more ports to do so.</div><p>Other Services that are only visible from inside the cluster are called internal Services.</p><p>Irrespective of the Service type, if you choose to create a Service and your container listens
on a port (incoming), you need to specify two ports.
The Service will be created mapping the port (incoming) to the target port seen by the container.
This Service will route to your deployed Pods. Supported protocols are TCP and UDP.
The internal DNS name for this Service will be the value you specified as application name above.</p></li></ul><p>If needed, you can expand the <strong>Advanced options</strong> section where you can specify more settings:</p><ul><li><p><strong>Description</strong>: The text you enter here will be added as an
<a href="/docs/concepts/overview/working-with-objects/annotations/">annotation</a>
to the Deployment and displayed in the application's details.</p></li><li><p><strong>Labels</strong>: Default <a href="/docs/concepts/overview/working-with-objects/labels/">labels</a> to be used
for your application are application name and version.
You can specify additional labels to be applied to the Deployment, Service (if any), and Pods,
such as release, environment, tier, partition, and release track.</p><p>Example:</p><pre tabindex="0"><code class="language-conf">release=1.0
tier=frontend
environment=pod
track=stable
</code></pre></li><li><p><strong>Namespace</strong>: Kubernetes supports multiple virtual clusters backed by the same physical cluster.
These virtual clusters are called <a href="/docs/tasks/administer-cluster/namespaces/">namespaces</a>.
They let you partition resources into logically named groups.</p><p>Dashboard offers all available namespaces in a dropdown list, and allows you to create a new namespace.
The namespace name may contain a maximum of 63 alphanumeric characters and dashes (-) but can not contain capital letters.
Namespace names should not consist of only numbers.
If the name is set as a number, such as 10, the pod will be put in the default namespace.</p><p>In case the creation of the namespace is successful, it is selected by default.
If the creation fails, the first namespace is selected.</p></li><li><p><strong>Image Pull Secret</strong>:
In case the specified Docker container image is private, it may require
<a href="/docs/concepts/configuration/secret/">pull secret</a> credentials.</p><p>Dashboard offers all available secrets in a dropdown list, and allows you to create a new secret.
The secret name must follow the DNS domain name syntax, for example <code>new.image-pull.secret</code>.
The content of a secret must be base64-encoded and specified in a
<a href="/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"><code>.dockercfg</code></a> file.
The secret name may consist of a maximum of 253 characters.</p><p>In case the creation of the image pull secret is successful, it is selected by default. If the creation fails, no secret is applied.</p></li><li><p><strong>CPU requirement (cores)</strong> and <strong>Memory requirement (MiB)</strong>:
You can specify the minimum <a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">resource limits</a>
for the container. By default, Pods run with unbounded CPU and memory limits.</p></li><li><p><strong>Run command</strong> and <strong>Run command arguments</strong>:
By default, your containers run the specified Docker image's default
<a href="/docs/tasks/inject-data-application/define-command-argument-container/">entrypoint command</a>.
You can use the command options and arguments to override the default.</p></li><li><p><strong>Run as privileged</strong>: This setting determines whether processes in
<a href="/docs/concepts/workloads/pods/#privileged-mode-for-containers">privileged containers</a>
are equivalent to processes running as root on the host.
Privileged containers can make use of capabilities like manipulating the network stack and accessing devices.</p></li><li><p><strong>Environment variables</strong>: Kubernetes exposes Services through
<a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a>.
You can compose environment variable or pass arguments to your commands using the values of environment variables.
They can be used in applications to find a Service.
Values can reference other variables using the <code>$(VAR_NAME)</code> syntax.</p></li></ul><h3 id="uploading-a-yaml-or-json-file">Uploading a YAML or JSON file</h3><p>Kubernetes supports declarative configuration.
In this style, all configuration is stored in manifests (YAML or JSON configuration files).
The manifests use Kubernetes <a href="/docs/concepts/overview/kubernetes-api/">API</a> resource schemas.</p><p>As an alternative to specifying application details in the deploy wizard,
you can define your application in one or more manifests, and upload the files using Dashboard.</p><h2 id="using-dashboard">Using Dashboard</h2><p>Following sections describe views of the Kubernetes Dashboard UI; what they provide and how can they be used.</p><h3 id="navigation">Navigation</h3><p>When there are Kubernetes objects defined in the cluster, Dashboard shows them in the initial view.
By default only objects from the <em>default</em> namespace are shown and
this can be changed using the namespace selector located in the navigation menu.</p><p>Dashboard shows most Kubernetes object kinds and groups them in a few menu categories.</p><h4 id="admin-overview">Admin overview</h4><p>For cluster and namespace administrators, Dashboard lists Nodes, Namespaces and PersistentVolumes and has detail views for them.
Node list view contains CPU and memory usage metrics aggregated across all Nodes.
The details view shows the metrics for a Node, its specification, status,
allocated resources, events and pods running on the node.</p><h4 id="workloads">Workloads</h4><p>Shows all applications running in the selected namespace.
The view lists applications by workload kind (for example: Deployments, ReplicaSets, StatefulSets).
Each workload kind can be viewed separately.
The lists summarize actionable information about the workloads,
such as the number of ready pods for a ReplicaSet or current memory usage for a Pod.</p><p>Detail views for workloads show status and specification information and
surface relationships between objects.
For example, Pods that ReplicaSet is controlling or new ReplicaSets and HorizontalPodAutoscalers for Deployments.</p><h4 id="services">Services</h4><p>Shows Kubernetes resources that allow for exposing services to external world and
discovering them within a cluster.
For that reason, Service and Ingress views show Pods targeted by them,
internal endpoints for cluster connections and external endpoints for external users.</p><h4 id="storage">Storage</h4><p>Storage view shows PersistentVolumeClaim resources which are used by applications for storing data.</p><h4 id="config-maps-and-secrets">ConfigMaps and Secrets</h4><p>Shows all Kubernetes resources that are used for live configuration of applications running in clusters.
The view allows for editing and managing config objects and displays secrets hidden by default.</p><h4 id="logs-viewer">Logs viewer</h4><p>Pod lists and detail pages link to a logs viewer that is built into Dashboard.
The viewer allows for drilling down logs from containers belonging to a single Pod.</p><p><img alt="Logs viewer" src="/images/docs/ui-dashboard-logs-view.png"></p><h2 id="what-s-next">What's next</h2><p>For more information, see the
<a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard project page</a>.</p></div></div><div><div class="td-content"><h1>Accessing Clusters</h1><p>This topic discusses multiple ways to interact with clusters.</p><h2 id="accessing-for-the-first-time-with-kubectl">Accessing for the first time with kubectl</h2><p>When accessing the Kubernetes API for the first time, we suggest using the
Kubernetes CLI, <code>kubectl</code>.</p><p>To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a <a href="/docs/setup/">Getting started guide</a>,
or someone else set up the cluster and provided you with credentials and a location.</p><p>Check the location and credentials that kubectl knows about with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view
</span></span></code></pre></div><p>Many of the <a href="/docs/reference/kubectl/quick-reference/">examples</a> provide an introduction to using
<code>kubectl</code>, and complete documentation is found in the
<a href="/docs/reference/kubectl/">kubectl reference</a>.</p><h2 id="directly-accessing-the-rest-api">Directly accessing the REST API</h2><p>Kubectl handles locating and authenticating to the apiserver.
If you want to directly access the REST API with an http client like
curl or wget, or a browser, there are several ways to locate and authenticate:</p><ul><li>Run kubectl in proxy mode.<ul><li>Recommended approach.</li><li>Uses stored apiserver location.</li><li>Verifies identity of apiserver using self-signed cert. No MITM possible.</li><li>Authenticates to apiserver.</li><li>In future, may do intelligent client-side load-balancing and failover.</li></ul></li><li>Provide the location and credentials directly to the http client.<ul><li>Alternate approach.</li><li>Works with some types of client code that are confused by using a proxy.</li><li>Need to import a root cert into your browser to protect against MITM.</li></ul></li></ul><h3 id="using-kubectl-proxy">Using kubectl proxy</h3><p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the apiserver and authenticating.
Run it like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span>
</span></span></code></pre></div><p>See <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl proxy</a> for more details.</p><p>Then you can explore the API with curl, wget, or a browser, replacing localhost
with [::1] for IPv6, like so:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://localhost:8080/api/
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"APIVersions"</span>,
</span></span><span><span>  <span>"versions"</span>: [
</span></span><span><span>    <span>"v1"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"serverAddressByClientCIDRs"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"clientCIDR"</span>: <span>"0.0.0.0/0"</span>,
</span></span><span><span>      <span>"serverAddress"</span>: <span>"10.0.1.149:443"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><h3 id="without-kubectl-proxy">Without kubectl proxy</h3><p>Use <code>kubectl apply</code> and <code>kubectl describe secret...</code> to create a token for the default service account with grep/cut:</p><p>First, create the Secret, requesting a token for the default ServiceAccount:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f - <span>&lt;&lt;EOF
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Secret
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: default-token
</span></span></span><span><span><span>  annotations:
</span></span></span><span><span><span>    kubernetes.io/service-account.name: default
</span></span></span><span><span><span>type: kubernetes.io/service-account-token
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Next, wait for the token controller to populate the Secret with a token:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>while</span> ! kubectl describe secret default-token | grep -E <span>'^token'</span> &gt;/dev/null; <span>do</span>
</span></span><span><span>  <span>echo</span> <span>"waiting for token..."</span> &gt;&amp;<span>2</span>
</span></span><span><span>  sleep <span>1</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>Capture and use the generated token:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>APISERVER</span><span>=</span><span>$(</span>kubectl config view --minify | grep server | cut -f 2- -d <span>":"</span> | tr -d <span>" "</span><span>)</span>
</span></span><span><span><span>TOKEN</span><span>=</span><span>$(</span>kubectl describe secret default-token | grep -E <span>'^token'</span> | cut -f2 -d<span>':'</span> | tr -d <span>" "</span><span>)</span>
</span></span><span><span>
</span></span><span><span>curl <span>$APISERVER</span>/api --header <span>"Authorization: Bearer </span><span>$TOKEN</span><span>"</span> --insecure
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"APIVersions"</span>,
</span></span><span><span>  <span>"versions"</span>: [
</span></span><span><span>    <span>"v1"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"serverAddressByClientCIDRs"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"clientCIDR"</span>: <span>"0.0.0.0/0"</span>,
</span></span><span><span>      <span>"serverAddress"</span>: <span>"10.0.1.149:443"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>Using <code>jsonpath</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>APISERVER</span><span>=</span><span>$(</span>kubectl config view --minify -o <span>jsonpath</span><span>=</span><span>'{.clusters[0].cluster.server}'</span><span>)</span>
</span></span><span><span><span>TOKEN</span><span>=</span><span>$(</span>kubectl get secret default-token -o <span>jsonpath</span><span>=</span><span>'{.data.token}'</span> | base64 --decode<span>)</span>
</span></span><span><span>
</span></span><span><span>curl <span>$APISERVER</span>/api --header <span>"Authorization: Bearer </span><span>$TOKEN</span><span>"</span> --insecure
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"APIVersions"</span>,
</span></span><span><span>  <span>"versions"</span>: [
</span></span><span><span>    <span>"v1"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"serverAddressByClientCIDRs"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"clientCIDR"</span>: <span>"0.0.0.0/0"</span>,
</span></span><span><span>      <span>"serverAddress"</span>: <span>"10.0.1.149:443"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>The above examples use the <code>--insecure</code> flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
<code>~/.kube</code> directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.</p><p>On some clusters, the apiserver does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. <a href="/docs/concepts/security/controlling-access/">Controlling Access to the API</a>
describes how a cluster admin can configure this.</p><h2 id="programmatic-access-to-the-api">Programmatic access to the API</h2><p>Kubernetes officially supports <a href="#go-client">Go</a> and <a href="#python-client">Python</a>
client libraries.</p><h3 id="go-client">Go client</h3><ul><li>To get the library, run the following command: <code>go get k8s.io/client-go@kubernetes-&lt;kubernetes-version-number&gt;</code>,
see <a href="https://github.com/kubernetes/client-go/blob/master/INSTALL.md#for-the-casual-user">INSTALL.md</a>
for detailed installation instructions. See
<a href="https://github.com/kubernetes/client-go#compatibility-matrix">https://github.com/kubernetes/client-go</a>
to see which versions are supported.</li><li>Write an application atop of the client-go clients. Note that client-go defines its own API objects,
so if needed, please import API definitions from client-go rather than from the main repository,
e.g., <code>import "k8s.io/client-go/kubernetes"</code> is correct.</li></ul><p>The Go client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the apiserver. See this
<a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">example</a>.</p><p>If the application is deployed as a Pod in the cluster, please refer to the <a href="#accessing-the-api-from-a-pod">next section</a>.</p><h3 id="python-client">Python client</h3><p>To use <a href="https://github.com/kubernetes-client/python">Python client</a>, run the following command:
<code>pip install kubernetes</code>. See <a href="https://github.com/kubernetes-client/python">Python Client Library page</a>
for more installation options.</p><p>The Python client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the apiserver. See this
<a href="https://github.com/kubernetes-client/python/tree/master/examples">example</a>.</p><h3 id="other-languages">Other languages</h3><p>There are <a href="/docs/reference/using-api/client-libraries/">client libraries</a> for accessing the API from other languages.
See documentation for other libraries for how they authenticate.</p><h2 id="accessing-the-api-from-a-pod">Accessing the API from a Pod</h2><p>When accessing the API from a pod, locating and authenticating
to the API server are somewhat different.</p><p>Please check <a href="/docs/tasks/run-application/access-api-from-pod/">Accessing the API from within a Pod</a>
for more details.</p><h2 id="accessing-services-running-on-the-cluster">Accessing services running on the cluster</h2><p>The previous section describes how to connect to the Kubernetes API server.
For information about connecting to other services running on a Kubernetes cluster, see
<a href="/docs/tasks/access-application-cluster/access-cluster-services/">Access Cluster Services</a>.</p><h2 id="requesting-redirects">Requesting redirects</h2><p>The redirect capabilities have been deprecated and removed. Please use a proxy (see below) instead.</p><h2 id="so-many-proxies">So many proxies</h2><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li><p>The <a href="#directly-accessing-the-rest-api">kubectl proxy</a>:</p><ul><li>runs on a user's desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li><p>The <a href="/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services">apiserver proxy</a>:</p><ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li><p>The <a href="/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>:</p><ul><li>runs on each node</li><li>proxies UDP and TCP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is only used to reach services</li></ul></li><li><p>A Proxy/Load-balancer in front of apiserver(s):</p><ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li><p>Cloud Load Balancers on external services:</p><ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <code>LoadBalancer</code></li><li>use UDP/TCP only</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin
will typically ensure that the latter types are set up correctly.</p></div></div><div><div class="td-content"><h1>Configure Access to Multiple Clusters</h1><p>This page shows how to configure access to multiple clusters by using
configuration files. After your clusters, users, and contexts are defined in
one or more configuration files, you can quickly switch between clusters by using the
<code>kubectl config use-context</code> command.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A file that is used to configure access to a cluster is sometimes called
a <em>kubeconfig file</em>. This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.</div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig
file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check that <a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a> is installed,
run <code>kubectl version --client</code>. The kubectl version should be
<a href="/releases/version-skew-policy/#kubectl">within one minor version</a> of your
cluster's API server.</p><h2 id="define-clusters-users-and-contexts">Define clusters, users, and contexts</h2><p>Suppose you have two clusters, one for development work and one for test work.
In the <code>development</code> cluster, your frontend developers work in a namespace called <code>frontend</code>,
and your storage developers work in a namespace called <code>storage</code>. In your <code>test</code> cluster,
developers work in the default namespace, or they create auxiliary namespaces as they
see fit. Access to the development cluster requires authentication by certificate. Access
to the test cluster requires authentication by username and password.</p><p>Create a directory named <code>config-exercise</code>. In your
<code>config-exercise</code> directory, create a file named <code>config-demo</code> with this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Config<span>
</span></span></span><span><span><span></span><span>preferences</span>:<span> </span>{}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>clusters</span>:<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>development<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>users</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>developer<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>experimenter<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-frontend<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-storage<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>exp-test<span>
</span></span></span></code></pre></div><p>A configuration file describes clusters, users, and contexts. Your <code>config-demo</code> file
has the framework to describe two clusters, two users, and three contexts.</p><p>Go to your <code>config-exercise</code> directory. Enter these commands to add cluster details to
your configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-cluster development --server<span>=</span>https://1.2.3.4 --certificate-authority<span>=</span>fake-ca-file
</span></span><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-cluster <span>test</span> --server<span>=</span>https://5.6.7.8 --insecure-skip-tls-verify
</span></span></code></pre></div><p>Add user details to your configuration file:</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Storing passwords in Kubernetes client config is risky. A better alternative would be to use a credential plugin and store them separately. See: <a href="/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">client-go credential plugins</a></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-credentials developer --client-certificate<span>=</span>fake-cert-file --client-key<span>=</span>fake-key-seefile
</span></span><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-credentials experimenter --username<span>=</span>exp --password<span>=</span>some-password
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>To delete a user you can run <code>kubectl --kubeconfig=config-demo config unset users.&lt;name&gt;</code></li><li>To remove a cluster, you can run <code>kubectl --kubeconfig=config-demo config unset clusters.&lt;name&gt;</code></li><li>To remove a context, you can run <code>kubectl --kubeconfig=config-demo config unset contexts.&lt;name&gt;</code></li></ul></div><p>Add context details to your configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-context dev-frontend --cluster<span>=</span>development --namespace<span>=</span>frontend --user<span>=</span>developer
</span></span><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-context dev-storage --cluster<span>=</span>development --namespace<span>=</span>storage --user<span>=</span>developer
</span></span><span><span>kubectl config --kubeconfig<span>=</span>config-demo set-context exp-test --cluster<span>=</span><span>test</span> --namespace<span>=</span>default --user<span>=</span>experimenter
</span></span></code></pre></div><p>Open your <code>config-demo</code> file to see the added details. As an alternative to opening the
<code>config-demo</code> file, you can use the <code>config view</code> command.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo view
</span></span></code></pre></div><p>The output shows the two clusters, two users, and three contexts:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>clusters</span>:<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>    </span><span>certificate-authority</span>:<span> </span>fake-ca-file<span>
</span></span></span><span><span><span>    </span><span>server</span>:<span> </span>https://1.2.3.4<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>development<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>    </span><span>insecure-skip-tls-verify</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>server</span>:<span> </span>https://5.6.7.8<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span></span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-frontend<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>storage<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-storage<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>test<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>experimenter<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>exp-test<span>
</span></span></span><span><span><span></span><span>current-context</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Config<span>
</span></span></span><span><span><span></span><span>preferences</span>:<span> </span>{}<span>
</span></span></span><span><span><span></span><span>users</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>user</span>:<span>
</span></span></span><span><span><span>    </span><span>client-certificate</span>:<span> </span>fake-cert-file<span>
</span></span></span><span><span><span>    </span><span>client-key</span>:<span> </span>fake-key-file<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>experimenter<span>
</span></span></span><span><span><span>  </span><span>user</span>:<span>
</span></span></span><span><span><span>    </span><span># Documentation note (this comment is NOT part of the command output).</span><span>
</span></span></span><span><span><span>    </span><span># Storing passwords in Kubernetes client config is risky.</span><span>
</span></span></span><span><span><span>    </span><span># A better alternative would be to use a credential plugin</span><span>
</span></span></span><span><span><span>    </span><span># and store the credentials separately.</span><span>
</span></span></span><span><span><span>    </span><span># See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins</span><span>
</span></span></span><span><span><span>    </span><span>password</span>:<span> </span>some-password<span>
</span></span></span><span><span><span>    </span><span>username</span>:<span> </span>exp<span>
</span></span></span></code></pre></div><p>The <code>fake-ca-file</code>, <code>fake-cert-file</code> and <code>fake-key-file</code> above are the placeholders
for the pathnames of the certificate files. You need to change these to the actual pathnames
of certificate files in your environment.</p><p>Sometimes you may want to use Base64-encoded data embedded here instead of separate
certificate files; in that case you need to add the suffix <code>-data</code> to the keys, for example,
<code>certificate-authority-data</code>, <code>client-certificate-data</code>, <code>client-key-data</code>.</p><p>Each context is a triple (cluster, user, namespace). For example, the
<code>dev-frontend</code> context says, "Use the credentials of the <code>developer</code>
user to access the <code>frontend</code> namespace of the <code>development</code> cluster".</p><p>Set the current context:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo use-context dev-frontend
</span></span></code></pre></div><p>Now whenever you enter a <code>kubectl</code> command, the action will apply to the cluster,
and namespace listed in the <code>dev-frontend</code> context. And the command will use
the credentials of the user listed in the <code>dev-frontend</code> context.</p><p>To see only the configuration information associated with
the current context, use the <code>--minify</code> flag.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo view --minify
</span></span></code></pre></div><p>The output shows configuration information associated with the <code>dev-frontend</code> context:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>clusters</span>:<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>    </span><span>certificate-authority</span>:<span> </span>fake-ca-file<span>
</span></span></span><span><span><span>    </span><span>server</span>:<span> </span>https://1.2.3.4<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>development<span>
</span></span></span><span><span><span></span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-frontend<span>
</span></span></span><span><span><span></span><span>current-context</span>:<span> </span>dev-frontend<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Config<span>
</span></span></span><span><span><span></span><span>preferences</span>:<span> </span>{}<span>
</span></span></span><span><span><span></span><span>users</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>user</span>:<span>
</span></span></span><span><span><span>    </span><span>client-certificate</span>:<span> </span>fake-cert-file<span>
</span></span></span><span><span><span>    </span><span>client-key</span>:<span> </span>fake-key-file<span>
</span></span></span></code></pre></div><p>Now suppose you want to work for a while in the test cluster.</p><p>Change the current context to <code>exp-test</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo use-context exp-test
</span></span></code></pre></div><p>Now any <code>kubectl</code> command you give will apply to the default namespace of
the <code>test</code> cluster. And the command will use the credentials of the user
listed in the <code>exp-test</code> context.</p><p>View configuration associated with the new current context, <code>exp-test</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo view --minify
</span></span></code></pre></div><p>Finally, suppose you want to work for a while in the <code>storage</code> namespace of the
<code>development</code> cluster.</p><p>Change the current context to <code>dev-storage</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo use-context dev-storage
</span></span></code></pre></div><p>View configuration associated with the new current context, <code>dev-storage</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config --kubeconfig<span>=</span>config-demo view --minify
</span></span></code></pre></div><h2 id="create-a-second-configuration-file">Create a second configuration file</h2><p>In your <code>config-exercise</code> directory, create a file named <code>config-demo-2</code> with this content:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Config<span>
</span></span></span><span><span><span></span><span>preferences</span>:<span> </span>{}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>ramp<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-ramp-up<span>
</span></span></span></code></pre></div><p>The preceding configuration file defines a new context named <code>dev-ramp-up</code>.</p><h2 id="set-the-kubeconfig-environment-variable">Set the KUBECONFIG environment variable</h2><p>See whether you have an environment variable named <code>KUBECONFIG</code>. If so, save the
current value of your <code>KUBECONFIG</code> environment variable, so you can restore it later.
For example:</p><h3 id="linux">Linux</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>KUBECONFIG_SAVED</span><span>=</span><span>"</span><span>$KUBECONFIG</span><span>"</span>
</span></span></code></pre></div><h3 id="windows-powershell">Windows PowerShell</h3><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>$Env:KUBECONFIG_SAVED</span>=<span>$ENV:KUBECONFIG</span>
</span></span></code></pre></div><p>The <code>KUBECONFIG</code> environment variable is a list of paths to configuration files. The list is
colon-delimited for Linux and Mac, and semicolon-delimited for Windows. If you have
a <code>KUBECONFIG</code> environment variable, familiarize yourself with the configuration files
in the list.</p><p>Temporarily append two paths to your <code>KUBECONFIG</code> environment variable. For example:</p><h3 id="linux-1">Linux</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>KUBECONFIG</span><span>=</span><span>"</span><span>${</span><span>KUBECONFIG</span><span>}</span><span>:config-demo:config-demo-2"</span>
</span></span></code></pre></div><h3 id="windows-powershell-1">Windows PowerShell</h3><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>$Env:KUBECONFIG</span>=(<span>"config-demo;config-demo-2"</span>)
</span></span></code></pre></div><p>In your <code>config-exercise</code> directory, enter this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view
</span></span></code></pre></div><p>The output shows merged information from all the files listed in your <code>KUBECONFIG</code>
environment variable. In particular, notice that the merged information has the
<code>dev-ramp-up</code> context from the <code>config-demo-2</code> file and the three contexts from
the <code>config-demo</code> file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-frontend<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>ramp<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-ramp-up<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>development<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>storage<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>developer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dev-storage<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>test<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>experimenter<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>exp-test<span>
</span></span></span></code></pre></div><p>For more information about how kubeconfig files are merged, see
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></p><h2 id="explore-the-home-kube-directory">Explore the $HOME/.kube directory</h2><p>If you already have a cluster, and you can use <code>kubectl</code> to interact with
the cluster, then you probably have a file named <code>config</code> in the <code>$HOME/.kube</code>
directory.</p><p>Go to <code>$HOME/.kube</code>, and see what files are there. Typically, there is a file named
<code>config</code>. There might also be other configuration files in this directory. Briefly
familiarize yourself with the contents of these files.</p><h2 id="append-home-kube-config-to-your-kubeconfig-environment-variable">Append $HOME/.kube/config to your KUBECONFIG environment variable</h2><p>If you have a <code>$HOME/.kube/config</code> file, and it's not already listed in your
<code>KUBECONFIG</code> environment variable, append it to your <code>KUBECONFIG</code> environment variable now.
For example:</p><h3 id="linux-2">Linux</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>KUBECONFIG</span><span>=</span><span>"</span><span>${</span><span>KUBECONFIG</span><span>}</span><span>:</span><span>${</span><span>HOME</span><span>}</span><span>/.kube/config"</span>
</span></span></code></pre></div><h3 id="windows-powershell-2">Windows Powershell</h3><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>$Env:KUBECONFIG</span>=<span>"</span><span>$Env:KUBECONFIG</span><span>;</span><span>$HOME</span><span>\.kube\config"</span>
</span></span></code></pre></div><p>View configuration information merged from all the files that are now listed
in your <code>KUBECONFIG</code> environment variable. In your config-exercise directory, enter:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view
</span></span></code></pre></div><h2 id="clean-up">Clean up</h2><p>Return your <code>KUBECONFIG</code> environment variable to its original value. For example:<br></p><h3 id="linux-3">Linux</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>KUBECONFIG</span><span>=</span><span>"</span><span>$KUBECONFIG_SAVED</span><span>"</span>
</span></span></code></pre></div><h3 id="windows-powershell-3">Windows PowerShell</h3><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>$Env:KUBECONFIG</span>=<span>$ENV:KUBECONFIG_SAVED</span>
</span></span></code></pre></div><h2 id="check-the-subject-represented-by-the-kubeconfig">Check the subject represented by the kubeconfig</h2><p>It is not always obvious what attributes (username, groups) you will get after authenticating to the cluster.
It can be even more challenging if you are managing more than one cluster at the same time.</p><p>There is a <code>kubectl</code> subcommand to check subject attributes, such as username, for your selected Kubernetes
client context: <code>kubectl auth whoami</code>.</p><p>Read <a href="/docs/reference/access-authn-authz/authentication/#self-subject-review">API access to authentication information for a client</a>
to learn about this in more detail.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands#config">kubectl config</a></li></ul></div></div><div><div class="td-content"><h1>Use Port Forwarding to Access Applications in a Cluster</h1><p>This page shows how to use <code>kubectl port-forward</code> to connect to a MongoDB
server running in a Kubernetes cluster. This type of connection can be useful
for database debugging.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.10.<p>To check the version, enter <code>kubectl version</code>.</p></li><li>Install <a href="https://www.mongodb.com/try/download/shell">MongoDB Shell</a>.</li></ul><h2 id="creating-mongodb-deployment-and-service">Creating MongoDB deployment and service</h2><ol><li><p>Create a Deployment that runs MongoDB:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-deployment.yaml
</span></span></code></pre></div><p>The output of a successful command verifies that the deployment was created:</p><pre tabindex="0"><code>deployment.apps/mongo created
</code></pre><p>View the pod status to check that it is ready:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output displays the pod created:</p><pre tabindex="0"><code>NAME                     READY   STATUS    RESTARTS   AGE
mongo-75f59d57f4-4nd6q   1/1     Running   0          2m4s
</code></pre><p>View the Deployment's status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment
</span></span></code></pre></div><p>The output displays that the Deployment was created:</p><pre tabindex="0"><code>NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mongo   1/1     1            1           2m21s
</code></pre><p>The Deployment automatically manages a ReplicaSet.
View the ReplicaSet status using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get replicaset
</span></span></code></pre></div><p>The output displays that the ReplicaSet was created:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY   AGE
mongo-75f59d57f4   1         1         1       3m12s
</code></pre></li><li><p>Create a Service to expose MongoDB on the network:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-service.yaml
</span></span></code></pre></div><p>The output of a successful command verifies that the Service was created:</p><pre tabindex="0"><code>service/mongo created
</code></pre><p>Check the Service created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get service mongo
</span></span></code></pre></div><p>The output displays the service created:</p><pre tabindex="0"><code>NAME    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
mongo   ClusterIP   10.96.41.183   &lt;none&gt;        27017/TCP   11s
</code></pre></li><li><p>Verify that the MongoDB server is running in the Pod, and listening on port 27017:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Change mongo-75f59d57f4-4nd6q to the name of the Pod</span>
</span></span><span><span>kubectl get pod mongo-75f59d57f4-4nd6q --template<span>=</span><span>'{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}'</span>
</span></span></code></pre></div><p>The output displays the port for MongoDB in that Pod:</p><pre tabindex="0"><code>27017
</code></pre><p>27017 is the official TCP port for MongoDB.</p></li></ol><h2 id="forward-a-local-port-to-a-port-on-the-pod">Forward a local port to a port on the Pod</h2><ol><li><p><code>kubectl port-forward</code> allows using resource name, such as a pod name, to select a matching pod to port forward to.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Change mongo-75f59d57f4-4nd6q to the name of the Pod</span>
</span></span><span><span>kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017
</span></span></code></pre></div><p>which is the same as</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl port-forward deployment/mongo 28015:27017
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl port-forward replicaset/mongo-75f59d57f4 28015:27017
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl port-forward service/mongo 28015:27017
</span></span></code></pre></div><p>Any of the above commands works. The output is similar to this:</p><pre tabindex="0"><code>Forwarding from 127.0.0.1:28015 -&gt; 27017
Forwarding from [::1]:28015 -&gt; 27017
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>kubectl port-forward</code> does not return. To continue with the exercises, you will need to open another terminal.</div></li><li><p>Start the MongoDB command line interface:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>mongosh --port <span>28015</span>
</span></span></code></pre></div></li><li><p>At the MongoDB command line prompt, enter the <code>ping</code> command:</p><pre tabindex="0"><code>db.runCommand( { ping: 1 } )
</code></pre><p>A successful ping request returns:</p><pre tabindex="0"><code>{ ok: 1 }
</code></pre></li></ol><h3 id="let-kubectl-choose-local-port">Optionally let <em>kubectl</em> choose the local port</h3><p>If you don't need a specific local port, you can let <code>kubectl</code> choose and allocate
the local port and thus relieve you from having to manage local port conflicts, with
the slightly simpler syntax:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl port-forward deployment/mongo :27017
</span></span></code></pre></div><p>The <code>kubectl</code> tool finds a local port number that is not in use (avoiding low ports numbers,
because these might be used by other applications). The output is similar to:</p><pre tabindex="0"><code>Forwarding from 127.0.0.1:63753 -&gt; 27017
Forwarding from [::1]:63753 -&gt; 27017
</code></pre><h2 id="discussion">Discussion</h2><p>Connections made to local port 28015 are forwarded to port 27017 of the Pod that
is running the MongoDB server. With this connection in place, you can use your
local workstation to debug the database that is running in the Pod.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>kubectl port-forward</code> is implemented for TCP ports only.
The support for UDP protocol is tracked in
<a href="https://github.com/kubernetes/kubernetes/issues/47862">issue 47862</a>.</div><h2 id="what-s-next">What's next</h2><p>Learn more about <a href="/docs/reference/generated/kubectl/kubectl-commands/#port-forward">kubectl port-forward</a>.</p></div></div><div><div class="td-content"><h1>Use a Service to Access an Application in a Cluster</h1><p>This page shows how to create a Kubernetes Service object that external
clients can use to access an application running in a cluster. The Service
provides load balancing for an application that has two running instances.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="objectives">Objectives</h2><ul><li>Run two instances of a Hello World application.</li><li>Create a Service object that exposes a node port.</li><li>Use the Service object to access the running application.</li></ul><h2 id="creating-a-service-for-an-application-running-in-two-pods">Creating a service for an application running in two pods</h2><p>Here is the configuration file for the application Deployment:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/hello-application.yaml"><code>service/access/hello-application.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/access/hello-application.yaml to clipboard"></div><div class="includecode" id="service-access-hello-application-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hello-world<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>run</span>:<span> </span>load-balancer-example<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>run</span>:<span> </span>load-balancer-example<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>hello-world<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span>us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0<span>
</span></span></span><span><span><span>          </span><span>ports</span>:<span>
</span></span></span><span><span><span>            </span>- <span>containerPort</span>:<span> </span><span>8080</span><span>
</span></span></span><span><span><span>              </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span></code></pre></div></div></div><ol><li><p>Run a Hello World application in your cluster:
Create the application Deployment using the file above:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml
</span></span></code></pre></div><p>The preceding command creates a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>
and an associated
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank">ReplicaSet</a>.
The ReplicaSet has two
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>
each of which runs the Hello World application.</p></li><li><p>Display information about the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployments hello-world
</span></span><span><span>kubectl describe deployments hello-world
</span></span></code></pre></div></li><li><p>Display information about your ReplicaSet objects:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get replicasets
</span></span><span><span>kubectl describe replicasets
</span></span></code></pre></div></li><li><p>Create a Service object that exposes the deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl expose deployment hello-world --type<span>=</span>NodePort --name<span>=</span>example-service
</span></span></code></pre></div></li><li><p>Display information about the Service:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe services example-service
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">Name:                   example-service
Namespace:              default
Labels:                 run=load-balancer-example
Annotations:            &lt;none&gt;
Selector:               run=load-balancer-example
Type:                   NodePort
IP:                     10.32.0.16
Port:                   &lt;unset&gt; 8080/TCP
TargetPort:             8080/TCP
NodePort:               &lt;unset&gt; 31496/TCP
Endpoints:              10.200.1.4:8080,10.200.2.5:8080
Session Affinity:       None
Events:                 &lt;none&gt;
</code></pre><p>Make a note of the NodePort value for the Service. For example,
in the preceding output, the NodePort value is 31496.</p></li><li><p>List the pods that are running the Hello World application:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --selector<span>=</span><span>"run=load-balancer-example"</span> --output<span>=</span>wide
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none">NAME                           READY   STATUS    ...  IP           NODE
hello-world-2895499144-bsbk5   1/1     Running   ...  10.200.1.4   worker1
hello-world-2895499144-m1pwt   1/1     Running   ...  10.200.2.5   worker2
</code></pre></li><li><p>Get the public IP address of one of your nodes that is running
a Hello World pod. How you get this address depends on how you set
up your cluster. For example, if you are using Minikube, you can
see the node address by running <code>kubectl cluster-info</code>. If you are
using Google Compute Engine instances, you can use the
<code>gcloud compute instances list</code> command to see the public addresses of your
nodes.</p></li><li><p>On your chosen node, create a firewall rule that allows TCP traffic
on your node port. For example, if your Service has a NodePort value of
31568, create a firewall rule that allows TCP traffic on port 31568. Different
cloud providers offer different ways of configuring firewall rules.</p></li><li><p>Use the node address and node port to access the Hello World application:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://&lt;public-node-ip&gt;:&lt;node-port&gt;
</span></span></code></pre></div><p>where <code>&lt;public-node-ip&gt;</code> is the public IP address of your node,
and <code>&lt;node-port&gt;</code> is the NodePort value for your service. The
response to a successful request is a hello message:</p><pre tabindex="0"><code class="language-none">Hello, world!
Version: 2.0.0
Hostname: hello-world-cdd4458f4-m47c8
</code></pre></li></ol><h2 id="using-a-service-configuration-file">Using a service configuration file</h2><p>As an alternative to using <code>kubectl expose</code>, you can use a
<a href="/docs/concepts/services-networking/service/">service configuration file</a>
to create a Service.</p><h2 id="cleaning-up">Cleaning up</h2><p>To delete the Service, enter this command:</p><pre><code>kubectl delete services example-service
</code></pre><p>To delete the Deployment, the ReplicaSet, and the Pods that are running
the Hello World application, enter this command:</p><pre><code>kubectl delete deployment hello-world
</code></pre><h2 id="what-s-next">What's next</h2><p>Follow the
<a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a>
tutorial.</p></div></div><div><div class="td-content"><h1>Connect a Frontend to a Backend Using Services</h1><p>This task shows how to create a <em>frontend</em> and a <em>backend</em> microservice. The backend
microservice is a hello greeter. The frontend exposes the backend using nginx and a
Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> object.</p><h2 id="objectives">Objectives</h2><ul><li>Create and run a sample <code>hello</code> backend microservice using a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> object.</li><li>Use a Service object to send traffic to the backend microservice's multiple replicas.</li><li>Create and run a <code>nginx</code> frontend microservice, also using a Deployment object.</li><li>Configure the frontend microservice to send traffic to the backend microservice.</li><li>Use a Service object of <code>type=LoadBalancer</code> to expose the frontend microservice
outside the cluster.</li></ul><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>This task uses
<a href="/docs/tasks/access-application-cluster/create-external-load-balancer/">Services with external load balancers</a>, which
require a supported environment. If your environment does not support this, you can use a Service of type
<a href="/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> instead.</p><h2 id="creating-the-backend-using-a-deployment">Creating the backend using a Deployment</h2><p>The backend is a simple hello greeter microservice. Here is the configuration
file for the backend Deployment:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/backend-deployment.yaml"><code>service/access/backend-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/access/backend-deployment.yaml to clipboard"></div><div class="includecode" id="service-access-backend-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>backend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>hello<span>
</span></span></span><span><span><span>      </span><span>tier</span>:<span> </span>backend<span>
</span></span></span><span><span><span>      </span><span>track</span>:<span> </span>stable<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>hello<span>
</span></span></span><span><span><span>        </span><span>tier</span>:<span> </span>backend<span>
</span></span></span><span><span><span>        </span><span>track</span>:<span> </span>stable<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span><span>"gcr.io/google-samples/hello-go-gke:1.0"</span><span>
</span></span></span><span><span><span>          </span><span>ports</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>              </span><span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span></span><span>...</span></span></span></code></pre></div></div></div><p>Create the backend Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/service/access/backend-deployment.yaml
</span></span></code></pre></div><p>View information about the backend Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployment backend
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                           backend
Namespace:                      default
CreationTimestamp:              Mon, 24 Oct 2016 14:21:02 -0700
Labels:                         app=hello
                                tier=backend
                                track=stable
Annotations:                    deployment.kubernetes.io/revision=1
Selector:                       app=hello,tier=backend,track=stable
Replicas:                       3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:                   RollingUpdate
MinReadySeconds:                0
RollingUpdateStrategy:          1 max unavailable, 1 max surge
Pod Template:
  Labels:       app=hello
                tier=backend
                track=stable
  Containers:
   hello:
    Image:              "gcr.io/google-samples/hello-go-gke:1.0"
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
OldReplicaSets:                 &lt;none&gt;
NewReplicaSet:                  hello-3621623197 (3/3 replicas created)
Events:
...
</code></pre><h2 id="creating-the-hello-service-object">Creating the <code>hello</code> Service object</h2><p>The key to sending requests from a frontend to a backend is the backend
Service. A Service creates a persistent IP address and DNS name entry
so that the backend microservice can always be reached. A Service uses
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selectors</a> to find
the Pods that it routes traffic to.</p><p>First, explore the Service configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/backend-service.yaml"><code>service/access/backend-service.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/access/backend-service.yaml to clipboard"></div><div class="includecode" id="service-access-backend-service-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>hello<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>backend<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span>http<span>
</span></span></span><span><span><span></span><span>...</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Service, named <code>hello</code> routes
traffic to Pods that have the labels <code>app: hello</code> and <code>tier: backend</code>.</p><p>Create the backend Service:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/service/access/backend-service.yaml
</span></span></code></pre></div><p>At this point, you have a <code>backend</code> Deployment running three replicas of your <code>hello</code>
application, and you have a Service that can route traffic to them. However, this
service is neither available nor resolvable outside the cluster.</p><h2 id="creating-the-frontend">Creating the frontend</h2><p>Now that you have your backend running, you can create a frontend that is accessible
outside the cluster, and connects to the backend by proxying requests to it.</p><p>The frontend sends requests to the backend worker Pods by using the DNS name
given to the backend Service. The DNS name is <code>hello</code>, which is the value
of the <code>name</code> field in the <code>examples/service/access/backend-service.yaml</code>
configuration file.</p><p>The Pods in the frontend Deployment run a nginx image that is configured
to proxy requests to the <code>hello</code> backend Service. Here is the nginx configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/frontend-nginx.conf"><code>service/access/frontend-nginx.conf</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/access/frontend-nginx.conf to clipboard"></div><div class="includecode" id="service-access-frontend-nginx-conf"><pre tabindex="0"><code class="language-conf"># The identifier Backend is internal to nginx, and used to name this specific upstream
upstream Backend {
    # hello is the internal DNS name used by the backend Service inside Kubernetes
    server hello;
}
<p>server {
listen 80;</p>
<pre><code>location / {
    # The following statement will proxy traffic to the upstream named Backend
    proxy_pass http://Backend;
}
</code></pre><p>}</p></code></pre></div></div><p>Similar to the backend, the frontend has a Deployment and a Service. An important
difference to notice between the backend and frontend services, is that the
configuration for the frontend Service has <code>type: LoadBalancer</code>, which means that
the Service uses a load balancer provisioned by your cloud provider and will be
accessible from outside the cluster.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/frontend-service.yaml"><code>service/access/frontend-service.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/access/frontend-service.yaml to clipboard"></div><div class="includecode" id="service-access-frontend-service-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>hello<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>protocol</span>:<span> </span><span>"TCP"</span><span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span><span><span><span></span><span>...</span></span></span></code></pre></div></div></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/frontend-deployment.yaml"><code>service/access/frontend-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/access/frontend-deployment.yaml to clipboard"></div><div class="includecode" id="service-access-frontend-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>hello<span>
</span></span></span><span><span><span>      </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>      </span><span>track</span>:<span> </span>stable<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>hello<span>
</span></span></span><span><span><span>        </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>        </span><span>track</span>:<span> </span>stable<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span><span>"gcr.io/google-samples/hello-frontend:1.0"</span><span>
</span></span></span><span><span><span>          </span><span>lifecycle</span>:<span>
</span></span></span><span><span><span>            </span><span>preStop</span>:<span>
</span></span></span><span><span><span>              </span><span>exec</span>:<span>
</span></span></span><span><span><span>                </span><span>command</span>:<span> </span>[<span>"/usr/sbin/nginx"</span>,<span>"-s"</span>,<span>"quit"</span>]<span>
</span></span></span><span><span><span></span><span>...</span></span></span></code></pre></div></div></div><p>Create the frontend Deployment and Service:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/service/access/frontend-deployment.yaml
</span></span><span><span>kubectl apply -f https://k8s.io/examples/service/access/frontend-service.yaml
</span></span></code></pre></div><p>The output verifies that both resources were created:</p><pre tabindex="0"><code>deployment.apps/frontend created
service/frontend created
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The nginx configuration is baked into the
<a href="/examples/service/access/Dockerfile">container image</a>. A better way to do this would
be to use a
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>,
so that you can change the configuration more easily.</div><h2 id="interact-with-the-frontend-service">Interact with the frontend Service</h2><p>Once you've created a Service of type LoadBalancer, you can use this
command to find the external IP:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get service frontend --watch
</span></span></code></pre></div><p>This displays the configuration for the <code>frontend</code> Service and watches for
changes. Initially, the external IP is listed as <code>&lt;pending&gt;</code>:</p><pre tabindex="0"><code>NAME       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)  AGE
frontend   LoadBalancer   10.51.252.116   &lt;pending&gt;     80/TCP   10s
</code></pre><p>As soon as an external IP is provisioned, however, the configuration updates
to include the new IP under the <code>EXTERNAL-IP</code> heading:</p><pre tabindex="0"><code>NAME       TYPE           CLUSTER-IP      EXTERNAL-IP        PORT(S)  AGE
frontend   LoadBalancer   10.51.252.116   XXX.XXX.XXX.XXX    80/TCP   1m
</code></pre><p>That IP can now be used to interact with the <code>frontend</code> service from outside the
cluster.</p><h2 id="send-traffic-through-the-frontend">Send traffic through the frontend</h2><p>The frontend and backend are now connected. You can hit the endpoint
by using the curl command on the external IP of your frontend Service.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://<span>${</span><span>EXTERNAL_IP</span><span>}</span> <span># replace this with the EXTERNAL-IP you saw earlier</span>
</span></span></code></pre></div><p>The output shows the message generated by the backend:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{<span>"message"</span>:<span>"Hello"</span>}
</span></span></code></pre></div><h2 id="cleaning-up">Cleaning up</h2><p>To delete the Services, enter this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete services frontend backend
</span></span></code></pre></div><p>To delete the Deployments, the ReplicaSets and the Pods that are running the backend and frontend applications, enter this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment frontend backend
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/services-networking/service/">Services</a></li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a></li><li>Learn more about <a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Service and Pods</a></li></ul></div></div><div><div class="td-content"><h1>Create an External Load Balancer</h1><p>This page shows how to create an external load balancer.</p><p>When creating a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>, you have
the option of automatically creating a cloud load balancer. This provides an
externally-accessible IP address that sends traffic to the correct port on your cluster
nodes,
<em>provided your cluster runs in a supported environment and is configured with
the correct cloud load balancer provider package</em>.</p><p>You can also use an <a class="glossary-tooltip" title="An API object that manages external access to the services in a cluster, typically HTTP." href="/docs/concepts/services-networking/ingress/" target="_blank">Ingress</a> in place of Service.
For more information, check the <a href="/docs/concepts/services-networking/ingress/">Ingress</a>
documentation.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your cluster must be running in a cloud or other environment that already has support
for configuring external load balancers.</p><h2 id="create-a-service">Create a Service</h2><h3 id="create-a-service-from-a-manifest">Create a Service from a manifest</h3><p>To create an external load balancer, add the following line to your
Service manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>    </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span></code></pre></div><p>Your manifest might then look like:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>example<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>port</span>:<span> </span><span>8765</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span></code></pre></div><h3 id="create-a-service-using-kubectl">Create a Service using kubectl</h3><p>You can alternatively create the service with the <code>kubectl expose</code> command and
its <code>--type=LoadBalancer</code> flag:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl expose deployment example --port<span>=</span><span>8765</span> --target-port<span>=</span><span>9376</span> <span>\
</span></span></span><span><span><span></span>        --name<span>=</span>example-service --type<span>=</span>LoadBalancer
</span></span></code></pre></div><p>This command creates a new Service using the same selectors as the referenced
resource (in the case of the example above, a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> named <code>example</code>).</p><p>For more information, including optional flags, refer to the
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose"><code>kubectl expose</code> reference</a>.</p><h2 id="finding-your-ip-address">Finding your IP address</h2><p>You can find the IP address created for your service by getting the service
information through <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl describe services example-service
</span></span></code></pre></div><p>which should produce output similar to:</p><pre tabindex="0"><code>Name:                     example-service
Namespace:                default
Labels:                   app=example
Annotations:              &lt;none&gt;
Selector:                 app=example
Type:                     LoadBalancer
IP Families:              &lt;none&gt;
IP:                       10.3.22.96
IPs:                      10.3.22.96
LoadBalancer Ingress:     192.0.2.89
Port:                     &lt;unset&gt;  8765/TCP
TargetPort:               9376/TCP
NodePort:                 &lt;unset&gt;  30593/TCP
Endpoints:                172.17.0.3:9376
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre><p>The load balancer's IP address is listed next to <code>LoadBalancer Ingress</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you are running your service on Minikube, you can find the assigned IP address and port with:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>minikube service example-service --url
</span></span></code></pre></div></div><h2 id="preserving-the-client-source-ip">Preserving the client source IP</h2><p>By default, the source IP seen in the target container is <em>not the original
source IP</em> of the client. To enable preservation of the client IP, the following
fields can be configured in the <code>.spec</code> of the Service:</p><ul><li><code>.spec.externalTrafficPolicy</code> - denotes if this Service desires to route
external traffic to node-local or cluster-wide endpoints. There are two available
options: <code>Cluster</code> (default) and <code>Local</code>. <code>Cluster</code> obscures the client source
IP and may cause a second hop to another node, but should have good overall
load-spreading. <code>Local</code> preserves the client source IP and avoids a second hop
for LoadBalancer and NodePort type Services, but risks potentially imbalanced
traffic spreading.</li><li><code>.spec.healthCheckNodePort</code> - specifies the health check node port
(numeric port number) for the service. If you don't specify
<code>healthCheckNodePort</code>, the service controller allocates a port from your
cluster's NodePort range.<br>You can configure that range by setting an API server command line option,
<code>--service-node-port-range</code>. The Service will use the user-specified
<code>healthCheckNodePort</code> value if you specify it, provided that the
Service <code>type</code> is set to LoadBalancer and <code>externalTrafficPolicy</code> is set
to <code>Local</code>.</li></ul><p>Setting <code>externalTrafficPolicy</code> to Local in the Service manifest
activates this feature. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>example<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>port</span>:<span> </span><span>8765</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span>  </span><span>externalTrafficPolicy</span>:<span> </span>Local<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span></code></pre></div><h3 id="caveats-and-limitations-when-preserving-source-ips">Caveats and limitations when preserving source IPs</h3><p>Load balancing services from some cloud providers do not let you configure different weights for each target.</p><p>With each target weighted equally in terms of sending traffic to Nodes, external
traffic is not equally load balanced across different Pods. The external load balancer
is unaware of the number of Pods on each node that are used as a target.</p><p>Where <code>NumServicePods &lt;&lt; NumNodes</code> or <code>NumServicePods &gt;&gt; NumNodes</code>, a fairly close-to-equal
distribution will be seen, even without weights.</p><p>Internal pod to pod traffic should behave similar to ClusterIP services, with equal probability across all pods.</p><h2 id="garbage-collecting-load-balancers">Garbage collecting load balancers</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>In usual case, the correlating load balancer resources in cloud provider should
be cleaned up soon after a LoadBalancer type Service is deleted. But it is known
that there are various corner cases where cloud resources are orphaned after the
associated Service is deleted. Finalizer Protection for Service LoadBalancers was
introduced to prevent this from happening. By using finalizers, a Service resource
will never be deleted until the correlating load balancer resources are also deleted.</p><p>Specifically, if a Service has <code>type</code> LoadBalancer, the service controller will attach
a finalizer named <code>service.kubernetes.io/load-balancer-cleanup</code>.
The finalizer will only be removed after the load balancer resource is cleaned up.
This prevents dangling load balancer resources even in corner cases such as the
service controller crashing.</p><h2 id="external-load-balancer-providers">External load balancer providers</h2><p>It is important to note that the datapath for this functionality is provided by a load balancer external to the Kubernetes cluster.</p><p>When the Service <code>type</code> is set to LoadBalancer, Kubernetes provides functionality equivalent to <code>type</code> equals ClusterIP to pods
within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the nodes
hosting the relevant Kubernetes pods. The Kubernetes control plane automates the creation of the external load balancer,
health checks (if needed), and packet filtering rules (if needed). Once the cloud provider allocates an IP address for the load
balancer, the control plane looks up that external IP address and populates it into the Service object.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li><li>Read about <a href="/docs/concepts/services-networking/service/">Service</a></li><li>Read about <a href="/docs/concepts/services-networking/ingress/">Ingress</a></li></ul></div></div><div><div class="td-content"><h1>List All Container Images Running in a Cluster</h1><p>This page shows how to use kubectl to list all of the Container images
for Pods running in a cluster.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>In this exercise you will use kubectl to fetch all of the Pods
running in a cluster, and format the output to pull out the list
of Containers for each.</p><h2 id="list-all-container-images-in-all-namespaces">List all Container images in all namespaces</h2><ul><li>Fetch all Pods in all namespaces using <code>kubectl get pods --all-namespaces</code></li><li>Format the output to include only the list of Container image names
using <code>-o jsonpath={.items[*].spec['initContainers', 'containers'][*].image}</code>. This will recursively parse out the
<code>image</code> field from the returned json.<ul><li>See the <a href="/docs/reference/kubectl/jsonpath/">jsonpath reference</a>
for further information on how to use jsonpath.</li></ul></li><li>Format the output using standard tools: <code>tr</code>, <code>sort</code>, <code>uniq</code><ul><li>Use <code>tr</code> to replace spaces with newlines</li><li>Use <code>sort</code> to sort the results</li><li>Use <code>uniq</code> to aggregate image counts</li></ul></li></ul><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --all-namespaces -o <span>jsonpath</span><span>=</span><span>"{.items[*].spec['initContainers', 'containers'][*].image}"</span> |<span>\
</span></span></span><span><span><span></span>tr -s <span>'[[:space:]]'</span> <span>'\n'</span> |<span>\
</span></span></span><span><span><span></span>sort |<span>\
</span></span></span><span><span><span></span>uniq -c
</span></span></code></pre></div><p>The jsonpath is interpreted as follows:</p><ul><li><code>.items[*]</code>: for each returned value</li><li><code>.spec</code>: get the spec</li><li><code>['initContainers', 'containers'][*]</code>: for each container</li><li><code>.image</code>: get the image</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When fetching a single Pod by name, for example <code>kubectl get pod nginx</code>,
the <code>.items[*]</code> portion of the path should be omitted because a single
Pod is returned instead of a list of items.</div><h2 id="list-container-images-by-pod">List Container images by Pod</h2><p>The formatting can be controlled further by using the <code>range</code> operation to
iterate over elements individually.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --all-namespaces -o <span>jsonpath</span><span>=</span><span>'{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}'</span> |<span>\
</span></span></span><span><span><span></span>sort
</span></span></code></pre></div><h2 id="list-container-images-filtering-by-pod-label">List Container images filtering by Pod label</h2><p>To target only Pods matching a specific label, use the -l flag. The
following matches only Pods with labels matching <code>app=nginx</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --all-namespaces -o <span>jsonpath</span><span>=</span><span>"{.items[*].spec.containers[*].image}"</span> -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><h2 id="list-container-images-filtering-by-pod-namespace">List Container images filtering by Pod namespace</h2><p>To target only pods in a specific namespace, use the namespace flag. The
following matches only Pods in the <code>kube-system</code> namespace.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --namespace kube-system -o <span>jsonpath</span><span>=</span><span>"{.items[*].spec.containers[*].image}"</span>
</span></span></code></pre></div><h2 id="list-container-images-using-a-go-template-instead-of-jsonpath">List Container images using a go-template instead of jsonpath</h2><p>As an alternative to jsonpath, Kubectl supports using <a href="https://pkg.go.dev/text/template">go-templates</a>
for formatting the output:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --all-namespaces -o go-template --template<span>=</span><span>"{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="reference">Reference</h3><ul><li><a href="/docs/reference/kubectl/jsonpath/">Jsonpath</a> reference guide</li><li><a href="https://pkg.go.dev/text/template">Go template</a> reference guide</li></ul></div></div><div><div class="td-content"><h1>Set up Ingress on Minikube with the NGINX Ingress Controller</h1><p>An <a href="/docs/concepts/services-networking/ingress/">Ingress</a> is an API object that defines rules
which allow external access to services in a cluster. An
<a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
fulfills the rules set in the Ingress.</p><p>This page shows you how to set up a simple Ingress which routes requests to Service 'web' or
'web2' depending on the HTTP URI.</p><h2 id="before-you-begin">Before you begin</h2><p>This tutorial assumes that you are using <code>minikube</code> to run a local Kubernetes cluster.
Visit <a href="/docs/tasks/tools/#minikube">Install tools</a> to learn how to install <code>minikube</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This tutorial uses a container that requires the AMD64 architecture.
If you are using minikube on a computer with a different CPU architecture,
you could try using minikube with a driver that can emulate AMD64.
For example, the Docker Desktop driver can do this.</div><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.19.<p>To check the version, enter <code>kubectl version</code>.</p>If you are using an older Kubernetes version, switch to the documentation for that version.</p><h3 id="create-a-minikube-cluster">Create a minikube cluster</h3><p>If you haven't already set up a cluster locally, run <code>minikube start</code> to create a cluster.</p><h2 id="enable-the-ingress-controller">Enable the Ingress controller</h2><ol><li><p>To enable the NGINX Ingress controller, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube addons <span>enable</span> ingress
</span></span></code></pre></div></li><li><p>Verify that the NGINX Ingress controller is running</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -n ingress-nginx
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>It can take up to a minute before you see these pods running OK.</div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">NAME                                        READY   STATUS      RESTARTS    AGE
ingress-nginx-admission-create-g9g49        0/1     Completed   0          11m
ingress-nginx-admission-patch-rqp78         0/1     Completed   1          11m
ingress-nginx-controller-59b45fb494-26npt   1/1     Running     0          11m
</code></pre></li></ol><h2 id="deploy-a-hello-world-app">Deploy a hello, world app</h2><ol><li><p>Create a Deployment using the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment web --image<span>=</span>gcr.io/google-samples/hello-app:1.0
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code class="language-none">deployment.apps/web created
</code></pre><p>Verify that the Deployment is in a Ready state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment web 
</span></span></code></pre></div><p>The output should be similar to:</p><pre tabindex="0"><code class="language-none">NAME   READY   UP-TO-DATE   AVAILABLE   AGE
web    1/1     1            1           53s
</code></pre></li><li><p>Expose the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl expose deployment web --type<span>=</span>NodePort --port<span>=</span><span>8080</span>
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code class="language-none">service/web exposed
</code></pre></li><li><p>Verify the Service is created and is available on a node port:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get service web
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web       NodePort   10.104.133.249   &lt;none&gt;        8080:31637/TCP   12m
</code></pre></li><li><p>Visit the Service via NodePort, using the <a href="https://minikube.sigs.k8s.io/docs/handbook/accessing/#using-minikube-service-with-tunnel"><code>minikube service</code></a> command. Follow the instructions for your platform:</p><p><ul class="nav nav-tabs" id="minikube-service"><li class="nav-item"><a class="nav-link active" href="#minikube-service-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#minikube-service-1">MacOS</a></li></ul><div class="tab-content" id="minikube-service"><div id="minikube-service-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube service web --url
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">http://172.17.0.15:31637
</code></pre><p>Invoke the URL obtained in the output of the previous step:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://172.17.0.15:31637 
</span></span></code></pre></div></p></div><div id="minikube-service-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># The command must be run in a separate terminal.</span>
</span></span><span><span>minikube service web --url 
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">http://127.0.0.1:62445
! Because you are using a Docker driver on darwin, the terminal needs to be open to run it.
</code></pre><p>From a different terminal, invoke the URL obtained in the output of the previous step:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl http://127.0.0.1:62445 
</span></span></code></pre></div></p></div></div><br>The output is similar to:</p><pre tabindex="0"><code class="language-none">Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre><p>You can now access the sample application via the Minikube IP address and NodePort.
The next step lets you access the application using the Ingress resource.</p></li></ol><h2 id="create-an-ingress">Create an Ingress</h2><p>The following manifest defines an Ingress that sends traffic to your Service via
<code>hello-world.example</code>.</p><ol><li><p>Create <code>example-ingress.yaml</code> from the following file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/example-ingress.yaml"><code>service/networking/example-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/example-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-example-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-ingress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ingressClassName</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>host</span>:<span> </span>hello-world.example<span>
</span></span></span><span><span><span>      </span><span>http</span>:<span>
</span></span></span><span><span><span>        </span><span>paths</span>:<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span>/<span>
</span></span></span><span><span><span>            </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>            </span><span>backend</span>:<span>
</span></span></span><span><span><span>              </span><span>service</span>:<span>
</span></span></span><span><span><span>                </span><span>name</span>:<span> </span>web<span>
</span></span></span><span><span><span>                </span><span>port</span>:<span>
</span></span></span><span><span><span>                  </span><span>number</span>:<span> </span><span>8080</span></span></span></code></pre></div></div></div></li><li><p>Create the Ingress object by running the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code class="language-none">ingress.networking.k8s.io/example-ingress created
</code></pre></li><li><p>Verify the IP address is set:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get ingress
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This can take a couple of minutes.</div><p>You should see an IPv4 address in the <code>ADDRESS</code> column; for example:</p><pre tabindex="0"><code class="language-none">NAME              CLASS   HOSTS                 ADDRESS        PORTS   AGE
example-ingress   nginx   hello-world.example   172.17.0.15    80      38s
</code></pre></li><li><p>Verify that the Ingress controller is directing traffic, by following the instructions for your platform:</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The network is limited if using the Docker driver on MacOS (Darwin) and the Node IP is not reachable directly. To get ingress to work you&#8217;ll need to open a new terminal and run <code>minikube tunnel</code>.<br><code>sudo</code> permission is required for it, so provide the password when prompted.</div><p><ul class="nav nav-tabs" id="ingress"><li class="nav-item"><a class="nav-link active" href="#ingress-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#ingress-1">MacOS</a></li></ul><div class="tab-content" id="ingress"><div id="ingress-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --resolve <span>"hello-world.example:80:</span><span>$(</span> minikube ip <span>)</span><span>"</span> -i http://hello-world.example
</span></span></code></pre></div></p></div><div id="ingress-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube tunnel
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">Tunnel successfully started

NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...

The service/ingress example-ingress requires privileged ports to be exposed: [80 443]
sudo permission will be asked for it.
Starting tunnel for service example-ingress.
</code></pre><p>From within a new terminal, invoke the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --resolve <span>"hello-world.example:80:127.0.0.1"</span> -i http://hello-world.example
</span></span></code></pre></div></p></div></div><br>You should see:</p><pre tabindex="0"><code class="language-none">Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre></li><li><p>Optionally, you can also visit <code>hello-world.example</code> from your browser.</p><p>Add a line to the bottom of the <code>/etc/hosts</code> file on
your computer (you will need administrator access):</p><ul class="nav nav-tabs" id="hosts"><li class="nav-item"><a class="nav-link active" href="#hosts-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#hosts-1">MacOS</a></li></ul><div class="tab-content" id="hosts"><div id="hosts-0" class="tab-pane show active"><p><p>Look up the external IP address as reported by minikube</p><pre tabindex="0"><code class="language-none">  minikube ip 
</code></pre><br><pre tabindex="0"><code class="language-none">  172.17.0.15 hello-world.example
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Change the IP address to match the output from <code>minikube ip</code>.</div></p></div><div id="hosts-1" class="tab-pane"><p><pre tabindex="0"><code class="language-none">127.0.0.1 hello-world.example
</code></pre></p></div></div><br><p>After you make this change, your web browser sends requests for
<code>hello-world.example</code> URLs to Minikube.</p></li></ol><h2 id="create-a-second-deployment">Create a second Deployment</h2><ol><li><p>Create another Deployment using the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment web2 --image<span>=</span>gcr.io/google-samples/hello-app:2.0
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code class="language-none">deployment.apps/web2 created
</code></pre><p>Verify that the Deployment is in a Ready state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment web2 
</span></span></code></pre></div><p>The output should be similar to:</p><pre tabindex="0"><code class="language-none">NAME   READY   UP-TO-DATE   AVAILABLE   AGE
web2   1/1     1            1           16s
</code></pre></li><li><p>Expose the second Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl expose deployment web2 --port<span>=</span><span>8080</span> --type<span>=</span>NodePort
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code class="language-none">service/web2 exposed
</code></pre></li></ol><h2 id="edit-ingress">Edit the existing Ingress</h2><ol><li><p>Edit the existing <code>example-ingress.yaml</code> manifest, and add the
following lines at the end:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>- <span>path</span>:<span> </span>/v2<span>
</span></span></span><span><span><span>  </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>  </span><span>backend</span>:<span>
</span></span></span><span><span><span>    </span><span>service</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>web2<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span>
</span></span></span><span><span><span>        </span><span>number</span>:<span> </span><span>8080</span><span>
</span></span></span></code></pre></div></li><li><p>Apply the changes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f example-ingress.yaml
</span></span></code></pre></div><p>You should see:</p><pre tabindex="0"><code class="language-none">ingress.networking/example-ingress configured
</code></pre></li></ol><h2 id="test-your-ingress">Test your Ingress</h2><ol><li><p>Access the 1st version of the Hello World app.</p><p><ul class="nav nav-tabs" id="ingress2-v1"><li class="nav-item"><a class="nav-link active" href="#ingress2-v1-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#ingress2-v1-1">MacOS</a></li></ul><div class="tab-content" id="ingress2-v1"><div id="ingress2-v1-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --resolve <span>"hello-world.example:80:</span><span>$(</span> minikube ip <span>)</span><span>"</span> -i http://hello-world.example
</span></span></code></pre></div></p></div><div id="ingress2-v1-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube tunnel
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">Tunnel successfully started

NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...

The service/ingress example-ingress requires privileged ports to be exposed: [80 443]
sudo permission will be asked for it.
Starting tunnel for service example-ingress.
</code></pre><p>From within a new terminal, invoke the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --resolve <span>"hello-world.example:80:127.0.0.1"</span> -i http://hello-world.example
</span></span></code></pre></div></p></div></div><br></p><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre></li><li><p>Access the 2nd version of the Hello World app.</p><ul class="nav nav-tabs" id="ingress2-v2"><li class="nav-item"><a class="nav-link active" href="#ingress2-v2-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#ingress2-v2-1">MacOS</a></li></ul><div class="tab-content" id="ingress2-v2"><div id="ingress2-v2-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --resolve <span>"hello-world.example:80:</span><span>$(</span> minikube ip <span>)</span><span>"</span> -i http://hello-world.example/v2
</span></span></code></pre></div></p></div><div id="ingress2-v2-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>minikube tunnel
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">Tunnel successfully started

NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...

The service/ingress example-ingress requires privileged ports to be exposed: [80 443]
sudo permission will be asked for it.
Starting tunnel for service example-ingress.
</code></pre><p>From within a new terminal, invoke the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --resolve <span>"hello-world.example:80:127.0.0.1"</span> -i http://hello-world.example/v2
</span></span></code></pre></div></p></div></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">Hello, world!
Version: 2.0.0
Hostname: web2-75cd47646f-t8cjk
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you did the optional step to update <code>/etc/hosts</code>, you can also visit <code>hello-world.example</code> and
<code>hello-world.example/v2</code> from your browser.</div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Read more about <a href="/docs/concepts/services-networking/ingress/">Ingress</a></li><li>Read more about <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</a></li><li>Read more about <a href="/docs/concepts/services-networking/service/">Services</a></li></ul></div></div><div><div class="td-content"><h1>Communicate Between Containers in the Same Pod Using a Shared Volume</h1><p>This page shows how to use a Volume to communicate between two Containers running
in the same Pod. See also how to allow processes to communicate by
<a href="/docs/tasks/configure-pod-container/share-process-namespace/">sharing process namespace</a>
between containers.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="creating-a-pod-that-runs-two-containers">Creating a Pod that runs two Containers</h2><p>In this exercise, you create a Pod that runs two Containers. The two containers
share a Volume that they can use to communicate. Here is the configuration file
for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/two-container-pod.yaml"><code>pods/two-container-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/two-container-pod.yaml to clipboard"></div><div class="includecode" id="pods-two-container-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>two-containers<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shared-data<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>shared-data<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/usr/share/nginx/html<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>debian-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>shared-data<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/pod-data<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"-c"</span>,<span> </span><span>"echo Hello from the debian container &gt; /pod-data/index.html"</span>]<span>
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Pod has a Volume named
<code>shared-data</code>.</p><p>The first container listed in the configuration file runs an nginx server. The
mount path for the shared Volume is <code>/usr/share/nginx/html</code>.
The second container is based on the debian image, and has a mount path of
<code>/pod-data</code>. The second container runs the following command and then terminates.</p><pre><code>echo Hello from the debian container &gt; /pod-data/index.html
</code></pre><p>Notice that the second container writes the <code>index.html</code> file in the root
directory of the nginx server.</p><p>Create the Pod and the two Containers:</p><pre><code>kubectl apply -f https://k8s.io/examples/pods/two-container-pod.yaml
</code></pre><p>View information about the Pod and the Containers:</p><pre><code>kubectl get pod two-containers --output=yaml
</code></pre><p>Here is a portion of the output:</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  ...
  name: two-containers
  namespace: default
  ...
spec:
  ...
  containerStatuses:

  - containerID: docker://c1d8abd1 ...
    image: debian
    ...
    lastState:
      terminated:
        ...
    name: debian-container
    ...

  - containerID: docker://96c1ff2c5bb ...
    image: nginx
    ...
    name: nginx-container
    ...
    state:
      running:
    ...
</code></pre><p>You can see that the debian Container has terminated, and the nginx Container
is still running.</p><p>Get a shell to nginx Container:</p><pre><code>kubectl exec -it two-containers -c nginx-container -- /bin/bash
</code></pre><p>In your shell, verify that nginx is running:</p><pre><code>root@two-containers:/# apt-get update
root@two-containers:/# apt-get install curl procps
root@two-containers:/# ps aux
</code></pre><p>The output is similar to this:</p><pre><code>USER       PID  ...  STAT START   TIME COMMAND
root         1  ...  Ss   21:12   0:00 nginx: master process nginx -g daemon off;
</code></pre><p>Recall that the debian Container created the <code>index.html</code> file in the nginx root
directory. Use <code>curl</code> to send a GET request to the nginx server:</p><pre tabindex="0"><code>root@two-containers:/# curl localhost
</code></pre><p>The output shows that nginx serves a web page written by the debian container:</p><pre tabindex="0"><code>Hello from the debian container
</code></pre><h2 id="discussion">Discussion</h2><p>The primary reason that Pods can have multiple containers is to support
helper applications that assist a primary application. Typical examples of
helper applications are data pullers, data pushers, and proxies.
Helper and primary applications often need to communicate with each other.
Typically this is done through a shared filesystem, as shown in this exercise,
or through the loopback network interface, localhost. An example of this pattern is a
web server along with a helper program that polls a Git repository for new updates.</p><p>The Volume in this exercise provides a way for Containers to communicate during
the life of the Pod. If the Pod is deleted and recreated, any data stored in
the shared Volume is lost.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Learn more about <a href="/blog/2015/06/the-distributed-system-toolkit-patterns/">patterns for composite containers</a>.</p></li><li><p>Learn about <a href="https://www.slideshare.net/Docker/slideshare-burns">composite containers for modular architecture</a>.</p></li><li><p>See <a href="/docs/tasks/configure-pod-container/configure-volume-storage/">Configuring a Pod to Use a Volume for Storage</a>.</p></li><li><p>See <a href="/docs/tasks/configure-pod-container/share-process-namespace/">Configure a Pod to share process namespace between containers in a Pod</a></p></li><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#volume-v1-core">Volume</a>.</p></li><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#pod-v1-core">Pod</a>.</p></li></ul></div></div><div><div class="td-content"><h1>Configure DNS for a Cluster</h1><p>Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm.</p><p>For more information on how to configure CoreDNS for a Kubernetes cluster, see the <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a>. An example demonstrating how to use Kubernetes DNS with kube-dns, see the <a href="https://github.com/kubernetes/examples/tree/master/staging/cluster-dns">Kubernetes DNS sample plugin</a>.</p></div></div><div><div class="td-content"><h1>Access Services Running on Clusters</h1><p>This page shows how to connect to services running on the Kubernetes cluster.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="accessing-services-running-on-the-cluster">Accessing services running on the cluster</h2><p>In Kubernetes, <a href="/docs/concepts/architecture/nodes/">nodes</a>,
<a href="/docs/concepts/workloads/pods/">pods</a> and <a href="/docs/concepts/services-networking/service/">services</a> all have
their own IPs. In many cases, the node IPs, pod IPs, and some service IPs on a cluster will not be
routable, so they will not be reachable from a machine outside the cluster,
such as your desktop machine.</p><h3 id="ways-to-connect">Ways to connect</h3><p>You have several options for connecting to nodes, pods and services from outside the cluster:</p><ul><li>Access services through public IPs.<ul><li>Use a service with type <code>NodePort</code> or <code>LoadBalancer</code> to make the service reachable outside
the cluster. See the <a href="/docs/concepts/services-networking/service/">services</a> and
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose">kubectl expose</a> documentation.</li><li>Depending on your cluster environment, this may only expose the service to your corporate network,
or it may expose it to the internet. Think about whether the service being exposed is secure.
Does it do its own authentication?</li><li>Place pods behind services. To access one specific pod from a set of replicas, such as for debugging,
place a unique label on the pod and create a new service which selects this label.</li><li>In most cases, it should not be necessary for application developer to directly access
nodes via their nodeIPs.</li></ul></li><li>Access services, nodes, or pods using the Proxy Verb.<ul><li>Does apiserver authentication and authorization prior to accessing the remote service.
Use this if the services are not secure enough to expose to the internet, or to gain
access to ports on the node IP, or for debugging.</li><li>Proxies may cause problems for some web applications.</li><li>Only works for HTTP/HTTPS.</li><li>Described <a href="#manually-constructing-apiserver-proxy-urls">here</a>.</li></ul></li><li>Access from a node or pod in the cluster.<ul><li>Run a pod, and then connect to a shell in it using <a href="/docs/reference/generated/kubectl/kubectl-commands/#exec">kubectl exec</a>.
Connect to other nodes, pods, and services from that shell.</li><li>Some clusters may allow you to ssh to a node in the cluster. From there you may be able to
access cluster services. This is a non-standard method, and will work on some clusters but
not others. Browsers and other tools may or may not be installed. Cluster DNS may not work.</li></ul></li></ul><h3 id="discovering-builtin-services">Discovering builtin services</h3><p>Typically, there are several services which are started on a cluster by kube-system. Get a list of these
with the <code>kubectl cluster-info</code> command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cluster-info
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Kubernetes master is running at https://192.0.2.1
elasticsearch-logging is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
kibana-logging is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/kibana-logging/proxy
kube-dns is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/kube-dns/proxy
grafana is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
heapster is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
</code></pre><p>This shows the proxy-verb URL for accessing each service.
For example, this cluster has cluster-level logging enabled (using Elasticsearch), which can be reached
at <code>https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>
if suitable credentials are passed, or through a kubectl proxy at, for example:
<code>http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>See <a href="/docs/tasks/administer-cluster/access-cluster-api/#accessing-the-kubernetes-api">Access Clusters Using the Kubernetes API</a>
for how to pass credentials or use kubectl proxy.</div><h4 id="manually-constructing-apiserver-proxy-urls">Manually constructing apiserver proxy URLs</h4><p>As mentioned above, you use the <code>kubectl cluster-info</code> command to retrieve the service's proxy URL. To create
proxy URLs that include service endpoints, suffixes, and parameters, you append to the service's proxy URL:
<code>http://</code><em><code>kubernetes_master_address</code></em><code>/api/v1/namespaces/</code><em><code>namespace_name</code></em><code>/services/</code><em><code>[https:]service_name[:port_name]</code></em><code>/proxy</code></p><p>If you haven't specified a name for your port, you don't have to specify <em>port_name</em> in the URL. You can also
use the port number in place of the <em>port_name</em> for both named and unnamed ports.</p><p>By default, the API server proxies to your service using HTTP. To use HTTPS, prefix the service name with <code>https:</code>:
<code>http://&lt;kubernetes_master_address&gt;/api/v1/namespaces/&lt;namespace_name&gt;/services/&lt;service_name&gt;/proxy</code></p><p>The supported formats for the <code>&lt;service_name&gt;</code> segment of the URL are:</p><ul><li><code>&lt;service_name&gt;</code> - proxies to the default or unnamed port using http</li><li><code>&lt;service_name&gt;:&lt;port_name&gt;</code> - proxies to the specified port name or port number using http</li><li><code>https:&lt;service_name&gt;:</code> - proxies to the default or unnamed port using https (note the trailing colon)</li><li><code>https:&lt;service_name&gt;:&lt;port_name&gt;</code> - proxies to the specified port name or port number using https</li></ul><h5 id="examples">Examples</h5><ul><li><p>To access the Elasticsearch service endpoint <code>_search?q=user:kimchy</code>, you would use:</p><pre tabindex="0"><code>http://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy
</code></pre></li><li><p>To access the Elasticsearch cluster health information <code>_cluster/health?pretty=true</code>, you would use:</p><pre tabindex="0"><code>https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true
</code></pre><p>The health information is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"cluster_name"</span> : <span>"kubernetes_logging"</span>,
</span></span><span><span>  <span>"status"</span> : <span>"yellow"</span>,
</span></span><span><span>  <span>"timed_out"</span> : <span>false</span>,
</span></span><span><span>  <span>"number_of_nodes"</span> : <span>1</span>,
</span></span><span><span>  <span>"number_of_data_nodes"</span> : <span>1</span>,
</span></span><span><span>  <span>"active_primary_shards"</span> : <span>5</span>,
</span></span><span><span>  <span>"active_shards"</span> : <span>5</span>,
</span></span><span><span>  <span>"relocating_shards"</span> : <span>0</span>,
</span></span><span><span>  <span>"initializing_shards"</span> : <span>0</span>,
</span></span><span><span>  <span>"unassigned_shards"</span> : <span>5</span>
</span></span><span><span>}
</span></span></code></pre></div></li><li><p>To access the <em>https</em> Elasticsearch service health information <code>_cluster/health?pretty=true</code>, you would use:</p><pre tabindex="0"><code>https://192.0.2.1/api/v1/namespaces/kube-system/services/https:elasticsearch-logging:/proxy/_cluster/health?pretty=true
</code></pre></li></ul><h4 id="using-web-browsers-to-access-services-running-on-the-cluster">Using web browsers to access services running on the cluster</h4><p>You may be able to put an apiserver proxy URL into the address bar of a browser. However:</p><ul><li>Web browsers cannot usually pass tokens, so you may need to use basic (password) auth.
Apiserver can be configured to accept basic auth,
but your cluster may not be configured to accept basic auth.</li><li>Some web apps may not work, particularly those with client side javascript that construct URLs in a
way that is unaware of the proxy path prefix.</li></ul></div></div><div><div class="td-content"><h1>Extend Kubernetes</h1><div class="lead">Understand advanced ways to adapt your Kubernetes cluster to the needs of your work environment.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">Configure the Aggregation Layer</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/custom-resources/">Use Custom Resources</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/setup-extension-api-server/">Set up an Extension API Server</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">Configure Multiple Schedulers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/http-proxy-access-api/">Use an HTTP Proxy to Access the Kubernetes API</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/socks5-proxy-access-api/">Use a SOCKS5 Proxy to Access the Kubernetes API</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/setup-konnectivity/">Set up Konnectivity service</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Configure the Aggregation Layer</h1><p>Configuring the <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer</a>
allows the Kubernetes apiserver to be extended with additional APIs, which are not
part of the core Kubernetes APIs.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There are a few setup requirements for getting the aggregation layer working in
your environment to support mutual TLS auth between the proxy and extension apiservers.
Kubernetes and the kube-apiserver have multiple CAs, so make sure that the proxy is
signed by the aggregation layer CA and not by something else, like the Kubernetes general CA.</div><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Reusing the same CA for different client types can negatively impact the cluster's
ability to function. For more information, see <a href="#ca-reusage-and-conflicts">CA Reusage and Conflicts</a>.</div><h2 id="authentication-flow">Authentication Flow</h2><p>Unlike Custom Resource Definitions (CRDs), the Aggregation API involves
another server - your Extension apiserver - in addition to the standard Kubernetes apiserver.
The Kubernetes apiserver will need to communicate with your extension apiserver,
and your extension apiserver will need to communicate with the Kubernetes apiserver.
In order for this communication to be secured, the Kubernetes apiserver uses x509
certificates to authenticate itself to the extension apiserver.</p><p>This section describes how the authentication and authorization flows work,
and how to configure them.</p><p>The high-level flow is as follows:</p><ol><li>Kubernetes apiserver: authenticate the requesting user and authorize their
rights to the requested API path.</li><li>Kubernetes apiserver: proxy the request to the extension apiserver</li><li>Extension apiserver: authenticate the request from the Kubernetes apiserver</li><li>Extension apiserver: authorize the request from the original user</li><li>Extension apiserver: execute</li></ol><p>The rest of this section describes these steps in detail.</p><p>The flow can be seen in the following diagram.</p><p><img alt="aggregation auth flows" src="/images/docs/aggregation-api-auth-flow.png"></p><p>The source for the above swimlanes can be found in the source of this document.</p><h3 id="kubernetes-apiserver-authentication-and-authorization">Kubernetes Apiserver Authentication and Authorization</h3><p>A request to an API path that is served by an extension apiserver begins
the same way as all API requests: communication to the Kubernetes apiserver.
This path already has been registered with the Kubernetes apiserver by the extension apiserver.</p><p>The user communicates with the Kubernetes apiserver, requesting access to the path.
The Kubernetes apiserver uses standard authentication and authorization configured
with the Kubernetes apiserver to authenticate the user and authorize access to the specific path.</p><p>For an overview of authenticating to a Kubernetes cluster, see
<a href="/docs/reference/access-authn-authz/authentication/">"Authenticating to a Cluster"</a>.
For an overview of authorization of access to Kubernetes cluster resources, see
<a href="/docs/reference/access-authn-authz/authorization/">"Authorization Overview"</a>.</p><p>Everything to this point has been standard Kubernetes API requests, authentication and authorization.</p><p>The Kubernetes apiserver now is prepared to send the request to the extension apiserver.</p><h3 id="kubernetes-apiserver-proxies-the-request">Kubernetes Apiserver Proxies the Request</h3><p>The Kubernetes apiserver now will send, or proxy, the request to the extension
apiserver that registered to handle the request. In order to do so,
it needs to know several things:</p><ol><li>How should the Kubernetes apiserver authenticate to the extension apiserver,
informing the extension apiserver that the request, which comes over the network,
is coming from a valid Kubernetes apiserver?</li><li>How should the Kubernetes apiserver inform the extension apiserver of the
username and group for which the original request was authenticated?</li></ol><p>In order to provide for these two, you must configure the Kubernetes apiserver using several flags.</p><h4 id="kubernetes-apiserver-client-authentication">Kubernetes Apiserver Client Authentication</h4><p>The Kubernetes apiserver connects to the extension apiserver over TLS,
authenticating itself using a client certificate. You must provide the
following to the Kubernetes apiserver upon startup, using the provided flags:</p><ul><li>private key file via <code>--proxy-client-key-file</code></li><li>signed client certificate file via <code>--proxy-client-cert-file</code></li><li>certificate of the CA that signed the client certificate file via <code>--requestheader-client-ca-file</code></li><li>valid Common Name values (CNs) in the signed client certificate via <code>--requestheader-allowed-names</code></li></ul><p>The Kubernetes apiserver will use the files indicated by <code>--proxy-client-*-file</code>
to authenticate to the extension apiserver. In order for the request to be considered
valid by a compliant extension apiserver, the following conditions must be met:</p><ol><li>The connection must be made using a client certificate that is signed by
the CA whose certificate is in <code>--requestheader-client-ca-file</code>.</li><li>The connection must be made using a client certificate whose CN is one of
those listed in <code>--requestheader-allowed-names</code>.</li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can set this option to blank as <code>--requestheader-allowed-names=""</code>.
This will indicate to an extension apiserver that <em>any</em> CN is acceptable.</div><p>When started with these options, the Kubernetes apiserver will:</p><ol><li>Use them to authenticate to the extension apiserver.</li><li>Create a configmap in the <code>kube-system</code> namespace called <code>extension-apiserver-authentication</code>,
in which it will place the CA certificate and the allowed CNs. These in turn can be retrieved
by extension apiservers to validate requests.</li></ol><p>Note that the same client certificate is used by the Kubernetes apiserver to authenticate
against <em>all</em> extension apiservers. It does not create a client certificate per extension
apiserver, but rather a single one to authenticate as the Kubernetes apiserver.
This same one is reused for all extension apiserver requests.</p><h4 id="original-request-username-and-group">Original Request Username and Group</h4><p>When the Kubernetes apiserver proxies the request to the extension apiserver,
it informs the extension apiserver of the username and group with which the
original request successfully authenticated. It provides these in http headers
of its proxied request. You must inform the Kubernetes apiserver of the names
of the headers to be used.</p><ul><li>the header in which to store the username via <code>--requestheader-username-headers</code></li><li>the header in which to store the group via <code>--requestheader-group-headers</code></li><li>the prefix to append to all extra headers via <code>--requestheader-extra-headers-prefix</code></li></ul><p>These header names are also placed in the <code>extension-apiserver-authentication</code> configmap,
so they can be retrieved and used by extension apiservers.</p><h3 id="extension-apiserver-authenticates-the-request">Extension Apiserver Authenticates the Request</h3><p>The extension apiserver, upon receiving a proxied request from the Kubernetes apiserver,
must validate that the request actually did come from a valid authenticating proxy,
which role the Kubernetes apiserver is fulfilling. The extension apiserver validates it via:</p><ol><li>Retrieve the following from the configmap in <code>kube-system</code>, as described above:<ul><li>Client CA certificate</li><li>List of allowed names (CNs)</li><li>Header names for username, group and extra info</li></ul></li><li>Check that the TLS connection was authenticated using a client certificate which:<ul><li>Was signed by the CA whose certificate matches the retrieved CA certificate.</li><li>Has a CN in the list of allowed CNs, unless the list is blank, in which case all CNs are allowed.</li><li>Extract the username and group from the appropriate headers</li></ul></li></ol><p>If the above passes, then the request is a valid proxied request from a legitimate
authenticating proxy, in this case the Kubernetes apiserver.</p><p>Note that it is the responsibility of the extension apiserver implementation to provide
the above. Many do it by default, leveraging the <code>k8s.io/apiserver/</code> package.
Others may provide options to override it using command-line options.</p><p>In order to have permission to retrieve the configmap, an extension apiserver
requires the appropriate role. There is a default role named <code>extension-apiserver-authentication-reader</code>
in the <code>kube-system</code> namespace which can be assigned.</p><h3 id="extension-apiserver-authorizes-the-request">Extension Apiserver Authorizes the Request</h3><p>The extension apiserver now can validate that the user/group retrieved from
the headers are authorized to execute the given request. It does so by sending
a standard <a href="/docs/reference/access-authn-authz/authorization/">SubjectAccessReview</a>
request to the Kubernetes apiserver.</p><p>In order for the extension apiserver to be authorized itself to submit the
<code>SubjectAccessReview</code> request to the Kubernetes apiserver, it needs the correct permissions.
Kubernetes includes a default <code>ClusterRole</code> named <code>system:auth-delegator</code> that
has the appropriate permissions. It can be granted to the extension apiserver's service account.</p><h3 id="extension-apiserver-executes">Extension Apiserver Executes</h3><p>If the <code>SubjectAccessReview</code> passes, the extension apiserver executes the request.</p><h2 id="enable-kubernetes-apiserver-flags">Enable Kubernetes Apiserver flags</h2><p>Enable the aggregation layer via the following <code>kube-apiserver</code> flags.
They may have already been taken care of by your provider.</p><pre><code>--requestheader-client-ca-file=&lt;path to aggregator CA cert&gt;
--requestheader-allowed-names=front-proxy-client
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=&lt;path to aggregator proxy cert&gt;
--proxy-client-key-file=&lt;path to aggregator proxy key&gt;
</code></pre><h3 id="ca-reusage-and-conflicts">CA Reusage and Conflicts</h3><p>The Kubernetes apiserver has two client CA options:</p><ul><li><code>--client-ca-file</code></li><li><code>--requestheader-client-ca-file</code></li></ul><p>Each of these functions independently and can conflict with each other,
if not used correctly.</p><ul><li><code>--client-ca-file</code>: When a request arrives to the Kubernetes apiserver,
if this option is enabled, the Kubernetes apiserver checks the certificate
of the request. If it is signed by one of the CA certificates in the file referenced by
<code>--client-ca-file</code>, then the request is treated as a legitimate request,
and the user is the value of the common name <code>CN=</code>, while the group is the organization <code>O=</code>.
See the <a href="/docs/reference/access-authn-authz/authentication/#x509-client-certificates">documentation on TLS authentication</a>.</li><li><code>--requestheader-client-ca-file</code>: When a request arrives to the Kubernetes apiserver,
if this option is enabled, the Kubernetes apiserver checks the certificate of the request.
If it is signed by one of the CA certificates in the file reference by <code>--requestheader-client-ca-file</code>,
then the request is treated as a potentially legitimate request. The Kubernetes apiserver then
checks if the common name <code>CN=</code> is one of the names in the list provided by <code>--requestheader-allowed-names</code>.
If the name is allowed, the request is approved; if it is not, the request is not.</li></ul><p>If <em>both</em> <code>--client-ca-file</code> and <code>--requestheader-client-ca-file</code> are provided,
then the request first checks the <code>--requestheader-client-ca-file</code> CA and then the
<code>--client-ca-file</code>. Normally, different CAs, either root CAs or intermediate CAs,
are used for each of these options; regular client requests match against <code>--client-ca-file</code>,
while aggregation requests match against <code>--requestheader-client-ca-file</code>. However,
if both use the <em>same</em> CA, then client requests that normally would pass via <code>--client-ca-file</code>
will fail, because the CA will match the CA in <code>--requestheader-client-ca-file</code>,
but the common name <code>CN=</code> will <strong>not</strong> match one of the acceptable common names in
<code>--requestheader-allowed-names</code>. This can cause your kubelets and other control plane components,
as well as end-users, to be unable to authenticate to the Kubernetes apiserver.</p><p>For this reason, use different CA certs for the <code>--client-ca-file</code>
option - to authorize control plane components and end-users - and the <code>--requestheader-client-ca-file</code> option - to authorize aggregation apiserver requests.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Do <strong>not</strong> reuse a CA that is used in a different context unless you understand
the risks and the mechanisms to protect the CA's usage.</div><p>If you are not running kube-proxy on a host running the API server,
then you must make sure that the system is enabled with the following
<code>kube-apiserver</code> flag:</p><pre><code>--enable-aggregator-routing=true
</code></pre><h3 id="register-apiservice-objects">Register APIService objects</h3><p>You can dynamically configure what client requests are proxied to extension
apiserver. The following is an example registration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiregistration.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>APIService<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>&lt;name of the registration object&gt;<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>&lt;API group name this extension apiserver hosts&gt;<span>
</span></span></span><span><span><span>  </span><span>version</span>:<span> </span>&lt;API version this extension apiserver hosts&gt;<span>
</span></span></span><span><span><span>  </span><span>groupPriorityMinimum</span>:<span> </span>&lt;priority this APIService for this group, see API documentation&gt;<span>
</span></span></span><span><span><span>  </span><span>versionPriority</span>:<span> </span>&lt;prioritizes ordering of this version within a group, see API documentation&gt;<span>
</span></span></span><span><span><span>  </span><span>service</span>:<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>&lt;namespace of the extension apiserver service&gt;<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>&lt;name of the extension apiserver service&gt;<span>
</span></span></span><span><span><span>  </span><span>caBundle</span>:<span> </span>&lt;pem encoded ca cert that signs the server cert used by the webhook&gt;<span>
</span></span></span></code></pre></div><p>The name of an APIService object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#path-segment-names">path segment name</a>.</p><h4 id="contacting-the-extension-apiserver">Contacting the extension apiserver</h4><p>Once the Kubernetes apiserver has determined a request should be sent to an extension apiserver,
it needs to know how to contact it.</p><p>The <code>service</code> stanza is a reference to the service for an extension apiserver.
The service namespace and name are required. The port is optional and defaults to 443.</p><p>Here is an example of an extension apiserver that is configured to be called on port "1234",
and to verify the TLS connection against the ServerName
<code>my-service-name.my-service-namespace.svc</code> using a custom CA bundle.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiregistration.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>APIService<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>service</span>:<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>my-service-namespace<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>my-service-name<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>1234</span><span>
</span></span></span><span><span><span>  </span><span>caBundle</span>:<span> </span><span>"Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K"</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/extend-kubernetes/setup-extension-api-server/">Set up an extension api-server</a>
to work with the aggregation layer.</li><li>For a high level overview, see
<a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Extending the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Extend the Kubernetes API Using Custom Resource Definitions</a>.</li></ul></div></div><div><div class="td-content"><h1>Use Custom Resources</h1><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinitions</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">Versions in CustomResourceDefinitions</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Extend the Kubernetes API with CustomResourceDefinitions</h1><p>This page shows how to install a
<a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource</a>
into the Kubernetes API by creating a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#customresourcedefinition-v1-apiextensions-k8s-io">CustomResourceDefinition</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.16.<p>To check the version, enter <code>kubectl version</code>.</p>If you are using an older version of Kubernetes that is still supported, switch to
the documentation for that version to see advice that is relevant for your cluster.</p><h2 id="create-a-customresourcedefinition">Create a CustomResourceDefinition</h2><p>When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server
creates a new RESTful resource path for each version you specify. The custom
resource created from a CRD object can be either namespaced or cluster-scoped,
as specified in the CRD's <code>spec.scope</code> field. As with existing built-in
objects, deleting a namespace deletes all custom objects in that namespace.
CustomResourceDefinitions themselves are non-namespaced and are available to
all namespaces.</p><p>For example, if you save the following CustomResourceDefinition to <code>resourcedefinition.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span># list of versions supported by this CustomResourceDefinition</span><span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>      </span><span># Each version can be enabled/disabled by Served flag.</span><span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span># One and only one version must be marked as the storage version.</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>spec</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>              </span><span>properties</span>:<span>
</span></span></span><span><span><span>                </span><span>cronSpec</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>image</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>  </span><span># either Namespaced or Cluster</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span># singular name to be used as an alias on the CLI and for display</span><span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span># kind is normally the CamelCased singular type. Your resource manifests use this.</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span># shortNames allow shorter string to match your resource on the CLI</span><span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f resourcedefinition.yaml
</span></span></code></pre></div><p>Then a new namespaced RESTful API endpoint is created at:</p><pre tabindex="0"><code>/apis/stable.example.com/v1/namespaces/*/crontabs/...
</code></pre><p>This endpoint URL can then be used to create and manage custom objects.
The <code>kind</code> of these objects will be <code>CronTab</code> from the spec of the
CustomResourceDefinition object you created above.</p><p>It might take a few seconds for the endpoint to be created.
You can watch the <code>Established</code> condition of your CustomResourceDefinition
to be true or watch the discovery information of the API server for your
resource to show up.</p><h2 id="create-custom-objects">Create custom objects</h2><p>After the CustomResourceDefinition object has been created, you can create
custom objects. Custom objects can contain custom fields. These fields can
contain arbitrary JSON.
In the following example, the <code>cronSpec</code> and <code>image</code> custom fields are set in a
custom object of kind <code>CronTab</code>. The kind <code>CronTab</code> comes from the spec of the
CustomResourceDefinition object you created above.</p><p>If you save the following YAML to <code>my-crontab.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"* * * * */5"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-crontab.yaml
</span></span></code></pre></div><p>You can then manage your CronTab objects using kubectl. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get crontab
</span></span></code></pre></div><p>Should print a list like this:</p><pre tabindex="0"><code class="language-none">NAME                 AGE
my-new-cron-object   6s
</code></pre><p>Resource names are not case-sensitive when using kubectl, and you can use either
the singular or plural forms defined in the CRD, as well as any short names.</p><p>You can also view the raw YAML data:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get ct -o yaml
</span></span></code></pre></div><p>You should see that it contains the custom <code>cronSpec</code> and <code>image</code> fields
from the YAML you used to create it:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>items</span>:<span>
</span></span></span><span><span><span></span>- <span>apiVersion</span>:<span> </span>stable.example.com/v1<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>  </span><span>metadata</span>:<span>
</span></span></span><span><span><span>    </span><span>annotations</span>:<span>
</span></span></span><span><span><span>      </span><span>kubectl.kubernetes.io/last-applied-configuration</span>:<span> </span>|<span>
</span></span></span><span><span><span>        {"apiVersion":"stable.example.com/v1","kind":"CronTab","metadata":{"annotations":{},"name":"my-new-cron-object","namespace":"default"},"spec":{"cronSpec":"* * * * */5","image":"my-awesome-cron-image"}}</span><span>        
</span></span></span><span><span><span>    </span><span>creationTimestamp</span>:<span> </span><span>"2021-06-20T07:35:27Z"</span><span>
</span></span></span><span><span><span>    </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>    </span><span>resourceVersion</span>:<span> </span><span>"1326"</span><span>
</span></span></span><span><span><span>    </span><span>uid</span>:<span> </span>9aab1d66-628e-41bb-a422-57b8b3b1f5a9<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>cronSpec</span>:<span> </span><span>'* * * * */5'</span><span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>List<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>  </span><span>selfLink</span>:<span> </span><span>""</span><span>
</span></span></span></code></pre></div><h2 id="delete-a-customresourcedefinition">Delete a CustomResourceDefinition</h2><p>When you delete a CustomResourceDefinition, the server will uninstall the RESTful API endpoint
and delete all custom objects stored in it.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f resourcedefinition.yaml
</span></span><span><span>kubectl get crontabs
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Error from server (NotFound): Unable to list {"stable.example.com" "v1" "crontabs"}: the server could not
find the requested resource (get crontabs.stable.example.com)
</code></pre><p>If you later recreate the same CustomResourceDefinition, it will start out empty.</p><h2 id="specifying-a-structural-schema">Specifying a structural schema</h2><p>CustomResources store structured data in custom fields (alongside the built-in
fields <code>apiVersion</code>, <code>kind</code> and <code>metadata</code>, which the API server validates
implicitly). With <a href="#validation">OpenAPI v3.0 validation</a> a schema can be
specified, which is validated during creation and updates, compare below for
details and limits of such a schema.</p><p>With <code>apiextensions.k8s.io/v1</code> the definition of a structural schema is
mandatory for CustomResourceDefinitions. In the beta version of
CustomResourceDefinition, the structural schema was optional.</p><p>A structural schema is an <a href="#validation">OpenAPI v3.0 validation schema</a> which:</p><ol><li>specifies a non-empty type (via <code>type</code> in OpenAPI) for the root, for each specified field of an object node
(via <code>properties</code> or <code>additionalProperties</code> in OpenAPI) and for each item in an array node
(via <code>items</code> in OpenAPI), with the exception of:<ul><li>a node with <code>x-kubernetes-int-or-string: true</code></li><li>a node with <code>x-kubernetes-preserve-unknown-fields: true</code></li></ul></li><li>for each field in an object and each item in an array which is specified within any of <code>allOf</code>, <code>anyOf</code>,
<code>oneOf</code> or <code>not</code>, the schema also specifies the field/item outside of those logical junctors (compare example 1 and 2).</li><li>does not set <code>description</code>, <code>type</code>, <code>default</code>, <code>additionalProperties</code>, <code>nullable</code> within an <code>allOf</code>, <code>anyOf</code>,
<code>oneOf</code> or <code>not</code>, with the exception of the two pattern for <code>x-kubernetes-int-or-string: true</code> (see below).</li><li>if <code>metadata</code> is specified, then only restrictions on <code>metadata.name</code> and <code>metadata.generateName</code> are allowed.</li></ol><p>Non-structural example 1:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>allOf</span>:<span>
</span></span></span><span><span><span></span>- <span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span></code></pre></div><p>conflicts with rule 2. The following would be correct:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>allOf</span>:<span>
</span></span></span><span><span><span></span>- <span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span></code></pre></div><p>Non-structural example 2:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>allOf</span>:<span>
</span></span></span><span><span><span></span>- <span>items</span>:<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>foo</span>:<span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span></code></pre></div><p>conflicts with rule 2. The following would be correct:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>items</span>:<span>
</span></span></span><span><span><span>  </span><span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span></span><span>allOf</span>:<span>
</span></span></span><span><span><span></span>- <span>items</span>:<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>foo</span>:<span>
</span></span></span><span><span><span>        </span><span># ...</span><span>
</span></span></span></code></pre></div><p>Non-structural example 3:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span>
</span></span></span><span><span><span>    </span><span>pattern</span>:<span> </span><span>"abc"</span><span>
</span></span></span><span><span><span>  </span><span>metadata</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>pattern</span>:<span> </span><span>"^a"</span><span>
</span></span></span><span><span><span>      </span><span>finalizers</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>          </span><span>pattern</span>:<span> </span><span>"my-finalizer"</span><span>
</span></span></span><span><span><span></span><span>anyOf</span>:<span>
</span></span></span><span><span><span></span>- <span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>bar</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>      </span><span>minimum</span>:<span> </span><span>42</span><span>
</span></span></span><span><span><span>  </span><span>required</span>:<span> </span>[<span>"bar"</span>]<span>
</span></span></span><span><span><span>  </span><span>description</span>:<span> </span><span>"foo bar object"</span><span>
</span></span></span></code></pre></div><p>is not a structural schema because of the following violations:</p><ul><li>the type at the root is missing (rule 1).</li><li>the type of <code>foo</code> is missing (rule 1).</li><li><code>bar</code> inside of <code>anyOf</code> is not specified outside (rule 2).</li><li><code>bar</code>'s <code>type</code> is within <code>anyOf</code> (rule 3).</li><li>the description is set within <code>anyOf</code> (rule 3).</li><li><code>metadata.finalizers</code> might not be restricted (rule 4).</li></ul><p>In contrast, the following, corresponding schema is structural:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>description</span>:<span> </span><span>"foo bar object"</span><span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>    </span><span>pattern</span>:<span> </span><span>"abc"</span><span>
</span></span></span><span><span><span>  </span><span>bar</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>  </span><span>metadata</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>pattern</span>:<span> </span><span>"^a"</span><span>
</span></span></span><span><span><span></span><span>anyOf</span>:<span>
</span></span></span><span><span><span></span>- <span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>bar</span>:<span>
</span></span></span><span><span><span>      </span><span>minimum</span>:<span> </span><span>42</span><span>
</span></span></span><span><span><span>  </span><span>required</span>:<span> </span>[<span>"bar"</span>]<span>
</span></span></span></code></pre></div><p>Violations of the structural schema rules are reported in the <code>NonStructural</code> condition in the
CustomResourceDefinition.</p><h3 id="field-pruning">Field pruning</h3><p>CustomResourceDefinitions store validated resource data in the cluster's persistence store, <a class="glossary-tooltip" title="Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data." href="/docs/tasks/administer-cluster/configure-upgrade-etcd/" target="_blank">etcd</a>.
As with native Kubernetes resources such as <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a>,
if you specify a field that the API server does not recognize, the unknown field is <em>pruned</em> (removed) before being persisted.</p><p>CRDs converted from <code>apiextensions.k8s.io/v1beta1</code> to <code>apiextensions.k8s.io/v1</code> might lack
structural schemas, and <code>spec.preserveUnknownFields</code> might be <code>true</code>.</p><p>For legacy CustomResourceDefinition objects created as
<code>apiextensions.k8s.io/v1beta1</code> with <code>spec.preserveUnknownFields</code> set to
<code>true</code>, the following is also true:</p><ul><li>Pruning is not enabled.</li><li>You can store arbitrary data.</li></ul><p>For compatibility with <code>apiextensions.k8s.io/v1</code>, update your custom
resource definitions to:</p><ol><li>Use a structural OpenAPI schema.</li><li>Set <code>spec.preserveUnknownFields</code> to <code>false</code>.</li></ol><p>If you save the following YAML to <code>my-crontab.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"* * * * */5"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span><span><span><span>  </span><span>someRandomField</span>:<span> </span><span>42</span><span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create --validate<span>=</span><span>false</span> -f my-crontab.yaml -o yaml
</span></span></code></pre></div><p>Your output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>stable.example.com/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span>2017-05-31T12:56:35Z<span>
</span></span></span><span><span><span>  </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"285"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>9423255b-4600-11e7-af6a-28d2447dc82b<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>'* * * * */5'</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span></code></pre></div><p>Notice that the field <code>someRandomField</code> was pruned.</p><p>This example turned off client-side validation to demonstrate the API server's behavior, by adding
the <code>--validate=false</code> command line option.
Because the <a href="#publish-validation-schema-in-openapi">OpenAPI validation schemas are also published</a>
to clients, <code>kubectl</code> also checks for unknown fields and rejects those objects well before they
would be sent to the API server.</p><h4 id="controlling-pruning">Controlling pruning</h4><p>By default, all unspecified fields for a custom resource, across all versions, are pruned. It is possible though to
opt-out of that for specific sub-trees of fields by adding <code>x-kubernetes-preserve-unknown-fields: true</code> in the
<a href="#specifying-a-structural-schema">structural OpenAPI v3 validation schema</a>.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>json</span>:<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-preserve-unknown-fields</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>The field <code>json</code> can store any JSON value, without anything being pruned.</p><p>You can also partially specify the permitted JSON; for example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>json</span>:<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-preserve-unknown-fields</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>description</span>:<span> </span>this is arbitrary JSON<span>
</span></span></span></code></pre></div><p>With this, only <code>object</code> type values are allowed.</p><p>Pruning is enabled again for each specified property (or <code>additionalProperties</code>):</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>json</span>:<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-preserve-unknown-fields</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>spec</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>foo</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>          </span><span>bar</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span></code></pre></div><p>With this, the value:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>json</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span> </span>abc<span>
</span></span></span><span><span><span>    </span><span>bar</span>:<span> </span>def<span>
</span></span></span><span><span><span>    </span><span>something</span>:<span> </span>x<span>
</span></span></span><span><span><span>  </span><span>status</span>:<span>
</span></span></span><span><span><span>    </span><span>something</span>:<span> </span>x<span>
</span></span></span></code></pre></div><p>is pruned to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>json</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span> </span>abc<span>
</span></span></span><span><span><span>    </span><span>bar</span>:<span> </span>def<span>
</span></span></span><span><span><span>  </span><span>status</span>:<span>
</span></span></span><span><span><span>    </span><span>something</span>:<span> </span>x<span>
</span></span></span></code></pre></div><p>This means that the <code>something</code> field in the specified <code>spec</code> object is pruned, but everything outside is not.</p><h3 id="intorstring">IntOrString</h3><p>Nodes in a schema with <code>x-kubernetes-int-or-string: true</code> are excluded from rule 1, such that the
following is structural:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-int-or-string</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>Also those nodes are partially excluded from rule 3 in the sense that the following two patterns are allowed
(exactly those, without variations in order to additional fields):</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>x-kubernetes-int-or-string</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span></span><span>anyOf</span>:<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>  </span>- <span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>and</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>x-kubernetes-int-or-string</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span></span><span>allOf</span>:<span>
</span></span></span><span><span><span>  </span>- <span>anyOf</span>:<span>
</span></span></span><span><span><span>      </span>- <span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>      </span>- <span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span>- <span># ... zero or more</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>With one of those specification, both an integer and a string validate.</p><p>In <a href="#publish-validation-schema-in-openapi">Validation Schema Publishing</a>,
<code>x-kubernetes-int-or-string: true</code> is unfolded to one of the two patterns shown above.</p><h3 id="rawextension">RawExtension</h3><p>RawExtensions (as in <a href="/docs/reference//kubernetes-api/workload-resources/controller-revision-v1#RawExtension"><code>runtime.RawExtension</code></a>)
holds complete Kubernetes objects, i.e. with <code>apiVersion</code> and <code>kind</code> fields.</p><p>It is possible to specify those embedded objects (both completely without constraints or partially specified)
by setting <code>x-kubernetes-embedded-resource: true</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-embedded-resource</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-preserve-unknown-fields</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>Here, the field <code>foo</code> holds a complete object, e.g.:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>foo</span>:<span>
</span></span></span><span><span><span>  </span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span># ...</span><span>
</span></span></span></code></pre></div><p>Because <code>x-kubernetes-preserve-unknown-fields: true</code> is specified alongside, nothing is pruned.
The use of <code>x-kubernetes-preserve-unknown-fields: true</code> is optional though.</p><p>With <code>x-kubernetes-embedded-resource: true</code>, the <code>apiVersion</code>, <code>kind</code> and <code>metadata</code> are implicitly specified and validated.</p><h2 id="serving-multiple-versions-of-a-crd">Serving multiple versions of a CRD</h2><p>See <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">Custom resource definition versioning</a>
for more information about serving multiple versions of your
CustomResourceDefinition and migrating your objects from one version to another.</p><h2 id="advanced-topics">Advanced topics</h2><h3 id="finalizers">Finalizers</h3><p><em>Finalizers</em> allow controllers to implement asynchronous pre-delete hooks.
Custom objects support finalizers similar to built-in objects.</p><p>You can add a finalizer to a custom object like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>finalizers</span>:<span>
</span></span></span><span><span><span>  </span>- stable.example.com/finalizer<span>
</span></span></span></code></pre></div><p>Identifiers of custom finalizers consist of a domain name, a forward slash and the name of
the finalizer. Any controller can add a finalizer to any object's list of finalizers.</p><p>The first delete request on an object with finalizers sets a value for the
<code>metadata.deletionTimestamp</code> field but does not delete it. Once this value is set,
entries in the <code>finalizers</code> list can only be removed. While any finalizers remain it is also
impossible to force the deletion of an object.</p><p>When the <code>metadata.deletionTimestamp</code> field is set, controllers watching the object execute any
finalizers they handle and remove the finalizer from the list after they are done. It is the
responsibility of each controller to remove its finalizer from the list.</p><p>The value of <code>metadata.deletionGracePeriodSeconds</code> controls the interval between polling updates.</p><p>Once the list of finalizers is empty, meaning all finalizers have been executed, the resource is
deleted by Kubernetes.</p><h3 id="validation">Validation</h3><p>Custom resources are validated via
<a href="https://github.com/OAI/OpenAPI-Specification/blob/3.0.0/versions/3.0.0.md#schema-object">OpenAPI v3.0 schemas</a>,
by x-kubernetes-validations when the <a href="#validation-rules">Validation Rules feature</a> is enabled, and you
can add additional validation using
<a href="/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">admission webhooks</a>.</p><p>Additionally, the following restrictions are applied to the schema:</p><ul><li><p>These fields cannot be set:</p><ul><li><code>definitions</code>,</li><li><code>dependencies</code>,</li><li><code>deprecated</code>,</li><li><code>discriminator</code>,</li><li><code>id</code>,</li><li><code>patternProperties</code>,</li><li><code>readOnly</code>,</li><li><code>writeOnly</code>,</li><li><code>xml</code>,</li><li><code>$ref</code>.</li></ul></li><li><p>The field <code>uniqueItems</code> cannot be set to <code>true</code>.</p></li><li><p>The field <code>additionalProperties</code> cannot be set to <code>false</code>.</p></li><li><p>The field <code>additionalProperties</code> is mutually exclusive with <code>properties</code>.</p></li></ul><p>The <code>x-kubernetes-validations</code> extension can be used to validate custom resources using
<a href="https://github.com/google/cel-spec">Common Expression Language (CEL)</a> expressions when the
<a href="#validation-rules">Validation rules</a> feature is enabled and the CustomResourceDefinition schema is a
<a href="#specifying-a-structural-schema">structural schema</a>.</p><p>Refer to the <a href="#specifying-a-structural-schema">structural schemas</a> section for other
restrictions and CustomResourceDefinition features.</p><p>The schema is defined in the CustomResourceDefinition. In the following example, the
CustomResourceDefinition applies the following validations on the custom object:</p><ul><li><code>spec.cronSpec</code> must be a string and must be of the form described by the regular expression.</li><li><code>spec.replicas</code> must be an integer and must have a minimum value of 1 and a maximum value of 10.</li></ul><p>Save the CustomResourceDefinition to <code>resourcedefinition.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span># openAPIV3Schema is the schema for validating custom objects.</span><span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>spec</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>              </span><span>properties</span>:<span>
</span></span></span><span><span><span>                </span><span>cronSpec</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                  </span><span>pattern</span>:<span> </span><span>'^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'</span><span>
</span></span></span><span><span><span>                </span><span>image</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>                  </span><span>minimum</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>                  </span><span>maximum</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f resourcedefinition.yaml
</span></span></code></pre></div><p>A request to create a custom object of kind CronTab is rejected if there are invalid values in its fields.
In the following example, the custom object contains fields with invalid values:</p><ul><li><code>spec.cronSpec</code> does not match the regular expression.</li><li><code>spec.replicas</code> is greater than 10.</li></ul><p>If you save the following YAML to <code>my-crontab.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"* * * *"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>15</span><span>
</span></span></span></code></pre></div><p>and attempt to create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-crontab.yaml
</span></span></code></pre></div><p>then you get an error:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>The CronTab "my-new-cron-object" is invalid: []: Invalid value: map[string]interface {}{"apiVersion":"stable.example.com/v1", "kind":"CronTab", "metadata":map[string]interface {}{"name":"my-new-cron-object", "namespace":"default", "deletionTimestamp":interface {}(nil), "deletionGracePeriodSeconds":(*int64)(nil), "creationTimestamp":"2017-09-05T05:20:07Z", "uid":"e14d79e7-91f9-11e7-a598-f0761cb232d1", "clusterName":""}, "spec":map[string]interface {}{"cronSpec":"* * * *", "image":"my-awesome-cron-image", "replicas":15}}:
</span></span></span><span><span><span>validation failure list:
</span></span></span><span><span><span>spec.cronSpec in body should match '^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'
</span></span></span><span><span><span>spec.replicas in body should be less than or equal to 10
</span></span></span></code></pre></div><p>If the fields contain valid values, the object creation request is accepted.</p><p>Save the following YAML to <code>my-crontab.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"* * * * */5"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>5</span><span>
</span></span></span></code></pre></div><p>And create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-crontab.yaml
</span></span><span><span>crontab <span>"my-new-cron-object"</span> created
</span></span></code></pre></div><h3 id="validation-ratcheting">Validation ratcheting</h3><div class="feature-state-notice feature-stable" title="Feature Gate: CRDValidationRatcheting"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>If you are using a version of Kubernetes older than v1.30, you need to explicitly
enable the <code>CRDValidationRatcheting</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> to
use this behavior, which then applies to all CustomResourceDefinitions in your
cluster.</p><p>Provided you enabled the feature gate, Kubernetes implements <em>validation ratcheting</em>
for CustomResourceDefinitions. The API server is willing to accept updates to resources that
are not valid after the update, provided that each part of the resource that failed to validate
was not changed by the update operation. In other words, any invalid part of the resource
that remains invalid must have already been wrong.
You cannot use this mechanism to update a valid resource so that it becomes invalid.</p><p>This feature allows authors of CRDs to confidently add new validations to the
OpenAPIV3 schema under certain conditions. Users can update to the new schema
safely without bumping the version of the object or breaking workflows.</p><p>While most validations placed in the OpenAPIV3 schema of a CRD support
ratcheting, there are a few exceptions. The following OpenAPIV3 schema
validations are not supported by ratcheting under the implementation in Kubernetes
1.34 and if violated will continue to throw an error as normally:</p><ul><li><p>Quantors</p><ul><li><code>allOf</code></li><li><code>oneOf</code></li><li><code>anyOf</code></li><li><code>not</code></li><li>any validations in a descendent of one of these fields</li></ul></li><li><p><code>x-kubernetes-validations</code>
For Kubernetes 1.28, CRD <a href="#validation-rules">validation rules</a> are ignored by
ratcheting. Starting with Alpha 2 in Kubernetes 1.29, <code>x-kubernetes-validations</code>
are ratcheted only if they do not refer to <code>oldSelf</code>.</p><p>Transition Rules are never ratcheted: only errors raised by rules that do not
use <code>oldSelf</code> will be automatically ratcheted if their values are unchanged.</p><p>To write custom ratcheting logic for CEL expressions, check out <a href="#field-optional-oldself">optionalOldSelf</a>.</p></li><li><p><code>x-kubernetes-list-type</code>
Errors arising from changing the list type of a subschema will not be
ratcheted. For example adding <code>set</code> onto a list with duplicates will always
result in an error.</p></li><li><p><code>x-kubernetes-list-map-keys</code>
Errors arising from changing the map keys of a list schema will not be
ratcheted.</p></li><li><p><code>required</code>
Errors arising from changing the list of required fields will not be ratcheted.</p></li><li><p><code>properties</code>
Adding/removing/modifying the names of properties is not ratcheted, but
changes to validations in each properties' schemas and subschemas may be ratcheted
if the name of the property stays the same.</p></li><li><p><code>additionalProperties</code>
To remove a previously specified <code>additionalProperties</code> validation will not be
ratcheted.</p></li><li><p><code>metadata</code>
Errors that come from Kubernetes' built-in validation of an object's <code>metadata</code>
are not ratcheted (such as object name, or characters in a label value).
If you specify your own additional rules for the metadata of a custom resource,
that additional validation will be ratcheted.</p></li></ul><h3 id="validation-rules">Validation rules</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div><p>Validation rules use the <a href="https://github.com/google/cel-spec">Common Expression Language (CEL)</a>
to validate custom resource values. Validation rules are included in
CustomResourceDefinition schemas using the <code>x-kubernetes-validations</code> extension.</p><p>The Rule is scoped to the location of the <code>x-kubernetes-validations</code> extension in the schema.
And <code>self</code> variable in the CEL expression is bound to the scoped value.</p><p>All validation rules are scoped to the current object: no cross-object or stateful validation
rules are supported.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>spec</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"self.minReplicas &lt;= self.replicas"</span><span>
</span></span></span><span><span><span>            </span><span>message</span>:<span> </span><span>"replicas should be greater than or equal to minReplicas."</span><span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"self.replicas &lt;= self.maxReplicas"</span><span>
</span></span></span><span><span><span>            </span><span>message</span>:<span> </span><span>"replicas should be smaller than or equal to maxReplicas."</span><span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span># ...</span><span>
</span></span></span><span><span><span>          </span><span>minReplicas</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>          </span><span>replicas</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>          </span><span>maxReplicas</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>        </span><span>required</span>:<span>
</span></span></span><span><span><span>          </span>- minReplicas<span>
</span></span></span><span><span><span>          </span>- replicas<span>
</span></span></span><span><span><span>          </span>- maxReplicas<span>
</span></span></span></code></pre></div><p>will reject a request to create this custom resource:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>minReplicas</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>20</span><span>
</span></span></span><span><span><span>  </span><span>maxReplicas</span>:<span> </span><span>10</span><span>
</span></span></span></code></pre></div><p>with the response:</p><pre tabindex="0"><code>The CronTab "my-new-cron-object" is invalid:
* spec: Invalid value: map[string]interface {}{"maxReplicas":10, "minReplicas":0, "replicas":20}: replicas should be smaller than or equal to maxReplicas.
</code></pre><p><code>x-kubernetes-validations</code> could have multiple rules.
The <code>rule</code> under <code>x-kubernetes-validations</code> represents the expression which will be evaluated by CEL.
The <code>message</code> represents the message displayed when validation fails. If message is unset, the
above response would be:</p><pre tabindex="0"><code>The CronTab "my-new-cron-object" is invalid:
* spec: Invalid value: map[string]interface {}{"maxReplicas":10, "minReplicas":0, "replicas":20}: failed rule: self.replicas &lt;= self.maxReplicas
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can quickly test CEL expressions in <a href="https://playcel.undistro.io">CEL Playground</a>.</div><p>Validation rules are compiled when CRDs are created/updated.
The request of CRDs create/update will fail if compilation of validation rules fail.
Compilation process includes type checking as well.</p><p>The compilation failure:</p><ul><li><p><code>no_matching_overload</code>: this function has no overload for the types of the arguments.</p><p>For example, a rule like <code>self == true</code> against a field of integer type will get error:</p><pre tabindex="0"><code class="language-none">Invalid value: apiextensions.ValidationRule{Rule:"self == true", Message:""}: compilation failed: ERROR: \&lt;input&gt;:1:6: found no matching overload for '_==_' applied to '(int, bool)'
</code></pre></li><li><p><code>no_such_field</code>: does not contain the desired field.</p><p>For example, a rule like <code>self.nonExistingField &gt; 0</code> against a non-existing field will return
the following error:</p><pre tabindex="0"><code class="language-none">Invalid value: apiextensions.ValidationRule{Rule:"self.nonExistingField &gt; 0", Message:""}: compilation failed: ERROR: \&lt;input&gt;:1:5: undefined field 'nonExistingField'
</code></pre></li><li><p><code>invalid argument</code>: invalid argument to macros.</p><p>For example, a rule like <code>has(self)</code> will return error:</p><pre tabindex="0"><code class="language-none">Invalid value: apiextensions.ValidationRule{Rule:"has(self)", Message:""}: compilation failed: ERROR: &lt;input&gt;:1:4: invalid argument to has() macro
</code></pre></li></ul><p>Validation Rules Examples:</p><table><thead><tr><th>Rule</th><th>Purpose</th></tr></thead><tbody><tr><td><code>self.minReplicas &lt;= self.replicas &amp;&amp; self.replicas &lt;= self.maxReplicas</code></td><td>Validate that the three fields defining replicas are ordered appropriately</td></tr><tr><td><code>'Available' in self.stateCounts</code></td><td>Validate that an entry with the 'Available' key exists in a map</td></tr><tr><td><code>(size(self.list1) == 0) != (size(self.list2) == 0)</code></td><td>Validate that one of two lists is non-empty, but not both</td></tr><tr><td><code>!('MY_KEY' in self.map1) || self['MY_KEY'].matches('^[a-zA-Z]*$')</code></td><td>Validate the value of a map for a specific key, if it is in the map</td></tr><tr><td><code>self.envars.filter(e, e.name == 'MY_ENV').all(e, e.value.matches('^[a-zA-Z]*$')</code></td><td>Validate the 'value' field of a listMap entry where key field 'name' is 'MY_ENV'</td></tr><tr><td><code>has(self.expired) &amp;&amp; self.created + self.ttl &lt; self.expired</code></td><td>Validate that 'expired' date is after a 'create' date plus a 'ttl' duration</td></tr><tr><td><code>self.health.startsWith('ok')</code></td><td>Validate a 'health' string field has the prefix 'ok'</td></tr><tr><td><code>self.widgets.exists(w, w.key == 'x' &amp;&amp; w.foo &lt; 10)</code></td><td>Validate that the 'foo' property of a listMap item with a key 'x' is less than 10</td></tr><tr><td><code>type(self) == string ? self == '100%' : self == 1000</code></td><td>Validate an int-or-string field for both the int and string cases</td></tr><tr><td><code>self.metadata.name.startsWith(self.prefix)</code></td><td>Validate that an object's name has the prefix of another field value</td></tr><tr><td><code>self.set1.all(e, !(e in self.set2))</code></td><td>Validate that two listSets are disjoint</td></tr><tr><td><code>size(self.names) == size(self.details) &amp;&amp; self.names.all(n, n in self.details)</code></td><td>Validate the 'details' map is keyed by the items in the 'names' listSet</td></tr><tr><td><code>size(self.clusters.filter(c, c.name == self.primary)) == 1</code></td><td>Validate that the 'primary' property has one and only one occurrence in the 'clusters' listMap</td></tr></tbody></table><p>Xref: <a href="https://github.com/google/cel-spec/blob/v0.6.0/doc/langdef.md#evaluation">Supported evaluation on CEL</a></p><ul><li><p>If the Rule is scoped to the root of a resource, it may make field selection into any fields
declared in the OpenAPIv3 schema of the CRD as well as <code>apiVersion</code>, <code>kind</code>, <code>metadata.name</code> and
<code>metadata.generateName</code>. This includes selection of fields in both the <code>spec</code> and <code>status</code> in the
same expression:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>      </span>- <span>rule</span>:<span> </span><span>"self.status.availableReplicas &gt;= self.spec.minReplicas"</span><span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>        </span><span>spec</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>minReplicas</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>            </span><span># ...</span><span>
</span></span></span><span><span><span>        </span><span>status</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>availableReplicas</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>integer<span>
</span></span></span></code></pre></div></li><li><p>If the Rule is scoped to an object with properties, the accessible properties of the object are field selectable
via <code>self.field</code> and field presence can be checked via <code>has(self.field)</code>. Null valued fields are treated as
absent fields in CEL expressions.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>spec</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"has(self.foo)"</span><span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span># ...</span><span>
</span></span></span><span><span><span>          </span><span>foo</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>integer<span>
</span></span></span></code></pre></div></li><li><p>If the Rule is scoped to an object with additionalProperties (i.e. a map) the value of the map
are accessible via <code>self[mapKey]</code>, map containment can be checked via <code>mapKey in self</code> and all
entries of the map are accessible via CEL macros and functions such as <code>self.all(...)</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>spec</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"self['xyz'].foo &gt; 0"</span><span>
</span></span></span><span><span><span>        </span><span>additionalProperties</span>:<span>
</span></span></span><span><span><span>          </span><span># ...</span><span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>foo</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>integer<span>
</span></span></span></code></pre></div></li><li><p>If the Rule is scoped to an array, the elements of the array are accessible via <code>self[i]</code> and
also by macros and functions.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span># ...</span><span>
</span></span></span><span><span><span>      </span><span>foo</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>        </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"size(self) == 1"</span><span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>string<span>
</span></span></span></code></pre></div></li><li><p>If the Rule is scoped to a scalar, <code>self</code> is bound to the scalar value.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span># ...</span><span>
</span></span></span><span><span><span>  </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>spec</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span># ...</span><span>
</span></span></span><span><span><span>          </span><span>foo</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>            </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>            </span>- <span>rule</span>:<span> </span><span>"self &gt; 0"</span><span>
</span></span></span></code></pre></div></li></ul><p>Examples:</p><table><thead><tr><th>type of the field rule scoped to</th><th>Rule example</th></tr></thead><tbody><tr><td>root object</td><td><code>self.status.actual &lt;= self.spec.maxDesired</code></td></tr><tr><td>map of objects</td><td><code>self.components['Widget'].priority &lt; 10</code></td></tr><tr><td>list of integers</td><td><code>self.values.all(value, value &gt;= 0 &amp;&amp; value &lt; 100)</code></td></tr><tr><td>string</td><td><code>self.startsWith('kube')</code></td></tr></tbody></table><p>The <code>apiVersion</code>, <code>kind</code>, <code>metadata.name</code> and <code>metadata.generateName</code> are always accessible from
the root of the object and from any <code>x-kubernetes-embedded-resource</code> annotated objects. No other
metadata properties are accessible.</p><p>Unknown data preserved in custom resources via <code>x-kubernetes-preserve-unknown-fields</code> is not
accessible in CEL expressions. This includes:</p><ul><li><p>Unknown field values that are preserved by object schemas with <code>x-kubernetes-preserve-unknown-fields</code>.</p></li><li><p>Object properties where the property schema is of an "unknown type". An "unknown type" is
recursively defined as:</p><ul><li>A schema with no type and x-kubernetes-preserve-unknown-fields set to true</li><li>An array where the items schema is of an "unknown type"</li><li>An object where the additionalProperties schema is of an "unknown type"</li></ul></li></ul><p>Only property names of the form <code>[a-zA-Z_.-/][a-zA-Z0-9_.-/]*</code> are accessible.
Accessible property names are escaped according to the following rules when accessed in the expression:</p><table><thead><tr><th>escape sequence</th><th>property name equivalent</th></tr></thead><tbody><tr><td><code>__underscores__</code></td><td><code>__</code></td></tr><tr><td><code>__dot__</code></td><td><code>.</code></td></tr><tr><td><code>__dash__</code></td><td><code>-</code></td></tr><tr><td><code>__slash__</code></td><td><code>/</code></td></tr><tr><td><code>__{keyword}__</code></td><td><a href="https://github.com/google/cel-spec/blob/v0.6.0/doc/langdef.md#syntax">CEL RESERVED keyword</a></td></tr></tbody></table><p>Note: CEL RESERVED keyword needs to match the exact property name to be escaped (e.g. int in the word sprint would not be escaped).</p><p>Examples on escaping:</p><table><thead><tr><th>property name</th><th>rule with escaped property name</th></tr></thead><tbody><tr><td>namespace</td><td><code>self.__namespace__ &gt; 0</code></td></tr><tr><td>x-prop</td><td><code>self.x__dash__prop &gt; 0</code></td></tr><tr><td>redact__d</td><td><code>self.redact__underscores__d &gt; 0</code></td></tr><tr><td>string</td><td><code>self.startsWith('kube')</code></td></tr></tbody></table><p>Equality on arrays with <code>x-kubernetes-list-type</code> of <code>set</code> or <code>map</code> ignores element order,
i.e., <code>[1, 2] == [2, 1]</code>. Concatenation on arrays with x-kubernetes-list-type use the semantics of
the list type:</p><ul><li><p><code>set</code>: <code>X + Y</code> performs a union where the array positions of all elements in <code>X</code> are preserved
and non-intersecting elements in <code>Y</code> are appended, retaining their partial order.</p></li><li><p><code>map</code>: <code>X + Y</code> performs a merge where the array positions of all keys in <code>X</code> are preserved but
the values are overwritten by values in <code>Y</code> when the key sets of <code>X</code> and <code>Y</code> intersect. Elements
in <code>Y</code> with non-intersecting keys are appended, retaining their partial order.</p></li></ul><p>Here is the declarations type mapping between OpenAPIv3 and CEL type:</p><table><thead><tr><th>OpenAPIv3 type</th><th>CEL type</th></tr></thead><tbody><tr><td>'object' with Properties</td><td>object / "message type"</td></tr><tr><td>'object' with AdditionalProperties</td><td>map</td></tr><tr><td>'object' with x-kubernetes-embedded-type</td><td>object / "message type", 'apiVersion', 'kind', 'metadata.name' and 'metadata.generateName' are implicitly included in schema</td></tr><tr><td>'object' with x-kubernetes-preserve-unknown-fields</td><td>object / "message type", unknown fields are NOT accessible in CEL expression</td></tr><tr><td>x-kubernetes-int-or-string</td><td>dynamic object that is either an int or a string, <code>type(value)</code> can be used to check the type</td></tr><tr><td>'array</td><td>list</td></tr><tr><td>'array' with x-kubernetes-list-type=map</td><td>list with map based Equality &amp; unique key guarantees</td></tr><tr><td>'array' with x-kubernetes-list-type=set</td><td>list with set based Equality &amp; unique entry guarantees</td></tr><tr><td>'boolean'</td><td>boolean</td></tr><tr><td>'number' (all formats)</td><td>double</td></tr><tr><td>'integer' (all formats)</td><td>int (64)</td></tr><tr><td>'null'</td><td>null_type</td></tr><tr><td>'string'</td><td>string</td></tr><tr><td>'string' with format=byte (base64 encoded)</td><td>bytes</td></tr><tr><td>'string' with format=date</td><td>timestamp (google.protobuf.Timestamp)</td></tr><tr><td>'string' with format=datetime</td><td>timestamp (google.protobuf.Timestamp)</td></tr><tr><td>'string' with format=duration</td><td>duration (google.protobuf.Duration)</td></tr></tbody></table><p>xref: <a href="https://github.com/google/cel-spec/blob/v0.6.0/doc/langdef.md#values">CEL types</a>,
<a href="https://swagger.io/specification/#data-types">OpenAPI types</a>,
<a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#specifying-a-structural-schema">Kubernetes Structural Schemas</a>.</p><h4 id="the-messageexpression-field">The messageExpression field</h4><p>Similar to the <code>message</code> field, which defines the string reported for a validation rule failure,
<code>messageExpression</code> allows you to use a CEL expression to construct the message string.
This allows you to insert more descriptive information into the validation failure message.
<code>messageExpression</code> must evaluate a string and may use the same variables that are available to the <code>rule</code>
field. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span></span>- <span>rule</span>:<span> </span><span>"self.x &lt;= self.maxLimit"</span><span>
</span></span></span><span><span><span>  </span><span>messageExpression</span>:<span> </span><span>'"x exceeded max limit of " + string(self.maxLimit)'</span><span>
</span></span></span></code></pre></div><p>Keep in mind that CEL string concatenation (<code>+</code> operator) does not auto-cast to string. If
you have a non-string scalar, use the <code>string(&lt;value&gt;)</code> function to cast the scalar to a string
like shown in the above example.</p><p><code>messageExpression</code> must evaluate to a string, and this is checked while the CRD is being written. Note that it is possible
to set <code>message</code> and <code>messageExpression</code> on the same rule, and if both are present, <code>messageExpression</code>
will be used. However, if <code>messageExpression</code> evaluates to an error, the string defined in <code>message</code>
will be used instead, and the <code>messageExpression</code> error will be logged. This fallback will also occur if
the CEL expression defined in <code>messageExpression</code> generates an empty string, or a string containing line
breaks.</p><p>If one of the above conditions are met and no <code>message</code> has been set, then the default validation failure
message will be used instead.</p><p><code>messageExpression</code> is a CEL expression, so the restrictions listed in <a href="#resource-use-by-validation-functions">Resource use by validation functions</a> apply. If evaluation halts due to resource constraints
during <code>messageExpression</code> execution, then no further validation rules will be executed.</p><p>Setting <code>messageExpression</code> is optional.</p><h4 id="field-message">The <code>message</code> field</h4><p>If you want to set a static message, you can supply <code>message</code> rather than <code>messageExpression</code>.
The value of <code>message</code> is used as an opaque error string if validation fails.</p><p>Setting <code>message</code> is optional.</p><h4 id="field-reason">The <code>reason</code> field</h4><p>You can add a machine-readable validation failure reason within a <code>validation</code>, to be returned
whenever a request fails this validation rule.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span></span>- <span>rule</span>:<span> </span><span>"self.x &lt;= self.maxLimit"</span><span>
</span></span></span><span><span><span>  </span><span>reason</span>:<span> </span><span>"FieldValueInvalid"</span><span>
</span></span></span></code></pre></div><p>The HTTP status code returned to the caller will match the reason of the first failed validation rule.
The currently supported reasons are: "FieldValueInvalid", "FieldValueForbidden", "FieldValueRequired", "FieldValueDuplicate".
If not set or unknown reasons, default to use "FieldValueInvalid".</p><p>Setting <code>reason</code> is optional.</p><h4 id="field-field-path">The <code>fieldPath</code> field</h4><p>You can specify the field path returned when the validation fails.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span></span>- <span>rule</span>:<span> </span><span>"self.foo.test.x &lt;= self.maxLimit"</span><span>
</span></span></span><span><span><span>  </span><span>fieldPath</span>:<span> </span><span>".foo.test.x"</span><span>
</span></span></span></code></pre></div><p>In the example above, the validation checks the value of field <code>x</code> should be less than the value of <code>maxLimit</code>.
If no <code>fieldPath</code> specified, when validation fails, the fieldPath would be default to wherever <code>self</code> scoped.
With <code>fieldPath</code> specified, the returned error will have <code>fieldPath</code> properly refer to the location of field <code>x</code>.</p><p>The <code>fieldPath</code> value must be a relative JSON path that is scoped to the location of this x-kubernetes-validations extension in the schema.
Additionally, it should refer to an existing field within the schema.
For example when validation checks if a specific attribute <code>foo</code> under a map <code>testMap</code>, you could set
<code>fieldPath</code> to <code>".testMap.foo"</code> or <code>.testMap['foo']'</code>.
If the validation requires checking for unique attributes in two lists, the fieldPath can be set to either of the lists.
For example, it can be set to <code>.testList1</code> or <code>.testList2</code>.
It supports child operation to refer to an existing field currently.
Refer to <a href="/docs/reference/kubectl/jsonpath/">JSONPath support in Kubernetes</a> for more info.
The <code>fieldPath</code> field does not support indexing arrays numerically.</p><p>Setting <code>fieldPath</code> is optional.</p><h4 id="field-optional-oldself">The <code>optionalOldSelf</code> field</h4><div class="feature-state-notice feature-stable" title="Feature Gate: CRDValidationRatcheting"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>If your cluster does not have <a href="#validation-ratcheting">CRD validation ratcheting</a> enabled,
the CustomResourceDefinition API doesn't include this field, and trying to set it may result
in an error.</p><p>The <code>optionalOldSelf</code> field is a boolean field that alters the behavior of <a href="#transition-rules">Transition Rules</a> described
below. Normally, a transition rule will not evaluate if <code>oldSelf</code> cannot be determined:
during object creation or when a new value is introduced in an update.</p><p>If <code>optionalOldSelf</code> is set to true, then transition rules will always be
evaluated and the type of <code>oldSelf</code> be changed to a CEL <a href="https://pkg.go.dev/github.com/google/cel-go/cel#OptionalTypes"><code>Optional</code></a> type.</p><p><code>optionalOldSelf</code> is useful in cases where schema authors would like a more
control tool <a href="#validation-ratcheting">than provided by the default equality based behavior of</a>
to introduce newer, usually stricter constraints on new values, while still
allowing old values to be "grandfathered" or ratcheted using the older validation.</p><p>Example Usage:</p><table><thead><tr><th>CEL</th><th>Description</th></tr></thead><tbody><tr><td><code>self.foo == "foo" || (oldSelf.hasValue() &amp;&amp; oldSelf.value().foo != "foo")</code></td><td>Ratcheted rule. Once a value is set to "foo", it must stay foo. But if it existed before the "foo" constraint was introduced, it may use any value</td></tr><tr><td><code>[oldSelf.orValue(""), self].all(x, ["OldCase1", "OldCase2"].exists(case, x == case)) || ["NewCase1", "NewCase2"].exists(case, self == case) || ["NewCase"].has(self)</code></td><td>"Ratcheted validation for removed enum cases if oldSelf used them"</td></tr><tr><td><code>oldSelf.optMap(o, o.size()).orValue(0) &lt; 4 || self.size() &gt;= 4</code></td><td>Ratcheted validation of newly increased minimum map or list size</td></tr></tbody></table><h4 id="available-validation-functions">Validation functions</h4><p>Functions available include:</p><ul><li>CEL standard functions, defined in the <a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#list-of-standard-definitions">list of standard definitions</a></li><li>CEL standard <a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#macros">macros</a></li><li>CEL <a href="https://pkg.go.dev/github.com/google/cel-go@v0.11.2/ext#Strings">extended string function library</a></li><li>Kubernetes <a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver@v0.24.0/pkg/apiserver/schema/cel/library#pkg-functions">CEL extension library</a></li></ul><h4 id="transition-rules">Transition rules</h4><p>A rule that contains an expression referencing the identifier <code>oldSelf</code> is implicitly considered a
<em>transition rule</em>. Transition rules allow schema authors to prevent certain transitions between two
otherwise valid states. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span></span><span>enum</span>:<span> </span>[<span>"low"</span>,<span> </span><span>"medium"</span>,<span> </span><span>"high"</span>]<span>
</span></span></span><span><span><span></span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span></span>- <span>rule</span>:<span> </span><span>"!(self == 'high' &amp;&amp; oldSelf == 'low') &amp;&amp; !(self == 'low' &amp;&amp; oldSelf == 'high')"</span><span>
</span></span></span><span><span><span>  </span><span>message</span>:<span> </span>cannot transition directly between 'low' and 'high'<span>
</span></span></span></code></pre></div><p>Unlike other rules, transition rules apply only to operations meeting the following criteria:</p><ul><li><p>The operation updates an existing object. Transition rules never apply to create operations.</p></li><li><p>Both an old and a new value exist. It remains possible to check if a value has been added or
removed by placing a transition rule on the parent node. Transition rules are never applied to
custom resource creation. When placed on an optional field, a transition rule will not apply to
update operations that set or unset the field.</p></li><li><p>The path to the schema node being validated by a transition rule must resolve to a node that is
comparable between the old object and the new object. For example, list items and their
descendants (<code>spec.foo[10].bar</code>) can't necessarily be correlated between an existing object and a
later update to the same object.</p></li></ul><p>Errors will be generated on CRD writes if a schema node contains a transition rule that can never be
applied, e.g. "oldSelf cannot be used on the uncorrelatable portion of the schema within <em>path</em>".</p><p>Transition rules are only allowed on <em>correlatable portions</em> of a schema.
A portion of the schema is correlatable if all <code>array</code> parent schemas are of type <code>x-kubernetes-list-type=map</code>;
any <code>set</code>or <code>atomic</code>array parent schemas make it impossible to unambiguously correlate a <code>self</code> with <code>oldSelf</code>.</p><p>Here are some examples for transition rules:</p><table><caption>Transition rules examples</caption><thead><tr><th>Use Case</th><th>Rule</th></tr></thead><tbody><tr><td>Immutability</td><td><code>self.foo == oldSelf.foo</code></td></tr><tr><td>Prevent modification/removal once assigned</td><td><code>oldSelf != 'bar' || self == 'bar'</code> or <code>!has(oldSelf.field) || has(self.field)</code></td></tr><tr><td>Append-only set</td><td><code>self.all(element, element in oldSelf)</code></td></tr><tr><td>If previous value was X, new value can only be A or B, not Y or Z</td><td><code>oldSelf != 'X' || self in ['A', 'B']</code></td></tr><tr><td>Monotonic (non-decreasing) counters</td><td><code>self &gt;= oldSelf</code></td></tr></tbody></table><h4 id="resource-use-by-validation-functions">Resource use by validation functions</h4><p>When you create or update a CustomResourceDefinition that uses validation rules,
the API server checks the likely impact of running those validation rules. If a rule is
estimated to be prohibitively expensive to execute, the API server rejects the create
or update operation, and returns an error message.
A similar system is used at runtime that observes the actions the interpreter takes. If the interpreter executes
too many instructions, execution of the rule will be halted, and an error will result.
Each CustomResourceDefinition is also allowed a certain amount of resources to finish executing all of
its validation rules. If the sum total of its rules are estimated at creation time to go over that limit,
then a validation error will also occur.</p><p>You are unlikely to encounter issues with the resource budget for validation if you only
specify rules that always take the same amount of time regardless of how large their input is.
For example, a rule that asserts that <code>self.foo == 1</code> does not by itself have any
risk of rejection on validation resource budget groups.
But if <code>foo</code> is a string and you define a validation rule <code>self.foo.contains("someString")</code>, that rule takes
longer to execute depending on how long <code>foo</code> is.
Another example would be if <code>foo</code> were an array, and you specified a validation rule <code>self.foo.all(x, x &gt; 5)</code>.
The cost system always assumes the worst-case scenario if a limit on the length of <code>foo</code> is not
given, and this will happen for anything that can be iterated over (lists, maps, etc.).</p><p>Because of this, it is considered best practice to put a limit via <code>maxItems</code>, <code>maxProperties</code>, and
<code>maxLength</code> for anything that will be processed in a validation rule in order to prevent validation
errors during cost estimation. For example, given this schema with one rule:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>  </span><span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>      </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>        </span>- <span>rule</span>:<span> </span><span>"self.all(x, x.contains('a string'))"</span><span>
</span></span></span></code></pre></div><p>then the API server rejects this rule on validation budget grounds with error:</p><pre tabindex="0"><code>spec.validation.openAPIV3Schema.properties[spec].properties[foo].x-kubernetes-validations[0].rule: Forbidden:
CEL rule exceeded budget by more than 100x (try simplifying the rule, or adding maxItems, maxProperties, and
maxLength where arrays, maps, and strings are used)
</code></pre><p>The rejection happens because <code>self.all</code> implies calling <code>contains()</code> on every string in <code>foo</code>,
which in turn will check the given string to see if it contains <code>'a string'</code>. Without limits, this
is a very expensive rule.</p><p>If you do not specify any validation limit, the estimated cost of this rule will exceed the
per-rule cost limit. But if you add limits in the appropriate places, the rule will be allowed:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>  </span><span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>      </span><span>maxItems</span>:<span> </span><span>25</span><span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>maxLength</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>      </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>        </span>- <span>rule</span>:<span> </span><span>"self.all(x, x.contains('a string'))"</span><span>
</span></span></span></code></pre></div><p>The cost estimation system takes into account how many times the rule will be executed in addition to the
estimated cost of the rule itself. For instance, the following rule will have the same estimated cost as the
previous example (despite the rule now being defined on the individual array items):</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>  </span><span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>      </span><span>maxItems</span>:<span> </span><span>25</span><span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"self.contains('a string'))"</span><span>
</span></span></span><span><span><span>        </span><span>maxLength</span>:<span> </span><span>10</span><span>
</span></span></span></code></pre></div><p>If a list inside of a list has a validation rule that uses <code>self.all</code>, that is significantly more expensive
than a non-nested list with the same rule. A rule that would have been allowed on a non-nested list might need
lower limits set on both nested lists in order to be allowed. For example, even without having limits set,
the following rule is allowed:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>  </span><span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>    </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>      </span>- <span>rule</span>:<span> </span><span>"self.all(x, x == 5)"</span><span>
</span></span></span></code></pre></div><p>But the same rule on the following schema (with a nested array added) produces a validation error:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>  </span><span>properties</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>array<span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>        </span><span>x-kubernetes-validations</span>:<span>
</span></span></span><span><span><span>          </span>- <span>rule</span>:<span> </span><span>"self.all(x, x == 5)"</span><span>
</span></span></span></code></pre></div><p>This is because each item of <code>foo</code> is itself an array, and each subarray in turn calls <code>self.all</code>.
Avoid nested lists and maps if possible where validation rules are used.</p><h3 id="defaulting">Defaulting</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To use defaulting, your CustomResourceDefinition must use API version <code>apiextensions.k8s.io/v1</code>.</div><p>Defaulting allows to specify default values in the <a href="#validation">OpenAPI v3 validation schema</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span># openAPIV3Schema is the schema for validating custom objects.</span><span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>spec</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>              </span><span>properties</span>:<span>
</span></span></span><span><span><span>                </span><span>cronSpec</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                  </span><span>pattern</span>:<span> </span><span>'^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'</span><span>
</span></span></span><span><span><span>                  </span><span>default</span>:<span> </span><span>"5 0 * * *"</span><span>
</span></span></span><span><span><span>                </span><span>image</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>                  </span><span>minimum</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>                  </span><span>maximum</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>                  </span><span>default</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div><p>With this both <code>cronSpec</code> and <code>replicas</code> are defaulted:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span></code></pre></div><p>leads to</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"5 0 * * *"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div><p>Defaulting happens on the object</p><ul><li>in the request to the API server using the request version defaults,</li><li>when reading from etcd using the storage version defaults,</li><li>after mutating admission plugins with non-empty patches using the admission webhook object version defaults.</li></ul><p>Defaults applied when reading data from etcd are not automatically written back to etcd.
An update request via the API is required to persist those defaults back into etcd.</p><p>Default values for non-leaf fields must be pruned (with the exception of defaults for <code>metadata</code> fields) and must
validate against a provided schema. For example in the above example, a default of <code>{"replicas": "foo", "badger": 1}</code>
for the <code>spec</code> field would be invalid, because <code>badger</code> is an unknown field, and <code>replicas</code> is not a string.</p><p>Default values for <code>metadata</code> fields of <code>x-kubernetes-embedded-resources: true</code> nodes (or parts of
a default value covering <code>metadata</code>) are not pruned during CustomResourceDefinition creation, but
through the pruning step during handling of requests.</p><h4 id="defaulting-and-nullable">Defaulting and Nullable</h4><p>Null values for fields that either don't specify the nullable flag, or give it a
<code>false</code> value, will be pruned before defaulting happens. If a default is present, it will be
applied. When nullable is <code>true</code>, null values will be conserved and won't be defaulted.</p><p>For example, given the OpenAPI schema below:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span></span><span>properties</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>    </span><span>properties</span>:<span>
</span></span></span><span><span><span>      </span><span>foo</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>nullable</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>        </span><span>default</span>:<span> </span><span>"default"</span><span>
</span></span></span><span><span><span>      </span><span>bar</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>nullable</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>baz</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>string<span>
</span></span></span></code></pre></div><p>creating an object with null values for <code>foo</code> and <code>bar</code> and <code>baz</code></p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>  </span><span>bar</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>  </span><span>baz</span>:<span> </span><span>null</span><span>
</span></span></span></code></pre></div><p>leads to</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>foo</span>:<span> </span><span>"default"</span><span>
</span></span></span><span><span><span>  </span><span>bar</span>:<span> </span><span>null</span><span>
</span></span></span></code></pre></div><p>with <code>foo</code> pruned and defaulted because the field is non-nullable, <code>bar</code> maintaining the null
value due to <code>nullable: true</code>, and <code>baz</code> pruned because the field is non-nullable and has no
default.</p><h3 id="publish-validation-schema-in-openapi">Publish Validation Schema in OpenAPI</h3><p>CustomResourceDefinition <a href="#validation">OpenAPI v3 validation schemas</a> which are
<a href="#specifying-a-structural-schema">structural</a> and <a href="#field-pruning">enable pruning</a> are published
as <a href="/docs/concepts/overview/kubernetes-api/#openapi-and-swagger-definitions">OpenAPI v3</a> and
OpenAPI v2 from Kubernetes API server. It is recommended to use the OpenAPI v3 document
as it is a lossless representation of the CustomResourceDefinition OpenAPI v3 validation schema
while OpenAPI v2 represents a lossy conversion.</p><p>The <a href="/docs/reference/kubectl/">kubectl</a> command-line tool consumes the published schema to perform
client-side validation (<code>kubectl create</code> and <code>kubectl apply</code>), schema explanation (<code>kubectl explain</code>)
on custom resources. The published schema can be consumed for other purposes as well, like client generation or documentation.</p><h4 id="compatibility-with-openapi-v2">Compatibility with OpenAPI V2</h4><p>For compatibility with OpenAPI V2, the OpenAPI v3 validation schema performs a lossy conversion
to the OpenAPI v2 schema. The schema show up in <code>definitions</code> and <code>paths</code> fields in the
<a href="/docs/concepts/overview/kubernetes-api/#openapi-and-swagger-definitions">OpenAPI v2 spec</a>.</p><p>The following modifications are applied during the conversion to keep backwards compatibility with
kubectl in previous 1.13 version. These modifications prevent kubectl from being over-strict and rejecting
valid OpenAPI schemas that it doesn't understand. The conversion won't modify the validation schema defined in CRD,
and therefore won't affect <a href="#validation">validation</a> in the API server.</p><ol><li><p>The following fields are removed as they aren't supported by OpenAPI v2.</p><ul><li>The fields <code>allOf</code>, <code>anyOf</code>, <code>oneOf</code> and <code>not</code> are removed</li></ul></li><li><p>If <code>nullable: true</code> is set, we drop <code>type</code>, <code>nullable</code>, <code>items</code> and <code>properties</code> because OpenAPI v2 is
not able to express nullable. To avoid kubectl to reject good objects, this is necessary.</p></li></ol><h3 id="additional-printer-columns">Additional printer columns</h3><p>The kubectl tool relies on server-side output formatting. Your cluster's API server decides which
columns are shown by the <code>kubectl get</code> command. You can customize these columns for a
CustomResourceDefinition. The following example adds the <code>Spec</code>, <code>Replicas</code>, and <code>Age</code>
columns.</p><p>Save the CustomResourceDefinition to <code>resourcedefinition.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>spec</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>            </span><span>properties</span>:<span>
</span></span></span><span><span><span>              </span><span>cronSpec</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>              </span><span>image</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>              </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>    </span><span>additionalPrinterColumns</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>Spec<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>      </span><span>description</span>:<span> </span>The cron spec defining the interval a CronJob is run<span>
</span></span></span><span><span><span>      </span><span>jsonPath</span>:<span> </span>.spec.cronSpec<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>Replicas<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>      </span><span>description</span>:<span> </span>The number of jobs launched by the CronJob<span>
</span></span></span><span><span><span>      </span><span>jsonPath</span>:<span> </span>.spec.replicas<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>Age<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>date<span>
</span></span></span><span><span><span>      </span><span>jsonPath</span>:<span> </span>.metadata.creationTimestamp<span>
</span></span></span></code></pre></div><p>Create the CustomResourceDefinition:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f resourcedefinition.yaml
</span></span></code></pre></div><p>Create an instance using the <code>my-crontab.yaml</code> from the previous section.</p><p>Invoke the server-side printing:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get crontab my-new-cron-object
</span></span></code></pre></div><p>Notice the <code>NAME</code>, <code>SPEC</code>, <code>REPLICAS</code>, and <code>AGE</code> columns in the output:</p><pre tabindex="0"><code>NAME                 SPEC        REPLICAS   AGE
my-new-cron-object   * * * * *   1          7s
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>NAME</code> column is implicit and does not need to be defined in the CustomResourceDefinition.</div><h4 id="priority">Priority</h4><p>Each column includes a <code>priority</code> field. Currently, the priority
differentiates between columns shown in standard view or wide view (using the <code>-o wide</code> flag).</p><ul><li>Columns with priority <code>0</code> are shown in standard view.</li><li>Columns with priority greater than <code>0</code> are shown only in wide view.</li></ul><h4 id="type">Type</h4><p>A column's <code>type</code> field can be any of the following (compare
<a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#dataTypes">OpenAPI v3 data types</a>):</p><ul><li><code>integer</code> &#8211; non-floating-point numbers</li><li><code>number</code> &#8211; floating point numbers</li><li><code>string</code> &#8211; strings</li><li><code>boolean</code> &#8211; <code>true</code> or <code>false</code></li><li><code>date</code> &#8211; rendered differentially as time since this timestamp.</li></ul><p>If the value inside a CustomResource does not match the type specified for the column,
the value is omitted. Use CustomResource validation to ensure that the value
types are correct.</p><h4 id="format">Format</h4><p>A column's <code>format</code> field can be any of the following:</p><ul><li><code>int32</code></li><li><code>int64</code></li><li><code>float</code></li><li><code>double</code></li><li><code>byte</code></li><li><code>date</code></li><li><code>date-time</code></li><li><code>password</code></li></ul><p>The column's <code>format</code> controls the style used when <code>kubectl</code> prints the value.</p><h3 id="field-selectors">Field selectors</h3><p><a href="/docs/concepts/overview/working-with-objects/field-selectors/">Field Selectors</a>
let clients select custom resources based on the value of one or more resource
fields.</p><p>All custom resources support the <code>metadata.name</code> and <code>metadata.namespace</code> field
selectors.</p><p>Fields declared in a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a>
may also be used with field selectors when included in the <code>spec.versions[*].selectableFields</code> field of the
<a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a>.</p><h4 id="crd-selectable-fields">Selectable fields for custom resources</h4><div class="feature-state-notice feature-stable" title="Feature Gate: CustomResourceFieldSelectors"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The <code>spec.versions[*].selectableFields</code> field of a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a> may be used to
declare which other fields in a custom resource may be used in field selectors
with the feature of <code>CustomResourceFieldSelectors</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> (This feature gate is enabled by default since Kubernetes v1.31).
The following example adds the <code>.spec.color</code> and <code>.spec.size</code> fields as
selectable fields.</p><p>Save the CustomResourceDefinition to <code>shirt-resource-definition.yaml</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/customresourcedefinition/shirt-resource-definition.yaml"><code>customresourcedefinition/shirt-resource-definition.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy customresourcedefinition/shirt-resource-definition.yaml to clipboard"></div><div class="includecode" id="customresourcedefinition-shirt-resource-definition-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>shirts.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>shirts<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>shirt<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>Shirt<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>spec</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>            </span><span>properties</span>:<span>
</span></span></span><span><span><span>              </span><span>color</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>              </span><span>size</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>    </span><span>selectableFields</span>:<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.color<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.size<span>
</span></span></span><span><span><span>    </span><span>additionalPrinterColumns</span>:<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.color<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>Color<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.size<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>Size<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>string<span>
</span></span></span></code></pre></div></div></div><p>Create the CustomResourceDefinition:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/customresourcedefinition/shirt-resource-definition.yaml
</span></span></code></pre></div><p>Define some Shirts by editing <code>shirt-resources.yaml</code>; for example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/customresourcedefinition/shirt-resources.yaml"><code>customresourcedefinition/shirt-resources.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy customresourcedefinition/shirt-resources.yaml to clipboard"></div><div class="includecode" id="customresourcedefinition-shirt-resources-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>stable.example.com/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Shirt<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>color</span>:<span> </span>blue<span>
</span></span></span><span><span><span>  </span><span>size</span>:<span> </span>S<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>stable.example.com/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Shirt<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>color</span>:<span> </span>blue<span>
</span></span></span><span><span><span>  </span><span>size</span>:<span> </span>M<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>stable.example.com/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Shirt<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example3<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>color</span>:<span> </span>green<span>
</span></span></span><span><span><span>  </span><span>size</span>:<span> </span>M<span>
</span></span></span></code></pre></div></div></div><p>Create the custom resources:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/customresourcedefinition/shirt-resources.yaml
</span></span></code></pre></div><p>Get all the resources:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get shirts.stable.example.com
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>NAME       COLOR  SIZE
example1   blue   S
example2   blue   M
example3   green  M
</code></pre><p>Fetch blue shirts (retrieve Shirts with a <code>color</code> of <code>blue</code>):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get shirts.stable.example.com --field-selector spec.color<span>=</span>blue
</span></span></code></pre></div><p>Should output:</p><pre tabindex="0"><code>NAME       COLOR  SIZE
example1   blue   S
example2   blue   M
</code></pre><p>Get only resources with a <code>color</code> of <code>green</code> and a <code>size</code> of <code>M</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get shirts.stable.example.com --field-selector spec.color<span>=</span>green,spec.size<span>=</span>M
</span></span></code></pre></div><p>Should output:</p><pre tabindex="0"><code>NAME       COLOR  SIZE
example2   blue   M
</code></pre><h3 id="subresources">Subresources</h3><p>Custom resources support <code>/status</code> and <code>/scale</code> subresources.</p><p>The status and scale subresources can be optionally enabled by
defining them in the CustomResourceDefinition.</p><h4 id="status-subresource">Status subresource</h4><p>When the status subresource is enabled, the <code>/status</code> subresource for the custom resource is exposed.</p><ul><li><p>The status and the spec stanzas are represented by the <code>.status</code> and <code>.spec</code> JSONPaths
respectively inside of a custom resource.</p></li><li><p><code>PUT</code> requests to the <code>/status</code> subresource take a custom resource object and ignore changes to
anything except the status stanza.</p></li><li><p><code>PUT</code> requests to the <code>/status</code> subresource only validate the status stanza of the custom
resource.</p></li><li><p><code>PUT</code>/<code>POST</code>/<code>PATCH</code> requests to the custom resource ignore changes to the status stanza.</p></li><li><p>The <code>.metadata.generation</code> value is incremented for all changes, except for changes to
<code>.metadata</code> or <code>.status</code>.</p></li><li><p>Only the following constructs are allowed at the root of the CRD OpenAPI validation schema:</p><ul><li><code>description</code></li><li><code>example</code></li><li><code>exclusiveMaximum</code></li><li><code>exclusiveMinimum</code></li><li><code>externalDocs</code></li><li><code>format</code></li><li><code>items</code></li><li><code>maximum</code></li><li><code>maxItems</code></li><li><code>maxLength</code></li><li><code>minimum</code></li><li><code>minItems</code></li><li><code>minLength</code></li><li><code>multipleOf</code></li><li><code>pattern</code></li><li><code>properties</code></li><li><code>required</code></li><li><code>title</code></li><li><code>type</code></li><li><code>uniqueItems</code></li></ul></li></ul><h4 id="scale-subresource">Scale subresource</h4><p>When the scale subresource is enabled, the <code>/scale</code> subresource for the custom resource is exposed.
The <code>autoscaling/v1.Scale</code> object is sent as the payload for <code>/scale</code>.</p><p>To enable the scale subresource, the following fields are defined in the CustomResourceDefinition.</p><ul><li><p><code>specReplicasPath</code> defines the JSONPath inside of a custom resource that corresponds to <code>scale.spec.replicas</code>.</p><ul><li>It is a required value.</li><li>Only JSONPaths under <code>.spec</code> and with the dot notation are allowed.</li><li>If there is no value under the <code>specReplicasPath</code> in the custom resource,
the <code>/scale</code> subresource will return an error on GET.</li></ul></li><li><p><code>statusReplicasPath</code> defines the JSONPath inside of a custom resource that corresponds to <code>scale.status.replicas</code>.</p><ul><li>It is a required value.</li><li>Only JSONPaths under <code>.status</code> and with the dot notation are allowed.</li><li>If there is no value under the <code>statusReplicasPath</code> in the custom resource,
the status replica value in the <code>/scale</code> subresource will default to 0.</li></ul></li><li><p><code>labelSelectorPath</code> defines the JSONPath inside of a custom resource that corresponds to
<code>Scale.Status.Selector</code>.</p><ul><li>It is an optional value.</li><li>It must be set to work with HPA and VPA.</li><li>Only JSONPaths under <code>.status</code> or <code>.spec</code> and with the dot notation are allowed.</li><li>If there is no value under the <code>labelSelectorPath</code> in the custom resource,
the status selector value in the <code>/scale</code> subresource will default to the empty string.</li><li>The field pointed by this JSON path must be a string field (not a complex selector struct)
which contains a serialized label selector in string form.</li></ul></li></ul><p>In the following example, both status and scale subresources are enabled.</p><p>Save the CustomResourceDefinition to <code>resourcedefinition.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>spec</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>              </span><span>properties</span>:<span>
</span></span></span><span><span><span>                </span><span>cronSpec</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>image</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>            </span><span>status</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>              </span><span>properties</span>:<span>
</span></span></span><span><span><span>                </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>                </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>      </span><span># subresources describes the subresources for custom resources.</span><span>
</span></span></span><span><span><span>      </span><span>subresources</span>:<span>
</span></span></span><span><span><span>        </span><span># status enables the status subresource.</span><span>
</span></span></span><span><span><span>        </span><span>status</span>:<span> </span>{}<span>
</span></span></span><span><span><span>        </span><span># scale enables the scale subresource.</span><span>
</span></span></span><span><span><span>        </span><span>scale</span>:<span>
</span></span></span><span><span><span>          </span><span># specReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Spec.Replicas.</span><span>
</span></span></span><span><span><span>          </span><span>specReplicasPath</span>:<span> </span>.spec.replicas<span>
</span></span></span><span><span><span>          </span><span># statusReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Replicas.</span><span>
</span></span></span><span><span><span>          </span><span>statusReplicasPath</span>:<span> </span>.status.replicas<span>
</span></span></span><span><span><span>          </span><span># labelSelectorPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Selector.</span><span>
</span></span></span><span><span><span>          </span><span>labelSelectorPath</span>:<span> </span>.status.labelSelector<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div><p>And create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f resourcedefinition.yaml
</span></span></code></pre></div><p>After the CustomResourceDefinition object has been created, you can create custom objects.</p><p>If you save the following YAML to <code>my-crontab.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"* * * * */5"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-crontab.yaml
</span></span></code></pre></div><p>Then new namespaced RESTful API endpoints are created at:</p><pre tabindex="0"><code class="language-none">/apis/stable.example.com/v1/namespaces/*/crontabs/status
</code></pre><p>and</p><pre tabindex="0"><code class="language-none">/apis/stable.example.com/v1/namespaces/*/crontabs/scale
</code></pre><p>A custom resource can be scaled using the <code>kubectl scale</code> command.
For example, the following command sets <code>.spec.replicas</code> of the
custom resource created above to 5:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale --replicas<span>=</span><span>5</span> crontabs/my-new-cron-object
</span></span><span><span>crontabs <span>"my-new-cron-object"</span> scaled
</span></span><span><span>
</span></span><span><span>kubectl get crontabs my-new-cron-object -o <span>jsonpath</span><span>=</span><span>'{.spec.replicas}'</span>
</span></span><span><span><span>5</span>
</span></span></code></pre></div><p>You can use a <a href="/docs/tasks/run-application/configure-pdb/">PodDisruptionBudget</a> to protect custom
resources that have the scale subresource enabled.</p><h3 id="categories">Categories</h3><p>Categories is a list of grouped resources the custom resource belongs to (eg. <code>all</code>).
You can use <code>kubectl get &lt;category-name&gt;</code> to list the resources belonging to the category.</p><p>The following example adds <code>all</code> in the list of categories in the CustomResourceDefinition
and illustrates how to output the custom resource using <code>kubectl get all</code>.</p><p>Save the following CustomResourceDefinition to <code>resourcedefinition.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>      </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>schema</span>:<span>
</span></span></span><span><span><span>        </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>          </span><span>properties</span>:<span>
</span></span></span><span><span><span>            </span><span>spec</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>              </span><span>properties</span>:<span>
</span></span></span><span><span><span>                </span><span>cronSpec</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>image</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>                </span><span>replicas</span>:<span>
</span></span></span><span><span><span>                  </span><span>type</span>:<span> </span>integer<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span><span><span><span>    </span><span># categories is a list of grouped resources the custom resource belongs to.</span><span>
</span></span></span><span><span><span>    </span><span>categories</span>:<span>
</span></span></span><span><span><span>    </span>- all<span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f resourcedefinition.yaml
</span></span></code></pre></div><p>After the CustomResourceDefinition object has been created, you can create custom objects.</p><p>Save the following YAML to <code>my-crontab.yaml</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span><span>"stable.example.com/v1"</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-new-cron-object<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cronSpec</span>:<span> </span><span>"* * * * */5"</span><span>
</span></span></span><span><span><span>  </span><span>image</span>:<span> </span>my-awesome-cron-image<span>
</span></span></span></code></pre></div><p>and create it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-crontab.yaml
</span></span></code></pre></div><p>You can specify the category when using <code>kubectl get</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get all
</span></span></code></pre></div><p>and it will include the custom resources of kind <code>CronTab</code>:</p><pre tabindex="0"><code class="language-none">NAME                          AGE
crontabs/my-new-cron-object   3s
</code></pre><h2 id="what-s-next">What's next</h2><ul><li><p>Read about <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</a>.</p></li><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#customresourcedefinition-v1-apiextensions-k8s-io">CustomResourceDefinition</a>.</p></li><li><p>Serve <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">multiple versions</a> of a
CustomResourceDefinition.</p></li></ul></div></div><div><div class="td-content"><h1>Versions in CustomResourceDefinitions</h1><p>This page explains how to add versioning information to
<a href="/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/">CustomResourceDefinitions</a>, to indicate the stability
level of your CustomResourceDefinitions or advance your API to a new version with conversion between API representations. It also describes how to upgrade an object from one version to another.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You should have an initial understanding of <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</a>.</p>Your Kubernetes server must be at or later than version v1.16.<p>To check the version, enter <code>kubectl version</code>.</p><h2 id="overview">Overview</h2><p>The CustomResourceDefinition API provides a workflow for introducing and upgrading
to new versions of a CustomResourceDefinition.</p><p>When a CustomResourceDefinition is created, the first version is set in the
CustomResourceDefinition <code>spec.versions</code> list to an appropriate stability level
and a version number. For example <code>v1beta1</code> would indicate that the first
version is not yet stable. All custom resource objects will initially be stored
at this version.</p><p>Once the CustomResourceDefinition is created, clients may begin using the
<code>v1beta1</code> API.</p><p>Later it might be necessary to add new version such as <code>v1</code>.</p><p>Adding a new version:</p><ol><li>Pick a conversion strategy. Since custom resource objects need the ability to
be served at both versions, that means they will sometimes be served in a
different version than the one stored. To make this possible, the custom resource objects must sometimes be converted between the
version they are stored at and the version they are served at. If the
conversion involves schema changes and requires custom logic, a conversion
webhook should be used. If there are no schema changes, the default <code>None</code>
conversion strategy may be used and only the <code>apiVersion</code> field will be
modified when serving different versions.</li><li>If using conversion webhooks, create and deploy the conversion webhook. See
the <a href="#webhook-conversion">Webhook conversion</a> for more details.</li><li>Update the CustomResourceDefinition to include the new version in the
<code>spec.versions</code> list with <code>served:true</code>. Also, set <code>spec.conversion</code> field
to the selected conversion strategy. If using a conversion webhook, configure
<code>spec.conversion.webhookClientConfig</code> field to call the webhook.</li></ol><p>Once the new version is added, clients may incrementally migrate to the new
version. It is perfectly safe for some clients to use the old version while
others use the new version.</p><p>Migrate stored objects to the new version:</p><ol><li>See the <a href="#upgrade-existing-objects-to-a-new-stored-version">upgrade existing objects to a new stored version</a> section.</li></ol><p>It is safe for clients to use both the old and new version before, during and
after upgrading the objects to a new stored version.</p><p>Removing an old version:</p><ol><li>Ensure all clients are fully migrated to the new version. The kube-apiserver
logs can be reviewed to help identify any clients that are still accessing via
the old version.</li><li>Set <code>served</code> to <code>false</code> for the old version in the <code>spec.versions</code> list. If
any clients are still unexpectedly using the old version they may begin reporting
errors attempting to access the custom resource objects at the old version.
If this occurs, switch back to using <code>served:true</code> on the old version, migrate the
remaining clients to the new version and repeat this step.</li><li>Ensure the <a href="#upgrade-existing-objects-to-a-new-stored-version">upgrade of existing objects to the new stored version</a> step has been completed.<ol><li>Verify that the <code>storage</code> is set to <code>true</code> for the new version in the <code>spec.versions</code> list in the CustomResourceDefinition.</li><li>Verify that the old version is no longer listed in the CustomResourceDefinition <code>status.storedVersions</code>.</li></ol></li><li>Remove the old version from the CustomResourceDefinition <code>spec.versions</code> list.</li><li>Drop conversion support for the old version in conversion webhooks.</li></ol><h2 id="specify-multiple-versions">Specify multiple versions</h2><p>The CustomResourceDefinition API <code>versions</code> field can be used to support multiple versions of custom resources that you
have developed. Versions can have different schemas, and conversion webhooks can convert custom resources between versions.
Webhook conversions should follow the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">Kubernetes API conventions</a> wherever applicable.
Specifically, See the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md">API change documentation</a> for a set of useful gotchas and suggestions.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In <code>apiextensions.k8s.io/v1beta1</code>, there was a <code>version</code> field instead of <code>versions</code>. The
<code>version</code> field is deprecated and optional, but if it is not empty, it must
match the first item in the <code>versions</code> field.</div><p>This example shows a CustomResourceDefinition with two versions. For the first
example, the assumption is all versions share the same schema with no conversion
between them. The comments in the YAML provide more context.</p><ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-1"><li class="nav-item"><a class="nav-link active" href="#customresourcedefinition-versioning-example-1-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#customresourcedefinition-versioning-example-1-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="customresourcedefinition-versioning-example-1"><div id="customresourcedefinition-versioning-example-1-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span># list of versions supported by this CustomResourceDefinition</span><span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span># Each version can be enabled/disabled by Served flag.</span><span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># One and only one version must be marked as the storage version.</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># A schema is required</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>host</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>          </span><span>port</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>host</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>          </span><span>port</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span><span># The conversion section is introduced in Kubernetes 1.13+ with a default value of</span><span>
</span></span></span><span><span><span>  </span><span># None conversion (strategy sub-field set to None).</span><span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span># None conversion assumes the same schema for all versions and only sets the apiVersion</span><span>
</span></span></span><span><span><span>    </span><span># field of custom resources to the proper value</span><span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span># either Namespaced or Cluster</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span># singular name to be used as an alias on the CLI and for display</span><span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span># kind is normally the CamelCased singular type. Your resource manifests use this.</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span># shortNames allow shorter string to match your resource on the CLI</span><span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div></p></div><div id="customresourcedefinition-versioning-example-1-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span># list of versions supported by this CustomResourceDefinition</span><span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span># Each version can be enabled/disabled by Served flag.</span><span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># One and only one version must be marked as the storage version.</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span><span>validation</span>:<span>
</span></span></span><span><span><span>    </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>      </span><span>properties</span>:<span>
</span></span></span><span><span><span>        </span><span>host</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span><span># The conversion section is introduced in Kubernetes 1.13+ with a default value of</span><span>
</span></span></span><span><span><span>  </span><span># None conversion (strategy sub-field set to None).</span><span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span># None conversion assumes the same schema for all versions and only sets the apiVersion</span><span>
</span></span></span><span><span><span>    </span><span># field of custom resources to the proper value</span><span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span># either Namespaced or Cluster</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span># singular name to be used as an alias on the CLI and for display</span><span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span># kind is normally the PascalCased singular type. Your resource manifests use this.</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span># shortNames allow shorter string to match your resource on the CLI</span><span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div></p></div></div><p>You can save the CustomResourceDefinition in a YAML file, then use
<code>kubectl apply</code> to create it.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-versioned-crontab.yaml
</span></span></code></pre></div><p>After creation, the API server starts to serve each enabled version at an HTTP
REST endpoint. In the above example, the API versions are available at
<code>/apis/example.com/v1beta1</code> and <code>/apis/example.com/v1</code>.</p><h3 id="version-priority">Version priority</h3><p>Regardless of the order in which versions are defined in a
CustomResourceDefinition, the version with the highest priority is used by
kubectl as the default version to access objects. The priority is determined
by parsing the <em>name</em> field to determine the version number, the stability
(GA, Beta, or Alpha), and the sequence within that stability level.</p><p>The algorithm used for sorting the versions is designed to sort versions in the
same way that the Kubernetes project sorts Kubernetes versions. Versions start with a
<code>v</code> followed by a number, an optional <code>beta</code> or <code>alpha</code> designation, and
optional additional numeric versioning information. Broadly, a version string might look
like <code>v2</code> or <code>v2beta1</code>. Versions are sorted using the following algorithm:</p><ul><li>Entries that follow Kubernetes version patterns are sorted before those that
do not.</li><li>For entries that follow Kubernetes version patterns, the numeric portions of
the version string is sorted largest to smallest.</li><li>If the strings <code>beta</code> or <code>alpha</code> follow the first numeric portion, they sorted
in that order, after the equivalent string without the <code>beta</code> or <code>alpha</code>
suffix (which is presumed to be the GA version).</li><li>If another number follows the <code>beta</code>, or <code>alpha</code>, those numbers are also
sorted from largest to smallest.</li><li>Strings that don't fit the above format are sorted alphabetically and the
numeric portions are not treated specially. Notice that in the example below,
<code>foo1</code> is sorted above <code>foo10</code>. This is different from the sorting of the
numeric portion of entries that do follow the Kubernetes version patterns.</li></ul><p>This might make sense if you look at the following sorted version list:</p><pre tabindex="0"><code class="language-none">- v10
- v2
- v1
- v11beta2
- v10beta3
- v3beta1
- v12alpha1
- v11alpha2
- foo1
- foo10
</code></pre><p>For the example in <a href="#specify-multiple-versions">Specify multiple versions</a>, the
version sort order is <code>v1</code>, followed by <code>v1beta1</code>. This causes the kubectl
command to use <code>v1</code> as the default version unless the provided object specifies
the version.</p><h3 id="version-deprecation">Version deprecation</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [stable]</code></div><p>Starting in v1.19, a CustomResourceDefinition can indicate a particular version of the resource it defines is deprecated.
When API requests to a deprecated version of that resource are made, a warning message is returned in the API response as a header.
The warning message for each deprecated version of the resource can be customized if desired.</p><p>A customized warning message should indicate the deprecated API group, version, and kind,
and should indicate what API group, version, and kind should be used instead, if applicable.</p><ul class="nav nav-tabs" id="customresourcedefinition-versioning-deprecated"><li class="nav-item"><a class="nav-link active" href="#customresourcedefinition-versioning-deprecated-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#customresourcedefinition-versioning-deprecated-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="customresourcedefinition-versioning-deprecated"><div id="customresourcedefinition-versioning-deprecated-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1alpha1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>    </span><span># This indicates the v1alpha1 version of the custom resource is deprecated.</span><span>
</span></span></span><span><span><span>    </span><span># API requests to this version receive a warning header in the server response.</span><span>
</span></span></span><span><span><span>    </span><span>deprecated</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># This overrides the default warning returned to API clients making v1alpha1 API requests.</span><span>
</span></span></span><span><span><span>    </span><span>deprecationWarning</span>:<span> </span><span>"example.com/v1alpha1 CronTab is deprecated; see http://example.com/v1alpha1-v1 for instructions to migrate to example.com/v1 CronTab"</span><span>
</span></span></span><span><span><span>    
</span></span></span><span><span><span>    </span><span>schema</span>:<span> </span>...<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># This indicates the v1beta1 version of the custom resource is deprecated.</span><span>
</span></span></span><span><span><span>    </span><span># API requests to this version receive a warning header in the server response.</span><span>
</span></span></span><span><span><span>    </span><span># A default warning message is returned for this version.</span><span>
</span></span></span><span><span><span>    </span><span>deprecated</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span> </span>...<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span> </span>...<span>
</span></span></span></code></pre></div></p></div><div id="customresourcedefinition-versioning-deprecated-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>validation</span>:<span> </span>...<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1alpha1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>    </span><span># This indicates the v1alpha1 version of the custom resource is deprecated.</span><span>
</span></span></span><span><span><span>    </span><span># API requests to this version receive a warning header in the server response.</span><span>
</span></span></span><span><span><span>    </span><span>deprecated</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># This overrides the default warning returned to API clients making v1alpha1 API requests.</span><span>
</span></span></span><span><span><span>    </span><span>deprecationWarning</span>:<span> </span><span>"example.com/v1alpha1 CronTab is deprecated; see http://example.com/v1alpha1-v1 for instructions to migrate to example.com/v1 CronTab"</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># This indicates the v1beta1 version of the custom resource is deprecated.</span><span>
</span></span></span><span><span><span>    </span><span># API requests to this version receive a warning header in the server response.</span><span>
</span></span></span><span><span><span>    </span><span># A default warning message is returned for this version.</span><span>
</span></span></span><span><span><span>    </span><span>deprecated</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div></p></div></div><h3 id="version-removal">Version removal</h3><p>An older API version cannot be dropped from a CustomResourceDefinition manifest until existing stored data has been migrated to the newer API version for all clusters that served the older version of the custom resource, and the old version is removed from the <code>status.storedVersions</code> of the CustomResourceDefinition.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span># This indicates the v1beta1 version of the custom resource is no longer served.</span><span>
</span></span></span><span><span><span>    </span><span># API requests to this version receive a not found error in the server response.</span><span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span> </span>...<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># The new served version should be set as the storage version</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span> </span>...<span>
</span></span></span></code></pre></div><h2 id="webhook-conversion">Webhook conversion</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.16 [stable]</code></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Webhook conversion is available as beta since 1.15, and as alpha since Kubernetes 1.13. The
<code>CustomResourceWebhookConversion</code> feature must be enabled, which is the case automatically for many clusters for beta features. Please refer to the <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> documentation for more information.</div><p>The above example has a None conversion between versions which only sets the <code>apiVersion</code> field
on conversion and does not change the rest of the object. The API server also supports webhook
conversions that call an external service in case a conversion is required. For example when:</p><ul><li>custom resource is requested in a different version than stored version.</li><li>Watch is created in one version but the changed object is stored in another version.</li><li>custom resource PUT request is in a different version than storage version.</li></ul><p>To cover all of these cases and to optimize conversion by the API server,
the conversion requests may contain multiple objects in order to minimize the external calls.
The webhook should perform these conversions independently.</p><h3 id="write-a-conversion-webhook-server">Write a conversion webhook server</h3><p>Please refer to the implementation of the <a href="https://github.com/kubernetes/kubernetes/tree/v1.25.3/test/images/agnhost/crd-conversion-webhook/main.go">custom resource conversion webhook
server</a>
that is validated in a Kubernetes e2e test. The webhook handles the
<code>ConversionReview</code> requests sent by the API servers, and sends back conversion
results wrapped in <code>ConversionResponse</code>. Note that the request
contains a list of custom resources that need to be converted independently without
changing the order of objects.
The example server is organized in a way to be reused for other conversions.
Most of the common code are located in the
<a href="https://github.com/kubernetes/kubernetes/tree/v1.25.3/test/images/agnhost/crd-conversion-webhook/converter/framework.go">framework file</a>
that leaves only
<a href="https://github.com/kubernetes/kubernetes/tree/v1.25.3/test/images/agnhost/crd-conversion-webhook/converter/example_converter.go#L29-L80">one function</a>
to be implemented for different conversions.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The example conversion webhook server leaves the <code>ClientAuth</code> field
<a href="https://github.com/kubernetes/kubernetes/tree/v1.25.3/test/images/agnhost/crd-conversion-webhook/config.go#L47-L48">empty</a>,
which defaults to <code>NoClientCert</code>. This means that the webhook server does not
authenticate the identity of the clients, supposedly API servers. If you need
mutual TLS or other ways to authenticate the clients, see
how to <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#authenticate-apiservers">authenticate API servers</a>.</div><h4 id="permissible-mutations">Permissible mutations</h4><p>A conversion webhook must not mutate anything inside of <code>metadata</code> of the converted object
other than <code>labels</code> and <code>annotations</code>.
Attempted changes to <code>name</code>, <code>UID</code> and <code>namespace</code> are rejected and fail the request
which caused the conversion. All other changes are ignored.</p><h3 id="deploy-the-conversion-webhook-service">Deploy the conversion webhook service</h3><p>Documentation for deploying the conversion webhook is the same as for the
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#deploy-the-admission-webhook-service">admission webhook example service</a>.
The assumption for next sections is that the conversion webhook server is deployed to a service
named <code>example-conversion-webhook-server</code> in <code>default</code> namespace and serving traffic on path <code>/crdconvert</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When the webhook server is deployed into the Kubernetes cluster as a
service, it has to be exposed via a service on port 443 (The server
itself can have an arbitrary port but the service object should map it to port 443).
The communication between the API server and the webhook service may fail
if a different port is used for the service.</div><h3 id="configure-customresourcedefinition-to-use-conversion-webhooks">Configure CustomResourceDefinition to use conversion webhooks</h3><p>The <code>None</code> conversion example can be extended to use the conversion webhook by modifying <code>conversion</code>
section of the <code>spec</code>:</p><ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-2"><li class="nav-item"><a class="nav-link active" href="#customresourcedefinition-versioning-example-2-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#customresourcedefinition-versioning-example-2-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="customresourcedefinition-versioning-example-2"><div id="customresourcedefinition-versioning-example-2-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span># list of versions supported by this CustomResourceDefinition</span><span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span># Each version can be enabled/disabled by Served flag.</span><span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># One and only one version must be marked as the storage version.</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># Each version can define its own schema when there is no top-level</span><span>
</span></span></span><span><span><span>    </span><span># schema is defined.</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>hostPort</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>host</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>          </span><span>port</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span># the Webhook strategy instructs the API server to call an external webhook for any conversion between custom resources.</span><span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span># webhook is required when strategy is `Webhook` and it configures the webhook endpoint to be called by API server.</span><span>
</span></span></span><span><span><span>    </span><span>webhook</span>:<span>
</span></span></span><span><span><span>      </span><span># conversionReviewVersions indicates what ConversionReview versions are understood/preferred by the webhook.</span><span>
</span></span></span><span><span><span>      </span><span># The first version in the list understood by the API server is sent to the webhook.</span><span>
</span></span></span><span><span><span>      </span><span># The webhook must respond with a ConversionReview object in the same version it received.</span><span>
</span></span></span><span><span><span>      </span><span>conversionReviewVersions</span>:<span> </span>[<span>"v1"</span>,<span>"v1beta1"</span>]<span>
</span></span></span><span><span><span>      </span><span>clientConfig</span>:<span>
</span></span></span><span><span><span>        </span><span>service</span>:<span>
</span></span></span><span><span><span>          </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>example-conversion-webhook-server<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/crdconvert<span>
</span></span></span><span><span><span>        </span><span>caBundle</span>:<span> </span><span>"Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K"</span><span>
</span></span></span><span><span><span>  </span><span># either Namespaced or Cluster</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span># singular name to be used as an alias on the CLI and for display</span><span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span># kind is normally the CamelCased singular type. Your resource manifests use this.</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span># shortNames allow shorter string to match your resource on the CLI</span><span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div></p></div><div id="customresourcedefinition-versioning-example-2-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>crontabs.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>example.com<span>
</span></span></span><span><span><span>  </span><span># prunes object fields that are not specified in OpenAPI schemas below.</span><span>
</span></span></span><span><span><span>  </span><span>preserveUnknownFields</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span><span># list of versions supported by this CustomResourceDefinition</span><span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1beta1<span>
</span></span></span><span><span><span>    </span><span># Each version can be enabled/disabled by Served flag.</span><span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># One and only one version must be marked as the storage version.</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span># Each version can define its own schema when there is no top-level</span><span>
</span></span></span><span><span><span>    </span><span># schema is defined.</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>hostPort</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>host</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>          </span><span>port</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span># the Webhook strategy instructs the API server to call an external webhook for any conversion between custom resources.</span><span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span># webhookClientConfig is required when strategy is `Webhook` and it configures the webhook endpoint to be called by API server.</span><span>
</span></span></span><span><span><span>    </span><span>webhookClientConfig</span>:<span>
</span></span></span><span><span><span>      </span><span>service</span>:<span>
</span></span></span><span><span><span>        </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>example-conversion-webhook-server<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/crdconvert<span>
</span></span></span><span><span><span>      </span><span>caBundle</span>:<span> </span><span>"Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K"</span><span>
</span></span></span><span><span><span>  </span><span># either Namespaced or Cluster</span><span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>crontabs<span>
</span></span></span><span><span><span>    </span><span># singular name to be used as an alias on the CLI and for display</span><span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>crontab<span>
</span></span></span><span><span><span>    </span><span># kind is normally the CamelCased singular type. Your resource manifests use this.</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>CronTab<span>
</span></span></span><span><span><span>    </span><span># shortNames allow shorter string to match your resource on the CLI</span><span>
</span></span></span><span><span><span>    </span><span>shortNames</span>:<span>
</span></span></span><span><span><span>    </span>- ct<span>
</span></span></span></code></pre></div></p></div></div><p>You can save the CustomResourceDefinition in a YAML file, then use
<code>kubectl apply</code> to apply it.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-versioned-crontab-with-conversion.yaml
</span></span></code></pre></div><p>Make sure the conversion service is up and running before applying new changes.</p><h3 id="contacting-the-webhook">Contacting the webhook</h3><p>Once the API server has determined a request should be sent to a conversion webhook,
it needs to know how to contact the webhook. This is specified in the <code>webhookClientConfig</code>
stanza of the webhook configuration.</p><p>Conversion webhooks can either be called via a URL or a service reference,
and can optionally include a custom CA bundle to use to verify the TLS connection.</p><h3 id="url">URL</h3><p><code>url</code> gives the location of the webhook, in standard URL form
(<code>scheme://host:port/path</code>).</p><p>The <code>host</code> should not refer to a service running in the cluster; use
a service reference by specifying the <code>service</code> field instead.
The host might be resolved via external DNS in some apiservers
(i.e., <code>kube-apiserver</code> cannot resolve in-cluster DNS as that would
be a layering violation). <code>host</code> may also be an IP address.</p><p>Please note that using <code>localhost</code> or <code>127.0.0.1</code> as a <code>host</code> is
risky unless you take great care to run this webhook on all hosts
which run an apiserver which might need to make calls to this
webhook. Such installations are likely to be non-portable or not readily run in a new cluster.</p><p>The scheme must be "https"; the URL must begin with "https://".</p><p>Attempting to use a user or basic auth (for example "user:password@") is not allowed.
Fragments ("#...") and query parameters ("?...") are also not allowed.</p><p>Here is an example of a conversion webhook configured to call a URL
(and expects the TLS certificate to be verified using system trust roots, so does not specify a caBundle):</p><ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-3"><li class="nav-item"><a class="nav-link active" href="#customresourcedefinition-versioning-example-3-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#customresourcedefinition-versioning-example-3-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="customresourcedefinition-versioning-example-3"><div id="customresourcedefinition-versioning-example-3-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhook</span>:<span>
</span></span></span><span><span><span>      </span><span>clientConfig</span>:<span>
</span></span></span><span><span><span>        </span><span>url</span>:<span> </span><span>"https://my-webhook.example.com:9443/my-webhook-path"</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div></p></div><div id="customresourcedefinition-versioning-example-3-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhookClientConfig</span>:<span>
</span></span></span><span><span><span>      </span><span>url</span>:<span> </span><span>"https://my-webhook.example.com:9443/my-webhook-path"</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div></p></div></div><h3 id="service-reference">Service Reference</h3><p>The <code>service</code> stanza inside <code>webhookClientConfig</code> is a reference to the service for a conversion webhook.
If the webhook is running within the cluster, then you should use <code>service</code> instead of <code>url</code>.
The service namespace and name are required. The port is optional and defaults to 443.
The path is optional and defaults to "/".</p><p>Here is an example of a webhook that is configured to call a service on port "1234"
at the subpath "/my-path", and to verify the TLS connection against the ServerName
<code>my-service-name.my-service-namespace.svc</code> using a custom CA bundle.</p><ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-4"><li class="nav-item"><a class="nav-link active" href="#customresourcedefinition-versioning-example-4-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#customresourcedefinition-versioning-example-4-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="customresourcedefinition-versioning-example-4"><div id="customresourcedefinition-versioning-example-4-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhook</span>:<span>
</span></span></span><span><span><span>      </span><span>clientConfig</span>:<span>
</span></span></span><span><span><span>        </span><span>service</span>:<span>
</span></span></span><span><span><span>          </span><span>namespace</span>:<span> </span>my-service-namespace<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>my-service-name<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/my-path<span>
</span></span></span><span><span><span>          </span><span>port</span>:<span> </span><span>1234</span><span>
</span></span></span><span><span><span>        </span><span>caBundle</span>:<span> </span><span>"Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K"</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div></p></div><div id="customresourcedefinition-versioning-example-4-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhookClientConfig</span>:<span>
</span></span></span><span><span><span>      </span><span>service</span>:<span>
</span></span></span><span><span><span>        </span><span>namespace</span>:<span> </span>my-service-namespace<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>my-service-name<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/my-path<span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span><span>1234</span><span>
</span></span></span><span><span><span>      </span><span>caBundle</span>:<span> </span><span>"Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K"</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div></p></div></div><h2 id="webhook-request-and-response">Webhook request and response</h2><h3 id="request">Request</h3><p>Webhooks are sent a POST request, with <code>Content-Type: application/json</code>,
with a <code>ConversionReview</code> API object in the <code>apiextensions.k8s.io</code> API group
serialized to JSON as the body.</p><p>Webhooks can specify what versions of <code>ConversionReview</code> objects they accept
with the <code>conversionReviewVersions</code> field in their CustomResourceDefinition:</p><ul class="nav nav-tabs" id="conversionreviewversions"><li class="nav-item"><a class="nav-link active" href="#conversionreviewversions-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#conversionreviewversions-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="conversionreviewversions"><div id="conversionreviewversions-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>webhook</span>:<span>
</span></span></span><span><span><span>      </span><span>conversionReviewVersions</span>:<span> </span>[<span>"v1"</span>,<span> </span><span>"v1beta1"</span>]<span>
</span></span></span><span><span><span>      </span>...<span>
</span></span></span></code></pre></div><p><code>conversionReviewVersions</code> is a required field when creating
<code>apiextensions.k8s.io/v1</code> custom resource definitions.
Webhooks are required to support at least one <code>ConversionReview</code>
version understood by the current and previous API server.</p></p></div><div id="conversionreviewversions-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>conversion</span>:<span>
</span></span></span><span><span><span>    </span><span>strategy</span>:<span> </span>Webhook<span>
</span></span></span><span><span><span>    </span><span>conversionReviewVersions</span>:<span> </span>[<span>"v1"</span>,<span> </span><span>"v1beta1"</span>]<span>
</span></span></span><span><span><span>    </span>...<span>
</span></span></span></code></pre></div><p>If no <code>conversionReviewVersions</code> are specified, the default when creating
<code>apiextensions.k8s.io/v1beta1</code> custom resource definitions is <code>v1beta1</code>.</p></p></div></div><p>API servers send the first <code>ConversionReview</code> version in the <code>conversionReviewVersions</code> list they support.
If none of the versions in the list are supported by the API server, the custom resource definition will not be allowed to be created.
If an API server encounters a conversion webhook configuration that was previously created and does not support any of the <code>ConversionReview</code>
versions the API server knows how to send, attempts to call to the webhook will fail.</p><p>This example shows the data contained in an <code>ConversionReview</code> object
for a request to convert <code>CronTab</code> objects to <code>example.com/v1</code>:</p><ul class="nav nav-tabs" id="conversionreview-request"><li class="nav-item"><a class="nav-link active" href="#conversionreview-request-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#conversionreview-request-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="conversionreview-request"><div id="conversionreview-request-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span>"apiVersion": </span><span>"apiextensions.k8s.io/v1"</span>,<span>
</span></span></span><span><span><span>  </span><span>"kind": </span><span>"ConversionReview"</span>,<span>
</span></span></span><span><span><span>  </span><span>"request": </span>{<span>
</span></span></span><span><span><span>    </span><span># Random uid uniquely identifying this conversion call</span><span>
</span></span></span><span><span><span>    </span><span>"uid": </span><span>"705ab4f5-6393-11e8-b7cc-42010a800002"</span>,<span>
</span></span></span><span><span><span>    
</span></span></span><span><span><span>    </span><span># The API group and version the objects should be converted to</span><span>
</span></span></span><span><span><span>    </span><span>"desiredAPIVersion": </span><span>"example.com/v1"</span>,<span>
</span></span></span><span><span><span>    
</span></span></span><span><span><span>    </span><span># The list of objects to convert.</span><span>
</span></span></span><span><span><span>    </span><span># May contain one or more objects, in one or more versions.</span><span>
</span></span></span><span><span><span>    </span><span>"objects": </span>[<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1beta1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-04T14:03:02Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"local-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"namespace": </span><span>"default"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"143"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"3415a7fc-162b-4300-b5da-fd6083580d66"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"hostPort": </span><span>"localhost:1234"</span><span>
</span></span></span><span><span><span>      </span>},<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1beta1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-03T13:02:01Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"remote-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"12893"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"359a83ec-b575-460d-b553-d859cedde8a0"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"hostPort": </span><span>"example.com:2345"</span><span>
</span></span></span><span><span><span>      </span>}<span>
</span></span></span><span><span><span>    </span>]<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div></p></div><div id="conversionreview-request-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span>  </span><span>"apiVersion": </span><span>"apiextensions.k8s.io/v1beta1"</span>,<span>
</span></span></span><span><span><span>  </span><span>"kind": </span><span>"ConversionReview"</span>,<span>
</span></span></span><span><span><span>  </span><span>"request": </span>{<span>
</span></span></span><span><span><span>    </span><span># Random uid uniquely identifying this conversion call</span><span>
</span></span></span><span><span><span>    </span><span>"uid": </span><span>"705ab4f5-6393-11e8-b7cc-42010a800002"</span>,<span>
</span></span></span><span><span><span>    
</span></span></span><span><span><span>    </span><span># The API group and version the objects should be converted to</span><span>
</span></span></span><span><span><span>    </span><span>"desiredAPIVersion": </span><span>"example.com/v1"</span>,<span>
</span></span></span><span><span><span>    
</span></span></span><span><span><span>    </span><span># The list of objects to convert.</span><span>
</span></span></span><span><span><span>    </span><span># May contain one or more objects, in one or more versions.</span><span>
</span></span></span><span><span><span>    </span><span>"objects": </span>[<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1beta1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-04T14:03:02Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"local-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"namespace": </span><span>"default"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"143"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"3415a7fc-162b-4300-b5da-fd6083580d66"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"hostPort": </span><span>"localhost:1234"</span><span>
</span></span></span><span><span><span>      </span>},<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1beta1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-03T13:02:01Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"remote-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"12893"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"359a83ec-b575-460d-b553-d859cedde8a0"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"hostPort": </span><span>"example.com:2345"</span><span>
</span></span></span><span><span><span>      </span>}<span>
</span></span></span><span><span><span>    </span>]<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div></p></div></div><h3 id="response">Response</h3><p>Webhooks respond with a 200 HTTP status code, <code>Content-Type: application/json</code>,
and a body containing a <code>ConversionReview</code> object (in the same version they were sent),
with the <code>response</code> stanza populated, serialized to JSON.</p><p>If conversion succeeds, a webhook should return a <code>response</code> stanza containing the following fields:</p><ul><li><code>uid</code>, copied from the <code>request.uid</code> sent to the webhook</li><li><code>result</code>, set to <code>{"status":"Success"}</code></li><li><code>convertedObjects</code>, containing all of the objects from <code>request.objects</code>, converted to <code>request.desiredAPIVersion</code></li></ul><p>Example of a minimal successful response from a webhook:</p><ul class="nav nav-tabs" id="conversionreview-response-success"><li class="nav-item"><a class="nav-link active" href="#conversionreview-response-success-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#conversionreview-response-success-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="conversionreview-response-success"><div id="conversionreview-response-success-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span>"apiVersion": </span><span>"apiextensions.k8s.io/v1"</span>,<span>
</span></span></span><span><span><span>  </span><span>"kind": </span><span>"ConversionReview"</span>,<span>
</span></span></span><span><span><span>  </span><span>"response": </span>{<span>
</span></span></span><span><span><span>    </span><span># must match &lt;request.uid&gt;</span><span>
</span></span></span><span><span><span>    </span><span>"uid": </span><span>"705ab4f5-6393-11e8-b7cc-42010a800002"</span>,<span>
</span></span></span><span><span><span>    </span><span>"result": </span>{<span>
</span></span></span><span><span><span>      </span><span>"status": </span><span>"Success"</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span><span># Objects must match the order of request.objects, and have apiVersion set to &lt;request.desiredAPIVersion&gt;.</span><span>
</span></span></span><span><span><span>    </span><span># kind, metadata.uid, metadata.name, and metadata.namespace fields must not be changed by the webhook.</span><span>
</span></span></span><span><span><span>    </span><span># metadata.labels and metadata.annotations fields may be changed by the webhook.</span><span>
</span></span></span><span><span><span>    </span><span># All other changes to metadata fields by the webhook are ignored.</span><span>
</span></span></span><span><span><span>    </span><span>"convertedObjects": </span>[<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-04T14:03:02Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"local-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"namespace": </span><span>"default"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"143"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"3415a7fc-162b-4300-b5da-fd6083580d66"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"host": </span><span>"localhost"</span>,<span>
</span></span></span><span><span><span>        </span><span>"port": </span><span>"1234"</span><span>
</span></span></span><span><span><span>      </span>},<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-03T13:02:01Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"remote-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"12893"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"359a83ec-b575-460d-b553-d859cedde8a0"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"host": </span><span>"example.com"</span>,<span>
</span></span></span><span><span><span>        </span><span>"port": </span><span>"2345"</span><span>
</span></span></span><span><span><span>      </span>}<span>
</span></span></span><span><span><span>    </span>]<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div></p></div><div id="conversionreview-response-success-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span>  </span><span>"apiVersion": </span><span>"apiextensions.k8s.io/v1beta1"</span>,<span>
</span></span></span><span><span><span>  </span><span>"kind": </span><span>"ConversionReview"</span>,<span>
</span></span></span><span><span><span>  </span><span>"response": </span>{<span>
</span></span></span><span><span><span>    </span><span># must match &lt;request.uid&gt;</span><span>
</span></span></span><span><span><span>    </span><span>"uid": </span><span>"705ab4f5-6393-11e8-b7cc-42010a800002"</span>,<span>
</span></span></span><span><span><span>    </span><span>"result": </span>{<span>
</span></span></span><span><span><span>      </span><span>"status": </span><span>"Failed"</span><span>
</span></span></span><span><span><span>    </span>},<span>
</span></span></span><span><span><span>    </span><span># Objects must match the order of request.objects, and have apiVersion set to &lt;request.desiredAPIVersion&gt;.</span><span>
</span></span></span><span><span><span>    </span><span># kind, metadata.uid, metadata.name, and metadata.namespace fields must not be changed by the webhook.</span><span>
</span></span></span><span><span><span>    </span><span># metadata.labels and metadata.annotations fields may be changed by the webhook.</span><span>
</span></span></span><span><span><span>    </span><span># All other changes to metadata fields by the webhook are ignored.</span><span>
</span></span></span><span><span><span>    </span><span>"convertedObjects": </span>[<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-04T14:03:02Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"local-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"namespace": </span><span>"default"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"143"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"3415a7fc-162b-4300-b5da-fd6083580d66"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"host": </span><span>"localhost"</span>,<span>
</span></span></span><span><span><span>        </span><span>"port": </span><span>"1234"</span><span>
</span></span></span><span><span><span>      </span>},<span>
</span></span></span><span><span><span>      </span>{<span>
</span></span></span><span><span><span>        </span><span>"kind": </span><span>"CronTab"</span>,<span>
</span></span></span><span><span><span>        </span><span>"apiVersion": </span><span>"example.com/v1"</span>,<span>
</span></span></span><span><span><span>        </span><span>"metadata": </span>{<span>
</span></span></span><span><span><span>          </span><span>"creationTimestamp": </span><span>"2019-09-03T13:02:01Z"</span>,<span>
</span></span></span><span><span><span>          </span><span>"name": </span><span>"remote-crontab"</span>,<span>
</span></span></span><span><span><span>          </span><span>"resourceVersion": </span><span>"12893"</span>,<span>
</span></span></span><span><span><span>          </span><span>"uid": </span><span>"359a83ec-b575-460d-b553-d859cedde8a0"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"host": </span><span>"example.com"</span>,<span>
</span></span></span><span><span><span>        </span><span>"port": </span><span>"2345"</span><span>
</span></span></span><span><span><span>      </span>}<span>
</span></span></span><span><span><span>    </span>]<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div></p></div></div><p>If conversion fails, a webhook should return a <code>response</code> stanza containing the following fields:</p><ul><li><code>uid</code>, copied from the <code>request.uid</code> sent to the webhook</li><li><code>result</code>, set to <code>{"status":"Failed"}</code></li></ul><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Failing conversion can disrupt read and write access to the custom resources,
including the ability to update or delete the resources. Conversion failures
should be avoided whenever possible, and should not be used to enforce validation
constraints (use validation schemas or webhook admission instead).</div><p>Example of a response from a webhook indicating a conversion request failed, with an optional message:<ul class="nav nav-tabs" id="conversionreview-response-failure"><li class="nav-item"><a class="nav-link active" href="#conversionreview-response-failure-0">apiextensions.k8s.io/v1</a></li><li class="nav-item"><a class="nav-link" href="#conversionreview-response-failure-1">apiextensions.k8s.io/v1beta1</a></li></ul><div class="tab-content" id="conversionreview-response-failure"><div id="conversionreview-response-failure-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span>"apiVersion": </span><span>"apiextensions.k8s.io/v1"</span>,<span>
</span></span></span><span><span><span>  </span><span>"kind": </span><span>"ConversionReview"</span>,<span>
</span></span></span><span><span><span>  </span><span>"response": </span>{<span>
</span></span></span><span><span><span>    </span><span>"uid": </span><span>"&lt;value from request.uid&gt;"</span>,<span>
</span></span></span><span><span><span>    </span><span>"result": </span>{<span>
</span></span></span><span><span><span>      </span><span>"status": </span><span>"Failed"</span>,<span>
</span></span></span><span><span><span>      </span><span>"message": </span><span>"hostPort could not be parsed into a separate host and port"</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div></p></div><div id="conversionreview-response-failure-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>  </span><span># Deprecated in v1.16 in favor of apiextensions.k8s.io/v1</span><span>
</span></span></span><span><span><span>  </span><span>"apiVersion": </span><span>"apiextensions.k8s.io/v1beta1"</span>,<span>
</span></span></span><span><span><span>  </span><span>"kind": </span><span>"ConversionReview"</span>,<span>
</span></span></span><span><span><span>  </span><span>"response": </span>{<span>
</span></span></span><span><span><span>    </span><span>"uid": </span><span>"&lt;value from request.uid&gt;"</span>,<span>
</span></span></span><span><span><span>    </span><span>"result": </span>{<span>
</span></span></span><span><span><span>      </span><span>"status": </span><span>"Failed"</span>,<span>
</span></span></span><span><span><span>      </span><span>"message": </span><span>"hostPort could not be parsed into a separate host and port"</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div></p></div></div></p><h2 id="writing-reading-and-updating-versioned-customresourcedefinition-objects">Writing, reading, and updating versioned CustomResourceDefinition objects</h2><p>When an object is written, it is stored at the version designated as the
storage version at the time of the write. If the storage version changes,
existing objects are never converted automatically. However, newly-created
or updated objects are written at the new storage version. It is possible for an
object to have been written at a version that is no longer served.</p><p>When you read an object, you specify the version as part of the path.
You can request an object at any version that is currently served.
If you specify a version that is different from the object's stored version,
Kubernetes returns the object to you at the version you requested, but the
stored object is not changed on disk.</p><p>What happens to the object that is being returned while serving the read
request depends on what is specified in the CRD's <code>spec.conversion</code>:</p><ul><li>if the default <code>strategy</code> value <code>None</code> is specified, the only modifications
to the object are changing the <code>apiVersion</code> string and perhaps <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning">pruning
unknown fields</a>
(depending on the configuration). Note that this is unlikely to lead to good
results if the schemas differ between the storage and requested version.
In particular, you should not use this strategy if the same data is
represented in different fields between versions.</li><li>if <a href="#webhook-conversion">webhook conversion</a> is specified, then this
mechanism controls the conversion.</li></ul><p>If you update an existing object, it is rewritten at the version that is
currently the storage version. This is the only way that objects can change from
one version to another.</p><p>To illustrate this, consider the following hypothetical series of events:</p><ol><li>The storage version is <code>v1beta1</code>. You create an object. It is stored at version <code>v1beta1</code></li><li>You add version <code>v1</code> to your CustomResourceDefinition and designate it as
the storage version. Here the schemas for <code>v1</code> and <code>v1beta1</code> are identical,
which is typically the case when promoting an API to stable in the
Kubernetes ecosystem.</li><li>You read your object at version <code>v1beta1</code>, then you read the object again at
version <code>v1</code>. Both returned objects are identical except for the apiVersion
field.</li><li>You create a new object. It is stored at version <code>v1</code>. You now
have two objects, one of which is at <code>v1beta1</code>, and the other of which is at
<code>v1</code>.</li><li>You update the first object. It is now stored at version <code>v1</code> since that
is the current storage version.</li></ol><h3 id="previous-storage-versions">Previous storage versions</h3><p>The API server records each version which has ever been marked as the storage
version in the status field <code>storedVersions</code>. Objects may have been stored
at any version that has ever been designated as a storage version. No objects
can exist in storage at a version that has never been a storage version.</p><h2 id="upgrade-existing-objects-to-a-new-stored-version">Upgrade existing objects to a new stored version</h2><p>When deprecating versions and dropping support, select a storage upgrade
procedure.</p><p><em>Option 1:</em> Use the Storage Version Migrator</p><ol><li>Run the <a href="https://github.com/kubernetes-sigs/kube-storage-version-migrator">storage Version migrator</a></li><li>Remove the old version from the CustomResourceDefinition <code>status.storedVersions</code> field.</li></ol><p><em>Option 2:</em> Manually upgrade the existing objects to a new stored version</p><p>The following is an example procedure to upgrade from <code>v1beta1</code> to <code>v1</code>.</p><ol><li>Set <code>v1</code> as the storage in the CustomResourceDefinition file and apply it
using kubectl. The <code>storedVersions</code> is now <code>v1beta1, v1</code>.</li><li>Write an upgrade procedure to list all existing objects and write them with
the same content. This forces the backend to write objects in the current
storage version, which is <code>v1</code>.</li><li>Remove <code>v1beta1</code> from the CustomResourceDefinition <code>status.storedVersions</code> field.</li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Here is an example of how to patch the <code>status</code> subresource for a CRD object using <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl patch customresourcedefinitions &lt;CRD_Name&gt; --subresource<span>=</span><span>'status'</span> --type<span>=</span><span>'merge'</span> -p <span>'{"status":{"storedVersions":["v1"]}}'</span>
</span></span></code></pre></div></div></div></div><div><div class="td-content"><h1>Set up an Extension API Server</h1><p>Setting up an extension API server to work with the aggregation layer allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>You must <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">configure the aggregation layer</a> and enable the apiserver flags.</li></ul><h2 id="set-up-an-extension-api-server-to-work-with-the-aggregation-layer">Set up an extension api-server to work with the aggregation layer</h2><p>The following steps describe how to set up an extension-apiserver <em>at a high level</em>. These steps apply regardless if you're using YAML configs or using APIs. An attempt is made to specifically identify any differences between the two. For a concrete example of how they can be implemented using YAML configs, you can look at the <a href="https://github.com/kubernetes/sample-apiserver/blob/master/README.md">sample-apiserver</a> in the Kubernetes repo.</p><p>Alternatively, you can use an existing 3rd party solution, such as <a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/README.md">apiserver-builder</a>, which should generate a skeleton and automate all of the following steps for you.</p><ol><li>Make sure the APIService API is enabled (check <code>--runtime-config</code>). It should be on by default, unless it's been deliberately turned off in your cluster.</li><li>You may need to make an RBAC rule allowing you to add APIService objects, or get your cluster administrator to make one. (Since API extensions affect the entire cluster, it is not recommended to do testing/development/debug of an API extension in a live cluster.)</li><li>Create the Kubernetes namespace you want to run your extension api-service in.</li><li>Create/get a CA cert to be used to sign the server cert the extension api-server uses for HTTPS.</li><li>Create a server cert/key for the api-server to use for HTTPS. This cert should be signed by the above CA. It should also have a CN of the Kube DNS name. This is derived from the Kubernetes service and be of the form <code>&lt;service name&gt;.&lt;service name namespace&gt;.svc</code></li><li>Create a Kubernetes secret with the server cert/key in your namespace.</li><li>Create a Kubernetes deployment for the extension api-server and make sure you are loading the secret as a volume. It should contain a reference to a working image of your extension api-server. The deployment should also be in your namespace.</li><li>Make sure that your extension-apiserver loads those certs from that volume and that they are used in the HTTPS handshake.</li><li>Create a Kubernetes service account in your namespace.</li><li>Create a Kubernetes cluster role for the operations you want to allow on your resources.</li><li>Create a Kubernetes cluster role binding from the service account in your namespace to the cluster role you created.</li><li>Create a Kubernetes cluster role binding from the service account in your namespace to the <code>system:auth-delegator</code> cluster role to delegate auth decisions to the Kubernetes core API server.</li><li>Create a Kubernetes role binding from the service account in your namespace to the <code>extension-apiserver-authentication-reader</code> role. This allows your extension api-server to access the <code>extension-apiserver-authentication</code> configmap.</li><li>Create a Kubernetes apiservice. The CA cert above should be base64 encoded, stripped of new lines and used as the spec.caBundle in the apiservice. This should not be namespaced. If using the <a href="https://github.com/kubernetes/kube-aggregator/">kube-aggregator API</a>, only pass in the PEM encoded CA bundle because the base 64 encoding is done for you.</li><li>Use kubectl to get your resource. When run, kubectl should return "No resources found.". This message
indicates that everything worked but you currently have no objects of that resource type created.</li></ol><h2 id="what-s-next">What's next</h2><ul><li>Walk through the steps to <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">configure the API aggregation layer</a> and enable the apiserver flags.</li><li>For a high level overview, see <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Extending the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Extend the Kubernetes API using Custom Resource Definitions</a>.</li></ul></div></div><div><div class="td-content"><h1>Configure Multiple Schedulers</h1><p>Kubernetes ships with a default scheduler that is described
<a href="/docs/reference/command-line-tools-reference/kube-scheduler/">here</a>.
If the default scheduler does not suit your needs you can implement your own scheduler.
Moreover, you can even run multiple schedulers simultaneously alongside the default
scheduler and instruct Kubernetes what scheduler to use for each of your pods. Let's
learn how to run multiple schedulers in Kubernetes with an example.</p><p>A detailed description of how to implement a scheduler is outside the scope of this
document. Please refer to the kube-scheduler implementation in
<a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler">pkg/scheduler</a>
in the Kubernetes source directory for a canonical example.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="package-the-scheduler">Package the scheduler</h2><p>Package your scheduler binary into a container image. For the purposes of this example,
you can use the default scheduler (kube-scheduler) as your second scheduler.
Clone the <a href="https://github.com/kubernetes/kubernetes">Kubernetes source code from GitHub</a>
and build the source.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>git clone https://github.com/kubernetes/kubernetes.git
</span></span><span><span><span>cd</span> kubernetes
</span></span><span><span>make
</span></span></code></pre></div><p>Create a container image containing the kube-scheduler binary. Here is the <code>Dockerfile</code>
to build the image:</p><div class="highlight"><pre tabindex="0"><code class="language-docker"><span><span><span>FROM</span><span> busybox</span><span>
</span></span></span><span><span><span></span><span>ADD</span> ./_output/local/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler<span>
</span></span></span></code></pre></div><p>Save the file as <code>Dockerfile</code>, build the image and push it to a registry. This example
pushes the image to
<a href="https://cloud.google.com/container-registry/">Google Container Registry (GCR)</a>.
For more details, please read the GCR
<a href="https://cloud.google.com/container-registry/docs/">documentation</a>. Alternatively
you can also use the <a href="https://hub.docker.com/search?q=">docker hub</a>. For more details
refer to the docker hub <a href="https://docs.docker.com/docker-hub/repos/create/#create-a-repository">documentation</a>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 .     <span># The image name and the repository</span>
</span></span><span><span>gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0 <span># used in here is just an example</span>
</span></span></code></pre></div><h2 id="define-a-kubernetes-deployment-for-the-scheduler">Define a Kubernetes Deployment for the scheduler</h2><p>Now that you have your scheduler in a container image, create a pod
configuration for it and run it in your Kubernetes cluster. But instead of creating a pod
directly in the cluster, you can use a <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>
for this example. A <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> manages a
<a href="/docs/concepts/workloads/controllers/replicaset/">Replica Set</a> which in turn manages the pods,
thereby making the scheduler resilient to failures. Here is the deployment
config. Save it as <code>my-scheduler.yaml</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/my-scheduler.yaml"><code>admin/sched/my-scheduler.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/sched/my-scheduler.yaml to clipboard"></div><div class="includecode" id="admin-sched-my-scheduler-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRoleBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler-as-kube-scheduler<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span></span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:kube-scheduler<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRoleBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler-as-volume-scheduler<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span></span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:volume-scheduler<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>RoleBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler-extension-apiserver-authentication-reader<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>Role<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>extension-apiserver-authentication-reader<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span></span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler-config<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>my-scheduler-config.yaml</span>:<span> </span>|<span>
</span></span></span><span><span><span>    apiVersion: kubescheduler.config.k8s.io/v1
</span></span></span><span><span><span>    kind: KubeSchedulerConfiguration
</span></span></span><span><span><span>    profiles:
</span></span></span><span><span><span>      - schedulerName: my-scheduler
</span></span></span><span><span><span>    leaderElection:
</span></span></span><span><span><span>      leaderElect: false</span><span>    
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>scheduler<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>control-plane<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>component</span>:<span> </span>scheduler<span>
</span></span></span><span><span><span>      </span><span>tier</span>:<span> </span>control-plane<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>component</span>:<span> </span>scheduler<span>
</span></span></span><span><span><span>        </span><span>tier</span>:<span> </span>control-plane<span>
</span></span></span><span><span><span>        </span><span>version</span>:<span> </span>second<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>serviceAccountName</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>command</span>:<span>
</span></span></span><span><span><span>        </span>- /usr/local/bin/kube-scheduler<span>
</span></span></span><span><span><span>        </span>- --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>gcr.io/my-gcp-project/my-kube-scheduler:1.0<span>
</span></span></span><span><span><span>        </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>          </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>            </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span> </span><span>10259</span><span>
</span></span></span><span><span><span>            </span><span>scheme</span>:<span> </span>HTTPS<span>
</span></span></span><span><span><span>          </span><span>initialDelaySeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>kube-second-scheduler<span>
</span></span></span><span><span><span>        </span><span>readinessProbe</span>:<span>
</span></span></span><span><span><span>          </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>            </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span> </span><span>10259</span><span>
</span></span></span><span><span><span>            </span><span>scheme</span>:<span> </span>HTTPS<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>'0.1'</span><span>
</span></span></span><span><span><span>        </span><span>securityContext</span>:<span>
</span></span></span><span><span><span>          </span><span>privileged</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>            </span><span>mountPath</span>:<span> </span>/etc/kubernetes/my-scheduler<span>
</span></span></span><span><span><span>      </span><span>hostNetwork</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>      </span><span>hostPID</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>          </span><span>configMap</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>my-scheduler-config<span>
</span></span></span></code></pre></div></div></div><p>In the above manifest, you use a <a href="/docs/reference/scheduling/config/">KubeSchedulerConfiguration</a>
to customize the behavior of your scheduler implementation. This configuration has been passed to
the <code>kube-scheduler</code> during initialization with the <code>--config</code> option. The <code>my-scheduler-config</code> ConfigMap stores the configuration file. The Pod of the<code>my-scheduler</code> Deployment mounts the <code>my-scheduler-config</code> ConfigMap as a volume.</p><p>In the aforementioned Scheduler Configuration, your scheduler implementation is represented via
a <a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-KubeSchedulerProfile">KubeSchedulerProfile</a>.<div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To determine if a scheduler is responsible for scheduling a specific Pod, the <code>spec.schedulerName</code> field in a
PodTemplate or Pod manifest must match the <code>schedulerName</code> field of the <code>KubeSchedulerProfile</code>.
All schedulers running in the cluster must have unique names.</div></p><p>Also, note that you create a dedicated service account <code>my-scheduler</code> and bind the ClusterRole
<code>system:kube-scheduler</code> to it so that it can acquire the same privileges as <code>kube-scheduler</code>.</p><p>Please see the
<a href="/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler documentation</a> for
detailed description of other command line arguments and
<a href="/docs/reference/config-api/kube-scheduler-config.v1/">Scheduler Configuration reference</a> for
detailed description of other customizable <code>kube-scheduler</code> configurations.</p><h2 id="run-the-second-scheduler-in-the-cluster">Run the second scheduler in the cluster</h2><p>In order to run your scheduler in a Kubernetes cluster, create the deployment
specified in the config above in a Kubernetes cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f my-scheduler.yaml
</span></span></code></pre></div><p>Verify that the scheduler pod is running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --namespace<span>=</span>kube-system
</span></span></code></pre></div><pre tabindex="0"><code>NAME                                           READY     STATUS    RESTARTS   AGE
....
my-scheduler-lnf4s-4744f                       1/1       Running   0          2m
...
</code></pre><p>You should see a "Running" my-scheduler pod, in addition to the default kube-scheduler
pod in this list.</p><h3 id="enable-leader-election">Enable leader election</h3><p>To run multiple-scheduler with leader election enabled, you must do the following:</p><p>Update the following fields for the KubeSchedulerConfiguration in the <code>my-scheduler-config</code> ConfigMap in your YAML file:</p><ul><li><code>leaderElection.leaderElect</code> to <code>true</code></li><li><code>leaderElection.resourceNamespace</code> to <code>&lt;lock-object-namespace&gt;</code></li><li><code>leaderElection.resourceName</code> to <code>&lt;lock-object-name&gt;</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The control plane creates the lock objects for you, but the namespace must already exist.
You can use the <code>kube-system</code> namespace.</div><p>If RBAC is enabled on your cluster, you must update the <code>system:kube-scheduler</code> cluster role.
Add your scheduler name to the resourceNames of the rule applied for <code>endpoints</code> and <code>leases</code> resources, as in the following example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit clusterrole system:kube-scheduler
</span></span></code></pre></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/clusterrole.yaml"><code>admin/sched/clusterrole.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/sched/clusterrole.yaml to clipboard"></div><div class="includecode" id="admin-sched-clusterrole-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>rbac.authorization.kubernetes.io/autoupdate</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/bootstrapping</span>:<span> </span>rbac-defaults<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:kube-scheduler<span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>      </span>- coordination.k8s.io<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- leases<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span>
</span></span></span><span><span><span>      </span>- create<span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>      </span>- coordination.k8s.io<span>
</span></span></span><span><span><span>    </span><span>resourceNames</span>:<span>
</span></span></span><span><span><span>      </span>- kube-scheduler<span>
</span></span></span><span><span><span>      </span>- my-scheduler<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- leases<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span>
</span></span></span><span><span><span>      </span>- get<span>
</span></span></span><span><span><span>      </span>- update<span>
</span></span></span><span><span><span>  </span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>      </span>- <span>""</span><span>
</span></span></span><span><span><span>    </span><span>resourceNames</span>:<span>
</span></span></span><span><span><span>      </span>- kube-scheduler<span>
</span></span></span><span><span><span>      </span>- my-scheduler<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span>- endpoints<span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span>
</span></span></span><span><span><span>      </span>- delete<span>
</span></span></span><span><span><span>      </span>- get<span>
</span></span></span><span><span><span>      </span>- patch<span>
</span></span></span><span><span><span>      </span>- update<span>
</span></span></span></code></pre></div></div></div><h2 id="specify-schedulers-for-pods">Specify schedulers for pods</h2><p>Now that your second scheduler is running, create some pods, and direct them
to be scheduled by either the default scheduler or the one you deployed.
In order to schedule a given pod using a specific scheduler, specify the name of the
scheduler in that pod spec. Let's look at three examples.</p><ul><li><p>Pod spec without any scheduler name</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/pod1.yaml"><code>admin/sched/pod1.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/sched/pod1.yaml to clipboard"></div><div class="includecode" id="admin-sched-pod1-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>no</span>-annotation<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>multischeduler-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-with-no-annotation-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8</span></span></code></pre></div></div></div><p>When no scheduler name is supplied, the pod is automatically scheduled using the
default-scheduler.</p><p>Save this file as <code>pod1.yaml</code> and submit it to the Kubernetes cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f pod1.yaml
</span></span></code></pre></div></li><li><p>Pod spec with <code>default-scheduler</code></p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/pod2.yaml"><code>admin/sched/pod2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/sched/pod2.yaml to clipboard"></div><div class="includecode" id="admin-sched-pod2-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>annotation-default-scheduler<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>multischeduler-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>schedulerName</span>:<span> </span>default-scheduler<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-with-default-annotation-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span></code></pre></div></div></div><p>A scheduler is specified by supplying the scheduler name as a value to <code>spec.schedulerName</code>. In this case, we supply the name of the
default scheduler which is <code>default-scheduler</code>.</p><p>Save this file as <code>pod2.yaml</code> and submit it to the Kubernetes cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f pod2.yaml
</span></span></code></pre></div></li><li><p>Pod spec with <code>my-scheduler</code></p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/pod3.yaml"><code>admin/sched/pod3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/sched/pod3.yaml to clipboard"></div><div class="includecode" id="admin-sched-pod3-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>annotation-second-scheduler<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>multischeduler-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>schedulerName</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-with-second-annotation-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span></code></pre></div></div></div><p>In this case, we specify that this pod should be scheduled using the scheduler that we
deployed - <code>my-scheduler</code>. Note that the value of <code>spec.schedulerName</code> should match the name supplied for the scheduler
in the <code>schedulerName</code> field of the mapping <code>KubeSchedulerProfile</code>.</p><p>Save this file as <code>pod3.yaml</code> and submit it to the Kubernetes cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f pod3.yaml
</span></span></code></pre></div><p>Verify that all three pods are running.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div></li></ul><h3 id="verifying-that-the-pods-were-scheduled-using-the-desired-schedulers">Verifying that the pods were scheduled using the desired schedulers</h3><p>In order to make it easier to work through these examples, we did not verify that the
pods were actually scheduled using the desired schedulers. We can verify that by
changing the order of pod and deployment config submissions above. If we submit all the
pod configs to a Kubernetes cluster before submitting the scheduler deployment config,
we see that the pod <code>annotation-second-scheduler</code> remains in "Pending" state forever
while the other two pods get scheduled. Once we submit the scheduler deployment config
and our new scheduler starts running, the <code>annotation-second-scheduler</code> pod gets
scheduled as well.</p><p>Alternatively, you can look at the "Scheduled" entries in the event logs to
verify that the pods were scheduled by the desired schedulers.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get events
</span></span></code></pre></div><p>You can also use a <a href="/docs/reference/scheduling/config/#multiple-profiles">custom scheduler configuration</a>
or a custom container image for the cluster's main scheduler by modifying its static pod manifest
on the relevant control plane nodes.</p></div></div><div><div class="td-content"><h1>Use an HTTP Proxy to Access the Kubernetes API</h1><p>This page shows how to use an HTTP proxy to access the Kubernetes API.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>If you do not already have an application running in your cluster, start
a Hello world application by entering this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment hello-app --image<span>=</span>gcr.io/google-samples/hello-app:2.0 --port<span>=</span><span>8080</span>
</span></span></code></pre></div><h2 id="using-kubectl-to-start-a-proxy-server">Using kubectl to start a proxy server</h2><p>This command starts a proxy to the Kubernetes API server:</p><pre><code>kubectl proxy --port=8080
</code></pre><h2 id="exploring-the-kubernetes-api">Exploring the Kubernetes API</h2><p>When the proxy server is running, you can explore the API using <code>curl</code>, <code>wget</code>,
or a browser.</p><p>Get the API versions:</p><pre><code>curl http://localhost:8080/api/
</code></pre><p>The output should look similar to this:</p><pre><code>{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "10.0.2.15:8443"
    }
  ]
}
</code></pre><p>Get a list of pods:</p><pre><code>curl http://localhost:8080/api/v1/namespaces/default/pods
</code></pre><p>The output should look similar to this:</p><pre><code>{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "resourceVersion": "33074"
  },
  "items": [
    {
      "metadata": {
        "name": "kubernetes-bootcamp-2321272333-ix8pt",
        "generateName": "kubernetes-bootcamp-2321272333-",
        "namespace": "default",
        "uid": "ba21457c-6b1d-11e6-85f7-1ef9f1dab92b",
        "resourceVersion": "33003",
        "creationTimestamp": "2016-08-25T23:43:30Z",
        "labels": {
          "pod-template-hash": "2321272333",
          "run": "kubernetes-bootcamp"
        },
        ...
}
</code></pre><h2 id="what-s-next">What's next</h2><p>Learn more about <a href="/docs/reference/generated/kubectl/kubectl-commands#proxy">kubectl proxy</a>.</p></div></div><div><div class="td-content"><h1>Use a SOCKS5 Proxy to Access the Kubernetes API</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>This page shows how to use a SOCKS5 proxy to access the API of a remote Kubernetes cluster.
This is useful when the cluster you want to access does not expose its API directly on the public internet.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.24.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>You need SSH client software (the <code>ssh</code> tool), and an SSH service running on the remote server.
You must be able to log in to the SSH service on the remote server.</p><h2 id="task-context">Task context</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This example tunnels traffic using SSH, with the SSH client and server acting as a SOCKS proxy.
You can instead use any other kind of <a href="https://en.wikipedia.org/wiki/SOCKS#SOCKS5">SOCKS5</a> proxies.</div><p>Figure 1 represents what you're going to achieve in this task.</p><ul><li>You have a client computer, referred to as local in the steps ahead, from where you're going to create requests to talk to the Kubernetes API.</li><li>The Kubernetes server/API is hosted on a remote server.</li><li>You will use SSH client and server software to create a secure SOCKS5 tunnel between the local and
the remote server. The HTTPS traffic between the client and the Kubernetes API will flow over the SOCKS5
tunnel, which is itself tunnelled over SSH.</li></ul><p><figure><div class="mermaid">graph LR;
subgraph local[Local client machine]
client([client])-. local<br>traffic .-&gt; local_ssh[Local SSH<br>SOCKS5 proxy];
end
local_ssh[SSH<br>SOCKS5<br>proxy]-- SSH Tunnel --&gt;sshd
subgraph remote[Remote server]
sshd[SSH<br>server]-- local traffic --&gt;service1;
end
client([client])-. proxied HTTPs traffic<br>going through the proxy .-&gt;service1[Kubernetes API];
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class ingress,service1,service2,pod1,pod2,pod3,pod4 k8s;
class client plain;
class cluster cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript>Figure 1. SOCKS5 tutorial components</p><h2 id="using-ssh-to-create-a-socks5-proxy">Using ssh to create a SOCKS5 proxy</h2><p>The following command starts a SOCKS5 proxy between your client machine and the remote SOCKS server:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># The SSH tunnel continues running in the foreground after you run this</span>
</span></span><span><span>ssh -D <span>1080</span> -q -N username@kubernetes-remote-server.example
</span></span></code></pre></div><p>The SOCKS5 proxy lets you connect to your cluster's API server based on the following configuration:</p><ul><li><code>-D 1080</code>: opens a SOCKS proxy on local port :1080.</li><li><code>-q</code>: quiet mode. Causes most warning and diagnostic messages to be suppressed.</li><li><code>-N</code>: Do not execute a remote command. Useful for just forwarding ports.</li><li><code>username@kubernetes-remote-server.example</code>: the remote SSH server behind which the Kubernetes cluster
is running (eg: a bastion host).</li></ul><h2 id="client-configuration">Client configuration</h2><p>To access the Kubernetes API server through the proxy you must instruct <code>kubectl</code> to send queries through
the <code>SOCKS</code> proxy we created earlier. Do this by either setting the appropriate environment variable,
or via the <code>proxy-url</code> attribute in the kubeconfig file. Using an environment variable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>export</span> <span>HTTPS_PROXY</span><span>=</span>socks5://localhost:1080
</span></span></code></pre></div><p>To always use this setting on a specific <code>kubectl</code> context, specify the <code>proxy-url</code> attribute in the relevant
<code>cluster</code> entry within the <code>~/.kube/config</code> file. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>clusters</span>:<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>    </span><span>certificate-authority-data</span>:<span> </span>LRMEMMW2<span> </span><span># shortened for readability </span><span>
</span></span></span><span><span><span>    </span><span>server</span>:<span> </span>https://&lt;API_SERVER_IP_ADDRESS&gt;:6443 <span> </span><span># the "Kubernetes API" server, in other words the IP address of kubernetes-remote-server.example</span><span>
</span></span></span><span><span><span>    </span><span>proxy-url</span>:<span> </span>socks5://localhost:1080  <span> </span><span># the "SSH SOCKS5 proxy" in the diagram above</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>    </span><span>cluster</span>:<span> </span>default<span>
</span></span></span><span><span><span>    </span><span>user</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>current-context</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Config<span>
</span></span></span><span><span><span></span><span>preferences</span>:<span> </span>{}<span>
</span></span></span><span><span><span></span><span>users</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>user</span>:<span>
</span></span></span><span><span><span>    </span><span>client-certificate-data</span>:<span> </span>LS0tLS1CR==<span> </span><span># shortened for readability</span><span>
</span></span></span><span><span><span>    </span><span>client-key-data</span>:<span> </span>LS0tLS1CRUdJT=     <span> </span><span># shortened for readability</span><span>
</span></span></span></code></pre></div><p>Once you have created the tunnel via the ssh command mentioned earlier, and defined either the environment variable or
the <code>proxy-url</code> attribute, you can interact with your cluster through that proxy. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>NAMESPACE     NAME                                     READY   STATUS      RESTARTS   AGE
</span></span></span><span><span><span>kube-system   coredns-85cb69466-klwq8                  1/1     Running     0          5m46s
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>Before <code>kubectl</code> 1.24, most <code>kubectl</code> commands worked when using a socks proxy, except <code>kubectl exec</code>.</li><li><code>kubectl</code> supports both <code>HTTPS_PROXY</code> and <code>https_proxy</code> environment variables. These are used by other
programs that support SOCKS, such as <code>curl</code>. Therefore in some cases it
will be better to define the environment variable on the command line:<div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>HTTPS_PROXY</span><span>=</span>socks5://localhost:1080 kubectl get pods
</span></span></code></pre></div></li><li>When using <code>proxy-url</code>, the proxy is used only for the relevant <code>kubectl</code> context,
whereas the environment variable will affect all contexts.</li><li>The k8s API server hostname can be further protected from DNS leakage by using the <code>socks5h</code> protocol name
instead of the more commonly known <code>socks5</code> protocol shown above. In this case, <code>kubectl</code> will ask the proxy server
(such as an ssh bastion) to resolve the k8s API server domain name, instead of resolving it on the system running
<code>kubectl</code>. Note also that with <code>socks5h</code>, a k8s API server URL like <code>https://localhost:6443/api</code> does not refer
to your local client computer. Instead, it refers to <code>localhost</code> as known on the proxy server (eg the ssh bastion).</li></ul></div><h2 id="clean-up">Clean up</h2><p>Stop the ssh port-forwarding process by pressing <code>CTRL+C</code> on the terminal where it is running.</p><p>Type <code>unset https_proxy</code> in a terminal to stop forwarding http traffic through the proxy.</p><h2 id="further-reading">Further reading</h2><ul><li><a href="https://man.openbsd.org/ssh">OpenSSH remote login client</a></li></ul></div></div><div><div class="td-content"><h1>Set up Konnectivity service</h1><p>The Konnectivity service provides a TCP level proxy for the control plane to cluster
communication.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this
tutorial on a cluster with at least two nodes that are not acting as control
plane hosts. If you do not already have a cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>.</p><h2 id="configure-the-konnectivity-service">Configure the Konnectivity service</h2><p>The following steps require an egress configuration, for example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/konnectivity/egress-selector-configuration.yaml"><code>admin/konnectivity/egress-selector-configuration.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/konnectivity/egress-selector-configuration.yaml to clipboard"></div><div class="includecode" id="admin-konnectivity-egress-selector-configuration-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EgressSelectorConfiguration<span>
</span></span></span><span><span><span></span><span>egressSelections</span>:<span>
</span></span></span><span><span><span></span><span># Since we want to control the egress traffic to the cluster, we use the</span><span>
</span></span></span><span><span><span></span><span># "cluster" as the name. Other supported values are "etcd", and "controlplane".</span><span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>cluster<span>
</span></span></span><span><span><span>  </span><span>connection</span>:<span>
</span></span></span><span><span><span>    </span><span># This controls the protocol between the API Server and the Konnectivity</span><span>
</span></span></span><span><span><span>    </span><span># server. Supported values are "GRPC" and "HTTPConnect". There is no</span><span>
</span></span></span><span><span><span>    </span><span># end user visible difference between the two modes. You need to set the</span><span>
</span></span></span><span><span><span>    </span><span># Konnectivity server to work in the same mode.</span><span>
</span></span></span><span><span><span>    </span><span>proxyProtocol</span>:<span> </span>GRPC<span>
</span></span></span><span><span><span>    </span><span>transport</span>:<span>
</span></span></span><span><span><span>      </span><span># This controls what transport the API Server uses to communicate with the</span><span>
</span></span></span><span><span><span>      </span><span># Konnectivity server. UDS is recommended if the Konnectivity server</span><span>
</span></span></span><span><span><span>      </span><span># locates on the same machine as the API Server. You need to configure the</span><span>
</span></span></span><span><span><span>      </span><span># Konnectivity server to listen on the same UDS socket.</span><span>
</span></span></span><span><span><span>      </span><span># The other supported transport is "tcp". You will need to set up TLS </span><span>
</span></span></span><span><span><span>      </span><span># config to secure the TCP transport.</span><span>
</span></span></span><span><span><span>      </span><span>uds</span>:<span>
</span></span></span><span><span><span>        </span><span>udsName</span>:<span> </span>/etc/kubernetes/konnectivity-server/konnectivity-server.socket<span>
</span></span></span></code></pre></div></div></div><p>You need to configure the API Server to use the Konnectivity service
and direct the network traffic to the cluster nodes:</p><ol><li>Make sure that
<a href="/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">Service Account Token Volume Projection</a>
feature enabled in your cluster. It is enabled by default since Kubernetes v1.20.</li><li>Create an egress configuration file such as <code>admin/konnectivity/egress-selector-configuration.yaml</code>.</li><li>Set the <code>--egress-selector-config-file</code> flag of the API Server to the path of
your API Server egress configuration file.</li><li>If you use UDS connection, add volumes config to the kube-apiserver:<div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>konnectivity-uds<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/etc/kubernetes/konnectivity-server<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>konnectivity-uds<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/etc/kubernetes/konnectivity-server<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>DirectoryOrCreate<span>
</span></span></span></code></pre></div></li></ol><p>Generate or obtain a certificate and kubeconfig for konnectivity-server.
For example, you can use the OpenSSL command line tool to issue a X.509 certificate,
using the cluster CA certificate <code>/etc/kubernetes/pki/ca.crt</code> from a control-plane host.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>openssl req -subj <span>"/CN=system:konnectivity-server"</span> -new -newkey rsa:2048 -nodes -out konnectivity.csr -keyout konnectivity.key
</span></span><span><span>openssl x509 -req -in konnectivity.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out konnectivity.crt -days <span>375</span> -sha256
</span></span><span><span><span>SERVER</span><span>=</span><span>$(</span>kubectl config view -o <span>jsonpath</span><span>=</span><span>'{.clusters..server}'</span><span>)</span>
</span></span><span><span>kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-credentials system:konnectivity-server --client-certificate konnectivity.crt --client-key konnectivity.key --embed-certs<span>=</span><span>true</span>
</span></span><span><span>kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-cluster kubernetes --server <span>"</span><span>$SERVER</span><span>"</span> --certificate-authority /etc/kubernetes/pki/ca.crt --embed-certs<span>=</span><span>true</span>
</span></span><span><span>kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-context system:konnectivity-server@kubernetes --cluster kubernetes --user system:konnectivity-server
</span></span><span><span>kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config use-context system:konnectivity-server@kubernetes
</span></span><span><span>rm -f konnectivity.crt konnectivity.key konnectivity.csr
</span></span></code></pre></div><p>Next, you need to deploy the Konnectivity server and agents.
<a href="https://github.com/kubernetes-sigs/apiserver-network-proxy">kubernetes-sigs/apiserver-network-proxy</a>
is a reference implementation.</p><p>Deploy the Konnectivity server on your control plane node. The provided
<code>konnectivity-server.yaml</code> manifest assumes
that the Kubernetes components are deployed as a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pod</a> in your cluster. If not, you can deploy the Konnectivity
server as a DaemonSet.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/konnectivity/konnectivity-server.yaml"><code>admin/konnectivity/konnectivity-server.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/konnectivity/konnectivity-server.yaml to clipboard"></div><div class="includecode" id="admin-konnectivity-konnectivity-server-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>konnectivity-server<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>priorityClassName</span>:<span> </span>system-cluster-critical<span>
</span></span></span><span><span><span>  </span><span>hostNetwork</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>konnectivity-server-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/kas-network-proxy/proxy-server:v0.0.37<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"/proxy-server"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>
</span></span></span><span><span><span>            </span><span>"--logtostderr=true"</span>,<span>
</span></span></span><span><span><span>            </span><span># This needs to be consistent with the value set in egressSelectorConfiguration.</span><span>
</span></span></span><span><span><span>            </span><span>"--uds-name=/etc/kubernetes/konnectivity-server/konnectivity-server.socket"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--delete-existing-uds-file"</span>,<span>
</span></span></span><span><span><span>            </span><span># The following two lines assume the Konnectivity server is</span><span>
</span></span></span><span><span><span>            </span><span># deployed on the same machine as the apiserver, and the certs and</span><span>
</span></span></span><span><span><span>            </span><span># key of the API Server are at the specified location.</span><span>
</span></span></span><span><span><span>            </span><span>"--cluster-cert=/etc/kubernetes/pki/apiserver.crt"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--cluster-key=/etc/kubernetes/pki/apiserver.key"</span>,<span>
</span></span></span><span><span><span>            </span><span># This needs to be consistent with the value set in egressSelectorConfiguration.</span><span>
</span></span></span><span><span><span>            </span><span>"--mode=grpc"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--server-port=0"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--agent-port=8132"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--admin-port=8133"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--health-port=8134"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--agent-namespace=kube-system"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--agent-service-account=konnectivity-agent"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--kubeconfig=/etc/kubernetes/konnectivity-server.conf"</span>,<span>
</span></span></span><span><span><span>            </span><span>"--authentication-audience=system:konnectivity-server"</span><span>
</span></span></span><span><span><span>            </span>]<span>
</span></span></span><span><span><span>    </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>      </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>        </span><span>scheme</span>:<span> </span>HTTP<span>
</span></span></span><span><span><span>        </span><span>host</span>:<span> </span><span>127.0.0.1</span><span>
</span></span></span><span><span><span>        </span><span>port</span>:<span> </span><span>8134</span><span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>      </span><span>initialDelaySeconds</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>      </span><span>timeoutSeconds</span>:<span> </span><span>60</span><span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>agentport<span>
</span></span></span><span><span><span>      </span><span>containerPort</span>:<span> </span><span>8132</span><span>
</span></span></span><span><span><span>      </span><span>hostPort</span>:<span> </span><span>8132</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>adminport<span>
</span></span></span><span><span><span>      </span><span>containerPort</span>:<span> </span><span>8133</span><span>
</span></span></span><span><span><span>      </span><span>hostPort</span>:<span> </span><span>8133</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>healthport<span>
</span></span></span><span><span><span>      </span><span>containerPort</span>:<span> </span><span>8134</span><span>
</span></span></span><span><span><span>      </span><span>hostPort</span>:<span> </span><span>8134</span><span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>k8s-certs<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/etc/kubernetes/pki<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>kubeconfig<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/etc/kubernetes/konnectivity-server.conf<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>konnectivity-uds<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/etc/kubernetes/konnectivity-server<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>k8s-certs<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/etc/kubernetes/pki<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>kubeconfig<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/etc/kubernetes/konnectivity-server.conf<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>FileOrCreate<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>konnectivity-uds<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/etc/kubernetes/konnectivity-server<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>DirectoryOrCreate<span>
</span></span></span></code></pre></div></div></div><p>Then deploy the Konnectivity agents in your cluster:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/konnectivity/konnectivity-agent.yaml"><code>admin/konnectivity/konnectivity-agent.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/konnectivity/konnectivity-agent.yaml to clipboard"></div><div class="includecode" id="admin-konnectivity-konnectivity-agent-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span># Alternatively, you can deploy the agents as Deployments. It is not necessary</span><span>
</span></span></span><span><span><span></span><span># to have an agent on each node.</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>addonmanager.kubernetes.io/mode</span>:<span> </span>Reconcile<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>k8s-app</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>k8s-app</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>priorityClassName</span>:<span> </span>system-cluster-critical<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>        </span>- <span>key</span>:<span> </span><span>"CriticalAddonsOnly"</span><span>
</span></span></span><span><span><span>          </span><span>operator</span>:<span> </span><span>"Exists"</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>image</span>:<span> </span>us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent:v0.0.37<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>"/proxy-agent"</span>]<span>
</span></span></span><span><span><span>          </span><span>args</span>:<span> </span>[<span>
</span></span></span><span><span><span>                  </span><span>"--logtostderr=true"</span>,<span>
</span></span></span><span><span><span>                  </span><span>"--ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"</span>,<span>
</span></span></span><span><span><span>                  </span><span># Since the konnectivity server runs with hostNetwork=true,</span><span>
</span></span></span><span><span><span>                  </span><span># this is the IP address of the master machine.</span><span>
</span></span></span><span><span><span>                  </span><span>"--proxy-server-host=35.225.206.7"</span>,<span>
</span></span></span><span><span><span>                  </span><span>"--proxy-server-port=8132"</span>,<span>
</span></span></span><span><span><span>                  </span><span>"--admin-server-port=8133"</span>,<span>
</span></span></span><span><span><span>                  </span><span>"--health-server-port=8134"</span>,<span>
</span></span></span><span><span><span>                  </span><span>"--service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token"</span><span>
</span></span></span><span><span><span>                  </span>]<span>
</span></span></span><span><span><span>          </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>            </span>- <span>mountPath</span>:<span> </span>/var/run/secrets/tokens<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>konnectivity-agent-token<span>
</span></span></span><span><span><span>          </span><span>livenessProbe</span>:<span>
</span></span></span><span><span><span>            </span><span>httpGet</span>:<span>
</span></span></span><span><span><span>              </span><span>port</span>:<span> </span><span>8134</span><span>
</span></span></span><span><span><span>              </span><span>path</span>:<span> </span>/healthz<span>
</span></span></span><span><span><span>            </span><span>initialDelaySeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>            </span><span>timeoutSeconds</span>:<span> </span><span>15</span><span>
</span></span></span><span><span><span>      </span><span>serviceAccountName</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>konnectivity-agent-token<span>
</span></span></span><span><span><span>          </span><span>projected</span>:<span>
</span></span></span><span><span><span>            </span><span>sources</span>:<span>
</span></span></span><span><span><span>              </span>- <span>serviceAccountToken</span>:<span>
</span></span></span><span><span><span>                  </span><span>path</span>:<span> </span>konnectivity-agent-token<span>
</span></span></span><span><span><span>                  </span><span>audience</span>:<span> </span>system:konnectivity-server<span>
</span></span></span></code></pre></div></div></div><p>Last, if RBAC is enabled in your cluster, create the relevant RBAC rules:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/konnectivity/konnectivity-rbac.yaml"><code>admin/konnectivity/konnectivity-rbac.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/konnectivity/konnectivity-rbac.yaml to clipboard"></div><div class="includecode" id="admin-konnectivity-konnectivity-rbac-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRoleBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:konnectivity-server<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>    </span><span>addonmanager.kubernetes.io/mode</span>:<span> </span>Reconcile<span>
</span></span></span><span><span><span></span><span>roleRef</span>:<span>
</span></span></span><span><span><span>  </span><span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span>  </span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>system:auth-delegator<span>
</span></span></span><span><span><span></span><span>subjects</span>:<span>
</span></span></span><span><span><span>  </span>- <span>apiGroup</span>:<span> </span>rbac.authorization.k8s.io<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>User<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>system:konnectivity-server<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>konnectivity-agent<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>    </span><span>addonmanager.kubernetes.io/mode</span>:<span> </span>Reconcile<span>
</span></span></span></code></pre></div></div></div></div></div><div><div class="td-content"><h1>TLS</h1><div class="lead">Understand how to protect traffic within your cluster using Transport Layer Security (TLS).</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/tls/certificate-issue-client-csr/">Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequest</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/tls/certificate-rotation/">Configure Certificate Rotation for the Kubelet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Manage TLS Certificates in a Cluster</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/tls/manual-rotation-of-ca-certificates/">Manual Rotation of CA Certificates</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequest</h1><p>Kubernetes lets you use a public key infrastructure (PKI) to authenticate to your cluster
as a client.</p><p>A few steps are required in order to get a normal user to be able to
authenticate and invoke an API. First, this user must have an <a href="https://www.itu.int/rec/T-REC-X.509">X.509</a> certificate
issued by an authority that your Kubernetes cluster trusts. The client must then present that certificate to the Kubernetes API.</p><p>You use a <a href="/concepts/security/certificate-signing-requests/">CertificateSigningRequest</a>
as part of this process, and either you or some other principal must approve the request.</p><p>You will create a private key, and then get a certificate issued, and finally configure
that private key for a client.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>You need the <code>kubectl</code>, <code>openssl</code> and <code>base64</code> utilities.</p></li></ul><p>This page assumes you are using Kubernetes <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." href="/docs/reference/access-authn-authz/rbac/" target="_blank">role based access control</a> (RBAC).
If you have alternative or additional security mechanisms around authorization, you need to account for those as well.</p><h2 id="create-private-key">Create private key</h2><p>In this step, you create a private key. You need to keep this document secret; anyone who has it can impersonate the user.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Create a private key</span>
</span></span><span><span>openssl genrsa -out myuser.key <span>3072</span>
</span></span></code></pre></div><h2 id="create-x.509-certificatessigningrequest">Create an X.509 certificate signing request</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This is not the same as the similarly-named CertificateSigningRequest API; the file you generate here goes into the
CertificateSigningRequest.</div><p>It is important to set CN and O attribute of the CSR. CN is the name of the user and O is the group that this user will belong to.
You can refer to <a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> for standard groups.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Change the common name "myuser" to the actual username that you want to use</span>
</span></span><span><span>openssl req -new -key myuser.key -out myuser.csr -subj <span>"/CN=myuser"</span>
</span></span></code></pre></div><h2 id="create-k8s-certificatessigningrequest">Create a Kubernetes CertificateSigningRequest</h2><p>Encode the CSR document using this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat myuser.csr | base64 | tr -d <span>"\n"</span>
</span></span></code></pre></div><p>Create a <a href="/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1/">CertificateSigningRequest</a>
and submit it to a Kubernetes Cluster via kubectl. Below is a snippet of shell that you can use to generate the
CertificateSigningRequest.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF | kubectl apply -f -
</span></span></span><span><span><span>apiVersion: certificates.k8s.io/v1
</span></span></span><span><span><span>kind: CertificateSigningRequest
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: myuser # example
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  # This is an encoded CSR. Change this to the base64-encoded contents of myuser.csr
</span></span></span><span><span><span>  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZVzVuWld4aE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTByczhJTHRHdTYxakx2dHhWTTJSVlRWMDNHWlJTWWw0dWluVWo4RElaWjBOCnR2MUZtRVFSd3VoaUZsOFEzcWl0Qm0wMUFSMkNJVXBGd2ZzSjZ4MXF3ckJzVkhZbGlBNVhwRVpZM3ExcGswSDQKM3Z3aGJlK1o2MVNrVHF5SVBYUUwrTWM5T1Nsbm0xb0R2N0NtSkZNMUlMRVI3QTVGZnZKOEdFRjJ6dHBoaUlFMwpub1dtdHNZb3JuT2wzc2lHQ2ZGZzR4Zmd4eW8ybmlneFNVekl1bXNnVm9PM2ttT0x1RVF6cXpkakJ3TFJXbWlECklmMXBMWnoyalVnald4UkhCM1gyWnVVV1d1T09PZnpXM01LaE8ybHEvZi9DdS8wYk83c0x0MCt3U2ZMSU91TFcKcW90blZtRmxMMytqTy82WDNDKzBERHk5aUtwbXJjVDBnWGZLemE1dHJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR05WdmVIOGR4ZzNvK21VeVRkbmFjVmQ1N24zSkExdnZEU1JWREkyQTZ1eXN3ZFp1L1BVCkkwZXpZWFV0RVNnSk1IRmQycVVNMjNuNVJsSXJ3R0xuUXFISUh5VStWWHhsdnZsRnpNOVpEWllSTmU3QlJvYXgKQVlEdUI5STZXT3FYbkFvczFqRmxNUG5NbFpqdU5kSGxpT1BjTU1oNndLaTZzZFhpVStHYTJ2RUVLY01jSVUyRgpvU2djUWdMYTk0aEpacGk3ZnNMdm1OQUxoT045UHdNMGM1dVJVejV4T0dGMUtCbWRSeEgvbUNOS2JKYjFRQm1HCkkwYitEUEdaTktXTU0xMzhIQXdoV0tkNjVoVHdYOWl4V3ZHMkh4TG1WQzg0L1BHT0tWQW9FNkpsYWFHdTlQVmkKdjlOSjVaZlZrcXdCd0hKbzZXdk9xVlA3SVFjZmg3d0drWm89Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
</span></span></span><span><span><span>  signerName: kubernetes.io/kube-apiserver-client
</span></span></span><span><span><span>  expirationSeconds: 86400  # one day
</span></span></span><span><span><span>  usages:
</span></span></span><span><span><span>  - client auth
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Some points to note:</p><ul><li><code>usages</code> has to be <code>client auth</code></li><li><code>expirationSeconds</code> could be made longer (i.e. <code>864000</code> for ten days) or shorter (i.e. <code>3600</code> for one hour).
You cannot request a duration shorter than 10 minutes.</li><li><code>request</code> is the base64 encoded value of the CSR file content.</li></ul><h2 id="approve-certificate-signing-request">Approve the CertificateSigningRequest</h2><p>Use kubectl to find the CSR you made, and manually approve it.</p><p>Get the list of CSRs:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr
</span></span></code></pre></div><p>Approve the CSR:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl certificate approve myuser
</span></span></code></pre></div><h2 id="get-the-certificate">Get the certificate</h2><p>Retrieve the certificate from the CSR, to check it looks OK.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr/myuser -o yaml
</span></span></code></pre></div><p>The certificate value is in Base64-encoded format under <code>.status.certificate</code>.</p><p>Export the issued certificate from the CertificateSigningRequest.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr myuser -o <span>jsonpath</span><span>=</span><span>'{.status.certificate}'</span>| base64 -d &gt; myuser.crt
</span></span></code></pre></div><h2 id="configure-the-certificate-into-kubeconfig">Configure the certificate into kubeconfig</h2><p>The next step is to add this user into the kubeconfig file.</p><p>First, you need to add new credentials:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config set-credentials myuser --client-key<span>=</span>myuser.key --client-certificate<span>=</span>myuser.crt --embed-certs<span>=</span><span>true</span>
</span></span></code></pre></div><p>Then, you need to add the context:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config set-context myuser --cluster<span>=</span>kubernetes --user<span>=</span>myuser
</span></span></code></pre></div><p>To test it:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl --context myuser auth whoami
</span></span></code></pre></div><p>You should see output confirming that you are &#8220;myuser&#8220;.</p><h2 id="create-role-and-rolebinding">Create Role and RoleBinding</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you don't use Kubernetes RBAC, skip this step and make the appropriate changes for the authorization mechanism
your cluster actually uses.</div><p>With the certificate created it is time to define the Role and RoleBinding for
this user to access Kubernetes cluster resources.</p><p>This is a sample command to create a Role for this new user:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create role developer --verb<span>=</span>create --verb<span>=</span>get --verb<span>=</span>list --verb<span>=</span>update --verb<span>=</span>delete --resource<span>=</span>pods
</span></span></code></pre></div><p>This is a sample command to create a RoleBinding for this new user:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create rolebinding developer-binding-myuser --role<span>=</span>developer --user<span>=</span>myuser
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Manage TLS Certificates in a Cluster</a></li><li>For details of X.509 itself, refer to <a href="https://tools.ietf.org/html/rfc5280#section-3.1">RFC 5280</a> section 3.1</li><li>For information on the syntax of PKCS#10 certificate signing requests, refer to <a href="https://tools.ietf.org/html/rfc2986">RFC 2986</a></li><li>Read about <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#cluster-trust-bundles">ClusterTrustBundles</a></li></ul></div></div><div><div class="td-content"><h1>Configure Certificate Rotation for the Kubelet</h1><p>This page shows how to enable and configure certificate rotation for the kubelet.</p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [stable]</code></div><h2 id="before-you-begin">Before you begin</h2><ul><li>Kubernetes version 1.8.0 or later is required</li></ul><h2 id="overview">Overview</h2><p>The kubelet uses certificates for authenticating to the Kubernetes API. By
default, these certificates are issued with one year expiration so that they do
not need to be renewed too frequently.</p><p>Kubernetes contains <a href="/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/">kubelet certificate
rotation</a>,
that will automatically generate a new key and request a new certificate from
the Kubernetes API as the current certificate approaches expiration. Once the
new certificate is available, it will be used for authenticating connections to
the Kubernetes API.</p><h2 id="enabling-client-certificate-rotation">Enabling client certificate rotation</h2><p>The <code>kubelet</code> process accepts an argument <code>--rotate-certificates</code> that controls
if the kubelet will automatically request a new certificate as the expiration of
the certificate currently in use approaches.</p><p>The <code>kube-controller-manager</code> process accepts an argument
<code>--cluster-signing-duration</code> (<code>--experimental-cluster-signing-duration</code> prior to 1.19)
that controls how long certificates will be issued for.</p><h2 id="understanding-the-certificate-rotation-configuration">Understanding the certificate rotation configuration</h2><p>When a kubelet starts up, if it is configured to bootstrap (using the
<code>--bootstrap-kubeconfig</code> flag), it will use its initial certificate to connect
to the Kubernetes API and issue a certificate signing request. You can view the
status of certificate signing requests using:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get csr
</span></span></code></pre></div><p>Initially a certificate signing request from the kubelet on a node will have a
status of <code>Pending</code>. If the certificate signing requests meets specific
criteria, it will be auto approved by the controller manager, then it will have
a status of <code>Approved</code>. Next, the controller manager will sign a certificate,
issued for the duration specified by the
<code>--cluster-signing-duration</code> parameter, and the signed certificate
will be attached to the certificate signing request.</p><p>The kubelet will retrieve the signed certificate from the Kubernetes API and
write that to disk, in the location specified by <code>--cert-dir</code>. Then the kubelet
will use the new certificate to connect to the Kubernetes API.</p><p>As the expiration of the signed certificate approaches, the kubelet will
automatically issue a new certificate signing request, using the Kubernetes API.
This can happen at any point between 30% and 10% of the time remaining on the
certificate. Again, the controller manager will automatically approve the certificate
request and attach a signed certificate to the certificate signing request. The
kubelet will retrieve the new signed certificate from the Kubernetes API and
write that to disk. Then it will update the connections it has to the
Kubernetes API to reconnect using the new certificate.</p></div></div><div><div class="td-content"><h1>Manage TLS Certificates in a Cluster</h1><p>Kubernetes provides a <code>certificates.k8s.io</code> API, which lets you provision TLS
certificates signed by a Certificate Authority (CA) that you control. These CA
and certificates can be used by your workloads to establish trust.</p><p><code>certificates.k8s.io</code> API uses a protocol that is similar to the <a href="https://github.com/ietf-wg-acme/acme/">ACME
draft</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Certificates created using the <code>certificates.k8s.io</code> API are signed by a
<a href="#configuring-your-cluster-to-provide-signing">dedicated CA</a>. It is possible to configure your cluster to use the cluster root
CA for this purpose, but you should never rely on this. Do not assume that
these certificates will validate against the cluster root CA.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You need the <code>cfssl</code> tool. You can download <code>cfssl</code> from
<a href="https://github.com/cloudflare/cfssl/releases">https://github.com/cloudflare/cfssl/releases</a>.</p><p>Some steps in this page use the <code>jq</code> tool. If you don't have <code>jq</code>, you can
install it via your operating system's software sources, or fetch it from
<a href="https://jqlang.github.io/jq/">https://jqlang.github.io/jq/</a>.</p><h2 id="trusting-tls-in-a-cluster">Trusting TLS in a cluster</h2><p>Trusting the <a href="#configuring-your-cluster-to-provide-signing">custom CA</a> from an application running as a pod usually requires
some extra application configuration. You will need to add the CA certificate
bundle to the list of CA certificates that the TLS client or server trusts. For
example, you would do this with a golang TLS config by parsing the certificate
chain and adding the parsed certificates to the <code>RootCAs</code> field in the
<a href="https://pkg.go.dev/crypto/tls#Config"><code>tls.Config</code></a> struct.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Even though the custom CA certificate may be included in the filesystem (in the
ConfigMap <code>kube-root-ca.crt</code>),
you should not use that certificate authority for any purpose other than to verify internal
Kubernetes endpoints. An example of an internal Kubernetes endpoint is the
Service named <code>kubernetes</code> in the default namespace.</p><p>If you want to use a custom certificate authority for your workloads, you should generate
that CA separately, and distribute its CA certificate using a
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> that your pods
have access to read.</p></div><h2 id="requesting-a-certificate">Requesting a certificate</h2><p>The following section demonstrates how to create a TLS certificate for a
Kubernetes service accessed through DNS.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This tutorial uses CFSSL: Cloudflare's PKI and TLS toolkit <a href="https://blog.cloudflare.com/introducing-cfssl/">click here</a> to know more.</div><h2 id="create-a-certificate-signing-request">Create a certificate signing request</h2><p>Generate a private key and certificate signing request (or CSR) by running
the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF | cfssl genkey - | cfssljson -bare server
</span></span></span><span><span><span>{
</span></span></span><span><span><span>  "hosts": [
</span></span></span><span><span><span>    "my-svc.my-namespace.svc.cluster.local",
</span></span></span><span><span><span>    "my-pod.my-namespace.pod.cluster.local",
</span></span></span><span><span><span>    "192.0.2.24",
</span></span></span><span><span><span>    "10.0.34.2"
</span></span></span><span><span><span>  ],
</span></span></span><span><span><span>  "CN": "my-pod.my-namespace.pod.cluster.local",
</span></span></span><span><span><span>  "key": {
</span></span></span><span><span><span>    "algo": "ecdsa",
</span></span></span><span><span><span>    "size": 256
</span></span></span><span><span><span>  }
</span></span></span><span><span><span>}
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Where <code>192.0.2.24</code> is the service's cluster IP,
<code>my-svc.my-namespace.svc.cluster.local</code> is the service's DNS name,
<code>10.0.34.2</code> is the pod's IP and <code>my-pod.my-namespace.pod.cluster.local</code>
is the pod's DNS name. You should see the output similar to:</p><pre tabindex="0"><code>2022/02/01 11:45:32 [INFO] generate received request
2022/02/01 11:45:32 [INFO] received CSR
2022/02/01 11:45:32 [INFO] generating key: ecdsa-256
2022/02/01 11:45:32 [INFO] encoded CSR
</code></pre><p>This command generates two files; it generates <code>server.csr</code> containing the PEM
encoded <a href="https://tools.ietf.org/html/rfc2986">PKCS#10</a> certification request,
and <code>server-key.pem</code> containing the PEM encoded key to the certificate that
is still to be created.</p><h2 id="create-a-certificatesigningrequest-object-to-send-to-the-kubernetes-api">Create a CertificateSigningRequest object to send to the Kubernetes API</h2><p>Generate a CSR manifest (in YAML), and send it to the API server. You can do that by
running the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF | kubectl apply -f -
</span></span></span><span><span><span>apiVersion: certificates.k8s.io/v1
</span></span></span><span><span><span>kind: CertificateSigningRequest
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: my-svc.my-namespace
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  request: $(cat server.csr | base64 | tr -d '\n')
</span></span></span><span><span><span>  signerName: example.com/serving
</span></span></span><span><span><span>  usages:
</span></span></span><span><span><span>  - digital signature
</span></span></span><span><span><span>  - key encipherment
</span></span></span><span><span><span>  - server auth
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>Notice that the <code>server.csr</code> file created in step 1 is base64 encoded
and stashed in the <code>.spec.request</code> field. You are also requesting a
certificate with the "digital signature", "key encipherment", and "server
auth" key usages, signed by an example <code>example.com/serving</code> signer.
A specific <code>signerName</code> must be requested.
View documentation for <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#signers">supported signer names</a>
for more information.</p><p>The CSR should now be visible from the API in a Pending state. You can see
it by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe csr my-svc.my-namespace
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Name:                   my-svc.my-namespace
Labels:                 &lt;none&gt;
Annotations:            &lt;none&gt;
CreationTimestamp:      Tue, 01 Feb 2022 11:49:15 -0500
Requesting User:        yourname@example.com
Signer:                 example.com/serving
Status:                 Pending
Subject:
        Common Name:    my-pod.my-namespace.pod.cluster.local
        Serial Number:
Subject Alternative Names:
        DNS Names:      my-pod.my-namespace.pod.cluster.local
                        my-svc.my-namespace.svc.cluster.local
        IP Addresses:   192.0.2.24
                        10.0.34.2
Events: &lt;none&gt;
</code></pre><h2 id="get-the-certificate-signing-request-approved">Get the CertificateSigningRequest approved</h2><p>Approving the <a href="/docs/reference/access-authn-authz/certificate-signing-requests/">certificate signing request</a>
is either done by an automated approval process or on a one off basis by a cluster
administrator. If you're authorized to approve a certificate request, you can do that
manually using <code>kubectl</code>; for example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl certificate approve my-svc.my-namespace
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">certificatesigningrequest.certificates.k8s.io/my-svc.my-namespace approved
</code></pre><p>You should now see the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                  AGE   SIGNERNAME            REQUESTOR              REQUESTEDDURATION   CONDITION
my-svc.my-namespace   10m   example.com/serving   yourname@example.com   &lt;none&gt;              Approved
</code></pre><p>This means the certificate request has been approved and is waiting for the
requested signer to sign it.</p><h2 id="sign-the-certificate-signing-request">Sign the CertificateSigningRequest</h2><p>Next, you'll play the part of a certificate signer, issue the certificate, and upload it to the API.</p><p>A signer would typically watch the CertificateSigningRequest API for objects with its <code>signerName</code>,
check that they have been approved, sign certificates for those requests,
and update the API object status with the issued certificate.</p><h3 id="create-a-certificate-authority">Create a Certificate Authority</h3><p>You need an authority to provide the digital signature on the new certificate.</p><p>First, create a signing certificate by running the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF | cfssl gencert -initca - | cfssljson -bare ca
</span></span></span><span><span><span>{
</span></span></span><span><span><span>  "CN": "My Example Signer",
</span></span></span><span><span><span>  "key": {
</span></span></span><span><span><span>    "algo": "rsa",
</span></span></span><span><span><span>    "size": 2048
</span></span></span><span><span><span>  }
</span></span></span><span><span><span>}
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>You should see output similar to:</p><pre tabindex="0"><code class="language-none">2022/02/01 11:50:39 [INFO] generating a new CA key and certificate from CSR
2022/02/01 11:50:39 [INFO] generate received request
2022/02/01 11:50:39 [INFO] received CSR
2022/02/01 11:50:39 [INFO] generating key: rsa-2048
2022/02/01 11:50:39 [INFO] encoded CSR
2022/02/01 11:50:39 [INFO] signed certificate with serial number 263983151013686720899716354349605500797834580472
</code></pre><p>This produces a certificate authority key file (<code>ca-key.pem</code>) and certificate (<code>ca.pem</code>).</p><h3 id="issue-a-certificate">Issue a certificate</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/tls/server-signing-config.json"><code>tls/server-signing-config.json</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy tls/server-signing-config.json to clipboard"></div><div class="includecode" id="tls-server-signing-config-json"><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>    <span>"signing"</span>: {
</span></span><span><span>        <span>"default"</span>: {
</span></span><span><span>            <span>"usages"</span>: [
</span></span><span><span>                <span>"digital signature"</span>,
</span></span><span><span>                <span>"key encipherment"</span>,
</span></span><span><span>                <span>"server auth"</span>
</span></span><span><span>            ],
</span></span><span><span>            <span>"expiry"</span>: <span>"876000h"</span>,
</span></span><span><span>            <span>"ca_constraint"</span>: {
</span></span><span><span>                <span>"is_ca"</span>: <span>false</span>
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}</span></span></code></pre></div></div></div><p>Use a <code>server-signing-config.json</code> signing configuration and the certificate authority key file
and certificate to sign the certificate request:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr my-svc.my-namespace -o <span>jsonpath</span><span>=</span><span>'{.spec.request}'</span> | <span>\
</span></span></span><span><span><span></span>  base64 --decode | <span>\
</span></span></span><span><span><span></span>  cfssl sign -ca ca.pem -ca-key ca-key.pem -config server-signing-config.json - | <span>\
</span></span></span><span><span><span></span>  cfssljson -bare ca-signed-server
</span></span></code></pre></div><p>You should see the output similar to:</p><pre tabindex="0"><code>2022/02/01 11:52:26 [INFO] signed certificate with serial number 576048928624926584381415936700914530534472870337
</code></pre><p>This produces a signed serving certificate file, <code>ca-signed-server.pem</code>.</p><h3 id="upload-the-signed-certificate">Upload the signed certificate</h3><p>Finally, populate the signed certificate in the API object's status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr my-svc.my-namespace -o json | <span>\
</span></span></span><span><span><span></span>  jq <span>'.status.certificate = "'</span><span>$(</span>base64 ca-signed-server.pem | tr -d <span>'\n'</span><span>)</span><span>'"'</span> | <span>\
</span></span></span><span><span><span></span>  kubectl replace --raw /apis/certificates.k8s.io/v1/certificatesigningrequests/my-svc.my-namespace/status -f -
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This uses the command line tool <a href="https://jqlang.github.io/jq/"><code>jq</code></a> to populate the base64-encoded
content in the <code>.status.certificate</code> field.
If you do not have <code>jq</code>, you can also save the JSON output to a file, populate this field manually, and
upload the resulting file.</div><p>Once the CSR is approved and the signed certificate is uploaded, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code class="language-none">NAME                  AGE   SIGNERNAME            REQUESTOR              REQUESTEDDURATION   CONDITION
my-svc.my-namespace   20m   example.com/serving   yourname@example.com   &lt;none&gt;              Approved,Issued
</code></pre><h2 id="download-the-certificate-and-use-it">Download the certificate and use it</h2><p>Now, as the requesting user, you can download the issued certificate
and save it to a <code>server.crt</code> file by running the following:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get csr my-svc.my-namespace -o <span>jsonpath</span><span>=</span><span>'{.status.certificate}'</span> <span>\
</span></span></span><span><span><span></span>    | base64 --decode &gt; server.crt
</span></span></code></pre></div><p>Now you can populate <code>server.crt</code> and <code>server-key.pem</code> in a
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a>
that you could later mount into a Pod (for example, to use with a webserver
that serves HTTPS).</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret tls server --cert server.crt --key server-key.pem
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">secret/server created
</code></pre><p>Finally, you can populate <code>ca.pem</code> into a <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a>
and use it as the trust root to verify the serving certificate:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create configmap example-serving-ca --from-file ca.crt<span>=</span>ca.pem
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">configmap/example-serving-ca created
</code></pre><h2 id="approving-certificate-signing-requests">Approving CertificateSigningRequests</h2><p>A Kubernetes administrator (with appropriate permissions) can manually approve
(or deny) CertificateSigningRequests by using the <code>kubectl certificate approve</code> and <code>kubectl certificate deny</code> commands. However if you intend
to make heavy usage of this API, you might consider writing an automated
certificates controller.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>The ability to approve CSRs decides who trusts whom within your environment. The
ability to approve CSRs should not be granted broadly or lightly.</p><p>You should make sure that you confidently understand both the verification requirements
that fall on the approver <strong>and</strong> the repercussions of issuing a specific certificate
before you grant the <code>approve</code> permission.</p></div><p>Whether a machine or a human using kubectl as above, the role of the <em>approver</em> is
to verify that the CSR satisfies two requirements:</p><ol><li>The subject of the CSR controls the private key used to sign the CSR. This
addresses the threat of a third party masquerading as an authorized subject.
In the above example, this step would be to verify that the pod controls the
private key used to generate the CSR.</li><li>The subject of the CSR is authorized to act in the requested context. This
addresses the threat of an undesired subject joining the cluster. In the
above example, this step would be to verify that the pod is allowed to
participate in the requested service.</li></ol><p>If and only if these two requirements are met, the approver should approve
the CSR and otherwise should deny the CSR.</p><p>For more information on certificate approval and access control, read
the <a href="/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>
reference page.</p><h2 id="configuring-your-cluster-to-provide-signing">Configuring your cluster to provide signing</h2><p>This page assumes that a signer is set up to serve the certificates API. The
Kubernetes controller manager provides a default implementation of a signer. To
enable it, pass the <code>--cluster-signing-cert-file</code> and
<code>--cluster-signing-key-file</code> parameters to the controller manager with paths to
your Certificate Authority's keypair.</p></div></div><div><div class="td-content"><h1>Manual Rotation of CA Certificates</h1><p>This page shows how to manually rotate the certificate authority (CA) certificates.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><ul><li>For more information about authentication in Kubernetes, see
<a href="/docs/reference/access-authn-authz/authentication/">Authenticating</a>.</li><li>For more information about best practices for CA certificates, see
<a href="/docs/setup/best-practices/certificates/#single-root-ca">Single root CA</a>.</li></ul><h2 id="rotate-the-ca-certificates-manually">Rotate the CA certificates manually</h2><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>Make sure to back up your certificate directory along with configuration files and any other necessary files.</p><p>This approach assumes operation of the Kubernetes control plane in a HA configuration with multiple API servers.
Graceful termination of the API server is also assumed so clients can cleanly disconnect from one API server and
reconnect to another.</p><p>Configurations with a single API server will experience unavailability while the API server is being restarted.</p></div><ol><li><p>Distribute the new CA certificates and private keys (for example: <code>ca.crt</code>, <code>ca.key</code>, <code>front-proxy-ca.crt</code>,
and <code>front-proxy-ca.key</code>) to all your control plane nodes in the Kubernetes certificates directory.</p></li><li><p>Update the <code>--root-ca-file</code> flag for the <a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a> to include
both old and new CA, then restart the kube-controller-manager.</p><p>Any <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank">ServiceAccount</a> created after this point will get
Secrets that include both old and new CAs.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The files specified by the kube-controller-manager flags <code>--client-ca-file</code> and <code>--cluster-signing-cert-file</code>
cannot be CA bundles. If these flags and <code>--root-ca-file</code> point to the same <code>ca.crt</code> file which is now a
bundle (includes both old and new CA) you will face an error. To workaround this problem you can copy the new CA
to a separate file and make the flags <code>--client-ca-file</code> and <code>--cluster-signing-cert-file</code> point to the copy.
Once <code>ca.crt</code> is no longer a bundle you can restore the problem flags to point to <code>ca.crt</code> and delete the copy.</p><p><a href="https://github.com/kubernetes/kubeadm/issues/1350">Issue 1350</a> for kubeadm tracks an bug with the
kube-controller-manager being unable to accept a CA bundle.</p></div></li><li><p>Wait for the controller manager to update <code>ca.crt</code> in the service account Secrets to include both old and new CA certificates.</p><p>If any Pods are started before new CA is used by API servers, the new Pods get this update and will trust both
old and new CAs.</p></li><li><p>Restart all pods using in-cluster configurations (for example: kube-proxy, CoreDNS, etc) so they can use the
updated certificate authority data from Secrets that link to ServiceAccounts.</p><ul><li>Make sure CoreDNS, kube-proxy and other Pods using in-cluster configurations are working as expected.</li></ul></li><li><p>Append the both old and new CA to the file against <code>--client-ca-file</code> and <code>--kubelet-certificate-authority</code>
flag in the <code>kube-apiserver</code> configuration.</p></li><li><p>Append the both old and new CA to the file against <code>--client-ca-file</code> flag in the <code>kube-scheduler</code> configuration.</p></li><li><p>Update certificates for user accounts by replacing the content of <code>client-certificate-data</code> and <code>client-key-data</code>
respectively.</p><p>For information about creating certificates for individual user accounts, see
<a href="/docs/setup/best-practices/certificates/#configure-certificates-for-user-accounts">Configure certificates for user accounts</a>.</p><p>Additionally, update the <code>certificate-authority-data</code> section in the kubeconfig files,
respectively with Base64-encoded old and new certificate authority data</p></li><li><p>Update the <code>--root-ca-file</code> flag for the <a class="glossary-tooltip" title="Control plane component that integrates Kubernetes with third-party cloud providers." href="/docs/concepts/architecture/cloud-controller/" target="_blank">Cloud Controller Manager</a> to include
both old and new CA, then restart the cloud-controller-manager.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If your cluster does not have a cloud-controller-manager, you can skip this step.</div></li><li><p>Follow the steps below in a rolling fashion.</p><ol><li><p>Restart any other
<a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregated API servers</a> or
webhook handlers to trust the new CA certificates.</p></li><li><p>Restart the kubelet by update the file against <code>clientCAFile</code> in kubelet configuration and
<code>certificate-authority-data</code> in <code>kubelet.conf</code> to use both the old and new CA on all nodes.</p><p>If your kubelet is not using client certificate rotation, update <code>client-certificate-data</code> and
<code>client-key-data</code> in <code>kubelet.conf</code> on all nodes along with the kubelet client certificate file
usually found in <code>/var/lib/kubelet/pki</code>.</p></li><li><p>Restart API servers with the certificates (<code>apiserver.crt</code>, <code>apiserver-kubelet-client.crt</code> and
<code>front-proxy-client.crt</code>) signed by new CA.
You can use the existing private keys or new private keys.
If you changed the private keys then update these in the Kubernetes certificates directory as well.</p><p>Since the Pods in your cluster trust both old and new CAs, there will be a momentarily disconnection
after which pods' Kubernetes clients reconnect to the new API server.
The new API server uses a certificate signed by the new CA.</p><ul><li>Restart the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a> to use and
trust the new CAs.</li><li>Make sure control plane components logs no TLS errors.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><pre><code>  To generate certificates and private keys for your cluster using the `openssl` command line tool,
  see [Certificates (`openssl`)](/docs/tasks/administer-cluster/certificates/#openssl).
  You can also use [`cfssl`](/docs/tasks/administer-cluster/certificates/#cfssl).
</code></pre></div></li><li><p>Annotate any DaemonSets and Deployments to trigger pod replacement in a safer rolling fashion.</p></li></ol><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> namespace in <span>$(</span>kubectl get namespace -o <span>jsonpath</span><span>=</span><span>'{.items[*].metadata.name}'</span><span>)</span>; <span>do</span>
</span></span><span><span>    <span>for</span> name in <span>$(</span>kubectl get deployments -n <span>$namespace</span> -o <span>jsonpath</span><span>=</span><span>'{.items[*].metadata.name}'</span><span>)</span>; <span>do</span>
</span></span><span><span>        kubectl patch deployment -n <span>${</span><span>namespace</span><span>}</span> <span>${</span><span>name</span><span>}</span> -p <span>'{"spec":{"template":{"metadata":{"annotations":{"ca-rotation": "1"}}}}}'</span>;
</span></span><span><span>    <span>done</span>
</span></span><span><span>    <span>for</span> name in <span>$(</span>kubectl get daemonset -n <span>$namespace</span> -o <span>jsonpath</span><span>=</span><span>'{.items[*].metadata.name}'</span><span>)</span>; <span>do</span>
</span></span><span><span>        kubectl patch daemonset -n <span>${</span><span>namespace</span><span>}</span> <span>${</span><span>name</span><span>}</span> -p <span>'{"spec":{"template":{"metadata":{"annotations":{"ca-rotation": "1"}}}}}'</span>;
</span></span><span><span>    <span>done</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><pre><code>  To limit the number of concurrent disruptions that your application experiences,
  see [configure pod disruption budget](/docs/tasks/run-application/configure-pdb/).
</code></pre></div><pre><code> Depending on how you use StatefulSets you may also need to perform similar rolling replacement.
</code></pre></li><li><p>If your cluster is using bootstrap tokens to join nodes, update the ConfigMap <code>cluster-info</code> in the <code>kube-public</code>
namespace with new CA.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>base64_encoded_ca</span><span>=</span><span>"</span><span>$(</span>base64 -w0 /etc/kubernetes/pki/ca.crt<span>)</span><span>"</span>
</span></span><span><span>
</span></span><span><span>kubectl get cm/cluster-info --namespace kube-public -o yaml | <span>\
</span></span></span><span><span><span></span>    /bin/sed <span>"s/\(certificate-authority-data:\).*/\1 </span><span>${</span><span>base64_encoded_ca</span><span>}</span><span>/"</span> | <span>\
</span></span></span><span><span><span></span>    kubectl apply -f -
</span></span></code></pre></div></li><li><p>Verify the cluster functionality.</p><ol><li><p>Check the logs from control plane components, along with the kubelet and the kube-proxy.
Ensure those components are not reporting any TLS errors; see
<a href="/docs/tasks/debug/debug-cluster/#looking-at-logs">looking at the logs</a> for more details.</p></li><li><p>Validate logs from any aggregated api servers and pods using in-cluster config.</p></li></ol></li><li><p>Once the cluster functionality is successfully verified:</p><ol><li><p>Update all service account tokens to include new CA certificate only.</p><ul><li>All pods using an in-cluster kubeconfig will eventually need to be restarted to pick up the new Secret,
so that no Pods are relying on the old cluster CA.</li></ul></li><li><p>Restart the control plane components by removing the old CA from the kubeconfig files and the files against
<code>--client-ca-file</code>, <code>--root-ca-file</code> flags resp.</p></li><li><p>On each node, restart the kubelet by removing the old CA from file against the <code>clientCAFile</code> flag
and from the kubelet kubeconfig file. You should carry this out as a rolling update.</p><p>If your cluster lets you make this change, you can also roll it out by replacing nodes rather than
reconfiguring them.</p></li></ol></li></ol></div></div><div><div class="td-content"><h1>Manage Cluster Daemons</h1><div class="lead">Perform common tasks for managing a DaemonSet, such as performing a rolling update.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/manage-daemon/create-daemon-set/">Building a Basic DaemonSet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-daemon/update-daemon-set/">Perform a Rolling Update on a DaemonSet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-daemon/rollback-daemon-set/">Perform a Rollback on a DaemonSet</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/manage-daemon/pods-some-nodes/">Running Pods on Only Some Nodes</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Building a Basic DaemonSet</h1><p>This page demonstrates how to build a basic <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a>
that runs a Pod on every node in a Kubernetes cluster.
It covers a simple use case of mounting a file from the host, logging its contents using
an <a href="/docs/concepts/workloads/pods/init-containers/">init container</a>, and utilizing a pause container.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>A Kubernetes cluster with at least two nodes (one control plane node and one worker node)
to demonstrate the behavior of DaemonSets.</p><h2 id="define-the-daemonset">Define the DaemonSet</h2><p>In this task, a basic DaemonSet is created which ensures that the copy of a Pod is scheduled on every node.
The Pod will use an init container to read and log the contents of <code>/etc/machine-id</code> from the host,
while the main container will be a <code>pause</code> container, which keeps the Pod running.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/basic-daemonset.yaml"><code>application/basic-daemonset.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/basic-daemonset.yaml to clipboard"></div><div class="includecode" id="application-basic-daemonset-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-daemonset<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app.kubernetes.io/name</span>:<span> </span>example<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app.kubernetes.io/name</span>:<span> </span>example<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/pause<span>
</span></span></span><span><span><span>      </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>log-machine-id<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>busybox:1.37<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'cat /etc/machine-id &gt; /var/log/machine-id.log'</span>]<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>machine-id<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/machine-id<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>log-dir<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>machine-id<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/etc/machine-id<span>
</span></span></span><span><span><span>          </span><span>type</span>:<span> </span>File<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>log-dir<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/log</span></span></code></pre></div></div></div><ol><li><p>Create a DaemonSet based on the (YAML) manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/basic-daemonset.yaml
</span></span></code></pre></div></li><li><p>Once applied, you can verify that the DaemonSet is running a Pod on every node in the cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -o wide
</span></span></code></pre></div><p>The output will list one Pod per node, similar to:</p><pre tabindex="0"><code>NAME                                READY   STATUS    RESTARTS   AGE    IP       NODE
example-daemonset-xxxxx             1/1     Running   0          5m     x.x.x.x  node-1
example-daemonset-yyyyy             1/1     Running   0          5m     x.x.x.x  node-2
</code></pre></li><li><p>You can inspect the contents of the logged <code>/etc/machine-id</code> file by checking
the log directory mounted from the host:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> &lt;pod-name&gt; -- cat /var/log/machine-id.log
</span></span></code></pre></div><p>Where <code>&lt;pod-name&gt;</code> is the name of one of your Pods.</p></li></ol><h2 id="cleaning-up">Cleaning up</h2><p>To delete the DaemonSet, run this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete --cascade<span>=</span>foreground --ignore-not-found --now daemonsets/example-daemonset
</span></span></code></pre></div><p>This simple DaemonSet example introduces key components like init containers and host path volumes,
which can be expanded upon for more advanced use cases. For more details refer to
<a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>See <a href="/docs/tasks/manage-daemon/update-daemon-set/">Performing a rolling update on a DaemonSet</a></li><li>See <a href="/docs/concepts/workloads/controllers/daemonset/">Creating a DaemonSet to adopt existing DaemonSet pods</a></li></ul></div></div><div><div class="td-content"><h1>Perform a Rolling Update on a DaemonSet</h1><p>This page shows how to perform a rolling update on a DaemonSet.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="daemonset-update-strategy">DaemonSet Update Strategy</h2><p>DaemonSet has two update strategy types:</p><ul><li><code>OnDelete</code>: With <code>OnDelete</code> update strategy, after you update a DaemonSet template, new
DaemonSet pods will <em>only</em> be created when you manually delete old DaemonSet
pods. This is the same behavior of DaemonSet in Kubernetes version 1.5 or
before.</li><li><code>RollingUpdate</code>: This is the default update strategy.<br>With <code>RollingUpdate</code> update strategy, after you update a
DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods
will be created automatically, in a controlled fashion. At most one pod of
the DaemonSet will be running on each node during the whole update process.</li></ul><h2 id="performing-a-rolling-update">Performing a Rolling Update</h2><p>To enable the rolling update feature of a DaemonSet, you must set its
<code>.spec.updateStrategy.type</code> to <code>RollingUpdate</code>.</p><p>You may want to set
<a href="/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/#DaemonSetSpec"><code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code></a>
(default to 1),
<a href="/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/#DaemonSetSpec"><code>.spec.minReadySeconds</code></a>
(default to 0) and
<a href="/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/#DaemonSetSpec"><code>.spec.updateStrategy.rollingUpdate.maxSurge</code></a>
(defaults to 0) as well.</p><h3 id="creating-a-daemonset-with-rollingupdate-update-strategy">Creating a DaemonSet with <code>RollingUpdate</code> update strategy</h3><p>This YAML file specifies a DaemonSet with an update strategy as 'RollingUpdate'</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/fluentd-daemonset.yaml"><code>controllers/fluentd-daemonset.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/fluentd-daemonset.yaml to clipboard"></div><div class="includecode" id="controllers-fluentd-daemonset-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>fluentd-logging<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>  </span><span>updateStrategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>RollingUpdate<span>
</span></span></span><span><span><span>    </span><span>rollingUpdate</span>:<span>
</span></span></span><span><span><span>      </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span><span># these tolerations are to have the daemonset runnable on control plane nodes</span><span>
</span></span></span><span><span><span>      </span><span># remove them if your control plane nodes should not run pods</span><span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/control-plane<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/master<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>quay.io/fluentd_elasticsearch/fluentd:v5.0.1<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>varlibdockercontainers<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/lib/docker/containers<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>varlibdockercontainers<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/lib/docker/containers<span>
</span></span></span></code></pre></div></div></div><p>After verifying the update strategy of the DaemonSet manifest, create the DaemonSet:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml
</span></span></code></pre></div><p>Alternatively, use <code>kubectl apply</code> to create the same DaemonSet if you plan to
update the DaemonSet with <code>kubectl apply</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml
</span></span></code></pre></div><h3 id="checking-daemonset-rollingupdate-update-strategy">Checking DaemonSet <code>RollingUpdate</code> update strategy</h3><p>Check the update strategy of your DaemonSet, and make sure it's set to
<code>RollingUpdate</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get ds/fluentd-elasticsearch -o go-template<span>=</span><span>'{{.spec.updateStrategy.type}}{{"\n"}}'</span> -n kube-system
</span></span></code></pre></div><p>If you haven't created the DaemonSet in the system, check your DaemonSet
manifest with the following command instead:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml --dry-run<span>=</span>client -o go-template<span>=</span><span>'{{.spec.updateStrategy.type}}{{"\n"}}'</span>
</span></span></code></pre></div><p>The output from both commands should be:</p><pre tabindex="0"><code>RollingUpdate
</code></pre><p>If the output isn't <code>RollingUpdate</code>, go back and modify the DaemonSet object or
manifest accordingly.</p><h3 id="updating-a-daemonset-template">Updating a DaemonSet template</h3><p>Any updates to a <code>RollingUpdate</code> DaemonSet <code>.spec.template</code> will trigger a rolling
update. Let's update the DaemonSet by applying a new YAML file. This can be done with several different <code>kubectl</code> commands.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/fluentd-daemonset-update.yaml"><code>controllers/fluentd-daemonset-update.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/fluentd-daemonset-update.yaml to clipboard"></div><div class="includecode" id="controllers-fluentd-daemonset-update-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>fluentd-logging<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>  </span><span>updateStrategy</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>RollingUpdate<span>
</span></span></span><span><span><span>    </span><span>rollingUpdate</span>:<span>
</span></span></span><span><span><span>      </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span><span># these tolerations are to have the daemonset runnable on control plane nodes</span><span>
</span></span></span><span><span><span>      </span><span># remove them if your control plane nodes should not run pods</span><span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/control-plane<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/master<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>quay.io/fluentd_elasticsearch/fluentd:v5.0.1<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>100m<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>varlibdockercontainers<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/lib/docker/containers<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>varlibdockercontainers<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/lib/docker/containers<span>
</span></span></span></code></pre></div></div></div><h4 id="declarative-commands">Declarative commands</h4><p>If you update DaemonSets using
<a href="/docs/tasks/manage-kubernetes-objects/declarative-config/">configuration files</a>,
use <code>kubectl apply</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset-update.yaml
</span></span></code></pre></div><h4 id="imperative-commands">Imperative commands</h4><p>If you update DaemonSets using
<a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">imperative commands</a>,
use <code>kubectl edit</code> :</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit ds/fluentd-elasticsearch -n kube-system
</span></span></code></pre></div><h5 id="updating-only-the-container-image">Updating only the container image</h5><p>If you only need to update the container image in the DaemonSet template, i.e.
<code>.spec.template.spec.containers[*].image</code>, use <code>kubectl set image</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> image ds/fluentd-elasticsearch fluentd-elasticsearch<span>=</span>quay.io/fluentd_elasticsearch/fluentd:v2.6.0 -n kube-system
</span></span></code></pre></div><h3 id="watching-the-rolling-update-status">Watching the rolling update status</h3><p>Finally, watch the rollout status of the latest DaemonSet rolling update:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout status ds/fluentd-elasticsearch -n kube-system
</span></span></code></pre></div><p>When the rollout is complete, the output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>daemonset <span>"fluentd-elasticsearch"</span> successfully rolled out
</span></span></code></pre></div><h2 id="troubleshooting">Troubleshooting</h2><h3 id="daemonset-rolling-update-is-stuck">DaemonSet rolling update is stuck</h3><p>Sometimes, a DaemonSet rolling update may be stuck. Here are some possible
causes:</p><h4 id="some-nodes-run-out-of-resources">Some nodes run out of resources</h4><p>The rollout is stuck because new DaemonSet pods can't be scheduled on at least one
node. This is possible when the node is
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">running out of resources</a>.</p><p>When this happens, find the nodes that don't have the DaemonSet pods scheduled on
by comparing the output of <code>kubectl get nodes</code> and the output of:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>name</span><span>=</span>fluentd-elasticsearch -o wide -n kube-system
</span></span></code></pre></div><p>Once you've found those nodes, delete some non-DaemonSet pods from the node to
make room for new DaemonSet pods.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This will cause service disruption when deleted pods are not controlled by any controllers or pods are not
replicated. This does not respect <a href="/docs/tasks/run-application/configure-pdb/">PodDisruptionBudget</a>
either.</div><h4 id="broken-rollout">Broken rollout</h4><p>If the recent DaemonSet template update is broken, for example, the container is
crash looping, or the container image doesn't exist (often due to a typo),
DaemonSet rollout won't progress.</p><p>To fix this, update the DaemonSet template again. New rollout won't be
blocked by previous unhealthy rollouts.</p><h4 id="clock-skew">Clock skew</h4><p>If <code>.spec.minReadySeconds</code> is specified in the DaemonSet, clock skew between
master and nodes will make DaemonSet unable to detect the right rollout
progress.</p><h2 id="clean-up">Clean up</h2><p>Delete DaemonSet from a namespace :</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete ds fluentd-elasticsearch -n kube-system
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>See <a href="/docs/tasks/manage-daemon/rollback-daemon-set/">Performing a rollback on a DaemonSet</a></li><li>See <a href="/docs/concepts/workloads/controllers/daemonset/">Creating a DaemonSet to adopt existing DaemonSet pods</a></li></ul></div></div><div><div class="td-content"><h1>Perform a Rollback on a DaemonSet</h1><p>This page shows how to perform a rollback on a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version 1.7.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>You should already know how to <a href="/docs/tasks/manage-daemon/update-daemon-set/">perform a rolling update on a
DaemonSet</a>.</p><h2 id="performing-a-rollback-on-a-daemonset">Performing a rollback on a DaemonSet</h2><h3 id="step-1-find-the-daemonset-revision-you-want-to-roll-back-to">Step 1: Find the DaemonSet revision you want to roll back to</h3><p>You can skip this step if you only want to roll back to the last revision.</p><p>List all revisions of a DaemonSet:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout <span>history</span> daemonset &lt;daemonset-name&gt;
</span></span></code></pre></div><p>This returns a list of DaemonSet revisions:</p><pre tabindex="0"><code>daemonsets "&lt;daemonset-name&gt;"
REVISION        CHANGE-CAUSE
1               ...
2               ...
...
</code></pre><ul><li>Change cause is copied from DaemonSet annotation <code>kubernetes.io/change-cause</code>
to its revisions upon creation. You may specify <code>--record=true</code> in <code>kubectl</code>
to record the command executed in the change cause annotation.</li></ul><p>To see the details of a specific revision:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout <span>history</span> daemonset &lt;daemonset-name&gt; --revision<span>=</span><span>1</span>
</span></span></code></pre></div><p>This returns the details of that revision:</p><pre tabindex="0"><code>daemonsets "&lt;daemonset-name&gt;" with revision #1
Pod Template:
Labels:       foo=bar
Containers:
app:
 Image:        ...
 Port:         ...
 Environment:  ...
 Mounts:       ...
Volumes:      ...
</code></pre><h3 id="step-2-roll-back-to-a-specific-revision">Step 2: Roll back to a specific revision</h3><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Specify the revision number you get from Step 1 in --to-revision</span>
</span></span><span><span>kubectl rollout undo daemonset &lt;daemonset-name&gt; --to-revision<span>=</span>&lt;revision&gt;
</span></span></code></pre></div><p>If it succeeds, the command returns:</p><pre tabindex="0"><code>daemonset "&lt;daemonset-name&gt;" rolled back
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If <code>--to-revision</code> flag is not specified, kubectl picks the most recent revision.</div><h3 id="step-3-watch-the-progress-of-the-daemonset-rollback">Step 3: Watch the progress of the DaemonSet rollback</h3><p><code>kubectl rollout undo daemonset</code> tells the server to start rolling back the
DaemonSet. The real rollback is done asynchronously inside the cluster
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>.</p><p>To watch the progress of the rollback:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout status ds/&lt;daemonset-name&gt;
</span></span></code></pre></div><p>When the rollback is complete, the output is similar to:</p><pre tabindex="0"><code>daemonset "&lt;daemonset-name&gt;" successfully rolled out
</code></pre><h2 id="understanding-daemonset-revisions">Understanding DaemonSet revisions</h2><p>In the previous <code>kubectl rollout history</code> step, you got a list of DaemonSet
revisions. Each revision is stored in a resource named ControllerRevision.</p><p>To see what is stored in each revision, find the DaemonSet revision raw
resources:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get controllerrevision -l &lt;daemonset-selector-key&gt;<span>=</span>&lt;daemonset-selector-value&gt;
</span></span></code></pre></div><p>This returns a list of ControllerRevisions:</p><pre tabindex="0"><code>NAME                               CONTROLLER                     REVISION   AGE
&lt;daemonset-name&gt;-&lt;revision-hash&gt;   DaemonSet/&lt;daemonset-name&gt;     1          1h
&lt;daemonset-name&gt;-&lt;revision-hash&gt;   DaemonSet/&lt;daemonset-name&gt;     2          1h
</code></pre><p>Each ControllerRevision stores the annotations and template of a DaemonSet
revision.</p><p><code>kubectl rollout undo</code> takes a specific ControllerRevision and replaces
DaemonSet template with the template stored in the ControllerRevision.
<code>kubectl rollout undo</code> is equivalent to updating DaemonSet template to a
previous revision through other commands, such as <code>kubectl edit</code> or <code>kubectl apply</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>DaemonSet revisions only roll forward. That is to say, after a
rollback completes, the revision number (<code>.revision</code> field) of the
ControllerRevision being rolled back to will advance. For example, if you
have revision 1 and 2 in the system, and roll back from revision 2 to revision
1, the ControllerRevision with <code>.revision: 1</code> will become <code>.revision: 3</code>.</div><h2 id="troubleshooting">Troubleshooting</h2><ul><li>See <a href="/docs/tasks/manage-daemon/update-daemon-set/#troubleshooting">troubleshooting DaemonSet rolling
update</a>.</li></ul></div></div><div><div class="td-content"><h1>Running Pods on Only Some Nodes</h1><p>This page demonstrates how can you run <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> on only some <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Nodes</a> as part of a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a></p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="running-pods-on-only-some-nodes">Running Pods on only some Nodes</h2><p>Imagine that you want to run a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a>, but you only need to run those daemon pods
on nodes that have local solid state (SSD) storage. For example, the Pod might provide cache service to the
node, and the cache is only useful when low-latency local storage is available.</p><h3 id="step-1-add-labels-to-your-nodes">Step 1: Add labels to your nodes</h3><p>Add the label <code>ssd=true</code> to the nodes which have SSDs.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label nodes example-node-1 example-node-2 <span>ssd</span><span>=</span><span>true</span>
</span></span></code></pre></div><h3 id="step-2-create-the-manifest">Step 2: Create the manifest</h3><p>Let's create a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a> which will provision the daemon pods on the SSD labeled <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a> only.</p><p>Next, use a <code>nodeSelector</code> to ensure that the DaemonSet only runs Pods on nodes
with the <code>ssd</code> label set to <code>"true"</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/daemonset-label-selector.yaml"><code>controllers/daemonset-label-selector.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/daemonset-label-selector.yaml to clipboard"></div><div class="includecode" id="controllers-daemonset-label-selector-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>ssd-driver<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>ssd-driver-pod<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>ssd-driver-pod<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>ssd</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>example-container<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span>example-image</span></span></code></pre></div></div></div><h3 id="step-3-create-the-daemonset">Step 3: Create the DaemonSet</h3><p>Create the DaemonSet from the manifest by using <code>kubectl create</code> or <code>kubectl apply</code></p><p>Let's label another node as <code>ssd=true</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label nodes example-node-3 <span>ssd</span><span>=</span><span>true</span>
</span></span></code></pre></div><p>Labelling the node automatically triggers the control plane (specifically, the DaemonSet controller)
to run a new daemon pod on that node.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -o wide
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>NAME                              READY     STATUS    RESTARTS   AGE    IP      NODE
&lt;daemonset-name&gt;&lt;some-hash-01&gt;    1/1       Running   0          13s    .....   example-node-1
&lt;daemonset-name&gt;&lt;some-hash-02&gt;    1/1       Running   0          13s    .....   example-node-2
&lt;daemonset-name&gt;&lt;some-hash-03&gt;    1/1       Running   0          5s     .....   example-node-3
</code></pre></div></div><div><div class="td-content"><h1>Networking</h1><div class="lead">Learn how to configure networking for your cluster.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/tasks/network/customize-hosts-file-for-pods/">Adding entries to Pod /etc/hosts with HostAliases</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/network/extend-service-ip-ranges/">Extend Service IP Ranges</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/network/reconfigure-default-service-ip-ranges/">Kubernetes Default ServiceCIDR Reconfiguration</a></h5><p></p></div><div class="entry"><h5><a href="/docs/tasks/network/validate-dual-stack/">Validate IPv4/IPv6 dual-stack</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Adding entries to Pod /etc/hosts with HostAliases</h1><p>Adding entries to a Pod's <code>/etc/hosts</code> file provides Pod-level override of hostname resolution when DNS and other options are not applicable. You can add these custom entries with the HostAliases field in PodSpec.</p><p>The Kubernetes project recommends modifying DNS configuration using the <code>hostAliases</code> field
(part of the <code>.spec</code> for a Pod), and not by using an init container or other means to edit <code>/etc/hosts</code>
directly.
Change made in other ways may be overwritten by the kubelet during Pod creation or restart.</p><h2 id="default-hosts-file-content">Default hosts file content</h2><p>Start an Nginx Pod which is assigned a Pod IP:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run nginx --image nginx
</span></span></code></pre></div><pre tabindex="0"><code>pod/nginx created
</code></pre><p>Examine a Pod IP:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --output<span>=</span>wide
</span></span></code></pre></div><pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre><p>The hosts file content would look like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> nginx -- cat /etc/hosts
</span></span></code></pre></div><pre tabindex="0"><code># Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.4	nginx
</code></pre><p>By default, the <code>hosts</code> file only includes IPv4 and IPv6 boilerplates like
<code>localhost</code> and its own hostname.</p><h2 id="adding-additional-entries-with-hostaliases">Adding additional entries with hostAliases</h2><p>In addition to the default boilerplate, you can add additional entries to the
<code>hosts</code> file.
For example: to resolve <code>foo.local</code>, <code>bar.local</code> to <code>127.0.0.1</code> and <code>foo.remote</code>,
<code>bar.remote</code> to <code>10.1.2.3</code>, you can configure HostAliases for a Pod under
<code>.spec.hostAliases</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/hostaliases-pod.yaml"><code>service/networking/hostaliases-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/hostaliases-pod.yaml to clipboard"></div><div class="includecode" id="service-networking-hostaliases-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hostaliases-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>hostAliases</span>:<span>
</span></span></span><span><span><span>  </span>- <span>ip</span>:<span> </span><span>"127.0.0.1"</span><span>
</span></span></span><span><span><span>    </span><span>hostnames</span>:<span>
</span></span></span><span><span><span>    </span>- <span>"foo.local"</span><span>
</span></span></span><span><span><span>    </span>- <span>"bar.local"</span><span>
</span></span></span><span><span><span>  </span>- <span>ip</span>:<span> </span><span>"10.1.2.3"</span><span>
</span></span></span><span><span><span>    </span><span>hostnames</span>:<span>
</span></span></span><span><span><span>    </span>- <span>"foo.remote"</span><span>
</span></span></span><span><span><span>    </span>- <span>"bar.remote"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cat-hosts<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- cat<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- <span>"/etc/hosts"</span><span>
</span></span></span></code></pre></div></div></div><p>You can start a Pod with that configuration by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/service/networking/hostaliases-pod.yaml
</span></span></code></pre></div><pre tabindex="0"><code>pod/hostaliases-pod created
</code></pre><p>Examine a Pod's details to see its IPv4 address and its status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pod --output<span>=</span>wide
</span></span></code></pre></div><pre tabindex="0"><code>NAME                           READY     STATUS      RESTARTS   AGE       IP              NODE
hostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0
</code></pre><p>The <code>hosts</code> file content looks like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs hostaliases-pod
</span></span></code></pre></div><pre tabindex="0"><code># Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.5	hostaliases-pod

# Entries added by HostAliases.
127.0.0.1	foo.local	bar.local
10.1.2.3	foo.remote	bar.remote
</code></pre><p>with the additional entries specified at the bottom.</p><h2 id="why-does-kubelet-manage-the-hosts-file">Why does the kubelet manage the hosts file?</h2><p>The kubelet manages the
<code>hosts</code> file for each container of the Pod to prevent the container runtime from
modifying the file after the containers have already been started.
Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would
then modify the <code>/etc/hosts</code> file after each container had started.</p><p>Current Kubernetes can use a variety of container runtimes; even so, the kubelet manages the
hosts file within each container so that the outcome is as intended regardless of which
container runtime you use.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>Avoid making manual changes to the hosts file inside a container.</p><p>If you make manual changes to the hosts file,
those changes are lost when the container exits.</p></div></div></div><div><div class="td-content"><h1>Extend Service IP Ranges</h1><div class="feature-state-notice feature-stable" title="Feature Gate: MultiCIDRServiceAllocator"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>This document shares how to extend the existing Service IP range assigned to a cluster.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.29.<p>To check the version, enter <code>kubectl version</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>While you can use this feature with an earlier version, the feature is only GA and officially supported since v1.33.</div><h2 id="extend-service-ip-ranges">Extend Service IP Ranges</h2><p>Kubernetes clusters with kube-apiservers that have enabled the <code>MultiCIDRServiceAllocator</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> and have the
<code>networking.k8s.io/v1</code> API group active, will create a ServiceCIDR object that takes
the well-known name <code>kubernetes</code>, and that specifies an IP address range
based on the value of the <code>--service-cluster-ip-range</code> command line argument to kube-apiserver.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get servicecidr
</span></span></code></pre></div><pre tabindex="0"><code>NAME         CIDRS          AGE
kubernetes   10.96.0.0/28   17d
</code></pre><p>The well-known <code>kubernetes</code> Service, that exposes the kube-apiserver endpoint to the Pods, calculates
the first IP address from the default ServiceCIDR range and uses that IP address as its
cluster IP address.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get service kubernetes
</span></span></code></pre></div><pre tabindex="0"><code>NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   17d
</code></pre><p>The default Service, in this case, uses the ClusterIP 10.96.0.1, that has the corresponding IPAddress object.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get ipaddress 10.96.0.1
</span></span></code></pre></div><pre tabindex="0"><code>NAME        PARENTREF
10.96.0.1   services/default/kubernetes
</code></pre><p>The ServiceCIDRs are protected with <a class="glossary-tooltip" title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." href="/docs/concepts/overview/working-with-objects/finalizers/" target="_blank">finalizers</a>,
to avoid leaving Service ClusterIPs orphans; the finalizer is only removed if there is another subnet
that contains the existing IPAddresses or there are no IPAddresses belonging to the subnet.</p><h2 id="extend-the-number-of-available-ips-for-services">Extend the number of available IPs for Services</h2><p>There are cases that users will need to increase the number addresses available to Services,
previously, increasing the Service range was a disruptive operation that could also cause data loss.
With this new feature users only need to add a new ServiceCIDR to increase the number of available addresses.</p><h3 id="adding-a-new-servicecidr">Adding a new ServiceCIDR</h3><p>On a cluster with a 10.96.0.0/28 range for Services, there is only 2^(32-28) - 2 = 14
IP addresses available. The <code>kubernetes.default</code> Service is always created; for this example,
that leaves you with only 13 possible Services.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span>for</span> i in <span>$(</span>seq <span>1</span> 13<span>)</span>; <span>do</span> kubectl create service clusterip <span>"test-</span><span>$i</span><span>"</span> --tcp <span>80</span> -o json | jq -r .spec.clusterIP; <span>done</span>
</span></span></code></pre></div><pre tabindex="0"><code>10.96.0.11
10.96.0.5
10.96.0.12
10.96.0.13
10.96.0.14
10.96.0.2
10.96.0.3
10.96.0.4
10.96.0.6
10.96.0.7
10.96.0.8
10.96.0.9
error: failed to create ClusterIP service: Internal error occurred: failed to allocate a serviceIP: range is full
</code></pre><p>You can increase the number of IP addresses available for Services, by creating a new ServiceCIDR
that extends or adds new IP address ranges.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>cat &lt;EOF | kubectl apply -f -
</span></span><span><span>apiVersion: networking.k8s.io/v1
</span></span><span><span>kind: ServiceCIDR
</span></span><span><span>metadata:
</span></span><span><span>  name: newcidr1
</span></span><span><span>spec:
</span></span><span><span>  cidrs:
</span></span><span><span>  - 10.96.0.0/24
</span></span><span><span>EOF
</span></span></code></pre></div><pre tabindex="0"><code>servicecidr.networking.k8s.io/newcidr1 created
</code></pre><p>and this will allow you to create new Services with ClusterIPs that will be picked from this new range.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span>for</span> i in <span>$(</span>seq <span>13</span> 16<span>)</span>; <span>do</span> kubectl create service clusterip <span>"test-</span><span>$i</span><span>"</span> --tcp <span>80</span> -o json | jq -r .spec.clusterIP; <span>done</span>
</span></span></code></pre></div><pre tabindex="0"><code>10.96.0.48
10.96.0.200
10.96.0.121
10.96.0.144
</code></pre><h3 id="deleting-a-servicecidr">Deleting a ServiceCIDR</h3><p>You cannot delete a ServiceCIDR if there are IPAddresses that depend on the ServiceCIDR.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete servicecidr newcidr1
</span></span></code></pre></div><pre tabindex="0"><code>servicecidr.networking.k8s.io "newcidr1" deleted
</code></pre><p>Kubernetes uses a finalizer on the ServiceCIDR to track this dependent relationship.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get servicecidr newcidr1 -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceCIDR<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2023-10-12T15:11:07Z"</span><span>
</span></span></span><span><span><span>  </span><span>deletionGracePeriodSeconds</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>  </span><span>deletionTimestamp</span>:<span> </span><span>"2023-10-12T15:12:45Z"</span><span>
</span></span></span><span><span><span>  </span><span>finalizers</span>:<span>
</span></span></span><span><span><span>  </span>- networking.k8s.io/service-cidr-finalizer<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>newcidr1<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"1133"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>5ffd8afe-c78f-4e60-ae76-cec448a8af40<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>cidrs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>10.96.0.0</span>/24<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>lastTransitionTime</span>:<span> </span><span>"2023-10-12T15:12:45Z"</span><span>
</span></span></span><span><span><span>    </span><span>message</span>:<span> </span>There are still IPAddresses referencing the ServiceCIDR, please remove<span>
</span></span></span><span><span><span>      </span>them or create a new ServiceCIDR<span>
</span></span></span><span><span><span>    </span><span>reason</span>:<span> </span>OrphanIPAddress<span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Ready<span>
</span></span></span></code></pre></div><p>By removing the Services containing the IP addresses that are blocking the deletion of the ServiceCIDR</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span><span>for</span> i in <span>$(</span>seq <span>13</span> 16<span>)</span>; <span>do</span> kubectl delete service <span>"test-</span><span>$i</span><span>"</span> ; <span>done</span>
</span></span></code></pre></div><pre tabindex="0"><code>service "test-13" deleted
service "test-14" deleted
service "test-15" deleted
service "test-16" deleted
</code></pre><p>the control plane notices the removal. The control plane then removes its finalizer,
so that the ServiceCIDR that was pending deletion will actually be removed.</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get servicecidr newcidr1
</span></span></code></pre></div><pre tabindex="0"><code>Error from server (NotFound): servicecidrs.networking.k8s.io "newcidr1" not found
</code></pre><h2 id="kubernetes-service-cidr-policies">Kubernetes Service CIDR Policies</h2><p>Cluster administrators can implement policies to control the creation and
modification of ServiceCIDR resources within the cluster. This allows for
centralized management of the IP address ranges used for Services and helps
prevent unintended or conflicting configurations. Kubernetes provides mechanisms
like Validating Admission Policies to enforce these rules.</p><h3 id="preventing-unauthorized-servicecidr-creation-update-using-validating-admission-policy">Preventing Unauthorized ServiceCIDR Creation/Update using Validating Admission Policy</h3><p>There can be situations that the cluster administrators want to restrict the
ranges that can be allowed or to completely deny any changes to the cluster
Service IP ranges.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The default "kubernetes" ServiceCIDR is created by the kube-apiserver
to provide consistency in the cluster and is required for the cluster to work,
so it always must be allowed. You can ensure your <code>ValidatingAdmissionPolicy</code>
doesn't restrict the default ServiceCIDR by adding the clause:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span>matchConditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span><span>'exclude-default-servicecidr'</span><span>
</span></span></span><span><span><span>    </span><span>expression</span>:<span> </span><span>"object.metadata.name != 'kubernetes'"</span><span>
</span></span></span></code></pre></div><p>as in the examples below.</p></div><h4 id="restrict-service-cidr-ranges-to-some-specific-ranges">Restrict Service CIDR ranges to some specific ranges</h4><p>The following is an example of a <code>ValidatingAdmissionPolicy</code> that only allows
ServiceCIDRs to be created if they are subranges of the given <code>allowed</code> ranges.
(So the example policy would allow a ServiceCIDR with <code>cidrs: ['10.96.1.0/24']</code>
or <code>cidrs: ['2001:db8:0:0:ffff::/80', '10.96.0.0/20']</code> but would not allow a
ServiceCIDR with <code>cidrs: ['172.20.0.0/16']</code>.) You can copy this policy and change
the value of <code>allowed</code> to something appropriate for you cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>admissionregistration.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ValidatingAdmissionPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>"servicecidrs.default"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>failurePolicy</span>:<span> </span>Fail<span>
</span></span></span><span><span><span>  </span><span>matchConstraints</span>:<span>
</span></span></span><span><span><span>    </span><span>resourceRules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>apiGroups</span>:<span>   </span>[<span>"networking.k8s.io"</span>]<span>
</span></span></span><span><span><span>      </span><span>apiVersions</span>:<span> </span>[<span>"v1"</span>]<span>
</span></span></span><span><span><span>      </span><span>operations</span>:<span>  </span>[<span>"CREATE"</span>,<span> </span><span>"UPDATE"</span>]<span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>   </span>[<span>"servicecidrs"</span>]<span>
</span></span></span><span><span><span>  </span><span>matchConditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span><span>'exclude-default-servicecidr'</span><span>
</span></span></span><span><span><span>    </span><span>expression</span>:<span> </span><span>"object.metadata.name != 'kubernetes'"</span><span>
</span></span></span><span><span><span>  </span><span>variables</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>allowed<span>
</span></span></span><span><span><span>    </span><span>expression</span>:<span> </span><span>"['10.96.0.0/16','2001:db8::/64']"</span><span>
</span></span></span><span><span><span>  </span><span>validations</span>:<span>
</span></span></span><span><span><span>  </span>- <span>expression</span>:<span> </span><span>"object.spec.cidrs.all(newCIDR, variables.allowed.exists(allowedCIDR, cidr(allowedCIDR).containsCIDR(newCIDR)))"</span><span>
</span></span></span><span><span><span>  </span><span># For all CIDRs (newCIDR) listed in the spec.cidrs of the submitted ServiceCIDR</span><span>
</span></span></span><span><span><span>  </span><span># object, check if there exists at least one CIDR (allowedCIDR) in the `allowed`</span><span>
</span></span></span><span><span><span>  </span><span># list of the VAP such that the allowedCIDR fully contains the newCIDR.</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>admissionregistration.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ValidatingAdmissionPolicyBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>"servicecidrs-binding"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>policyName</span>:<span> </span><span>"servicecidrs.default"</span><span>
</span></span></span><span><span><span>  </span><span>validationActions</span>:<span> </span>[Deny,Audit]<span>
</span></span></span></code></pre></div><p>Consult the <a href="https://kubernetes.io/docs/reference/using-api/cel/">CEL documentation</a>
to learn more about CEL if you want to write your own validation <code>expression</code>.</p><h4 id="restrict-any-usage-of-the-servicecidr-api">Restrict any usage of the ServiceCIDR API</h4><p>The following example demonstrates how to use a <code>ValidatingAdmissionPolicy</code> and
its binding to restrict the creation of any new Service CIDR ranges, excluding the default "kubernetes" ServiceCIDR:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>admissionregistration.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ValidatingAdmissionPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>"servicecidrs.deny"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>failurePolicy</span>:<span> </span>Fail<span>
</span></span></span><span><span><span>  </span><span>matchConstraints</span>:<span>
</span></span></span><span><span><span>    </span><span>resourceRules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>apiGroups</span>:<span>   </span>[<span>"networking.k8s.io"</span>]<span>
</span></span></span><span><span><span>      </span><span>apiVersions</span>:<span> </span>[<span>"v1"</span>]<span>
</span></span></span><span><span><span>      </span><span>operations</span>:<span>  </span>[<span>"CREATE"</span>,<span> </span><span>"UPDATE"</span>]<span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>   </span>[<span>"servicecidrs"</span>]<span>
</span></span></span><span><span><span>  </span><span>validations</span>:<span>
</span></span></span><span><span><span>  </span>- <span>expression</span>:<span> </span><span>"object.metadata.name == 'kubernetes'"</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>admissionregistration.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ValidatingAdmissionPolicyBinding<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>"servicecidrs-deny-binding"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>policyName</span>:<span> </span><span>"servicecidrs.deny"</span><span>
</span></span></span><span><span><span>  </span><span>validationActions</span>:<span> </span>[Deny,Audit]<span>
</span></span></span></code></pre></div></div></div><div><div class="td-content"><h1>Kubernetes Default ServiceCIDR Reconfiguration</h1><div class="feature-state-notice feature-stable" title="Feature Gate: MultiCIDRServiceAllocator"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>This document shares how to reconfigure the default Service IP range(s) assigned
to a cluster.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.33.<p>To check the version, enter <code>kubectl version</code>.</p><h2 id="kubernetes-default-servicecidr-reconfiguration">Kubernetes Default ServiceCIDR Reconfiguration</h2><p>This document explains how to manage the Service IP address range within a
Kubernetes cluster, which also influences the cluster's supported IP families
for Services.</p><p>The IP families available for Service ClusterIPs are determined by the
<code>--service-cluster-ip-range</code> flag to kube-apiserver. For a better
understanding of Service IP address allocation, refer to the
<a href="/docs/reference/networking/virtual-ips/#ip-address-objects">Services IP address allocation tracking</a> documentation.</p><p>Since Kubernetes 1.33, the Service IP families configured for the cluster are
reflected by the ServiceCIDR object named <code>kubernetes</code>. The <code>kubernetes</code> ServiceCIDR
object is created by the first kube-apiserver instance that starts, based on its
configured <code>--service-cluster-ip-range</code> flag. To ensure consistent cluster behavior,
all kube-apiserver instances must be configured with the same <code>--service-cluster-ip-range</code> values,
which must match the default <code>kubernetes</code> ServiceCIDR object.</p><h3 id="kubernetes-servicecidr-reconfiguration-categories">Kubernetes ServiceCIDR Reconfiguration Categories</h3><p>ServiceCIDR reconfiguration typically falls into one of the following categories:</p><ul><li><p><strong>Extending the existing ServiceCIDRs:</strong> This can be done dynamically by
adding new ServiceCIDR objects without the need for reconfiguring the
kube-apiserver. Please refer to the dedicated documentation on
<a href="/docs/tasks/network/extend-service-ip-ranges/">Extending Service IP Ranges</a>.</p></li><li><p><strong>Single-to-dual-stack conversion preserving the primary ServiceCIDR:</strong> This
involves introducing a secondary IP family (IPv6 to an IPv4-only cluster, or
IPv4 to an IPv6-only cluster) while keeping the original IP family as
primary. This requires an update to the kube-apiserver configuration and a
corresponding modification of various cluster components that need to handle
this additional IP family. These components include, but are not limited to,
kube-proxy, the CNI or network plugin, service mesh implementations, and DNS
services.</p></li><li><p><strong>Dual-to-single conversion preserving the primary ServiceCIDR:</strong> This
involves removing the secondary IP family from a dual-stack cluster,
reverting to a single IP family while retaining the original primary IP
family. In addition to reconfiguring the components to match the
new IP family, you might need to address Services that were explicitly
configured to use the removed IP family.</p></li><li><p><strong>Anything that results in changing the primary ServiceCIDR:</strong> Completely
replacing the default ServiceCIDR is a complex operation. If the new
ServiceCIDR does not overlap with the existing one, it will require
<a href="#illustrative-reconfiguration-steps">renumbering all existing Services and changing the <code>kubernetes.default</code> Service</a>.
The case where the primary IP family also changes is even more complicated,
and may require changing multiple cluster components (kubelet, network plugins, etc.)
to match the new primary IP family.</p></li></ul><h3 id="manual-operations-for-replacing-the-default-servicecidr">Manual Operations for Replacing the Default ServiceCIDR</h3><p>Reconfiguring the default ServiceCIDR necessitates manual steps performed by
the cluster operator, administrator, or the software managing the cluster
lifecycle. These typically include:</p><ol><li><strong>Updating</strong> the kube-apiserver configuration: Modify the
<code>--service-cluster-ip-range</code> flag with the new IP range(s).</li><li><strong>Reconfiguring</strong> the network components: This is a critical step and the
specific procedure depends on the different networking components in use. It
might involve updating configuration files, restarting agent pods, or
updating the components to manage the new ServiceCIDR(s) and the desired IP
family configuration for Pods. Typical components can be the implementation
of Kubernetes Services, such as kube-proxy, and the configured networking
plugin, and potentially other networking components like service mesh
controllers and DNS servers, to ensure they can correctly handle traffic and
perform service discovery with the new IP family configuration.</li><li><strong>Managing existing Services:</strong> Services with IPs from the old CIDR need to
be addressed if they are not within the new configured ranges. Options
include recreation (leading to downtime and new IP assignments) or
potentially more complex reconfiguration strategies.</li><li><strong>Recreating internal Kubernetes services:</strong> The <code>kubernetes.default</code>
Service must be deleted and recreated to obtain an IP address from the new
ServiceCIDR if the primary IP family is changed or replaced by a different
network.</li></ol><h3 id="illustrative-reconfiguration-steps">Illustrative Reconfiguration Steps</h3><p>The following steps describe a controlled reconfiguration focusing on the
complete replacement of the default ServiceCIDR and the recreation of the
<code>kubernetes.default</code> Service:</p><ol><li>Start the kube-apiserver with the initial <code>--service-cluster-ip-range</code>.</li><li>Create initial Services that obtain IPs from this range.</li><li>Introduce a new ServiceCIDR as a temporary target for reconfiguration.</li><li>Mark the <code>kubernetes</code> default ServiceCIDR for deletion (it will remain
pending due to existing IPs and finalizers). This prevents new allocations
from the old range.</li><li>Recreate existing Services. They should now be allocated IPs from the new,
temporary ServiceCIDR.</li><li>Restart the kube-apiserver with the new ServiceCIDR(s) configured and shut
down the old instance.</li><li>Delete the <code>kubernetes.default</code> Service. The new kube-apiserver will
recreate it within the new ServiceCIDR.</li></ol><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/cluster-administration/networking/">Kubernetes Networking Concepts</a></li><li><a href="/docs/concepts/services-networking/dual-stack/">Kubernetes Dual-Stack Services</a></li><li><a href="/docs/tasks/network/extend-service-ip-ranges/">Extending Kubernetes Service IP Ranges</a></li></ul></div></div><div><div class="td-content"><h1>Validate IPv4/IPv6 dual-stack</h1><p>This document shares how to validate IPv4/IPv6 dual-stack enabled Kubernetes clusters.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Provider support for dual-stack networking (Cloud provider or otherwise must be able to
provide Kubernetes nodes with routable IPv4/IPv6 network interfaces)</li><li>A <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a>
that supports dual-stack networking.</li><li><a href="/docs/concepts/services-networking/dual-stack/">Dual-stack enabled</a> cluster</li></ul>Your Kubernetes server must be at or later than version v1.23.<p>To check the version, enter <code>kubectl version</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>While you can validate with an earlier version, the feature is only GA and officially supported since v1.23.</div><h2 id="validate-addressing">Validate addressing</h2><h3 id="validate-node-addressing">Validate node addressing</h3><p>Each dual-stack Node should have a single IPv4 block and a single IPv6 block allocated.
Validate that IPv4/IPv6 Pod address ranges are configured by running the following command.
Replace the sample node name with a valid dual-stack Node from your cluster. In this example,
the Node's name is <code>k8s-linuxpool1-34450317-0</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template<span>=</span><span>'{{range .spec.podCIDRs}}{{printf "%s\n" .}}{{end}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code>10.244.1.0/24
2001:db8::/64
</code></pre><p>There should be one IPv4 block and one IPv6 block allocated.</p><p>Validate that the node has an IPv4 and IPv6 interface detected.
Replace node name with a valid node from the cluster.
In this example the node name is <code>k8s-linuxpool1-34450317-0</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template<span>=</span><span>'{{range .status.addresses}}{{printf "%s: %s\n" .type .address}}{{end}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code>Hostname: k8s-linuxpool1-34450317-0
InternalIP: 10.0.0.5
InternalIP: 2001:db8:10::5
</code></pre><h3 id="validate-pod-addressing">Validate Pod addressing</h3><p>Validate that a Pod has an IPv4 and IPv6 address assigned. Replace the Pod name with
a valid Pod in your cluster. In this example the Pod name is <code>pod01</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods pod01 -o go-template --template<span>=</span><span>'{{range .status.podIPs}}{{printf "%s\n" .ip}}{{end}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code>10.244.1.4
2001:db8::4
</code></pre><p>You can also validate Pod IPs using the Downward API via the <code>status.podIPs</code> fieldPath.
The following snippet demonstrates how you can expose the Pod IPs via an environment variable
called <code>MY_POD_IPS</code> within a container.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>        </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>MY_POD_IPS<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>              </span><span>fieldPath</span>:<span> </span>status.podIPs<span>
</span></span></span></code></pre></div><p>The following command prints the value of the <code>MY_POD_IPS</code> environment variable from
within a container. The value is a comma separated list that corresponds to the
Pod's IPv4 and IPv6 addresses.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it pod01 -- <span>set</span> | grep MY_POD_IPS
</span></span></code></pre></div><pre tabindex="0"><code>MY_POD_IPS=10.244.1.4,2001:db8::4
</code></pre><p>The Pod's IP addresses will also be written to <code>/etc/hosts</code> within a container.
The following command executes a cat on <code>/etc/hosts</code> on a dual stack Pod.
From the output you can verify both the IPv4 and IPv6 IP address for the Pod.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it pod01 -- cat /etc/hosts
</span></span></code></pre></div><pre tabindex="0"><code># Kubernetes-managed hosts file.
127.0.0.1    localhost
::1    localhost ip6-localhost ip6-loopback
fe00::0    ip6-localnet
fe00::0    ip6-mcastprefix
fe00::1    ip6-allnodes
fe00::2    ip6-allrouters
10.244.1.4    pod01
2001:db8::4    pod01
</code></pre><h2 id="validate-services">Validate Services</h2><p>Create the following Service that does not explicitly define <code>.spec.ipFamilyPolicy</code>.
Kubernetes will assign a cluster IP for the Service from the first configured
<code>service-cluster-ip-range</code> and set the <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Use <code>kubectl</code> to view the YAML for the Service.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><p>The Service has <code>.spec.ipFamilyPolicy</code> set to <code>SingleStack</code> and <code>.spec.clusterIP</code> set
to an IPv4 address from the first configured range set via <code>--service-cluster-ip-range</code>
flag on kube-controller-manager.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span><span>10.0.217.164</span><span>
</span></span></span><span><span><span>  </span><span>clusterIPs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>10.0.217.164</span><span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv4<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>SingleStack<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>sessionAffinity</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>ClusterIP<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>loadBalancer</span>:<span> </span>{}<span>
</span></span></span></code></pre></div><p>Create the following Service that explicitly defines <code>IPv6</code> as the first array element in
<code>.spec.ipFamilies</code>. Kubernetes will assign a cluster IP for the Service from the IPv6 range
configured <code>service-cluster-ip-range</code> and set the <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-ipfamilies-ipv6.yaml"><code>service/networking/dual-stack-ipfamilies-ipv6.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-ipfamilies-ipv6.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-ipfamilies-ipv6-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv6<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Use <code>kubectl</code> to view the YAML for the Service.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><p>The Service has <code>.spec.ipFamilyPolicy</code> set to <code>SingleStack</code> and <code>.spec.clusterIP</code> set to
an IPv6 address from the IPv6 range set via <code>--service-cluster-ip-range</code> flag on kube-controller-manager.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span><span>2001</span>:db8:fd00::5118<span>
</span></span></span><span><span><span>  </span><span>clusterIPs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>2001</span>:db8:fd00::5118<span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv6<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>SingleStack<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>sessionAffinity</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>ClusterIP<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>loadBalancer</span>:<span> </span>{}<span>
</span></span></span></code></pre></div><p>Create the following Service that explicitly defines <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>.
Kubernetes will assign both IPv4 and IPv6 addresses (as this cluster has dual-stack enabled) and
select the <code>.spec.ClusterIP</code> from the list of <code>.spec.ClusterIPs</code> based on the address family of
the first element in the <code>.spec.ipFamilies</code> array.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-svc.yaml"><code>service/networking/dual-stack-preferred-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-preferred-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-preferred-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>PreferDualStack<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>kubectl get svc</code> command will only show the primary IP in the <code>CLUSTER-IP</code> field.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc -l app.kubernetes.io/name<span>=</span>MyApp
</span></span></code></pre></div><pre tabindex="0"><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-service   ClusterIP   10.0.216.242   &lt;none&gt;        80/TCP    5s
</code></pre></div><p>Validate that the Service gets cluster IPs from the IPv4 and IPv6 address blocks using
<code>kubectl describe</code>. You may then validate access to the service via the IPs and ports.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe svc -l app.kubernetes.io/name<span>=</span>MyApp
</span></span></code></pre></div><pre tabindex="0"><code>Name:              my-service
Namespace:         default
Labels:            app.kubernetes.io/name=MyApp
Annotations:       &lt;none&gt;
Selector:          app.kubernetes.io/name=MyApp
Type:              ClusterIP
IP Family Policy:  PreferDualStack
IP Families:       IPv4,IPv6
IP:                10.0.216.242
IPs:               10.0.216.242,2001:db8:fd00::af55
Port:              &lt;unset&gt;  80/TCP
TargetPort:        9376/TCP
Endpoints:         &lt;none&gt;
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre><h3 id="create-a-dual-stack-load-balanced-service">Create a dual-stack load balanced Service</h3><p>If the cloud provider supports the provisioning of IPv6 enabled external load balancers,
create the following Service with <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>, <code>IPv6</code> as
the first element of the <code>.spec.ipFamilies</code> array and the <code>type</code> field set to <code>LoadBalancer</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-prefer-ipv6-lb-svc.yaml"><code>service/networking/dual-stack-prefer-ipv6-lb-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-prefer-ipv6-lb-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-prefer-ipv6-lb-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>PreferDualStack<span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv6<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Check the Service:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc -l app.kubernetes.io/name<span>=</span>MyApp
</span></span></code></pre></div><p>Validate that the Service receives a <code>CLUSTER-IP</code> address from the IPv6 address block
along with an <code>EXTERNAL-IP</code>. You may then validate access to the service via the IP and port.</p><pre tabindex="0"><code>NAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE
my-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s
</code></pre></div></div><div><div class="td-content"><h1>Extend kubectl with plugins</h1><div class="lead">Extend kubectl by creating and installing kubectl plugins.</div><p>This guide demonstrates how to install and write extensions for <a href="/docs/reference/kubectl/kubectl/">kubectl</a>.
By thinking of core <code>kubectl</code> commands as essential building blocks for interacting with a Kubernetes cluster,
a cluster administrator can think of plugins as a means of utilizing these building blocks to create more complex behavior.
Plugins extend <code>kubectl</code> with new sub-commands, allowing for new and custom features not included in the main distribution of <code>kubectl</code>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a working <code>kubectl</code> binary installed.</p><h2 id="installing-kubectl-plugins">Installing kubectl plugins</h2><p>A plugin is a standalone executable file, whose name begins with <code>kubectl-</code>. To install a plugin, move its executable file to anywhere on your <code>PATH</code>.</p><p>You can also discover and install kubectl plugins available in the open source
using <a href="https://krew.dev/">Krew</a>. Krew is a plugin manager maintained by
the Kubernetes SIG CLI community.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Kubectl plugins available via the Krew <a href="https://krew.sigs.k8s.io/plugins/">plugin index</a>
are not audited for security. You should install and run third-party plugins at your
own risk, since they are arbitrary programs running on your machine.</div><h3 id="discovering-plugins">Discovering plugins</h3><p><code>kubectl</code> provides a command <code>kubectl plugin list</code> that searches your <code>PATH</code> for valid plugin executables.
Executing this command causes a traversal of all files in your <code>PATH</code>. Any files that are executable, and
begin with <code>kubectl-</code> will show up <em>in the order in which they are present in your <code>PATH</code></em> in this command's output.
A warning will be included for any files beginning with <code>kubectl-</code> that are <em>not</em> executable.
A warning will also be included for any valid plugin files that overlap each other's name.</p><p>You can use <a href="https://krew.dev/">Krew</a> to discover and install <code>kubectl</code>
plugins from a community-curated
<a href="https://krew.sigs.k8s.io/plugins/">plugin index</a>.</p><h4 id="create-plugins">Create plugins</h4><p><code>kubectl</code> allows plugins to add custom create commands of the shape <code>kubectl create something</code> by providing a <code>kubectl-create-something</code> binary in the <code>PATH</code>.</p><h4 id="limitations">Limitations</h4><p>It is currently not possible to create plugins that overwrite existing <code>kubectl</code> commands or extend commands other than <code>create</code>.
For example, creating a plugin <code>kubectl-version</code> will cause that plugin to never be executed, as the existing <code>kubectl version</code>
command will always take precedence over it.
Due to this limitation, it is also <em>not</em> possible to use plugins to add new subcommands to existing <code>kubectl</code> commands.
For example, adding a subcommand <code>kubectl attach vm</code> by naming your plugin <code>kubectl-attach-vm</code> will cause that plugin to be ignored.</p><p><code>kubectl plugin list</code> shows warnings for any valid plugins that attempt to do this.</p><h2 id="writing-kubectl-plugins">Writing kubectl plugins</h2><p>You can write a plugin in any programming language or script that allows you to write command-line commands.</p><p>There is no plugin installation or pre-loading required. Plugin executables receive
the inherited environment from the <code>kubectl</code> binary.
A plugin determines which command path it wishes to implement based on its name.
For example, a plugin named <code>kubectl-foo</code> provides a command <code>kubectl foo</code>. You must
install the plugin executable somewhere in your <code>PATH</code>.</p><h3 id="example-plugin">Example plugin</h3><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>#!/bin/bash
</span></span></span><span><span><span></span>
</span></span><span><span><span># optional argument handling</span>
</span></span><span><span><span>if</span> <span>[[</span> <span>"</span><span>$1</span><span>"</span> <span>==</span> <span>"version"</span> <span>]]</span>
</span></span><span><span><span>then</span>
</span></span><span><span>    <span>echo</span> <span>"1.0.0"</span>
</span></span><span><span>    <span>exit</span> <span>0</span>
</span></span><span><span><span>fi</span>
</span></span><span><span>
</span></span><span><span><span># optional argument handling</span>
</span></span><span><span><span>if</span> <span>[[</span> <span>"</span><span>$1</span><span>"</span> <span>==</span> <span>"config"</span> <span>]]</span>
</span></span><span><span><span>then</span>
</span></span><span><span>    <span>echo</span> <span>"</span><span>$KUBECONFIG</span><span>"</span>
</span></span><span><span>    <span>exit</span> <span>0</span>
</span></span><span><span><span>fi</span>
</span></span><span><span>
</span></span><span><span><span>echo</span> <span>"I am a plugin named kubectl-foo"</span>
</span></span></code></pre></div><h3 id="using-a-plugin">Using a plugin</h3><p>To use a plugin, make the plugin executable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo chmod +x ./kubectl-foo
</span></span></code></pre></div><p>and place it anywhere in your <code>PATH</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sudo mv ./kubectl-foo /usr/local/bin
</span></span></code></pre></div><p>You may now invoke your plugin as a <code>kubectl</code> command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl foo
</span></span></code></pre></div><pre tabindex="0"><code>I am a plugin named kubectl-foo
</code></pre><p>All args and flags are passed as-is to the executable:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl foo version
</span></span></code></pre></div><pre tabindex="0"><code>1.0.0
</code></pre><p>All environment variables are also passed as-is to the executable:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>export</span> <span>KUBECONFIG</span><span>=</span>~/.kube/config
</span></span><span><span>kubectl foo config
</span></span></code></pre></div><pre tabindex="0"><code>/home/&lt;user&gt;/.kube/config
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>KUBECONFIG</span><span>=</span>/etc/kube/config kubectl foo config
</span></span></code></pre></div><pre tabindex="0"><code>/etc/kube/config
</code></pre><p>Additionally, the first argument that is passed to a plugin will always be the full path to the location where it was invoked (<code>$0</code> would equal <code>/usr/local/bin/kubectl-foo</code> in the example above).</p><h3 id="naming-a-plugin">Naming a plugin</h3><p>As seen in the example above, a plugin determines the command path that it will implement based on its filename. Every sub-command in the command path that a plugin targets, is separated by a dash (<code>-</code>).
For example, a plugin that wishes to be invoked whenever the command <code>kubectl foo bar baz</code> is invoked by the user, would have the filename of <code>kubectl-foo-bar-baz</code>.</p><h4 id="flags-and-argument-handling">Flags and argument handling</h4><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The plugin mechanism does <em>not</em> create any custom, plugin-specific values or environment variables for a plugin process.</p><p>An older kubectl plugin mechanism provided environment variables such as <code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>; that no longer happens.</p></div><p>kubectl plugins must parse and validate all of the arguments passed to them.
See <a href="#using-the-command-line-runtime-package">using the command line runtime package</a> for details of a Go library aimed at plugin authors.</p><p>Here are some additional cases where users invoke your plugin while providing additional flags and arguments. This builds upon the <code>kubectl-foo-bar-baz</code> plugin from the scenario above.</p><p>If you run <code>kubectl foo bar baz arg1 --flag=value arg2</code>, kubectl's plugin mechanism will first try to find the plugin with the longest possible name, which in this case
would be <code>kubectl-foo-bar-baz-arg1</code>. Upon not finding that plugin, kubectl then treats the last dash-separated value as an argument (<code>arg1</code> in this case), and attempts to find the next longest possible name, <code>kubectl-foo-bar-baz</code>.
Upon having found a plugin with this name, kubectl then invokes that plugin, passing all args and flags after the plugin's name as arguments to the plugin process.</p><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># create a plugin</span>
</span></span><span><span><span>echo</span> -e <span>'#!/bin/bash\n\necho "My first command-line argument was $1"'</span> &gt; kubectl-foo-bar-baz
</span></span><span><span>sudo chmod +x ./kubectl-foo-bar-baz
</span></span><span><span>
</span></span><span><span><span># "install" your plugin by moving it to a directory in your $PATH</span>
</span></span><span><span>sudo mv ./kubectl-foo-bar-baz /usr/local/bin
</span></span><span><span>
</span></span><span><span><span># check that kubectl recognizes your plugin</span>
</span></span><span><span>kubectl plugin list
</span></span></code></pre></div><pre tabindex="0"><code>The following kubectl-compatible plugins are available:

/usr/local/bin/kubectl-foo-bar-baz
</code></pre><pre tabindex="0"><code># test that calling your plugin via a "kubectl" command works
# even when additional arguments and flags are passed to your
# plugin executable by the user.
kubectl foo bar baz arg1 --meaningless-flag=true
</code></pre><pre tabindex="0"><code>My first command-line argument was arg1
</code></pre><p>As you can see, your plugin was found based on the <code>kubectl</code> command specified by a user, and all extra arguments and flags were passed as-is to the plugin executable once it was found.</p><h4 id="names-with-dashes-and-underscores">Names with dashes and underscores</h4><p>Although the <code>kubectl</code> plugin mechanism uses the dash (<code>-</code>) in plugin filenames to separate the sequence of sub-commands processed by the plugin, it is still possible to create a plugin
command containing dashes in its commandline invocation by using underscores (<code>_</code>) in its filename.</p><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># create a plugin containing an underscore in its filename</span>
</span></span><span><span><span>echo</span> -e <span>'#!/bin/bash\n\necho "I am a plugin with a dash in my name"'</span> &gt; ./kubectl-foo_bar
</span></span><span><span>sudo chmod +x ./kubectl-foo_bar
</span></span><span><span>
</span></span><span><span><span># move the plugin into your $PATH</span>
</span></span><span><span>sudo mv ./kubectl-foo_bar /usr/local/bin
</span></span><span><span>
</span></span><span><span><span># You can now invoke your plugin via kubectl:</span>
</span></span><span><span>kubectl foo-bar
</span></span></code></pre></div><pre tabindex="0"><code>I am a plugin with a dash in my name
</code></pre><p>Note that the introduction of underscores to a plugin filename does not prevent you from having commands such as <code>kubectl foo_bar</code>.
The command from the above example, can be invoked using either a dash (<code>-</code>) or an underscore (<code>_</code>):</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># You can invoke your custom command with a dash</span>
</span></span><span><span>kubectl foo-bar
</span></span></code></pre></div><pre tabindex="0"><code>I am a plugin with a dash in my name
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># You can also invoke your custom command with an underscore</span>
</span></span><span><span>kubectl foo_bar
</span></span></code></pre></div><pre tabindex="0"><code>I am a plugin with a dash in my name
</code></pre><h4 id="name-conflicts-and-overshadowing">Name conflicts and overshadowing</h4><p>It is possible to have multiple plugins with the same filename in different locations throughout your <code>PATH</code>.
For example, given a <code>PATH</code> with the following value: <code>PATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins</code>, a copy of plugin <code>kubectl-foo</code> could exist in <code>/usr/local/bin/plugins</code> and <code>/usr/local/bin/moreplugins</code>,
such that the output of the <code>kubectl plugin list</code> command is:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span>PATH</span><span>=</span>/usr/local/bin/plugins:/usr/local/bin/moreplugins kubectl plugin list
</span></span></code></pre></div><pre tabindex="0"><code>The following kubectl-compatible plugins are available:

/usr/local/bin/plugins/kubectl-foo
/usr/local/bin/moreplugins/kubectl-foo
  - warning: /usr/local/bin/moreplugins/kubectl-foo is overshadowed by a similarly named plugin: /usr/local/bin/plugins/kubectl-foo

error: one plugin warning was found
</code></pre><p>In the above scenario, the warning under <code>/usr/local/bin/moreplugins/kubectl-foo</code> tells you that this plugin will never be executed. Instead, the executable that appears first in your <code>PATH</code>, <code>/usr/local/bin/plugins/kubectl-foo</code>, will always be found and executed first by the <code>kubectl</code> plugin mechanism.</p><p>A way to resolve this issue is to ensure that the location of the plugin that you wish to use with <code>kubectl</code> always comes first in your <code>PATH</code>. For example, if you want to always use <code>/usr/local/bin/moreplugins/kubectl-foo</code> anytime that the <code>kubectl</code> command <code>kubectl foo</code> was invoked, change the value of your <code>PATH</code> to be <code>/usr/local/bin/moreplugins:/usr/local/bin/plugins</code>.</p><h4 id="invocation-of-the-longest-executable-filename">Invocation of the longest executable filename</h4><p>There is another kind of overshadowing that can occur with plugin filenames. Given two plugins present in a user's <code>PATH</code>: <code>kubectl-foo-bar</code> and <code>kubectl-foo-bar-baz</code>, the <code>kubectl</code> plugin mechanism will always choose the longest possible plugin name for a given user command. Some examples below, clarify this further:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># for a given kubectl command, the plugin with the longest possible filename will always be preferred</span>
</span></span><span><span>kubectl foo bar baz
</span></span></code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar-baz is executed
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl foo bar
</span></span></code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar is executed
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl foo bar baz buz
</span></span></code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar-baz is executed, with "buz" as its first argument
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl foo bar buz
</span></span></code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar is executed, with "buz" as its first argument
</code></pre><p>This design choice ensures that plugin sub-commands can be implemented across multiple files, if needed, and that these sub-commands can be nested under a "parent" plugin command:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>ls ./plugin_command_tree
</span></span></code></pre></div><pre tabindex="0"><code>kubectl-parent
kubectl-parent-subcommand
kubectl-parent-subcommand-subsubcommand
</code></pre><h3 id="checking-for-plugin-warnings">Checking for plugin warnings</h3><p>You can use the aforementioned <code>kubectl plugin list</code> command to ensure that your plugin is visible by <code>kubectl</code>, and verify that there are no warnings preventing it from being called as a <code>kubectl</code> command.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl plugin list
</span></span></code></pre></div><pre tabindex="0"><code>The following kubectl-compatible plugins are available:

test/fixtures/pkg/kubectl/plugins/kubectl-foo
/usr/local/bin/kubectl-foo
  - warning: /usr/local/bin/kubectl-foo is overshadowed by a similarly named plugin: test/fixtures/pkg/kubectl/plugins/kubectl-foo
plugins/kubectl-invalid
  - warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executable

error: 2 plugin warnings were found
</code></pre><h3 id="using-the-command-line-runtime-package">Using the command line runtime package</h3><p>If you're writing a plugin for kubectl and you're using Go, you can make use
of the
<a href="https://github.com/kubernetes/cli-runtime">cli-runtime</a> utility libraries.</p><p>These libraries provide helpers for parsing or updating a user's
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig</a>
file, for making REST-style requests to the API server, or to bind flags
associated with configuration and printing.</p><p>See the <a href="https://github.com/kubernetes/sample-cli-plugin">Sample CLI Plugin</a> for
an example usage of the tools provided in the CLI Runtime repo.</p><h2 id="distributing-kubectl-plugins">Distributing kubectl plugins</h2><p>If you have developed a plugin for others to use, you should consider how you
package it, distribute it and deliver updates to your users.</p><h3 id="distributing-krew">Krew</h3><p><a href="https://krew.dev/">Krew</a> offers a cross-platform way to package and
distribute your plugins. This way, you use a single packaging format for all
target platforms (Linux, Windows, macOS etc) and deliver updates to your users.
Krew also maintains a <a href="https://krew.sigs.k8s.io/plugins/">plugin
index</a> so that other people can
discover your plugin and install it.</p><h3 id="distributing-native">Native / platform specific package management</h3><p>Alternatively, you can use traditional package managers such as, <code>apt</code> or <code>yum</code>
on Linux, Chocolatey on Windows, and Homebrew on macOS. Any package
manager will be suitable if it can place new executables placed somewhere
in the user's <code>PATH</code>.
As a plugin author, if you pick this option then you also have the burden
of updating your kubectl plugin's distribution package across multiple
platforms for each release.</p><h3 id="distributing-source-code">Source code</h3><p>You can publish the source code; for example, as a Git repository. If you
choose this option, someone who wants to use that plugin must fetch the code,
set up a build environment (if it needs compiling), and deploy the plugin.
If you also make compiled packages available, or use Krew, that will make
installs easier.</p><h2 id="what-s-next">What's next</h2><ul><li>Check the Sample CLI Plugin repository for a
<a href="https://github.com/kubernetes/sample-cli-plugin">detailed example</a> of a
plugin written in Go.
In case of any questions, feel free to reach out to the
<a href="https://github.com/kubernetes/community/tree/master/sig-cli">SIG CLI team</a>.</li><li>Read about <a href="https://krew.dev/">Krew</a>, a package manager for kubectl plugins.</li></ul></div></div><div><div class="td-content"><h1>Manage HugePages</h1><div class="lead">Configure and manage huge pages as a schedulable resource in a cluster.</div><div class="feature-state-notice feature-stable" title="Feature Gate: HugePages"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code> (enabled by default: true)</div><p>Kubernetes supports the allocation and consumption of pre-allocated huge pages
by applications in a Pod. This page describes how users can consume huge pages.</p><h2 id="before-you-begin">Before you begin</h2><p>Kubernetes nodes must
<a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html">pre-allocate huge pages</a>
in order for the node to report its huge page capacity.</p><p>A node can pre-allocate huge pages for multiple sizes, for instance,
the following line in <code>/etc/default/grub</code> allocates <code>2*1GiB</code> of 1 GiB
and <code>512*2 MiB</code> of 2 MiB pages:</p><pre tabindex="0"><code>GRUB_CMDLINE_LINUX="hugepagesz=1G hugepages=2 hugepagesz=2M hugepages=512"
</code></pre><p>The nodes will automatically discover and report all huge page resources as
schedulable resources.</p><p>When you describe the Node, you should see something similar to the following
in the following in the <code>Capacity</code> and <code>Allocatable</code> sections:</p><pre tabindex="0"><code>Capacity:
  cpu:                ...
  ephemeral-storage:  ...
  hugepages-1Gi:      2Gi
  hugepages-2Mi:      1Gi
  memory:             ...
  pods:               ...
Allocatable:
  cpu:                ...
  ephemeral-storage:  ...
  hugepages-1Gi:      2Gi
  hugepages-2Mi:      1Gi
  memory:             ...
  pods:               ...
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For dynamically allocated pages (after boot), the Kubelet needs to be restarted
for the new allocations to be refrelected.</div><h2 id="api">API</h2><p>Huge pages can be consumed via container level resource requirements using the
resource name <code>hugepages-&lt;size&gt;</code>, where <code>&lt;size&gt;</code> is the most compact binary
notation using integer values supported on a particular node. For example, if a
node supports 2048KiB and 1048576KiB page sizes, it will expose a schedulable
resources <code>hugepages-2Mi</code> and <code>hugepages-1Gi</code>. Unlike CPU or memory, huge pages
do not support overcommit. Note that when requesting hugepage resources, either
memory or CPU resources must be requested as well.</p><p>A pod may consume multiple huge page sizes in a single pod spec. In this case it
must use <code>medium: HugePages-&lt;hugepagesize&gt;</code> notation for all volume mounts.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>huge-pages-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>fedora:latest<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- sleep<span>
</span></span></span><span><span><span>    </span>- inf<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/hugepages-2Mi<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>hugepage-2mi<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/hugepages-1Gi<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>hugepage-1gi<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>hugepages-2Mi</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>        </span><span>hugepages-1Gi</span>:<span> </span>2Gi<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>hugepage-2mi<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span>
</span></span></span><span><span><span>      </span><span>medium</span>:<span> </span>HugePages-2Mi<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>hugepage-1gi<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span>
</span></span></span><span><span><span>      </span><span>medium</span>:<span> </span>HugePages-1Gi<span>
</span></span></span></code></pre></div><p>A pod may use <code>medium: HugePages</code> only if it requests huge pages of one size.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>huge-pages-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>fedora:latest<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- sleep<span>
</span></span></span><span><span><span>    </span>- inf<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/hugepages<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>hugepage<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>hugepages-2Mi</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>hugepage<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span>
</span></span></span><span><span><span>      </span><span>medium</span>:<span> </span>HugePages<span>
</span></span></span></code></pre></div><ul><li>Huge page requests must equal the limits. This is the default if limits are
specified, but requests are not.</li><li>Huge pages are isolated at a container scope, so each container has own
limit on their cgroup sandbox as requested in a container spec.</li><li>EmptyDir volumes backed by huge pages may not consume more huge page memory
than the pod request.</li><li>Applications that consume huge pages via <code>shmget()</code> with <code>SHM_HUGETLB</code> must
run with a supplemental group that matches <code>proc/sys/vm/hugetlb_shm_group</code>.</li><li>Huge page usage in a namespace is controllable via ResourceQuota similar
to other compute resources like <code>cpu</code> or <code>memory</code> using the <code>hugepages-&lt;size&gt;</code>
token.</li></ul></div></div><div><div class="td-content"><h1>Schedule GPUs</h1><div class="lead">Configure and schedule GPUs for use as a resource by nodes in a cluster.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Kubernetes includes <strong>stable</strong> support for managing AMD and NVIDIA GPUs
(graphical processing units) across different nodes in your cluster, using
<a class="glossary-tooltip" title="Software extensions to let Pods access devices that need vendor-specific initialization or setup" href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/" target="_blank">device plugins</a>.</p><p>This page describes how users can consume GPUs, and outlines
some of the limitations in the implementation.</p><h2 id="using-device-plugins">Using device plugins</h2><p>Kubernetes implements device plugins to let Pods access specialized hardware features such as GPUs.</p><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>As an administrator, you have to install GPU drivers from the corresponding
hardware vendor on the nodes and run the corresponding device plugin from the
GPU vendor. Here are some links to vendors' instructions:</p><ul><li><a href="https://github.com/ROCm/k8s-device-plugin#deployment">AMD</a></li><li><a href="https://intel.github.io/intel-device-plugins-for-kubernetes/cmd/gpu_plugin/README.html">Intel</a></li><li><a href="https://github.com/NVIDIA/k8s-device-plugin#quick-start">NVIDIA</a></li></ul><p>Once you have installed the plugin, your cluster exposes a custom schedulable resource such as <code>amd.com/gpu</code> or <code>nvidia.com/gpu</code>.</p><p>You can consume these GPUs from your containers by requesting
the custom GPU resource, the same way you request <code>cpu</code> or <code>memory</code>.
However, there are some limitations in how you specify the resource
requirements for custom devices.</p><p>GPUs are only supposed to be specified in the <code>limits</code> section, which means:</p><ul><li>You can specify GPU <code>limits</code> without specifying <code>requests</code>, because
Kubernetes will use the limit as the request value by default.</li><li>You can specify GPU in both <code>limits</code> and <code>requests</code> but these two values
must be equal.</li><li>You cannot specify GPU <code>requests</code> without specifying <code>limits</code>.</li></ul><p>Here's an example manifest for a Pod that requests a GPU:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-vector-add<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>example-vector-add<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span><span>"registry.example/example-vector-add:v42"</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>limits</span>:<span>
</span></span></span><span><span><span>          </span><span>gpu-vendor.example/example-gpu</span>:<span> </span><span>1</span><span> </span><span># requesting 1 GPU</span><span>
</span></span></span></code></pre></div><h2 id="manage-clusters-with-different-types-of-gpus">Manage clusters with different types of GPUs</h2><p>If different nodes in your cluster have different types of GPUs, then you
can use <a href="/docs/tasks/configure-pod-container/assign-pods-nodes/">Node Labels and Node Selectors</a>
to schedule pods to appropriate nodes.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Label your nodes with the accelerator type they have.</span>
</span></span><span><span>kubectl label nodes node1 <span>accelerator</span><span>=</span>example-gpu-x100
</span></span><span><span>kubectl label nodes node2 <span>accelerator</span><span>=</span>other-gpu-k915
</span></span></code></pre></div><p>That label key <code>accelerator</code> is just an example; you can use
a different label key if you prefer.</p><h2 id="node-labeller">Automatic node labelling</h2><p>As an administrator, you can automatically discover and label all your GPU enabled nodes
by deploying Kubernetes <a href="https://github.com/kubernetes-sigs/node-feature-discovery">Node Feature Discovery</a> (NFD).
NFD detects the hardware features that are available on each node in a Kubernetes cluster.
Typically, NFD is configured to advertise those features as node labels, but NFD can also add extended resources, annotations, and node taints.
NFD is compatible with all <a href="/releases/version-skew-policy/#supported-versions">supported versions</a> of Kubernetes.
By default NFD create the <a href="https://kubernetes-sigs.github.io/node-feature-discovery/master/usage/features.html">feature labels</a> for the detected features.
Administrators can leverage NFD to also taint nodes with specific features, so that only pods that request those features can be scheduled on those nodes.</p><p>You also need a plugin for NFD that adds appropriate labels to your nodes; these might be generic
labels or they could be vendor specific. Your GPU vendor may provide a third party
plugin for NFD; check their documentation for more details.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-vector-add<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span><span><span><span>  </span><span># You can use Kubernetes node affinity to schedule this Pod onto a node</span><span>
</span></span></span><span><span><span>  </span><span># that provides the kind of GPU that its container needs in order to work</span><span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>        </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>        </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span><span>"gpu.gpu-vendor.example/installed-memory"</span><span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>Gt<span> </span><span># (greater than)</span><span>
</span></span></span><span><span><span>            </span><span>values</span>:<span> </span>[<span>"40535"</span>]<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span><span>"feature.node.kubernetes.io/pci-10.present"</span><span> </span><span># NFD Feature label</span><span>
</span></span></span><span><span><span>            </span><span>values</span>:<span> </span>[<span>"true"</span>]<span> </span><span># (optional) only schedule on nodes with PCI device 10</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>example-vector-add<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span><span>"registry.example/example-vector-add:v42"</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>limits</span>:<span>
</span></span></span><span><span><span>          </span><span>gpu-vendor.example/example-gpu</span>:<span> </span><span>1</span><span> </span><span># requesting 1 GPU</span></span></span></code></pre></div><h4 id="gpu-vendor-implementations">GPU vendor implementations</h4><ul><li><a href="https://intel.github.io/intel-device-plugins-for-kubernetes/cmd/gpu_plugin/README.html">Intel</a></li><li><a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA</a></li></ul></div></div>