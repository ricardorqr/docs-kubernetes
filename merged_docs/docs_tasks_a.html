<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Using a KMS provider for data encryption</h1><p>This page shows how to configure a Key Management Service (KMS) provider and plugin to enable secret data encryption.
In Kubernetes 1.34 there are two versions of KMS at-rest encryption.
You should use KMS v2 if feasible because KMS v1 is deprecated (since Kubernetes v1.28) and disabled by default (since Kubernetes v1.29).
KMS v2 offers significantly better performance characteristics than KMS v1.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>This documentation is for the generally available implementation of KMS v2 (and for the
deprecated version 1 implementation).
If you are using any control plane components older than Kubernetes v1.29, please check
the equivalent page in the documentation for the version of Kubernetes that your cluster
is running. Earlier releases of Kubernetes had different behavior that may be relevant
for information security.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>The version of Kubernetes that you need depends on which KMS API version
you have selected. Kubernetes recommends using KMS v2.</p><ul><li>If you selected KMS API v1 to support clusters prior to version v1.27
or if you have a legacy KMS plugin that only supports KMS v1,
any supported Kubernetes version will work. This API is deprecated as of Kubernetes v1.28.
Kubernetes does not recommend the use of this API.</li></ul><p>To check the version, enter <code>kubectl version</code>.</p><h3 id="kms-v1">KMS v1</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [deprecated]</code></div><ul><li><p>Kubernetes version 1.10.0 or later is required</p></li><li><p>For version 1.29 and later, the v1 implementation of KMS is disabled by default.
To enable the feature, set <code>--feature-gates=KMSv1=true</code> to configure a KMS v1 provider.</p></li><li><p>Your cluster must use etcd v3 or later</p></li></ul><h3 id="kms-v2">KMS v2</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div><ul><li>Your cluster must use etcd v3 or later</li></ul><h2 id="kms-encryption-and-per-object-encryption-keys">KMS encryption and per-object encryption keys</h2><p>The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd.
The data is encrypted using a data encryption key (DEK).
The DEKs are encrypted with a key encryption key (KEK) that is stored and managed in a remote KMS.</p><p>If you use the (deprecated) v1 implementation of KMS, a new DEK is generated for each encryption.</p><p>With KMS v2, a new DEK is generated <strong>per encryption</strong>: the API server uses a
<em>key derivation function</em> to generate single use data encryption keys from a secret seed
combined with some random data.
The seed is rotated whenever the KEK is rotated
(see the <em>Understanding key_id and Key Rotation</em> section below for more details).</p><p>The KMS provider uses gRPC to communicate with a specific KMS plugin over a UNIX domain socket.
The KMS plugin, which is implemented as a gRPC server and deployed on the same host(s)
as the Kubernetes control plane, is responsible for all communication with the remote KMS.</p><h2 id="configuring-the-kms-provider">Configuring the KMS provider</h2><p>To configure a KMS provider on the API server, include a provider of type <code>kms</code> in the
<code>providers</code> array in the encryption configuration file and set the following properties:</p><h3 id="configuring-the-kms-provider-kms-v1">KMS v1</h3><ul><li><code>apiVersion</code>: API Version for KMS provider. Leave this value empty or set it to <code>v1</code>.</li><li><code>name</code>: Display name of the KMS plugin. Cannot be changed once set.</li><li><code>endpoint</code>: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.</li><li><code>cachesize</code>: Number of data encryption keys (DEKs) to be cached in the clear.
When cached, DEKs can be used without another call to the KMS;
whereas DEKs that are not cached require a call to the KMS to unwrap.</li><li><code>timeout</code>: How long should <code>kube-apiserver</code> wait for kms-plugin to respond before
returning an error (default is 3 seconds).</li></ul><h3 id="configuring-the-kms-provider-kms-v2">KMS v2</h3><ul><li><code>apiVersion</code>: API Version for KMS provider. Set this to <code>v2</code>.</li><li><code>name</code>: Display name of the KMS plugin. Cannot be changed once set.</li><li><code>endpoint</code>: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.</li><li><code>timeout</code>: How long should <code>kube-apiserver</code> wait for kms-plugin to respond before
returning an error (default is 3 seconds).</li></ul><p>KMS v2 does not support the <code>cachesize</code> property. All data encryption keys (DEKs) will be cached in
the clear once the server has unwrapped them via a call to the KMS. Once cached, DEKs can be used
to perform decryption indefinitely without making a call to the KMS.</p><p>See <a href="/docs/tasks/administer-cluster/encrypt-data/">Understanding the encryption at rest configuration</a>.</p><h2 id="implementing-a-kms-plugin">Implementing a KMS plugin</h2><p>To implement a KMS plugin, you can develop a new plugin gRPC server or enable a KMS plugin
already provided by your cloud provider.
You then integrate the plugin with the remote KMS and deploy it on the Kubernetes control plane.</p><h3 id="enabling-the-kms-supported-by-your-cloud-provider">Enabling the KMS supported by your cloud provider</h3><p>Refer to your cloud provider for instructions on enabling the cloud provider-specific KMS plugin.</p><h3 id="developing-a-kms-plugin-grpc-server">Developing a KMS plugin gRPC server</h3><p>You can develop a KMS plugin gRPC server using a stub file available for Go. For other languages,
you use a proto file to create a stub file that you can use to develop the gRPC server code.</p><h4 id="developing-a-kms-plugin-gRPC-server-kms-v1">KMS v1</h4><ul><li><p>Using Go: Use the functions and data structures in the stub file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v1beta1/api.pb.go">api.pb.go</a>
to develop the gRPC server code</p></li><li><p>Using languages other than Go: Use the protoc compiler with the proto file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v1beta1/api.proto">api.proto</a>
to generate a stub file for the specific language</p></li></ul><h4 id="developing-a-kms-plugin-gRPC-server-kms-v2">KMS v2</h4><ul><li><p>Using Go: A high level
<a href="https://github.com/kubernetes/kms/blob/release-1.34/pkg/service/interface.go">library</a>
is provided to make the process easier. Low level implementations
can use the functions and data structures in the stub file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v2/api.pb.go">api.pb.go</a>
to develop the gRPC server code</p></li><li><p>Using languages other than Go: Use the protoc compiler with the proto file:
<a href="https://github.com/kubernetes/kms/blob/release-1.34/apis/v2/api.proto">api.proto</a>
to generate a stub file for the specific language</p></li></ul><p>Then use the functions and data structures in the stub file to develop the server code.</p><h4 id="notes">Notes</h4><h5 id="developing-a-kms-plugin-gRPC-server-notes-kms-v1">KMS v1</h5><ul><li><p>kms plugin version: <code>v1beta1</code></p><p>In response to procedure call Version, a compatible KMS plugin should return <code>v1beta1</code> as <code>VersionResponse.version</code>.</p></li><li><p>message version: <code>v1beta1</code></p><p>All messages from KMS provider have the version field set to <code>v1beta1</code>.</p></li><li><p>protocol: UNIX domain socket (<code>unix</code>)</p><p>The plugin is implemented as a gRPC server that listens at UNIX domain socket. The plugin deployment should create a file on the file system to run the gRPC unix domain socket connection. The API server (gRPC client) is configured with the KMS provider (gRPC server) unix domain socket endpoint in order to communicate with it. An abstract Linux socket may be used by starting the endpoint with <code>/@</code>, i.e. <code>unix:///@foo</code>. Care must be taken when using this type of socket as they do not have concept of ACL (unlike traditional file based sockets). However, they are subject to Linux networking namespace, so will only be accessible to containers within the same pod unless host networking is used.</p></li></ul><h5 id="developing-a-kms-plugin-gRPC-server-notes-kms-v2">KMS v2</h5><ul><li><p>KMS plugin version: <code>v2</code></p><p>In response to the <code>Status</code> remote procedure call, a compatible KMS plugin should return its KMS compatibility
version as <code>StatusResponse.version</code>. That status response should also include
"ok" as <code>StatusResponse.healthz</code> and a <code>key_id</code> (remote KMS KEK ID) as <code>StatusResponse.key_id</code>.
The Kubernetes project recommends you make your plugin
compatible with the stable <code>v2</code> KMS API. Kubernetes 1.34 also supports the
<code>v2beta1</code> API for KMS; future Kubernetes releases are likely to continue supporting that beta version.</p><p>The API server polls the <code>Status</code> procedure call approximately every minute when everything is healthy,
and every 10 seconds when the plugin is not healthy. Plugins must take care to optimize this call as it will be
under constant load.</p></li><li><p>Encryption</p><p>The <code>EncryptRequest</code> procedure call provides the plaintext and a UID for logging purposes. The response must include
the ciphertext, the <code>key_id</code> for the KEK used, and, optionally, any metadata that the KMS plugin needs to aid in
future <code>DecryptRequest</code> calls (via the <code>annotations</code> field). The plugin must guarantee that any distinct plaintext
results in a distinct response <code>(ciphertext, key_id, annotations)</code>.</p><p>If the plugin returns a non-empty <code>annotations</code> map, all map keys must be fully qualified domain names such as
<code>example.com</code>. An example use case of <code>annotation</code> is <code>{"kms.example.io/remote-kms-auditid":"&lt;audit ID used by the remote KMS&gt;"}</code></p><p>The API server does not perform the <code>EncryptRequest</code> procedure call at a high rate. Plugin implementations should
still aim to keep each request's latency at under 100 milliseconds.</p></li><li><p>Decryption</p><p>The <code>DecryptRequest</code> procedure call provides the <code>(ciphertext, key_id, annotations)</code> from <code>EncryptRequest</code> and a UID
for logging purposes. As expected, it is the inverse of the <code>EncryptRequest</code> call. Plugins must verify that the
<code>key_id</code> is one that they understand - they must not attempt to decrypt data unless they are sure that it was
encrypted by them at an earlier time.</p><p>The API server may perform thousands of <code>DecryptRequest</code> procedure calls on startup to fill its watch cache. Thus
plugin implementations must perform these calls as quickly as possible, and should aim to keep each request's latency
at under 10 milliseconds.</p></li><li><p>Understanding <code>key_id</code> and Key Rotation</p><p>The <code>key_id</code> is the public, non-secret name of the remote KMS KEK that is currently in use. It may be logged
during regular operation of the API server, and thus must not contain any private data. Plugin implementations
are encouraged to use a hash to avoid leaking any data. The KMS v2 metrics take care to hash this value before
exposing it via the <code>/metrics</code> endpoint.</p><p>The API server considers the <code>key_id</code> returned from the <code>Status</code> procedure call to be authoritative. Thus, a change
to this value signals to the API server that the remote KEK has changed, and data encrypted with the old KEK should
be marked stale when a no-op write is performed (as described below). If an <code>EncryptRequest</code> procedure call returns a
<code>key_id</code> that is different from <code>Status</code>, the response is thrown away and the plugin is considered unhealthy. Thus
implementations must guarantee that the <code>key_id</code> returned from <code>Status</code> will be the same as the one returned by
<code>EncryptRequest</code>. Furthermore, plugins must ensure that the <code>key_id</code> is stable and does not flip-flop between values
(i.e. during a remote KEK rotation).</p><p>Plugins must not re-use <code>key_id</code>s, even in situations where a previously used remote KEK has been reinstated. For
example, if a plugin was using <code>key_id=A</code>, switched to <code>key_id=B</code>, and then went back to <code>key_id=A</code> - instead of
reporting <code>key_id=A</code> the plugin should report some derivative value such as <code>key_id=A_001</code> or use a new value such
as <code>key_id=C</code>.</p><p>Since the API server polls <code>Status</code> about every minute, <code>key_id</code> rotation is not immediate. Furthermore, the API
server will coast on the last valid state for about three minutes. Thus if a user wants to take a passive approach
to storage migration (i.e. by waiting), they must schedule a migration to occur at <code>3 + N + M</code> minutes after the
remote KEK has been rotated (<code>N</code> is how long it takes the plugin to observe the <code>key_id</code> change and <code>M</code> is the
desired buffer to allow config changes to be processed - a minimum <code>M</code> of five minutes is recommend). Note that no
API server restart is required to perform KEK rotation.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Because you don't control the number of writes performed with the DEK,
the Kubernetes project recommends rotating the KEK at least every 90 days.</div></li><li><p>protocol: UNIX domain socket (<code>unix</code>)</p><p>The plugin is implemented as a gRPC server that listens at UNIX domain socket.
The plugin deployment should create a file on the file system to run the gRPC unix domain socket connection.
The API server (gRPC client) is configured with the KMS provider (gRPC server) unix
domain socket endpoint in order to communicate with it.
An abstract Linux socket may be used by starting the endpoint with <code>/@</code>, i.e. <code>unix:///@foo</code>.
Care must be taken when using this type of socket as they do not have concept of ACL
(unlike traditional file based sockets).
However, they are subject to Linux networking namespace, so will only be accessible to
containers within the same pod unless host networking is used.</p></li></ul><h3 id="integrating-a-kms-plugin-with-the-remote-kms">Integrating a KMS plugin with the remote KMS</h3><p>The KMS plugin can communicate with the remote KMS using any protocol supported by the KMS.
All configuration data, including authentication credentials the KMS plugin uses to communicate with the remote KMS,
are stored and managed by the KMS plugin independently.
The KMS plugin can encode the ciphertext with additional metadata that may be required before sending it to the KMS
for decryption (KMS v2 makes this process easier by providing a dedicated <code>annotations</code> field).</p><h3 id="deploying-the-kms-plugin">Deploying the KMS plugin</h3><p>Ensure that the KMS plugin runs on the same host(s) as the Kubernetes API server(s).</p><h2 id="encrypting-your-data-with-the-kms-provider">Encrypting your data with the KMS provider</h2><p>To encrypt the data:</p><ol><li><p>Create a new <code>EncryptionConfiguration</code> file using the appropriate properties for the <code>kms</code> provider
to encrypt resources like Secrets and ConfigMaps. If you want to encrypt an extension API that is
defined in a CustomResourceDefinition, your cluster must be running Kubernetes v1.26 or newer.</p></li><li><p>Set the <code>--encryption-provider-config</code> flag on the kube-apiserver to point to the location of the configuration file.</p></li><li><p><code>--encryption-provider-config-automatic-reload</code> boolean argument
determines if the file set by <code>--encryption-provider-config</code> should be
<a href="/docs/tasks/administer-cluster/encrypt-data/#configure-automatic-reloading">automatically reloaded</a>
if the disk contents change.</p></li><li><p>Restart your API server.</p></li></ol><h3 id="encrypting-your-data-with-the-kms-provider-kms-v1">KMS v1</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- configmaps<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- pandas.awesome.bears.example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">kms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myKmsPluginFoo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile-foo.sock<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">timeout</span>:<span style="color:#bbb"> </span>3s<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">kms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myKmsPluginBar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile-bar.sock<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">timeout</span>:<span style="color:#bbb"> </span>3s<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="encrypting-your-data-with-the-kms-provider-kms-v2">KMS v2</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- configmaps<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- pandas.awesome.bears.example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">kms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myKmsPluginFoo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile-foo.sock<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">timeout</span>:<span style="color:#bbb"> </span>3s<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">kms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myKmsPluginBar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile-bar.sock<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">timeout</span>:<span style="color:#bbb"> </span>3s<span style="color:#bbb">
</span></span></span></code></pre></div><p>Setting <code>--encryption-provider-config-automatic-reload</code> to <code>true</code> collapses all health checks to a single health check endpoint. Individual health checks are only available when KMS v1 providers are in use and the encryption config is not auto-reloaded.</p><p>The following table summarizes the health check endpoints for each KMS version:</p><table><thead><tr><th>KMS configurations</th><th>Without Automatic Reload</th><th>With Automatic Reload</th></tr></thead><tbody><tr><td>KMS v1 only</td><td>Individual Healthchecks</td><td>Single Healthcheck</td></tr><tr><td>KMS v2 only</td><td>Single Healthcheck</td><td>Single Healthcheck</td></tr><tr><td>Both KMS v1 and v2</td><td>Individual Healthchecks</td><td>Single Healthcheck</td></tr><tr><td>No KMS</td><td>None</td><td>Single Healthcheck</td></tr></tbody></table><p><code>Single Healthcheck</code> means that the only health check endpoint is <code>/healthz/kms-providers</code>.</p><p><code>Individual Healthchecks</code> means that each KMS plugin has an associated health check endpoint based on its location in the encryption config: <code>/healthz/kms-provider-0</code>, <code>/healthz/kms-provider-1</code> etc.</p><p>These healthcheck endpoint paths are hard coded and generated/controlled by the server. The indices for individual healthchecks corresponds to the order in which the KMS encryption config is processed.</p><p>Until the steps defined in <a href="#ensuring-all-secrets-are-encrypted">Ensuring all secrets are encrypted</a> are performed, the <code>providers</code> list should end with the <code>identity: {}</code> provider to allow unencrypted data to be read. Once all resources are encrypted, the <code>identity</code> provider should be removed to prevent the API server from honoring unencrypted data.</p><p>For details about the <code>EncryptionConfiguration</code> format, please check the
<a href="/docs/reference/config-api/apiserver-config.v1/">API server encryption API reference</a>.</p><h2 id="verifying-that-the-data-is-encrypted">Verifying that the data is encrypted</h2><p>When encryption at rest is correctly configured, resources are encrypted on write.
After restarting your <code>kube-apiserver</code>, any newly created or updated Secret or other resource types
configured in <code>EncryptionConfiguration</code> should be encrypted when stored. To verify,
you can use the <code>etcdctl</code> command line program to retrieve the contents of your secret data.</p><ol><li><p>Create a new secret called <code>secret1</code> in the <code>default</code> namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret generic secret1 -n default --from-literal<span style="color:#666">=</span><span style="color:#b8860b">mykey</span><span style="color:#666">=</span>mydata
</span></span></code></pre></div></li><li><p>Using the <code>etcdctl</code> command line, read that secret out of etcd:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl get /kubernetes.io/secrets/default/secret1 <span style="color:#666">[</span>...<span style="color:#666">]</span> | hexdump -C
</span></span></code></pre></div><p>where <code>[...]</code> contains the additional arguments for connecting to the etcd server.</p></li><li><p>Verify the stored secret is prefixed with <code>k8s:enc:kms:v1:</code> for KMS v1 or prefixed with <code>k8s:enc:kms:v2:</code> for KMS v2, which indicates that the <code>kms</code> provider has encrypted the resulting data.</p></li><li><p>Verify that the secret is correctly decrypted when retrieved via the API:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe secret secret1 -n default
</span></span></code></pre></div><p>The Secret should contain <code>mykey: mydata</code></p></li></ol><h2 id="ensuring-all-secrets-are-encrypted">Ensuring all secrets are encrypted</h2><p>When encryption at rest is correctly configured, resources are encrypted on write.
Thus we can perform an in-place no-op update to ensure that data is encrypted.</p><p>The following command reads all secrets and then updates them to apply server side encryption.
If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><h2 id="switching-from-a-local-encryption-provider-to-the-kms-provider">Switching from a local encryption provider to the KMS provider</h2><p>To switch from a local encryption provider to the <code>kms</code> provider and re-encrypt all of the secrets:</p><ol><li><p>Add the <code>kms</code> provider as the first entry in the configuration file as shown in the following example.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">kms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name </span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></span></span></code></pre></div></li><li><p>Restart all <code>kube-apiserver</code> processes.</p></li><li><p>Run the following command to force all secrets to be re-encrypted using the <code>kms</code> provider.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><a id="disabling-encryption-at-rest"><p>If you no longer want to use encryption for data persisted in the Kubernetes API, read
<a href="/docs/tasks/administer-cluster/decrypt-data/">decrypt data that are already stored at rest</a>.</p></a></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Change the Reclaim Policy of a PersistentVolume</h1><p>This page shows how to change the reclaim policy of a Kubernetes
PersistentVolume.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="why-change-reclaim-policy-of-a-persistentvolume">Why change reclaim policy of a PersistentVolume</h2><p>PersistentVolumes can have various reclaim policies, including "Retain",
"Recycle", and "Delete". For dynamically provisioned PersistentVolumes,
the default reclaim policy is "Delete". This means that a dynamically provisioned
volume is automatically deleted when a user deletes the corresponding
PersistentVolumeClaim. This automatic behavior might be inappropriate if the volume
contains precious data. In that case, it is more appropriate to use the "Retain"
policy. With the "Retain" policy, if a user deletes a PersistentVolumeClaim,
the corresponding PersistentVolume will not be deleted. Instead, it is moved to the
Released phase, where all of its data can be manually recovered.</p><h2 id="changing-the-reclaim-policy-of-a-persistentvolume">Changing the reclaim policy of a PersistentVolume</h2><ol><li><p>List the PersistentVolumes in your cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pv
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM             STORAGECLASS     REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1    manual                     10s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2    manual                     6s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim3    manual                     3s
</code></pre><p>This list also includes the name of the claims that are bound to each volume
for easier identification of dynamically provisioned volumes.</p></li><li><p>Choose one of your PersistentVolumes and change its reclaim policy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch pv &lt;your-pv-name&gt; -p <span style="color:#b44">'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'</span>
</span></span></code></pre></div><p>where <code>&lt;your-pv-name&gt;</code> is the name of your chosen PersistentVolume.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>On Windows, you must <em>double</em> quote any JSONPath template that contains spaces (not single
quote as shown above for bash). This in turn means that you must use a single quote or escaped
double quote around any literals in the template. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cmd" data-lang="cmd"><span style="display:flex"><span>kubectl patch pv &lt;your-pv-name&gt; -p <span style="color:#b44">"{\"</span>spec\<span style="color:#b44">":{\"</span>persistentVolumeReclaimPolicy\<span style="color:#b44">":\"</span>Retain\<span style="color:#b44">"}}"</span>
</span></span></code></pre></div></div></li><li><p>Verify that your chosen PersistentVolume has the right policy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pv
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM             STORAGECLASS     REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1    manual                     40s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2    manual                     36s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Retain          Bound     default/claim3    manual                     33s
</code></pre><p>In the preceding output, you can see that the volume bound to claim
<code>default/claim3</code> has reclaim policy <code>Retain</code>. It will not be automatically
deleted when a user deletes claim <code>default/claim3</code>.</p></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a>.</li></ul><h3 id="reference">References</h3><ul><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/">PersistentVolume</a><ul><li>Pay attention to the <code>.spec.persistentVolumeReclaimPolicy</code>
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec">field</a>
of PersistentVolume.</li></ul></li><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/">PersistentVolumeClaim</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Minimum and Maximum CPU Constraints for a Namespace</h1><div class="lead">Define a range of valid CPU resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</div><p>This page shows how to set minimum and maximum values for the CPU resources used by containers
and Pods in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>. You specify minimum
and maximum CPU values in a
<a href="/docs/reference/kubernetes-api/policy-resources/limit-range-v1/">LimitRange</a>
object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created
in the namespace.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 1.0 CPU available for Pods.
See <a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>
to learn what Kubernetes means by “1 CPU”.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace constraints-cpu-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's a manifest for an example <a class="glossary-tooltip" title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." data-toggle="tooltip" data-placement="top" href="/docs/concepts/policy/limit-range/" target="_blank" aria-label="LimitRange">LimitRange</a>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints.yaml" download="admin/resource/cpu-constraints.yaml"><code>admin/resource/cpu-constraints.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-constraints-yaml&quot;)" title="Copy admin/resource/cpu-constraints.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-constraints-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu-min-max-demo-lr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">max</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">min</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the LimitRange:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>View detailed information about the LimitRange:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get limitrange cpu-min-max-demo-lr --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows the minimum and maximum CPU constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">default</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">defaultRequest</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">max</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">min</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>200m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></span></span></code></pre></div><p>Now whenever you create a Pod in the constraints-cpu-example namespace (or some other client
of the Kubernetes API creates an equivalent Pod), Kubernetes performs these steps:</p><ul><li><p>If any container in that Pod does not specify its own CPU request and limit, the control plane
assigns the default CPU request and limit to that container.</p></li><li><p>Verify that every container in that Pod specifies a CPU request that is greater than or equal to 200 millicpu.</p></li><li><p>Verify that every container in that Pod specifies a CPU limit that is less than or equal to 800 millicpu.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When creating a <code>LimitRange</code> object, you can specify limits on huge-pages
or GPUs as well. However, when both <code>default</code> and <code>defaultRequest</code> are specified
on these resources, the two values must be the same.</div><p>Here's a manifest for a Pod that has one container. The container manifest
specifies a CPU request of 500 millicpu and a CPU limit of 800 millicpu. These satisfy the
minimum and maximum CPU constraints imposed by the LimitRange for this namespace.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod.yaml" download="admin/resource/cpu-constraints-pod.yaml"><code>admin/resource/cpu-constraints-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-constraints-pod-yaml&quot;)" title="Copy admin/resource/cpu-constraints-pod.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-constraints-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"500m"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>Verify that the Pod is running and that its container is healthy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod constraints-cpu-demo --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod constraints-cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod's only container has a CPU request of 500 millicpu and CPU limit
of 800 millicpu. These satisfy the constraints imposed by the LimitRange.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="delete-the-pod">Delete the Pod</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete pod constraints-cpu-demo --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><h2 id="attempt-to-create-a-pod-that-exceeds-the-maximum-cpu-constraint">Attempt to create a Pod that exceeds the maximum CPU constraint</h2><p>Here's a manifest for a Pod that has one container. The container specifies a
CPU request of 500 millicpu and a cpu limit of 1.5 cpu.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod-2.yaml" download="admin/resource/cpu-constraints-pod-2.yaml"><code>admin/resource/cpu-constraints-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-constraints-pod-2-yaml&quot;)" title="Copy admin/resource/cpu-constraints-pod-2.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-constraints-pod-2-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-2-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1.5"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"500m"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines an unacceptable container.
That container is not acceptable because it specifies a CPU limit that is too large:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-2.yaml":
pods "constraints-cpu-demo-2" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m.
</code></pre><h2 id="attempt-to-create-a-pod-that-does-not-meet-the-minimum-cpu-request">Attempt to create a Pod that does not meet the minimum CPU request</h2><p>Here's a manifest for a Pod that has one container. The container specifies a
CPU request of 100 millicpu and a CPU limit of 800 millicpu.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod-3.yaml" download="admin/resource/cpu-constraints-pod-3.yaml"><code>admin/resource/cpu-constraints-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-constraints-pod-3-yaml&quot;)" title="Copy admin/resource/cpu-constraints-pod-3.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-constraints-pod-3-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-3-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100m"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines an unacceptable container.
That container is not acceptable because it specifies a CPU request that is lower than the
enforced minimum:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-3.yaml":
pods "constraints-cpu-demo-3" is forbidden: minimum cpu usage per Container is 200m, but request is 100m.
</code></pre><h2 id="create-a-pod-that-does-not-specify-any-cpu-request-or-limit">Create a Pod that does not specify any CPU request or limit</h2><p>Here's a manifest for a Pod that has one container. The container does not
specify a CPU request, nor does it specify a CPU limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-constraints-pod-4.yaml" download="admin/resource/cpu-constraints-pod-4.yaml"><code>admin/resource/cpu-constraints-pod-4.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-constraints-pod-4-yaml&quot;)" title="Copy admin/resource/cpu-constraints-pod-4.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-constraints-pod-4-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-4-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><pre tabindex="0"><code>kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml
</code></pre><p>The output shows that the Pod's single container has a CPU request of 800 millicpu and a
CPU limit of 800 millicpu.
How did that container get those values?</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></span></span></code></pre></div><p>Because that container did not specify its own CPU request and limit, the control plane
applied the
<a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">default CPU request and limit</a>
from the LimitRange for this namespace.</p><p>At this point, your Pod may or may not be running. Recall that a prerequisite for
this task is that your Nodes must have at least 1 CPU available for use. If each of your Nodes has only 1 CPU,
then there might not be enough allocatable CPU on any Node to accommodate a request of 800 millicpu.
If you happen to be using Nodes with 2 CPU, then you probably have enough CPU to accommodate the 800 millicpu request.</p><p>Delete your Pod:</p><pre tabindex="0"><code>kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example
</code></pre><h2 id="enforcement-of-minimum-and-maximum-cpu-constraints">Enforcement of minimum and maximum CPU constraints</h2><p>The maximum and minimum CPU constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.</p><h2 id="motivation-for-minimum-and-maximum-cpu-constraints">Motivation for minimum and maximum CPU constraints</h2><p>As a cluster administrator, you might want to impose restrictions on the CPU resources that Pods can use.
For example:</p><ul><li><p>Each Node in a cluster has 2 CPU. You do not want to accept any Pod that requests
more than 2 CPU, because no Node in the cluster can support the request.</p></li><li><p>A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 3 CPU, but you want development workloads to be limited
to 1 CPU. You create separate namespaces for production and development, and you apply CPU constraints to
each namespace.</p></li></ul><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace constraints-cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Access to Multiple Clusters</h1><p>This page shows how to configure access to multiple clusters by using
configuration files. After your clusters, users, and contexts are defined in
one or more configuration files, you can quickly switch between clusters by using the
<code>kubectl config use-context</code> command.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A file that is used to configure access to a cluster is sometimes called
a <em>kubeconfig file</em>. This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.</div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig
file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.</div><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check that <a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/kubectl/" target="_blank" aria-label="kubectl">kubectl</a> is installed,
run <code>kubectl version --client</code>. The kubectl version should be
<a href="/releases/version-skew-policy/#kubectl">within one minor version</a> of your
cluster's API server.</p><h2 id="define-clusters-users-and-contexts">Define clusters, users, and contexts</h2><p>Suppose you have two clusters, one for development work and one for test work.
In the <code>development</code> cluster, your frontend developers work in a namespace called <code>frontend</code>,
and your storage developers work in a namespace called <code>storage</code>. In your <code>test</code> cluster,
developers work in the default namespace, or they create auxiliary namespaces as they
see fit. Access to the development cluster requires authentication by certificate. Access
to the test cluster requires authentication by username and password.</p><p>Create a directory named <code>config-exercise</code>. In your
<code>config-exercise</code> directory, create a file named <code>config-demo</code> with this content:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">clusters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">users</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">contexts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>exp-test<span style="color:#bbb">
</span></span></span></code></pre></div><p>A configuration file describes clusters, users, and contexts. Your <code>config-demo</code> file
has the framework to describe two clusters, two users, and three contexts.</p><p>Go to your <code>config-exercise</code> directory. Enter these commands to add cluster details to
your configuration file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-cluster development --server<span style="color:#666">=</span>https://1.2.3.4 --certificate-authority<span style="color:#666">=</span>fake-ca-file
</span></span><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-cluster <span style="color:#a2f">test</span> --server<span style="color:#666">=</span>https://5.6.7.8 --insecure-skip-tls-verify
</span></span></code></pre></div><p>Add user details to your configuration file:</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Storing passwords in Kubernetes client config is risky. A better alternative would be to use a credential plugin and store them separately. See: <a href="/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">client-go credential plugins</a></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-credentials developer --client-certificate<span style="color:#666">=</span>fake-cert-file --client-key<span style="color:#666">=</span>fake-key-seefile
</span></span><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-credentials experimenter --username<span style="color:#666">=</span>exp --password<span style="color:#666">=</span>some-password
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li>To delete a user you can run <code>kubectl --kubeconfig=config-demo config unset users.&lt;name&gt;</code></li><li>To remove a cluster, you can run <code>kubectl --kubeconfig=config-demo config unset clusters.&lt;name&gt;</code></li><li>To remove a context, you can run <code>kubectl --kubeconfig=config-demo config unset contexts.&lt;name&gt;</code></li></ul></div><p>Add context details to your configuration file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-context dev-frontend --cluster<span style="color:#666">=</span>development --namespace<span style="color:#666">=</span>frontend --user<span style="color:#666">=</span>developer
</span></span><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-context dev-storage --cluster<span style="color:#666">=</span>development --namespace<span style="color:#666">=</span>storage --user<span style="color:#666">=</span>developer
</span></span><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-context exp-test --cluster<span style="color:#666">=</span><span style="color:#a2f">test</span> --namespace<span style="color:#666">=</span>default --user<span style="color:#666">=</span>experimenter
</span></span></code></pre></div><p>Open your <code>config-demo</code> file to see the added details. As an alternative to opening the
<code>config-demo</code> file, you can use the <code>config view</code> command.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view
</span></span></code></pre></div><p>The output shows the two clusters, two users, and three contexts:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">clusters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">certificate-authority</span>:<span style="color:#bbb"> </span>fake-ca-file<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span>https://1.2.3.4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">insecure-skip-tls-verify</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span>https://5.6.7.8<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">contexts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>exp-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">current-context</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">users</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">client-certificate</span>:<span style="color:#bbb"> </span>fake-cert-file<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">client-key</span>:<span style="color:#bbb"> </span>fake-key-file<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Documentation note (this comment is NOT part of the command output).</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Storing passwords in Kubernetes client config is risky.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># A better alternative would be to use a credential plugin</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># and store the credentials separately.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">password</span>:<span style="color:#bbb"> </span>some-password<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">username</span>:<span style="color:#bbb"> </span>exp<span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>fake-ca-file</code>, <code>fake-cert-file</code> and <code>fake-key-file</code> above are the placeholders
for the pathnames of the certificate files. You need to change these to the actual pathnames
of certificate files in your environment.</p><p>Sometimes you may want to use Base64-encoded data embedded here instead of separate
certificate files; in that case you need to add the suffix <code>-data</code> to the keys, for example,
<code>certificate-authority-data</code>, <code>client-certificate-data</code>, <code>client-key-data</code>.</p><p>Each context is a triple (cluster, user, namespace). For example, the
<code>dev-frontend</code> context says, "Use the credentials of the <code>developer</code>
user to access the <code>frontend</code> namespace of the <code>development</code> cluster".</p><p>Set the current context:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo use-context dev-frontend
</span></span></code></pre></div><p>Now whenever you enter a <code>kubectl</code> command, the action will apply to the cluster,
and namespace listed in the <code>dev-frontend</code> context. And the command will use
the credentials of the user listed in the <code>dev-frontend</code> context.</p><p>To see only the configuration information associated with
the current context, use the <code>--minify</code> flag.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view --minify
</span></span></code></pre></div><p>The output shows configuration information associated with the <code>dev-frontend</code> context:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">clusters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">certificate-authority</span>:<span style="color:#bbb"> </span>fake-ca-file<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span>https://1.2.3.4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">contexts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">current-context</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">users</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">client-certificate</span>:<span style="color:#bbb"> </span>fake-cert-file<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">client-key</span>:<span style="color:#bbb"> </span>fake-key-file<span style="color:#bbb">
</span></span></span></code></pre></div><p>Now suppose you want to work for a while in the test cluster.</p><p>Change the current context to <code>exp-test</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo use-context exp-test
</span></span></code></pre></div><p>Now any <code>kubectl</code> command you give will apply to the default namespace of
the <code>test</code> cluster. And the command will use the credentials of the user
listed in the <code>exp-test</code> context.</p><p>View configuration associated with the new current context, <code>exp-test</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view --minify
</span></span></code></pre></div><p>Finally, suppose you want to work for a while in the <code>storage</code> namespace of the
<code>development</code> cluster.</p><p>Change the current context to <code>dev-storage</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo use-context dev-storage
</span></span></code></pre></div><p>View configuration associated with the new current context, <code>dev-storage</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view --minify
</span></span></code></pre></div><h2 id="create-a-second-configuration-file">Create a second configuration file</h2><p>In your <code>config-exercise</code> directory, create a file named <code>config-demo-2</code> with this content:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">contexts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>ramp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-ramp-up<span style="color:#bbb">
</span></span></span></code></pre></div><p>The preceding configuration file defines a new context named <code>dev-ramp-up</code>.</p><h2 id="set-the-kubeconfig-environment-variable">Set the KUBECONFIG environment variable</h2><p>See whether you have an environment variable named <code>KUBECONFIG</code>. If so, save the
current value of your <code>KUBECONFIG</code> environment variable, so you can restore it later.
For example:</p><h3 id="linux">Linux</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG_SAVED</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b8860b">$KUBECONFIG</span><span style="color:#b44">"</span>
</span></span></code></pre></div><h3 id="windows-powershell">Windows PowerShell</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#b8860b">$Env:KUBECONFIG_SAVED</span>=<span style="color:#b8860b">$ENV:KUBECONFIG</span>
</span></span></code></pre></div><p>The <code>KUBECONFIG</code> environment variable is a list of paths to configuration files. The list is
colon-delimited for Linux and Mac, and semicolon-delimited for Windows. If you have
a <code>KUBECONFIG</code> environment variable, familiarize yourself with the configuration files
in the list.</p><p>Temporarily append two paths to your <code>KUBECONFIG</code> environment variable. For example:</p><h3 id="linux-1">Linux</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">KUBECONFIG</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">:config-demo:config-demo-2"</span>
</span></span></code></pre></div><h3 id="windows-powershell-1">Windows PowerShell</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#b8860b">$Env:KUBECONFIG</span>=(<span style="color:#b44">"config-demo;config-demo-2"</span>)
</span></span></code></pre></div><p>In your <code>config-exercise</code> directory, enter this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config view
</span></span></code></pre></div><p>The output shows merged information from all the files listed in your <code>KUBECONFIG</code>
environment variable. In particular, notice that the merged information has the
<code>dev-ramp-up</code> context from the <code>config-demo-2</code> file and the three contexts from
the <code>config-demo</code> file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">contexts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>ramp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-ramp-up<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dev-storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>exp-test<span style="color:#bbb">
</span></span></span></code></pre></div><p>For more information about how kubeconfig files are merged, see
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></p><h2 id="explore-the-home-kube-directory">Explore the $HOME/.kube directory</h2><p>If you already have a cluster, and you can use <code>kubectl</code> to interact with
the cluster, then you probably have a file named <code>config</code> in the <code>$HOME/.kube</code>
directory.</p><p>Go to <code>$HOME/.kube</code>, and see what files are there. Typically, there is a file named
<code>config</code>. There might also be other configuration files in this directory. Briefly
familiarize yourself with the contents of these files.</p><h2 id="append-home-kube-config-to-your-kubeconfig-environment-variable">Append $HOME/.kube/config to your KUBECONFIG environment variable</h2><p>If you have a <code>$HOME/.kube/config</code> file, and it's not already listed in your
<code>KUBECONFIG</code> environment variable, append it to your <code>KUBECONFIG</code> environment variable now.
For example:</p><h3 id="linux-2">Linux</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">KUBECONFIG</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">:</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">HOME</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">/.kube/config"</span>
</span></span></code></pre></div><h3 id="windows-powershell-2">Windows Powershell</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#b8860b">$Env:KUBECONFIG</span>=<span style="color:#b44">"</span><span style="color:#b8860b">$Env:KUBECONFIG</span><span style="color:#b44">;</span><span style="color:#b8860b">$HOME</span><span style="color:#b44">\.kube\config"</span>
</span></span></code></pre></div><p>View configuration information merged from all the files that are now listed
in your <code>KUBECONFIG</code> environment variable. In your config-exercise directory, enter:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config view
</span></span></code></pre></div><h2 id="clean-up">Clean up</h2><p>Return your <code>KUBECONFIG</code> environment variable to its original value. For example:<br/></p><h3 id="linux-3">Linux</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b8860b">$KUBECONFIG_SAVED</span><span style="color:#b44">"</span>
</span></span></code></pre></div><h3 id="windows-powershell-3">Windows PowerShell</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#b8860b">$Env:KUBECONFIG</span>=<span style="color:#b8860b">$ENV:KUBECONFIG_SAVED</span>
</span></span></code></pre></div><h2 id="check-the-subject-represented-by-the-kubeconfig">Check the subject represented by the kubeconfig</h2><p>It is not always obvious what attributes (username, groups) you will get after authenticating to the cluster.
It can be even more challenging if you are managing more than one cluster at the same time.</p><p>There is a <code>kubectl</code> subcommand to check subject attributes, such as username, for your selected Kubernetes
client context: <code>kubectl auth whoami</code>.</p><p>Read <a href="/docs/reference/access-authn-authz/authentication/#self-subject-review">API access to authentication information for a client</a>
to learn about this in more detail.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands#config">kubectl config</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Set Kubelet Parameters Via A Configuration File</h1><h2 id="before-you-begin">Before you begin</h2><p>Some steps in this page use the <code>jq</code> tool. If you don't have <code>jq</code>, you can
install it via your operating system's software sources, or fetch it from
<a href="https://jqlang.github.io/jq/">https://jqlang.github.io/jq/</a>.</p><p>Some steps also involve installing <code>curl</code>, which can be installed via your
operating system's software sources.</p><p>A subset of the kubelet's configuration parameters may be
set via an on-disk config file, as a substitute for command-line flags.</p><p>Providing parameters via a config file is the recommended approach because
it simplifies node deployment and configuration management.</p><h2 id="create-the-config-file">Create the config file</h2><p>The subset of the kubelet's configuration that can be configured via a file
is defined by the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
struct.</p><p>The configuration file must be a JSON or YAML representation of the parameters
in this struct. Make sure the kubelet has read permissions on the file.</p><p>Here is an example of what this file might look like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">address</span>:<span style="color:#bbb"> </span><span style="color:#b44">"192.168.0.8"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">20250</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">serializeImagePulls</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">evictionHard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">memory.available</span>:<span style="color:#bbb">  </span><span style="color:#b44">"100Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodefs.available</span>:<span style="color:#bbb">  </span><span style="color:#b44">"10%"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodefs.inodesFree</span>:<span style="color:#bbb"> </span><span style="color:#b44">"5%"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"15%"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagefs.inodesFree</span>:<span style="color:#bbb"> </span><span style="color:#b44">"5%"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, the kubelet is configured with the following settings:</p><ol><li><p><code>address</code>: The kubelet will serve on IP address <code>192.168.0.8</code>.</p></li><li><p><code>port</code>: The kubelet will serve on port <code>20250</code>.</p></li><li><p><code>serializeImagePulls</code>: Image pulls will be done in parallel.</p></li><li><p><code>evictionHard</code>: The kubelet will evict Pods under one of the following conditions:</p><ul><li>When the node's available memory drops below 100MiB.</li><li>When the node's main filesystem's available space is less than 10%.</li><li>When the image filesystem's available space is less than 15%.</li><li>When more than 95% of the node's main filesystem's inodes are in use.</li></ul></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In the example, by changing the default value of only one parameter for
evictionHard, the default values of other parameters will not be inherited and
will be set to zero. In order to provide custom values, you should provide all
the threshold values respectively.
Alternatively, you can set the MergeDefaultEvictionSettings to true in the kubelet
configuration file, if any parameter is changed then the other parameters will inherit
their default values instead of 0.</div><p>The <code>imagefs</code> is an optional filesystem that container runtimes use to store container
images and container writable layers.</p><h2 id="start-a-kubelet-process-configured-via-the-config-file">Start a kubelet process configured via the config file</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you use kubeadm to initialize your cluster, use the kubelet-config while creating your cluster with <code>kubeadm init</code>.
See <a href="/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">configuring kubelet using kubeadm</a> for details.</div><p>Start the kubelet with the <code>--config</code> flag set to the path of the kubelet's config file.
The kubelet will then load its config from this file.</p><p>Note that command line flags which target the same value as a config file will override that value.
This helps ensure backwards compatibility with the command-line API.</p><p>Note that relative file paths in the kubelet config file are resolved relative to the
location of the kubelet config file, whereas relative paths in command line flags are resolved
relative to the kubelet's current working directory.</p><p>Note that some default values differ between command-line flags and the kubelet config file.
If <code>--config</code> is provided and the values are not specified via the command line, the
defaults for the <code>KubeletConfiguration</code> version apply.
In the above example, this version is <code>kubelet.config.k8s.io/v1beta1</code>.</p><h2 id="kubelet-conf-d">Drop-in directory for kubelet configuration files</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code></div><p>You can specify a drop-in configuration directory for the kubelet. By default, the kubelet does not look
for drop-in configuration files anywhere - you must specify a path.
For example: <code>--config-dir=/etc/kubernetes/kubelet.conf.d</code></p><p>For Kubernetes v1.28 to v1.29, you can only specify <code>--config-dir</code> if you also set
the environment variable <code>KUBELET_CONFIG_DROPIN_DIR_ALPHA</code> for the kubelet process (the value
of that variable does not matter).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The suffix of a valid kubelet drop-in configuration file <strong>must</strong> be <code>.conf</code>. For instance: <code>99-kubelet-address.conf</code></div><p>The kubelet processes files in its config drop-in directory by sorting the <strong>entire file name</strong> alphanumerically.
For instance, <code>00-kubelet.conf</code> is processed first, and then overridden with a file named <code>01-kubelet.conf</code>.</p><p>These files may contain partial configurations but should not be invalid and must include type metadata, specifically <code>apiVersion</code> and <code>kind</code>.
Validation is only performed on the final resulting configuration structure stored internally in the kubelet.
This offers flexibility in managing and merging kubelet configurations from different sources while preventing undesirable configurations.
However, it is important to note that behavior varies based on the data type of the configuration fields.</p><p>Different data types in the kubelet configuration structure merge differently. See the
<a href="/docs/reference/node/kubelet-config-directory-merging/">reference document</a>
for more information.</p><h3 id="kubelet-configuration-merging-order">Kubelet configuration merging order</h3><p>On startup, the kubelet merges configuration from:</p><ul><li>Feature gates specified over the command line (lowest precedence).</li><li>The kubelet configuration.</li><li>Drop-in configuration files, according to sort order.</li><li>Command line arguments excluding feature gates (highest precedence).</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The config drop-in dir mechanism for the kubelet is similar but different from how the <code>kubeadm</code> tool allows you to patch configuration.
The <code>kubeadm</code> tool uses a specific <a href="/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches">patching strategy</a>
for its configuration, whereas the only patch strategy for kubelet configuration drop-in files is <code>replace</code>.
The kubelet determines the order of merges based on sorting the <strong>suffixes</strong> alphanumerically,
and replaces every field present in a higher priority file.</div><h2 id="viewing-the-kubelet-configuration">Viewing the kubelet configuration</h2><p>Since the configuration could now be spread over multiple files with this feature, if someone wants to inspect the final actuated configuration,
they can follow these steps to inspect the kubelet configuration:</p><ol><li><p>Start a proxy server using <a href="/docs/reference/kubectl/generated/kubectl_proxy/"><code>kubectl proxy</code></a> in your terminal.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl proxy
</span></span></code></pre></div><p>Which gives output like:</p><pre tabindex="0"><code class="language-none" data-lang="none">Starting to serve on 127.0.0.1:8001
</code></pre></li><li><p>Open another terminal window and use <code>curl</code> to fetch the kubelet configuration.
Replace <code>&lt;node-name&gt;</code> with the actual name of your node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>curl -X GET http://127.0.0.1:8001/api/v1/nodes/&lt;node-name&gt;/proxy/configz | jq .
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kubeletconfig"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableServer"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"staticPodPath"</span>: <span style="color:#b44">"/var/run/kubernetes/static-pods"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"syncFrequency"</span>: <span style="color:#b44">"1m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"fileCheckFrequency"</span>: <span style="color:#b44">"20s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"httpCheckFrequency"</span>: <span style="color:#b44">"20s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"address"</span>: <span style="color:#b44">"192.168.1.16"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"port"</span>: <span style="color:#666">10250</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"readOnlyPort"</span>: <span style="color:#666">10255</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"tlsCertFile"</span>: <span style="color:#b44">"/var/lib/kubelet/pki/kubelet.crt"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"tlsPrivateKeyFile"</span>: <span style="color:#b44">"/var/lib/kubelet/pki/kubelet.key"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"rotateCertificates"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"authentication"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"x509"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"clientCAFile"</span>: <span style="color:#b44">"/var/run/kubernetes/client-ca.crt"</span>
</span></span><span style="display:flex"><span>      },
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"webhook"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"enabled"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"cacheTTL"</span>: <span style="color:#b44">"2m0s"</span>
</span></span><span style="display:flex"><span>      },
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"anonymous"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"enabled"</span>: <span style="color:#a2f;font-weight:700">true</span>
</span></span><span style="display:flex"><span>      }
</span></span><span style="display:flex"><span>    },
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"authorization"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"mode"</span>: <span style="color:#b44">"AlwaysAllow"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"webhook"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"cacheAuthorizedTTL"</span>: <span style="color:#b44">"5m0s"</span>,
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"cacheUnauthorizedTTL"</span>: <span style="color:#b44">"30s"</span>
</span></span><span style="display:flex"><span>      }
</span></span><span style="display:flex"><span>    },
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"registryPullQPS"</span>: <span style="color:#666">5</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"registryBurst"</span>: <span style="color:#666">10</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"eventRecordQPS"</span>: <span style="color:#666">50</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"eventBurst"</span>: <span style="color:#666">100</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableDebuggingHandlers"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"healthzPort"</span>: <span style="color:#666">10248</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"healthzBindAddress"</span>: <span style="color:#b44">"127.0.0.1"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"oomScoreAdj"</span>: <span style="color:#666">-999</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"clusterDomain"</span>: <span style="color:#b44">"cluster.local"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"clusterDNS"</span>: [
</span></span><span style="display:flex"><span>      <span style="color:#b44">"10.0.0.10"</span>
</span></span><span style="display:flex"><span>    ],
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"streamingConnectionIdleTimeout"</span>: <span style="color:#b44">"4h0m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"nodeStatusUpdateFrequency"</span>: <span style="color:#b44">"10s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"nodeStatusReportFrequency"</span>: <span style="color:#b44">"5m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"nodeLeaseDurationSeconds"</span>: <span style="color:#666">40</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"imageMinimumGCAge"</span>: <span style="color:#b44">"2m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"imageMaximumGCAge"</span>: <span style="color:#b44">"0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"imageGCHighThresholdPercent"</span>: <span style="color:#666">85</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"imageGCLowThresholdPercent"</span>: <span style="color:#666">80</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"volumeStatsAggPeriod"</span>: <span style="color:#b44">"1m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"cgroupsPerQOS"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"cgroupDriver"</span>: <span style="color:#b44">"systemd"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"cpuManagerPolicy"</span>: <span style="color:#b44">"none"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"cpuManagerReconcilePeriod"</span>: <span style="color:#b44">"10s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"memoryManagerPolicy"</span>: <span style="color:#b44">"None"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"topologyManagerPolicy"</span>: <span style="color:#b44">"none"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"topologyManagerScope"</span>: <span style="color:#b44">"container"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"runtimeRequestTimeout"</span>: <span style="color:#b44">"2m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"hairpinMode"</span>: <span style="color:#b44">"promiscuous-bridge"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"maxPods"</span>: <span style="color:#666">110</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"podPidsLimit"</span>: <span style="color:#666">-1</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"resolvConf"</span>: <span style="color:#b44">"/run/systemd/resolve/resolv.conf"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"cpuCFSQuota"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"cpuCFSQuotaPeriod"</span>: <span style="color:#b44">"100ms"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"nodeStatusMaxImages"</span>: <span style="color:#666">50</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"maxOpenFiles"</span>: <span style="color:#666">1000000</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"contentType"</span>: <span style="color:#b44">"application/vnd.kubernetes.protobuf"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"kubeAPIQPS"</span>: <span style="color:#666">50</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"kubeAPIBurst"</span>: <span style="color:#666">100</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"serializeImagePulls"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"evictionHard"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"imagefs.available"</span>: <span style="color:#b44">"15%"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"memory.available"</span>: <span style="color:#b44">"100Mi"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"nodefs.available"</span>: <span style="color:#b44">"10%"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"nodefs.inodesFree"</span>: <span style="color:#b44">"5%"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"imagefs.inodesFree"</span>: <span style="color:#b44">"5%"</span>
</span></span><span style="display:flex"><span>    },
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"evictionPressureTransitionPeriod"</span>: <span style="color:#b44">"1m0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"mergeDefaultEvictionSettings"</span>: <span style="color:#a2f;font-weight:700">false</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableControllerAttachDetach"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"makeIPTablesUtilChains"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"iptablesMasqueradeBit"</span>: <span style="color:#666">14</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"iptablesDropBit"</span>: <span style="color:#666">15</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"featureGates"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"AllAlpha"</span>: <span style="color:#a2f;font-weight:700">false</span>
</span></span><span style="display:flex"><span>    },
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"failSwapOn"</span>: <span style="color:#a2f;font-weight:700">false</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"memorySwap"</span>: {},
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"containerLogMaxSize"</span>: <span style="color:#b44">"10Mi"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"containerLogMaxFiles"</span>: <span style="color:#666">5</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"configMapAndSecretChangeDetectionStrategy"</span>: <span style="color:#b44">"Watch"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enforceNodeAllocatable"</span>: [
</span></span><span style="display:flex"><span>      <span style="color:#b44">"pods"</span>
</span></span><span style="display:flex"><span>    ],
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"volumePluginDir"</span>: <span style="color:#b44">"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"logging"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"format"</span>: <span style="color:#b44">"text"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"flushFrequency"</span>: <span style="color:#b44">"5s"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"verbosity"</span>: <span style="color:#666">3</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"options"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"json"</span>: {
</span></span><span style="display:flex"><span>          <span style="color:green;font-weight:700">"infoBufferSize"</span>: <span style="color:#b44">"0"</span>
</span></span><span style="display:flex"><span>        }
</span></span><span style="display:flex"><span>      }
</span></span><span style="display:flex"><span>    },
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableSystemLogHandler"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableSystemLogQuery"</span>: <span style="color:#a2f;font-weight:700">false</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"shutdownGracePeriod"</span>: <span style="color:#b44">"0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"shutdownGracePeriodCriticalPods"</span>: <span style="color:#b44">"0s"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableProfilingHandler"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"enableDebugFlagsHandler"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"seccompDefault"</span>: <span style="color:#a2f;font-weight:700">false</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"memoryThrottlingFactor"</span>: <span style="color:#666">0.9</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"registerNode"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"localStorageCapacityIsolation"</span>: <span style="color:#a2f;font-weight:700">true</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"containerRuntimeEndpoint"</span>: <span style="color:#b44">"unix:///var/run/crio/crio.sock"</span>
</span></span><span style="display:flex"><span>  }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about kubelet configuration by checking the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
reference.</li><li>Learn more about kubelet configuration merging in the
<a href="/docs/reference/node/kubelet-config-directory-merging/">reference document</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Change the Access Mode of a PersistentVolume to ReadWriteOncePod</h1><p>This page shows how to change the access mode on an existing PersistentVolume to
use <code>ReadWriteOncePod</code>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.22.<p>To check the version, enter <code>kubectl version</code>.</p></p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>ReadWriteOncePod</code> access mode graduated to stable in the Kubernetes v1.29
release. If you are running a version of Kubernetes older than v1.29, you might
need to enable a feature gate. Check the documentation for your version of
Kubernetes.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The <code>ReadWriteOncePod</code> access mode is only supported for
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> volumes.
To use this volume access mode you will need to update the following
<a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html">CSI sidecars</a>
to these versions or greater:</p><ul><li><a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+</a></li></ul></div><h2 id="why-should-i-use-readwriteoncepod">Why should I use <code>ReadWriteOncePod</code>?</h2><p>Prior to Kubernetes v1.22, the <code>ReadWriteOnce</code> access mode was commonly used to
restrict PersistentVolume access for workloads that required single-writer
access to storage. However, this access mode had a limitation: it restricted
volume access to a single <em>node</em>, allowing multiple pods on the same node to
read from and write to the same volume simultaneously. This could pose a risk
for applications that demand strict single-writer access for data safety.</p><p>If ensuring single-writer access is critical for your workloads, consider
migrating your volumes to <code>ReadWriteOncePod</code>.</p><h2 id="migrating-existing-persistentvolumes">Migrating existing PersistentVolumes</h2><p>If you have existing PersistentVolumes, they can be migrated to use
<code>ReadWriteOncePod</code>. Only migrations from <code>ReadWriteOnce</code> to <code>ReadWriteOncePod</code>
are supported.</p><p>In this example, there is already a <code>ReadWriteOnce</code> "cat-pictures-pvc"
PersistentVolumeClaim that is bound to a "cat-pictures-pv" PersistentVolume,
and a "cat-pictures-writer" Deployment that uses this PersistentVolumeClaim.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If your storage plugin supports
<a href="/docs/concepts/storage/dynamic-provisioning/">Dynamic provisioning</a>,
the "cat-picutres-pv" will be created for you, but its name may differ. To get
your PersistentVolume's name run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pvc cat-pictures-pvc -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.spec.volumeName}'</span>
</span></span></code></pre></div></div><p>And you can view the PVC before you make changes. Either view the manifest
locally, or run <code>kubectl get pvc &lt;name-of-pvc&gt; -o yaml</code>. The output is similar
to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># cat-pictures-pvc.yaml</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cat-pictures-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span></code></pre></div><p>Here's an example Deployment that relies on that PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># cat-pictures-writer-deployment.yaml</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cat-pictures-writer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>cat-pictures-writer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>cat-pictures-writer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cat-pictures<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cat-pictures<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">persistentVolumeClaim</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">claimName</span>:<span style="color:#bbb"> </span>cat-pictures-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>As a first step, you need to edit your PersistentVolume's
<code>spec.persistentVolumeReclaimPolicy</code> and set it to <code>Retain</code>. This ensures your
PersistentVolume will not be deleted when you delete the corresponding
PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch pv cat-pictures-pv -p <span style="color:#b44">'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'</span>
</span></span></code></pre></div><p>Next you need to stop any workloads that are using the PersistentVolumeClaim
bound to the PersistentVolume you want to migrate, and then delete the
PersistentVolumeClaim. Avoid making any other changes to the
PersistentVolumeClaim, such as volume resizes, until after the migration is
complete.</p><p>Once that is done, you need to clear your PersistentVolume's <code>spec.claimRef.uid</code>
to ensure PersistentVolumeClaims can bind to it upon recreation:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale --replicas<span style="color:#666">=</span><span style="color:#666">0</span> deployment cat-pictures-writer
</span></span><span style="display:flex"><span>kubectl delete pvc cat-pictures-pvc
</span></span><span style="display:flex"><span>kubectl patch pv cat-pictures-pv -p <span style="color:#b44">'{"spec":{"claimRef":{"uid":""}}}'</span>
</span></span></code></pre></div><p>After that, replace the PersistentVolume's list of valid access modes to be
(only) <code>ReadWriteOncePod</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch pv cat-pictures-pv -p <span style="color:#b44">'{"spec":{"accessModes":["ReadWriteOncePod"]}}'</span>
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>ReadWriteOncePod</code> access mode cannot be combined with other access modes.
Make sure <code>ReadWriteOncePod</code> is the only access mode on the PersistentVolume
when updating, otherwise the request will fail.</div><p>Next you need to modify your PersistentVolumeClaim to set <code>ReadWriteOncePod</code> as
the only access mode. You should also set the PersistentVolumeClaim's
<code>spec.volumeName</code> to the name of your PersistentVolume to ensure it binds to
this specific PersistentVolume.</p><p>Once this is done, you can recreate your PersistentVolumeClaim and start up your
workloads:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to:</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># - Set ReadWriteOncePod as the only access mode</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># - Set spec.volumeName to "cat-pictures-pv"</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>kubectl apply -f cat-pictures-pvc.yaml
</span></span><span style="display:flex"><span>kubectl apply -f cat-pictures-writer-deployment.yaml
</span></span></code></pre></div><p>Lastly you may edit your PersistentVolume's <code>spec.persistentVolumeReclaimPolicy</code>
and set to it back to <code>Delete</code> if you previously changed it.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch pv cat-pictures-pv -p <span style="color:#b44">'{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}'</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">Configuring a Pod to Use a PersistentVolume for Storage</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Minimum and Maximum Memory Constraints for a Namespace</h1><div class="lead">Define a range of valid memory resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</div><p>This page shows how to set minimum and maximum values for memory used by containers
running in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>.
You specify minimum and maximum memory values in a
<a href="/docs/reference/kubernetes-api/policy-resources/limit-range-v1/">LimitRange</a>
object. If a Pod does not meet the constraints imposed by the LimitRange,
it cannot be created in the namespace.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 1 GiB of memory available for Pods.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace constraints-mem-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's an example manifest for a LimitRange:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints.yaml" download="admin/resource/memory-constraints.yaml"><code>admin/resource/memory-constraints.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-constraints-yaml&quot;)" title="Copy admin/resource/memory-constraints.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-constraints-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mem-min-max-demo-lr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">max</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">min</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>500Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the LimitRange:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>View detailed information about the LimitRange:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get limitrange mem-min-max-demo-lr --namespace<span style="color:#666">=</span>constraints-mem-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output shows the minimum and maximum memory constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.</p><pre tabindex="0"><code>  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
</code></pre><p>Now whenever you define a Pod within the constraints-mem-example namespace, Kubernetes
performs these steps:</p><ul><li><p>If any container in that Pod does not specify its own memory request and limit,
the control plane assigns the default memory request and limit to that container.</p></li><li><p>Verify that every container in that Pod requests at least 500 MiB of memory.</p></li><li><p>Verify that every container in that Pod requests no more than 1024 MiB (1 GiB)
of memory.</p></li></ul><p>Here's a manifest for a Pod that has one container. Within the Pod spec, the sole
container specifies a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the
minimum and maximum memory constraints imposed by the LimitRange.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod.yaml" download="admin/resource/memory-constraints-pod.yaml"><code>admin/resource/memory-constraints-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-constraints-pod-yaml&quot;)" title="Copy admin/resource/memory-constraints-pod.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-constraints-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"600Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>Verify that the Pod is running and that its container is healthy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod constraints-mem-demo --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod constraints-mem-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>The output shows that the container within that Pod has a memory request of 600 MiB and
a memory limit of 800 MiB. These satisfy the constraints imposed by the LimitRange for
this namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>800Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>600Mi<span style="color:#bbb">
</span></span></span></code></pre></div><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete pod constraints-mem-demo --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><h2 id="attempt-to-create-a-pod-that-exceeds-the-maximum-memory-constraint">Attempt to create a Pod that exceeds the maximum memory constraint</h2><p>Here's a manifest for a Pod that has one container. The container specifies a
memory request of 800 MiB and a memory limit of 1.5 GiB.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod-2.yaml" download="admin/resource/memory-constraints-pod-2.yaml"><code>admin/resource/memory-constraints-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-constraints-pod-2-yaml&quot;)" title="Copy admin/resource/memory-constraints-pod-2.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-constraints-pod-2-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-2-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1.5Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines a container that
requests more memory than is allowed:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-2.yaml":
pods "constraints-mem-demo-2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
</code></pre><h2 id="attempt-to-create-a-pod-that-does-not-meet-the-minimum-memory-request">Attempt to create a Pod that does not meet the minimum memory request</h2><p>Here's a manifest for a Pod that has one container. That container specifies a
memory request of 100 MiB and a memory limit of 800 MiB.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod-3.yaml" download="admin/resource/memory-constraints-pod-3.yaml"><code>admin/resource/memory-constraints-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-constraints-pod-3-yaml&quot;)" title="Copy admin/resource/memory-constraints-pod-3.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-constraints-pod-3-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-3-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>The output shows that the Pod does not get created, because it defines a container
that requests less memory than the enforced minimum:</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-3.yaml":
pods "constraints-mem-demo-3" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
</code></pre><h2 id="create-a-pod-that-does-not-specify-any-memory-request-or-limit">Create a Pod that does not specify any memory request or limit</h2><p>Here's a manifest for a Pod that has one container. The container does not
specify a memory request, and it does not specify a memory limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-constraints-pod-4.yaml" download="admin/resource/memory-constraints-pod-4.yaml"><code>admin/resource/memory-constraints-pod-4.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-constraints-pod-4-yaml&quot;)" title="Copy admin/resource/memory-constraints-pod-4.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-constraints-pod-4-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-4-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod constraints-mem-demo-4 --namespace<span style="color:#666">=</span>constraints-mem-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output shows that the Pod's only container has a memory request of 1 GiB and a memory limit of 1 GiB.
How did that container get those values?</p><pre tabindex="0"><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><p>Because your Pod did not define any memory request and limit for that container, the cluster
applied a
<a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">default memory request and limit</a>
from the LimitRange.</p><p>This means that the definition of that Pod shows those values. You can check it using
<code>kubectl describe</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Look for the "Requests:" section of the output</span>
</span></span><span style="display:flex"><span>kubectl describe pod constraints-mem-demo-4 --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><p>At this point, your Pod might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 GiB of memory. If each of your Nodes has only
1 GiB of memory, then there is not enough allocatable memory on any Node to accommodate a memory
request of 1 GiB. If you happen to be using Nodes with 2 GiB of memory, then you probably have
enough space to accommodate the 1 GiB request.</p><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete pod constraints-mem-demo-4 --namespace<span style="color:#666">=</span>constraints-mem-example
</span></span></code></pre></div><h2 id="enforcement-of-minimum-and-maximum-memory-constraints">Enforcement of minimum and maximum memory constraints</h2><p>The maximum and minimum memory constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.</p><h2 id="motivation-for-minimum-and-maximum-memory-constraints">Motivation for minimum and maximum memory constraints</h2><p>As a cluster administrator, you might want to impose restrictions on the amount of memory that Pods can use.
For example:</p><ul><li><p>Each Node in a cluster has 2 GiB of memory. You do not want to accept any Pod that requests
more than 2 GiB of memory, because no Node in the cluster can support the request.</p></li><li><p>A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 8 GiB of memory, but
you want development workloads to be limited to 512 MiB. You create separate namespaces
for production and development, and you apply memory constraints to each namespace.</p></li></ul><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace constraints-mem-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Advertise Extended Resources for a Node</h1><p>This page shows how to specify extended resources for a Node.
Extended resources allow cluster administrators to advertise node-level
resources that would otherwise be unknown to Kubernetes.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="get-the-names-of-your-nodes">Get the names of your Nodes</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get nodes
</span></span></code></pre></div><p>Choose one of your Nodes to use for this exercise.</p><h2 id="advertise-a-new-extended-resource-on-one-of-your-nodes">Advertise a new extended resource on one of your Nodes</h2><p>To advertise a new extended resource on a Node, send an HTTP PATCH request to
the Kubernetes API server. For example, suppose one of your Nodes has four dongles
attached. Here's an example of a PATCH request that advertises four dongle resources
for your Node.</p><pre tabindex="0"><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "add",
    "path": "/status/capacity/example.com~1dongle",
    "value": "4"
  }
]
</code></pre><p>Note that Kubernetes does not need to know what a dongle is or what a dongle is for.
The preceding PATCH request tells Kubernetes that your Node has four things that
you call dongles.</p><p>Start a proxy, so that you can easily send requests to the Kubernetes API server:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy
</span></span></code></pre></div><p>In another command window, send the HTTP PATCH request.
Replace <code>&lt;your-node-name&gt;</code> with the name of your Node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl --header <span style="color:#b44">"Content-Type: application/json-patch+json"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --request PATCH <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --data <span style="color:#b44">'[{"op": "add", "path": "/status/capacity/example.com~1dongle", "value": "4"}]'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In the preceding request, <code>~1</code> is the encoding for the character / in
the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href="https://tools.ietf.org/html/rfc6901">IETF RFC 6901</a>, section 3.</div><p>The output shows that the Node has a capacity of 4 dongles:</p><pre tabindex="0"><code>"capacity": {
  "cpu": "2",
  "memory": "2049008Ki",
  "example.com/dongle": "4",
</code></pre><p>Describe your Node:</p><pre tabindex="0"><code>kubectl describe node &lt;your-node-name&gt;
</code></pre><p>Once again, the output shows the dongle resource:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">Capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>2049008Ki<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Now, application developers can create Pods that request a certain
number of dongles. See
<a href="/docs/tasks/configure-pod-container/extended-resource/">Assign Extended Resources to a Container</a>.</p><h2 id="discussion">Discussion</h2><p>Extended resources are similar to memory and CPU resources. For example,
just as a Node has a certain amount of memory and CPU to be shared by all components
running on the Node, it can have a certain number of dongles to be shared
by all components running on the Node. And just as application developers
can create Pods that request a certain amount of memory and CPU, they can
create Pods that request a certain number of dongles.</p><p>Extended resources are opaque to Kubernetes; Kubernetes does not
know anything about what they are. Kubernetes knows only that a Node
has a certain number of them. Extended resources must be advertised in integer
amounts. For example, a Node can advertise four dongles, but not 4.5 dongles.</p><h3 id="storage-example">Storage example</h3><p>Suppose a Node has 800 GiB of a special kind of disk storage. You could
create a name for the special storage, say example.com/special-storage.
Then you could advertise it in chunks of a certain size, say 100 GiB. In that case,
your Node would advertise that it has eight resources of type
example.com/special-storage.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">Capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">example.com/special-storage</span>:<span style="color:#bbb"> </span><span style="color:#666">8</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>If you want to allow arbitrary requests for special storage, you
could advertise special storage in chunks of size 1 byte. In that case, you would advertise
800Gi resources of type example.com/special-storage.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">Capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">example.com/special-storage</span>:<span style="color:#bbb">  </span>800Gi<span style="color:#bbb">
</span></span></span></code></pre></div><p>Then a Container could request any number of bytes of special storage, up to 800Gi.</p><h2 id="clean-up">Clean up</h2><p>Here is a PATCH request that removes the dongle advertisement from a Node.</p><pre tabindex="0"><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "remove",
    "path": "/status/capacity/example.com~1dongle",
  }
]
</code></pre><p>Start a proxy, so that you can easily send requests to the Kubernetes API server:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy
</span></span></code></pre></div><p>In another command window, send the HTTP PATCH request.
Replace <code>&lt;your-node-name&gt;</code> with the name of your Node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl --header <span style="color:#b44">"Content-Type: application/json-patch+json"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --request PATCH <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --data <span style="color:#b44">'[{"op": "remove", "path": "/status/capacity/example.com~1dongle"}]'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</span></span></code></pre></div><p>Verify that the dongle advertisement has been removed:</p><pre tabindex="0"><code>kubectl describe node &lt;your-node-name&gt; | grep dongle
</code></pre><p>(you should not see any output)</p><h2 id="what-s-next">What's next</h2><h3 id="for-application-developers">For application developers</h3><ul><li><a href="/docs/tasks/configure-pod-container/extended-resource/">Assign Extended Resources to a Container</a></li><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></li><li><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></li><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Change the default StorageClass</h1><p>This page shows how to change the default Storage Class that is used to
provision volumes for PersistentVolumeClaims that have no special requirements.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="why-change-the-default-storage-class">Why change the default storage class?</h2><p>Depending on the installation method, your Kubernetes cluster may be deployed with
an existing StorageClass that is marked as default. This default StorageClass
is then used to dynamically provision storage for PersistentVolumeClaims
that do not require any specific storage class. See
<a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim documentation</a>
for details.</p><p>The pre-installed default StorageClass may not fit well with your expected workload;
for example, it might provision storage that is too expensive. If this is the case,
you can either change the default StorageClass or disable it completely to avoid
dynamic provisioning of storage.</p><p>Deleting the default StorageClass may not work, as it may be re-created
automatically by the addon manager running in your cluster. Please consult the docs for your installation
for details about addon manager and how to disable individual addons.</p><h2 id="changing-the-default-storageclass">Changing the default StorageClass</h2><ol><li><p>List the StorageClasses in your cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get storageclass
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>NAME                 PROVISIONER               AGE
</span></span><span style="display:flex"><span>standard <span style="color:#666">(</span>default<span style="color:#666">)</span>   kubernetes.io/gce-pd      1d
</span></span><span style="display:flex"><span>gold                 kubernetes.io/gce-pd      1d
</span></span></code></pre></div><p>The default StorageClass is marked by <code>(default)</code>.</p></li><li><p>Mark the default StorageClass as non-default:</p><p>The default StorageClass has an annotation
<code>storageclass.kubernetes.io/is-default-class</code> set to <code>true</code>. Any other value
or absence of the annotation is interpreted as <code>false</code>.</p><p>To mark a StorageClass as non-default, you need to change its value to <code>false</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl patch storageclass standard -p <span style="color:#b44">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'</span>
</span></span></code></pre></div><p>where <code>standard</code> is the name of your chosen StorageClass.</p></li><li><p>Mark a StorageClass as default:</p><p>Similar to the previous step, you need to add/set the annotation
<code>storageclass.kubernetes.io/is-default-class=true</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl patch storageclass gold -p <span style="color:#b44">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span>
</span></span></code></pre></div><p>Please note you can have multiple <code>StorageClass</code> marked as default. If more
than one <code>StorageClass</code> is marked as default, a <code>PersistentVolumeClaim</code> without
an explicitly defined <code>storageClassName</code> will be created using the most recently
created default <code>StorageClass</code>.
When a <code>PersistentVolumeClaim</code> is created with a specified <code>volumeName</code>, it remains
in a pending state if the static volume's <code>storageClassName</code> does not match the
<code>StorageClass</code> on the <code>PersistentVolumeClaim</code>.</p></li><li><p>Verify that your chosen StorageClass is default:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get storageclass
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>NAME             PROVISIONER               AGE
</span></span><span style="display:flex"><span>standard         kubernetes.io/gce-pd      1d
</span></span><span style="display:flex"><span>gold <span style="color:#666">(</span>default<span style="color:#666">)</span>   kubernetes.io/gce-pd      1d
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Troubleshooting CNI plugin-related errors</h1><p>To avoid CNI plugin-related errors, verify that you are using or upgrading to a
container runtime that has been tested to work correctly with your version of
Kubernetes.</p><h2 id="about-the-incompatible-cni-versions-and-failed-to-destroy-network-for-sandbox-errors">About the "Incompatible CNI versions" and "Failed to destroy network for sandbox" errors</h2><p>Service issues exist for pod CNI network setup and tear down in containerd
v1.6.0-v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config
version is not declared in the CNI config files. The containerd team reports,
"these issues are resolved in containerd v1.6.4."</p><p>With containerd v1.6.0-v1.6.3, if you do not upgrade the CNI plugins and/or
declare the CNI config version, you might encounter the following "Incompatible
CNI versions" or "Failed to destroy network for sandbox" error conditions.</p><h3 id="incompatible-cni-versions-error">Incompatible CNI versions error</h3><p>If the version of your CNI plugin does not correctly match the plugin version in
the config because the config version is later than the plugin version, the
containerd log will likely show an error message on startup of a pod similar
to:</p><pre tabindex="0"><code>incompatible CNI versions; config is \"1.0.0\", plugin supports [\"0.1.0\" \"0.2.0\" \"0.3.0\" \"0.3.1\" \"0.4.0\"]"
</code></pre><p>To fix this issue, <a href="#updating-your-cni-plugins-and-cni-config-files">update your CNI plugins and CNI config files</a>.</p><h3 id="failed-to-destroy-network-for-sandbox-error">Failed to destroy network for sandbox error</h3><p>If the version of the plugin is missing in the CNI plugin config, the pod may
run. However, stopping the pod generates an error similar to:</p><pre tabindex="0"><code>ERROR[2022-04-26T00:43:24.518165483Z] StopPodSandbox for "b" failed
error="failed to destroy network for sandbox \"bbc85f891eaf060c5a879e27bba9b6b06450210161dfdecfbb2732959fb6500a\": invalid version \"\": the version is empty"
</code></pre><p>This error leaves the pod in the not-ready state with a network namespace still
attached. To recover from this problem, <a href="#updating-your-cni-plugins-and-cni-config-files">edit the CNI config file</a> to add
the missing version information. The next attempt to stop the pod should
be successful.</p><h3 id="updating-your-cni-plugins-and-cni-config-files">Updating your CNI plugins and CNI config files</h3><p>If you're using containerd v1.6.0-v1.6.3 and encountered "Incompatible CNI
versions" or "Failed to destroy network for sandbox" errors, consider updating
your CNI plugins and editing the CNI config files.</p><p>Here's an overview of the typical steps for each node:</p><ol><li><p><a href="/docs/tasks/administer-cluster/safely-drain-node/">Safely drain and cordon the node</a>.</p></li><li><p>After stopping your container runtime and kubelet services, perform the
following upgrade operations:</p><ul><li>If you're running CNI plugins, upgrade them to the latest version.</li><li>If you're using non-CNI plugins, replace them with CNI plugins. Use the
latest version of the plugins.</li><li>Update the plugin configuration file to specify or match a version of the
CNI specification that the plugin supports, as shown in the following
<a href="#an-example-containerd-configuration-file">"An example containerd configuration file"</a> section.</li><li>For <code>containerd</code>, ensure that you have installed the latest version (v1.0.0 or later)
of the CNI loopback plugin.</li><li>Upgrade node components (for example, the kubelet) to Kubernetes v1.24</li><li>Upgrade to or install the most current version of the container runtime.</li></ul></li><li><p>Bring the node back into your cluster by restarting your container runtime
and kubelet. Uncordon the node (<code>kubectl uncordon &lt;nodename&gt;</code>).</p></li></ol><h2 id="an-example-containerd-configuration-file">An example containerd configuration file</h2><p>The following example shows a configuration for <code>containerd</code> runtime v1.6.x,
which supports a recent version of the CNI specification (v1.0.0).</p><p>Please see the documentation from your plugin and networking provider for
further instructions on configuring your system.</p><p>On Kubernetes, containerd runtime adds a loopback interface, <code>lo</code>, to pods as a
default behavior. The containerd runtime configures the loopback interface via a
CNI plugin, <code>loopback</code>. The <code>loopback</code> plugin is distributed as part of the
<code>containerd</code> release packages that have the <code>cni</code> designation. <code>containerd</code>
v1.6.0 and later includes a CNI v1.0.0-compatible loopback plugin as well as
other default CNI plugins. The configuration for the loopback plugin is done
internally by containerd, and is set to use CNI v1.0.0. This also means that the
version of the <code>loopback</code> plugin must be v1.0.0 or later when this newer version
<code>containerd</code> is started.</p><p>The following bash command generates an example CNI config. Here, the 1.0.0
value for the config version is assigned to the <code>cniVersion</code> field for use when
<code>containerd</code> invokes the CNI bridge plugin.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>cat <span style="color:#b44">&lt;&lt; EOF | tee /etc/cni/net.d/10-containerd-net.conflist
</span></span></span><span style="display:flex"><span><span style="color:#b44">{
</span></span></span><span style="display:flex"><span><span style="color:#b44"> "cniVersion": "1.0.0",
</span></span></span><span style="display:flex"><span><span style="color:#b44"> "name": "containerd-net",
</span></span></span><span style="display:flex"><span><span style="color:#b44"> "plugins": [
</span></span></span><span style="display:flex"><span><span style="color:#b44">   {
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "type": "bridge",
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "bridge": "cni0",
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "isGateway": true,
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "ipMasq": true,
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "promiscMode": true,
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "ipam": {
</span></span></span><span style="display:flex"><span><span style="color:#b44">       "type": "host-local",
</span></span></span><span style="display:flex"><span><span style="color:#b44">       "ranges": [
</span></span></span><span style="display:flex"><span><span style="color:#b44">         [{
</span></span></span><span style="display:flex"><span><span style="color:#b44">           "subnet": "10.88.0.0/16"
</span></span></span><span style="display:flex"><span><span style="color:#b44">         }],
</span></span></span><span style="display:flex"><span><span style="color:#b44">         [{
</span></span></span><span style="display:flex"><span><span style="color:#b44">           "subnet": "2001:db8:4860::/64"
</span></span></span><span style="display:flex"><span><span style="color:#b44">         }]
</span></span></span><span style="display:flex"><span><span style="color:#b44">       ],
</span></span></span><span style="display:flex"><span><span style="color:#b44">       "routes": [
</span></span></span><span style="display:flex"><span><span style="color:#b44">         { "dst": "0.0.0.0/0" },
</span></span></span><span style="display:flex"><span><span style="color:#b44">         { "dst": "::/0" }
</span></span></span><span style="display:flex"><span><span style="color:#b44">       ]
</span></span></span><span style="display:flex"><span><span style="color:#b44">     }
</span></span></span><span style="display:flex"><span><span style="color:#b44">   },
</span></span></span><span style="display:flex"><span><span style="color:#b44">   {
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "type": "portmap",
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "capabilities": {"portMappings": true},
</span></span></span><span style="display:flex"><span><span style="color:#b44">     "externalSetMarkChain": "KUBE-MARK-MASQ"
</span></span></span><span style="display:flex"><span><span style="color:#b44">   }
</span></span></span><span style="display:flex"><span><span style="color:#b44"> ]
</span></span></span><span style="display:flex"><span><span style="color:#b44">}
</span></span></span><span style="display:flex"><span><span style="color:#b44">EOF</span>
</span></span></code></pre></div><p>Update the IP address ranges in the preceding example with ones that are based
on your use case and network addressing plan.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure a Pod Quota for a Namespace</h1><div class="lead">Restrict how many Pods you can create within a namespace.</div><p>This page shows how to set a quota for the total number of Pods that can run
in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="Namespace">Namespace</a>. You specify quotas in a
<a href="/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/">ResourceQuota</a>
object.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace quota-pod-example
</span></span></code></pre></div><h2 id="create-a-resourcequota">Create a ResourceQuota</h2><p>Here is an example manifest for a ResourceQuota:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-pod.yaml" download="admin/resource/quota-pod.yaml"><code>admin/resource/quota-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-pod-yaml&quot;)" title="Copy admin/resource/quota-pod.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace<span style="color:#666">=</span>quota-pod-example
</span></span></code></pre></div><p>View detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get resourcequota pod-demo --namespace<span style="color:#666">=</span>quota-pod-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output shows that the namespace has a quota of two Pods, and that currently there are
no Pods; that is, none of the quota is used.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">used</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Here is an example manifest for a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-pod-deployment.yaml" download="admin/resource/quota-pod-deployment.yaml"><code>admin/resource/quota-pod-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-pod-deployment-yaml&quot;)" title="Copy admin/resource/quota-pod-deployment.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-pod-deployment-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-quota-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">purpose</span>:<span style="color:#bbb"> </span>quota-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">purpose</span>:<span style="color:#bbb"> </span>quota-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-quota-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In that manifest, <code>replicas: 3</code> tells Kubernetes to attempt to create three new Pods, all
running the same application.</p><p>Create the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace<span style="color:#666">=</span>quota-pod-example
</span></span></code></pre></div><p>View detailed information about the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment pod-quota-demo --namespace<span style="color:#666">=</span>quota-pod-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output shows that even though the Deployment specifies three replicas, only two
Pods were created because of the quota you defined earlier:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">availableReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">lastUpdateTime</span>:<span style="color:#bbb"> </span>2021-04-02T20:57:05Z<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">message: 'unable to create pods</span>:<span style="color:#bbb"> </span>pods "pod-quota-demo-1650323038-" is forbidden:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited</span>:<span style="color:#bbb"> </span>pods=2'<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="choice-of-resource">Choice of resource</h3><p>In this task you have defined a ResourceQuota that limited the total number of Pods, but
you could also limit the total number of other kinds of object. For example, you
might decide to limit how many <a class="glossary-tooltip" title="A repeating task (a Job) that runs on a regular schedule." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/cron-jobs/" target="_blank" aria-label="CronJobs">CronJobs</a>
that can live in a single namespace.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace quota-pod-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Migrate Replicated Control Plane To Use Cloud Controller Manager</h1><p><p>The cloud-controller-manager is a Kubernetes <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p><p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p><h2 id="background">Background</h2><p>As part of the <a href="/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/">cloud provider extraction effort</a>,
all cloud specific controllers must be moved out of the <code>kube-controller-manager</code>.
All existing clusters that run cloud controllers in the <code>kube-controller-manager</code>
must migrate to instead run the controllers in a cloud provider specific
<code>cloud-controller-manager</code>.</p><p>Leader Migration provides a mechanism in which HA clusters can safely migrate "cloud
specific" controllers between the <code>kube-controller-manager</code> and the
<code>cloud-controller-manager</code> via a shared resource lock between the two components
while upgrading the replicated control plane. For a single-node control plane, or if
unavailability of controller managers can be tolerated during the upgrade, Leader
Migration is not needed and this guide can be ignored.</p><p>Leader Migration can be enabled by setting <code>--enable-leader-migration</code> on
<code>kube-controller-manager</code> or <code>cloud-controller-manager</code>. Leader Migration only
applies during the upgrade and can be safely disabled or left enabled after the
upgrade is complete.</p><p>This guide walks you through the manual process of upgrading the control plane from
<code>kube-controller-manager</code> with built-in cloud provider to running both
<code>kube-controller-manager</code> and <code>cloud-controller-manager</code>. If you use a tool to deploy
and manage the cluster, please refer to the documentation of the tool and the cloud
provider for specific instructions of the migration.</p><h2 id="before-you-begin">Before you begin</h2><p>It is assumed that the control plane is running Kubernetes version N and to be
upgraded to version N + 1. Although it is possible to migrate within the same
version, ideally the migration should be performed as part of an upgrade so that
changes of configuration can be aligned to each release. The exact versions of N and
N + 1 depend on each cloud provider. For example, if a cloud provider builds a
<code>cloud-controller-manager</code> to work with Kubernetes 1.24, then N can be 1.23 and N + 1
can be 1.24.</p><p>The control plane nodes should run <code>kube-controller-manager</code> with Leader Election
enabled, which is the default. As of version N, an in-tree cloud provider must be set
with <code>--cloud-provider</code> flag and <code>cloud-controller-manager</code> should not yet be
deployed.</p><p>The out-of-tree cloud provider must have built a <code>cloud-controller-manager</code> with
Leader Migration implementation. If the cloud provider imports
<code>k8s.io/cloud-provider</code> and <code>k8s.io/controller-manager</code> of version v0.21.0 or later,
Leader Migration will be available. However, for version before v0.22.0, Leader
Migration is alpha and requires feature gate <code>ControllerManagerLeaderMigration</code> to be
enabled in <code>cloud-controller-manager</code>.</p><p>This guide assumes that kubelet of each control plane node starts
<code>kube-controller-manager</code> and <code>cloud-controller-manager</code> as static pods defined by
their manifests. If the components run in a different setting, please adjust the
steps accordingly.</p><p>For authorization, this guide assumes that the cluster uses RBAC. If another
authorization mode grants permissions to <code>kube-controller-manager</code> and
<code>cloud-controller-manager</code> components, please grant the needed access in a way that
matches the mode.</p><h3 id="grant-access-to-migration-lease">Grant access to Migration Lease</h3><p>The default permissions of the controller manager allow only accesses to their main
Lease. In order for the migration to work, accesses to another Lease are required.</p><p>You can grant <code>kube-controller-manager</code> full access to the leases API by modifying
the <code>system::leader-locking-kube-controller-manager</code> role. This task guide assumes
that the name of the migration lease is <code>cloud-provider-extraction-migration</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch -n kube-system role <span style="color:#b44">'system::leader-locking-kube-controller-manager'</span> -p <span style="color:#b44">'{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}'</span> --type<span style="color:#666">=</span>merge<span style="color:#b44">`</span>
</span></span></code></pre></div><p>Do the same to the <code>system::leader-locking-cloud-controller-manager</code> role.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch -n kube-system role <span style="color:#b44">'system::leader-locking-cloud-controller-manager'</span> -p <span style="color:#b44">'{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}'</span> --type<span style="color:#666">=</span>merge<span style="color:#b44">`</span>
</span></span></code></pre></div><h3 id="initial-leader-migration-configuration">Initial Leader Migration configuration</h3><p>Leader Migration optionally takes a configuration file representing the state of
controller-to-manager assignment. At this moment, with in-tree cloud provider,
<code>kube-controller-manager</code> runs <code>route</code>, <code>service</code>, and <code>cloud-node-lifecycle</code>. The
following example configuration shows the assignment.</p><p>Leader Migration can be enabled without a configuration. Please see
<a href="#default-configuration">Default Configuration</a> for details.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controllerLeaders</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span></span></span></code></pre></div><p>Alternatively, because the controllers can run under either controller managers,
setting <code>component</code> to <code>*</code> for both sides makes the configuration file consistent
between both parties of the migration.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># wildcard version</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controllerLeaders</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span></code></pre></div><p>On each control plane node, save the content to <code>/etc/leadermigration.conf</code>, and
update the manifest of <code>kube-controller-manager</code> so that the file is mounted inside
the container at the same location. Also, update the same manifest to add the
following arguments:</p><ul><li><code>--enable-leader-migration</code> to enable Leader Migration on the controller manager</li><li><code>--leader-migration-config=/etc/leadermigration.conf</code> to set configuration file</li></ul><p>Restart <code>kube-controller-manager</code> on each node. At this moment,
<code>kube-controller-manager</code> has leader migration enabled and is ready for the
migration.</p><h3 id="deploy-cloud-controller-manager">Deploy Cloud Controller Manager</h3><p>In version N + 1, the desired state of controller-to-manager assignment can be
represented by a new configuration file, shown as follows. Please note <code>component</code>
field of each <code>controllerLeaders</code> changing from <code>kube-controller-manager</code> to
<code>cloud-controller-manager</code>. Alternatively, use the wildcard version mentioned above,
which has the same effect.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controllerLeaders</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span></code></pre></div><p>When creating control plane nodes of version N + 1, the content should be deployed to
<code>/etc/leadermigration.conf</code>. The manifest of <code>cloud-controller-manager</code> should be
updated to mount the configuration file in the same manner as
<code>kube-controller-manager</code> of version N. Similarly, add <code>--enable-leader-migration</code>
and <code>--leader-migration-config=/etc/leadermigration.conf</code> to the arguments of
<code>cloud-controller-manager</code>.</p><p>Create a new control plane node of version N + 1 with the updated
<code>cloud-controller-manager</code> manifest, and with the <code>--cloud-provider</code> flag set to
<code>external</code> for <code>kube-controller-manager</code>. <code>kube-controller-manager</code> of version N + 1
MUST NOT have Leader Migration enabled because, with an external cloud provider, it
does not run the migrated controllers anymore, and thus it is not involved in the
migration.</p><p>Please refer to <a href="/docs/tasks/administer-cluster/running-cloud-controller/">Cloud Controller Manager Administration</a>
for more detail on how to deploy <code>cloud-controller-manager</code>.</p><h3 id="upgrade-control-plane">Upgrade Control Plane</h3><p>The control plane now contains nodes of both version N and N + 1. The nodes of
version N run <code>kube-controller-manager</code> only, and these of version N + 1 run both
<code>kube-controller-manager</code> and <code>cloud-controller-manager</code>. The migrated controllers,
as specified in the configuration, are running under either <code>kube-controller-manager</code>
of version N or <code>cloud-controller-manager</code> of version N + 1 depending on which
controller manager holds the migration lease. No controller will ever be running
under both controller managers at any time.</p><p>In a rolling manner, create a new control plane node of version N + 1 and bring down
one of version N until the control plane contains only nodes of version N + 1.
If a rollback from version N + 1 to N is required, add nodes of version N with Leader
Migration enabled for <code>kube-controller-manager</code> back to the control plane, replacing
one of version N + 1 each time until there are only nodes of version N.</p><h3 id="disable-leader-migration">(Optional) Disable Leader Migration</h3><p>Now that the control plane has been upgraded to run both <code>kube-controller-manager</code>
and <code>cloud-controller-manager</code> of version N + 1, Leader Migration has finished its
job and can be safely disabled to save one Lease resource. It is safe to re-enable
Leader Migration for the rollback in the future.</p><p>In a rolling manager, update manifest of <code>cloud-controller-manager</code> to unset both
<code>--enable-leader-migration</code> and <code>--leader-migration-config=</code> flag, also remove the
mount of <code>/etc/leadermigration.conf</code>, and finally remove <code>/etc/leadermigration.conf</code>.
To re-enable Leader Migration, recreate the configuration file and add its mount and
the flags that enable Leader Migration back to <code>cloud-controller-manager</code>.</p><h3 id="default-configuration">Default Configuration</h3><p>Starting Kubernetes 1.22, Leader Migration provides a default configuration suitable
for the default controller-to-manager assignment.
The default configuration can be enabled by setting <code>--enable-leader-migration</code> but
without <code>--leader-migration-config=</code>.</p><p>For <code>kube-controller-manager</code> and <code>cloud-controller-manager</code>, if there are no flags
that enable any in-tree cloud provider or change ownership of controllers, the
default configuration can be used to avoid manual creation of the configuration file.</p><h3 id="node-ipam-controller-migration">Special case: migrating the Node IPAM controller</h3><p>If your cloud provider provides an implementation of Node IPAM controller, you should
switch to the implementation in <code>cloud-controller-manager</code>. Disable Node IPAM
controller in <code>kube-controller-manager</code> of version N + 1 by adding
<code>--controllers=*,-nodeipam</code> to its flags. Then add <code>nodeipam</code> to the list of migrated
controllers.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># wildcard version, with nodeipam</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controllerLeaders</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nodeipam<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:#bbb">  </span><span style="color:green;font-weight:700">component</span>:<span style="color:#bbb"> </span>*<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration">Controller Manager Leader Migration</a>
enhancement proposal.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use Calico for NetworkPolicy</h1><p>This page shows a couple of quick ways to create a Calico cluster on Kubernetes.</p><h2 id="before-you-begin">Before you begin</h2><p>Decide whether you want to deploy a <a href="#creating-a-calico-cluster-with-google-kubernetes-engine-gke">cloud</a> or <a href="#creating-a-local-calico-cluster-with-kubeadm">local</a> cluster.</p><h2 id="creating-a-calico-cluster-with-google-kubernetes-engine-gke">Creating a Calico cluster with Google Kubernetes Engine (GKE)</h2><p><strong>Prerequisite</strong>: <a href="https://cloud.google.com/sdk/docs/quickstarts">gcloud</a>.</p><ol><li><p>To launch a GKE cluster with Calico, include the <code>--enable-network-policy</code> flag.</p><p><strong>Syntax</strong></p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>gcloud container clusters create <span style="color:#666">[</span>CLUSTER_NAME<span style="color:#666">]</span> --enable-network-policy
</span></span></code></pre></div><p><strong>Example</strong></p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>gcloud container clusters create my-calico-cluster --enable-network-policy
</span></span></code></pre></div></li><li><p>To verify the deployment, use the following command.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The Calico pods begin with <code>calico</code>. Check to make sure each one has a status of <code>Running</code>.</p></li></ol><h2 id="creating-a-local-calico-cluster-with-kubeadm">Creating a local Calico cluster with kubeadm</h2><p>To get a local single-host Calico cluster in fifteen minutes using kubeadm, refer to the
<a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/">Calico Quickstart</a>.</p><h2 id="what-s-next">What's next</h2><p>Once your cluster is running, you can follow the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Default Memory Requests and Limits for a Namespace</h1><div class="lead">Define a default memory resource limit for a namespace, so that every new Pod in that namespace has a memory resource limit configured.</div><p>This page shows how to configure default memory requests and limits for a
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>.</p><p>A Kubernetes cluster can be divided into namespaces. Once you have a namespace that
has a default memory
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">limit</a>,
and you then try to create a Pod with a container that does not specify its own memory
limit, then the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> assigns the default
memory limit to that container.</p><p>Kubernetes assigns a default memory request under certain conditions that are explained later in this topic.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 2 GiB of memory.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace default-mem-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's a manifest for an example <a class="glossary-tooltip" title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." data-toggle="tooltip" data-placement="top" href="/docs/concepts/policy/limit-range/" target="_blank" aria-label="LimitRange">LimitRange</a>.
The manifest specifies a default memory
request and a default memory limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults.yaml" download="admin/resource/memory-defaults.yaml"><code>admin/resource/memory-defaults.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-defaults-yaml&quot;)" title="Copy admin/resource/memory-defaults.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-defaults-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mem-limit-range<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">default</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>512Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">defaultRequest</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>256Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the LimitRange in the default-mem-example namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>Now if you create a Pod in the default-mem-example namespace, and any container
within that Pod does not specify its own values for memory request and memory limit,
then the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>
applies default values: a memory request of 256MiB and a memory limit of 512MiB.</p><p>Here's an example manifest for a Pod that has one container. The container
does not specify a memory request and limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults-pod.yaml" download="admin/resource/memory-defaults-pod.yaml"><code>admin/resource/memory-defaults-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-defaults-pod-yaml&quot;)" title="Copy admin/resource/memory-defaults-pod.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-defaults-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-mem-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-mem-demo-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod default-mem-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>The output shows that the Pod's container has a memory request of 256 MiB and
a memory limit of 512 MiB. These are the default values specified by the LimitRange.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>containers:
</span></span><span style="display:flex"><span>- image: nginx
</span></span><span style="display:flex"><span>  imagePullPolicy: Always
</span></span><span style="display:flex"><span>  name: default-mem-demo-ctr
</span></span><span style="display:flex"><span>  resources:
</span></span><span style="display:flex"><span>    limits:
</span></span><span style="display:flex"><span>      memory: 512Mi
</span></span><span style="display:flex"><span>    requests:
</span></span><span style="display:flex"><span>      memory: 256Mi
</span></span></code></pre></div><p>Delete your Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete pod default-mem-demo --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><h2 id="what-if-you-specify-a-container-s-limit-but-not-its-request">What if you specify a container's limit, but not its request?</h2><p>Here's a manifest for a Pod that has one container. The container
specifies a memory limit, but not a request:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults-pod-2.yaml" download="admin/resource/memory-defaults-pod-2.yaml"><code>admin/resource/memory-defaults-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-defaults-pod-2-yaml&quot;)" title="Copy admin/resource/memory-defaults-pod-2.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-defaults-pod-2-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-mem-demo-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-mem-demo-2-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1Gi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>View detailed information about the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod default-mem-demo-2 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>The output shows that the container's memory request is set to match its memory limit.
Notice that the container was not assigned the default memory request value of 256Mi.</p><pre tabindex="0"><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><h2 id="what-if-you-specify-a-container-s-request-but-not-its-limit">What if you specify a container's request, but not its limit?</h2><p>Here's a manifest for a Pod that has one container. The container
specifies a memory request, but not a limit:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/memory-defaults-pod-3.yaml" download="admin/resource/memory-defaults-pod-3.yaml"><code>admin/resource/memory-defaults-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-memory-defaults-pod-3-yaml&quot;)" title="Copy admin/resource/memory-defaults-pod-3.yaml to clipboard"/></div><div class="includecode" id="admin-resource-memory-defaults-pod-3-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-mem-demo-3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-mem-demo-3-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"128Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>View the Pod's specification:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod default-mem-demo-3 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</span></span></code></pre></div><p>The output shows that the container's memory request is set to the value specified in the
container's manifest. The container is limited to use no more than 512MiB of
memory, which matches the default memory limit for the namespace.</p><pre tabindex="0"><code>resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A <code>LimitRange</code> does <strong>not</strong> check the consistency of the default values it applies. This means that a default value for the <em>limit</em> that is set by <code>LimitRange</code> may be less than the <em>request</em> value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be scheduleable.
See <a href="/docs/concepts/policy/limit-range/#constraints-on-resource-limits-and-requests">Constraints on resource limits and requests</a> for more details.</div><h2 id="motivation-for-default-memory-limits-and-requests">Motivation for default memory limits and requests</h2><p>If your namespace has a memory <a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." data-toggle="tooltip" data-placement="top" href="/docs/concepts/policy/resource-quotas/" target="_blank" aria-label="resource quota">resource quota</a>
configured,
it is helpful to have a default value in place for memory limit.
Here are three of the restrictions that a resource quota imposes on a namespace:</p><ul><li>For every Pod that runs in the namespace, the Pod and each of its containers must have a memory limit.
(If you specify a memory limit for every container in a Pod, Kubernetes can infer the Pod-level memory
limit by adding up the limits for its containers).</li><li>Memory limits apply a resource reservation on the node where the Pod in question is scheduled.
The total amount of memory reserved for all Pods in the namespace must not exceed a specified limit.</li><li>The total amount of memory actually used by all Pods in the namespace must also not exceed a specified limit.</li></ul><p>When you add a LimitRange:</p><p>If any Pod in that namespace that includes a container does not specify its own memory limit,
the control plane applies the default memory limit to that container, and the Pod can be
allowed to run in a namespace that is restricted by a memory ResourceQuota.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace default-mem-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Certificate Management with kubeadm</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.15 [stable]</code></div><p>Client certificates generated by <a href="/docs/reference/setup-tools/kubeadm/">kubeadm</a> expire after 1 year.
This page explains how to manage certificate renewals with kubeadm. It also covers other tasks related
to kubeadm certificate management.</p><p>The Kubernetes project recommends upgrading to the latest patch releases promptly, and
to ensure that you are running a supported minor release of Kubernetes.
Following this recommendation helps you to stay secure.</p><h2 id="before-you-begin">Before you begin</h2><p>You should be familiar with <a href="/docs/setup/best-practices/certificates/">PKI certificates and requirements in Kubernetes</a>.</p><p>You should be familiar with how to pass a <a href="/docs/reference/config-api/kubeadm-config.v1beta4/">configuration</a> file to the kubeadm commands.</p><p>This guide covers the usage of the <code>openssl</code> command (used for manual certificate signing,
if you choose that approach), but you can use your preferred tools.</p><p>Some of the steps here use <code>sudo</code> for administrator access. You can use any equivalent tool.</p><h2 id="custom-certificates">Using custom certificates</h2><p>By default, kubeadm generates all the certificates needed for a cluster to run.
You can override this behavior by providing your own certificates.</p><p>To do so, you must place them in whatever directory is specified by the
<code>--cert-dir</code> flag or the <code>certificatesDir</code> field of kubeadm's <code>ClusterConfiguration</code>.
By default this is <code>/etc/kubernetes/pki</code>.</p><p>If a given certificate and private key pair exists before running <code>kubeadm init</code>,
kubeadm does not overwrite them. This means you can, for example, copy an existing
CA into <code>/etc/kubernetes/pki/ca.crt</code> and <code>/etc/kubernetes/pki/ca.key</code>,
and kubeadm will use this CA for signing the rest of the certificates.</p><h2 id="choosing-encryption-algorithm">Choosing an encryption algorithm</h2><p>kubeadm allows you to choose an encryption algorithm that is used for creating
public and private keys. That can be done by using the <code>encryptionAlgorithm</code> field of the
kubeadm configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">encryptionAlgorithm</span>:<span style="color:#bbb"> </span>&lt;ALGORITHM&gt;<span style="color:#bbb">
</span></span></span></code></pre></div><p><code>&lt;ALGORITHM&gt;</code> can be one of <code>RSA-2048</code> (default), <code>RSA-3072</code>, <code>RSA-4096</code> or <code>ECDSA-P256</code>.</p><h2 id="choosing-cert-validity-period">Choosing certificate validity period</h2><p>kubeadm allows you to choose the validity period of CA and leaf certificates.
That can be done by using the <code>certificateValidityPeriod</code> and <code>caCertificateValidityPeriod</code>
fields of the kubeadm configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">certificateValidityPeriod: 8760h # Default</span>:<span style="color:#bbb"> </span><span style="color:#666">365</span><span style="color:#bbb"> </span>days × 24 hours = 1 year<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">caCertificateValidityPeriod: 87600h # Default</span>:<span style="color:#bbb"> </span><span style="color:#666">365</span><span style="color:#bbb"> </span>days × 24 hours * 10 = 10 years<span style="color:#bbb">
</span></span></span></code></pre></div><p>The values of the fields follow the accepted format for
<a href="https://pkg.go.dev/time#ParseDuration">Go's <code>time.Duration</code> values</a>, with the longest supported
unit being <code>h</code> (hours).</p><h2 id="external-ca-mode">External CA mode</h2><p>It is also possible to provide only the <code>ca.crt</code> file and not the
<code>ca.key</code> file (this is only available for the root CA file, not other cert pairs).
If all other certificates and kubeconfig files are in place, kubeadm recognizes
this condition and activates the "External CA" mode. kubeadm will proceed without the
CA key on disk.</p><p>Instead, run the controller-manager standalone with <code>--controllers=csrsigner</code> and
point to the CA certificate and key.</p><p>There are various ways to prepare the component credentials when using external CA mode.</p><h3 id="manual-preparation-of-component-credentials">Manual preparation of component credentials</h3><p><a href="/docs/setup/best-practices/certificates/">PKI certificates and requirements</a> includes information
on how to prepare all the required by kubeadm component credentials manually.</p><p>This guide covers the usage of the <code>openssl</code> command (used for manual certificate signing,
if you choose that approach), but you can use your preferred tools.</p><h3 id="preparation-of-credentials-by-signing-csrs-generated-by-kubeadm">Preparation of credentials by signing CSRs generated by kubeadm</h3><p>kubeadm can <a href="#signing-csr">generate CSR files</a> that you can sign manually with tools like
<code>openssl</code> and your external CA. These CSR files will include all the specification for credentials
that components deployed by kubeadm require.</p><h3 id="automated-preparation-of-component-credentials-by-using-kubeadm-phases">Automated preparation of component credentials by using kubeadm phases</h3><p>Alternatively, it is possible to use kubeadm phase commands to automate this process.</p><ul><li>Go to a host that you want to prepare as a kubeadm control plane node with external CA.</li><li>Copy the external CA files <code>ca.crt</code> and <code>ca.key</code> that you have into <code>/etc/kubernetes/pki</code> on the node.</li><li>Prepare a temporary <a href="/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file">kubeadm configuration file</a>
called <code>config.yaml</code> that can be used with <code>kubeadm init</code>. Make sure that this file includes
any relevant cluster wide or host-specific information that could be included in certificates, such as,
<code>ClusterConfiguration.controlPlaneEndpoint</code>, <code>ClusterConfiguration.certSANs</code> and <code>InitConfiguration.APIEndpoint</code>.</li><li>On the same host execute the commands <code>kubeadm init phase kubeconfig all --config config.yaml</code> and
<code>kubeadm init phase certs all --config config.yaml</code>. This will generate all required kubeconfig
files and certificates under <code>/etc/kubernetes/</code> and its <code>pki</code> sub directory.</li><li>Inspect the generated files. Delete <code>/etc/kubernetes/pki/ca.key</code>, delete or move to a safe location
the file <code>/etc/kubernetes/super-admin.conf</code>.</li><li>On nodes where <code>kubeadm join</code> will be called also delete <code>/etc/kubernetes/kubelet.conf</code>.
This file is only required on the first node where <code>kubeadm init</code> will be called.</li><li>Note that some files such <code>pki/sa.*</code>, <code>pki/front-proxy-ca.*</code> and <code>pki/etc/ca.*</code> are
shared between control plane nodes, You can generate them once and
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/#manual-certs">distribute them manually</a>
to nodes where <code>kubeadm join</code> will be called, or you can use the
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes"><code>--upload-certs</code></a>
functionality of <code>kubeadm init</code> and <code>--certificate-key</code> of <code>kubeadm join</code> to automate this distribution.</li></ul><p>Once the credentials are prepared on all nodes, call <code>kubeadm init</code> and <code>kubeadm join</code> for these nodes to
join the cluster. kubeadm will use the existing kubeconfig and certificate files under <code>/etc/kubernetes/</code>
and its <code>pki</code> sub directory.</p><h2 id="check-certificate-expiration">Certificate expiry and management</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>kubeadm</code> cannot manage certificates signed by an external CA.</div><p>You can use the <code>check-expiration</code> subcommand to check when certificates expire:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubeadm certs check-expiration
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
</span></span></span><span style="display:flex"><span><span style="color:#888">admin.conf                 Dec 30, 2020 23:36 UTC   364d                                    no
</span></span></span><span style="display:flex"><span><span style="color:#888">apiserver                  Dec 30, 2020 23:36 UTC   364d            ca                      no
</span></span></span><span style="display:flex"><span><span style="color:#888">apiserver-etcd-client      Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span style="display:flex"><span><span style="color:#888">apiserver-kubelet-client   Dec 30, 2020 23:36 UTC   364d            ca                      no
</span></span></span><span style="display:flex"><span><span style="color:#888">controller-manager.conf    Dec 30, 2020 23:36 UTC   364d                                    no
</span></span></span><span style="display:flex"><span><span style="color:#888">etcd-healthcheck-client    Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span style="display:flex"><span><span style="color:#888">etcd-peer                  Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span style="display:flex"><span><span style="color:#888">etcd-server                Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
</span></span></span><span style="display:flex"><span><span style="color:#888">front-proxy-client         Dec 30, 2020 23:36 UTC   364d            front-proxy-ca          no
</span></span></span><span style="display:flex"><span><span style="color:#888">scheduler.conf             Dec 30, 2020 23:36 UTC   364d                                    no
</span></span></span><span style="display:flex"><span><span style="color:#888"/><span>
</span></span></span><span style="display:flex"><span><span/><span style="color:#888">CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
</span></span></span><span style="display:flex"><span><span style="color:#888">ca                      Dec 28, 2029 23:36 UTC   9y              no
</span></span></span><span style="display:flex"><span><span style="color:#888">etcd-ca                 Dec 28, 2029 23:36 UTC   9y              no
</span></span></span><span style="display:flex"><span><span style="color:#888">front-proxy-ca          Dec 28, 2029 23:36 UTC   9y              no
</span></span></span></code></pre></div><p>The command shows expiration/residual time for the client certificates in the
<code>/etc/kubernetes/pki</code> folder and for the client certificate embedded in the kubeconfig files used
by kubeadm (<code>admin.conf</code>, <code>controller-manager.conf</code> and <code>scheduler.conf</code>).</p><p>Additionally, kubeadm informs the user if the certificate is externally managed; in this case, the
user should take care of managing certificate renewal manually/using other tools.</p><p>The <code>kubelet.conf</code> configuration file is not included in the list above because kubeadm
configures kubelet
for <a href="/docs/tasks/tls/certificate-rotation/">automatic certificate renewal</a>
with rotatable certificates under <code>/var/lib/kubelet/pki</code>.
To repair an expired kubelet client certificate see
<a href="/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert">Kubelet client certificate rotation fails</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>On nodes created with <code>kubeadm init</code> from versions prior to kubeadm version 1.17, there is a
<a href="https://github.com/kubernetes/kubeadm/issues/1753">bug</a> where you manually have to modify the
contents of <code>kubelet.conf</code>. After <code>kubeadm init</code> finishes, you should update <code>kubelet.conf</code> to
point to the rotated kubelet client certificates, by replacing <code>client-certificate-data</code> and
<code>client-key-data</code> with:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">client-certificate</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">client-key</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span></span></span></code></pre></div></div><h2 id="automatic-certificate-renewal">Automatic certificate renewal</h2><p>kubeadm renews all the certificates during control plane
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">upgrade</a>.</p><p>This feature is designed for addressing the simplest use cases;
if you don't have specific requirements on certificate renewal and perform Kubernetes version
upgrades regularly (less than 1 year in between each upgrade), kubeadm will take care of keeping
your cluster up to date and reasonably secure.</p><p>If you have more complex requirements for certificate renewal, you can opt out from the default
behavior by passing <code>--certificate-renewal=false</code> to <code>kubeadm upgrade apply</code> or to <code>kubeadm upgrade node</code>.</p><h2 id="manual-certificate-renewal">Manual certificate renewal</h2><p>You can renew your certificates manually at any time with the <code>kubeadm certs renew</code> command,
with the appropriate command line options. If you are running cluster with a replicated control
plane, this command needs to be executed on all the control-plane nodes.</p><p>This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in <code>/etc/kubernetes/pki</code>.</p><p><code>kubeadm certs renew</code> uses the existing certificates as the authoritative source for attributes
(Common Name, Organization, subject alternative name) and does not rely on the <code>kubeadm-config</code>
ConfigMap.
Even so, the Kubernetes project recommends keeping the served certificate and the associated
values in that ConfigMap synchronized, to avoid any risk of confusion.</p><p>After running the command you should restart the control plane Pods. This is required since
dynamic certificate reload is currently not supported for all components and certificates.
<a href="/docs/tasks/configure-pod-container/static-pod/">Static Pods</a> are managed by the local kubelet
and not by the API Server, thus kubectl cannot be used to delete and restart them.
To restart a static Pod you can temporarily remove its manifest file from <code>/etc/kubernetes/manifests/</code>
and wait for 20 seconds (see the <code>fileCheckFrequency</code> value in <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration struct</a>).
The kubelet will terminate the Pod if it's no longer in the manifest directory.
You can then move the file back and after another <code>fileCheckFrequency</code> period, the kubelet will recreate
the Pod and the certificate renewal for the component can complete.</p><p><code>kubeadm certs renew</code> can renew any specific certificate or, with the subcommand <code>all</code>, it can renew all of them:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># If you are running cluster with a replicated control plane, this command</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># needs to be executed on all the control-plane nodes.</span>
</span></span><span style="display:flex"><span>kubeadm certs renew all
</span></span></code></pre></div><h3 id="admin-certificate-copy">Copying the administrator certificate (optional)</h3><p>Clusters built with kubeadm often copy the <code>admin.conf</code> certificate into
<code>$HOME/.kube/config</code>, as instructed in <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>.
On such a system, to update the contents of <code>$HOME/.kube/config</code>
after renewing the <code>admin.conf</code>, you could run the following commands:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo cp -i /etc/kubernetes/admin.conf <span style="color:#b8860b">$HOME</span>/.kube/config
</span></span><span style="display:flex"><span>sudo chown <span style="color:#a2f;font-weight:700">$(</span>id -u<span style="color:#a2f;font-weight:700">)</span>:<span style="color:#a2f;font-weight:700">$(</span>id -g<span style="color:#a2f;font-weight:700">)</span> <span style="color:#b8860b">$HOME</span>/.kube/config
</span></span></code></pre></div><h2 id="renew-certificates-with-the-kubernetes-certificates-api">Renew certificates with the Kubernetes certificates API</h2><p>This section provides more details about how to execute manual certificate renewal using the Kubernetes certificates API.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>These are advanced topics for users who need to integrate their organization's certificate
infrastructure into a kubeadm-built cluster. If the default kubeadm configuration satisfies your
needs, you should let kubeadm manage certificates instead.</div><h3 id="set-up-a-signer">Set up a signer</h3><p>The Kubernetes Certificate Authority does not work out of the box.
You can configure an external signer such as <a href="https://cert-manager.io/docs/configuration/ca/">cert-manager</a>,
or you can use the built-in signer.</p><p>The built-in signer is part of <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a>.</p><p>To activate the built-in signer, you must pass the <code>--cluster-signing-cert-file</code> and
<code>--cluster-signing-key-file</code> flags.</p><p>If you're creating a new cluster, you can use a kubeadm
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/">configuration file</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controllerManager</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">extraArgs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"cluster-signing-cert-file"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/etc/kubernetes/pki/ca.crt"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"cluster-signing-key-file"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/etc/kubernetes/pki/ca.key"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="create-certificate-signing-requests-csr">Create certificate signing requests (CSR)</h3><p>See <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest">Create CertificateSigningRequest</a>
for creating CSRs with the Kubernetes API.</p><h2 id="renew-certificates-with-external-ca">Renew certificates with external CA</h2><p>This section provide more details about how to execute manual certificate renewal using an external CA.</p><p>To better integrate with external CAs, kubeadm can also produce certificate signing requests (CSRs).
A CSR represents a request to a CA for a signed certificate for a client.
In kubeadm terms, any certificate that would normally be signed by an on-disk CA can be produced
as a CSR instead. A CA, however, cannot be produced as a CSR.</p><h3 id="renewal-by-using-certificate-signing-requests-csr">Renewal by using certificate signing requests (CSR)</h3><p>Renewal of ceritficates is possible by generating new CSRs and signing them with the external CA.
For more details about working with CSRs generated by kubeadm see the section
<a href="#signing-csr">Signing certificate signing requests (CSR) generated by kubeadm</a>.</p><h2 id="certificate-authority-rotation">Certificate authority (CA) rotation</h2><p>Kubeadm does not support rotation or replacement of CA certificates out of the box.</p><p>For more information about manual rotation or replacement of CA, see <a href="/docs/tasks/tls/manual-rotation-of-ca-certificates/">manual rotation of CA certificates</a>.</p><h2 id="kubelet-serving-certs">Enabling signed kubelet serving certificates</h2><p>By default the kubelet serving certificate deployed by kubeadm is self-signed.
This means a connection from external services like the
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a> to a
kubelet cannot be secured with TLS.</p><p>To configure the kubelets in a new kubeadm cluster to obtain properly signed serving
certificates you must pass the following minimal configuration to <code>kubeadm init</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">serverTLSBootstrap</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>If you have already created the cluster you must adapt it by doing the following:</p><ul><li>Find and edit the <code>kubelet-config</code> ConfigMap in the <code>kube-system</code> namespace.
In that ConfigMap, the <code>kubelet</code> key has a
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration</a>
document as its value. Edit the KubeletConfiguration document to set <code>serverTLSBootstrap: true</code>.</li><li>On each node, add the <code>serverTLSBootstrap: true</code> field in <code>/var/lib/kubelet/config.yaml</code>
and restart the kubelet with <code>systemctl restart kubelet</code></li></ul><p>The field <code>serverTLSBootstrap: true</code> will enable the bootstrap of kubelet serving
certificates by requesting them from the <code>certificates.k8s.io</code> API. One known limitation
is that the CSRs (Certificate Signing Requests) for these certificates cannot be automatically
approved by the default signer in the kube-controller-manager -
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers"><code>kubernetes.io/kubelet-serving</code></a>.
This will require action from the user or a third party controller.</p><p>These CSRs can be viewed using:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get csr
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME        AGE     SIGNERNAME                        REQUESTOR                      CONDITION
</span></span></span><span style="display:flex"><span><span style="color:#888">csr-9wvgt   112s    kubernetes.io/kubelet-serving     system:node:worker-1           Pending
</span></span></span><span style="display:flex"><span><span style="color:#888">csr-lz97v   1m58s   kubernetes.io/kubelet-serving     system:node:control-plane-1    Pending
</span></span></span></code></pre></div><p>To approve them you can do the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl certificate approve &lt;CSR-name&gt;
</span></span></code></pre></div><p>By default, these serving certificate will expire after one year. Kubeadm sets the
<code>KubeletConfiguration</code> field <code>rotateCertificates</code> to <code>true</code>, which means that close
to expiration a new set of CSRs for the serving certificates will be created and must
be approved to complete the rotation. To understand more see
<a href="/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#certificate-rotation">Certificate Rotation</a>.</p><p>If you are looking for a solution for automatic approval of these CSRs it is recommended
that you contact your cloud provider and ask if they have a CSR signer that verifies
the node identity with an out of band mechanism.</p><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Third party custom controllers can be used:</p><ul><li><a href="https://github.com/postfinance/kubelet-csr-approver">kubelet-csr-approver</a></li></ul><p>Such a controller is not a secure mechanism unless it not only verifies the CommonName
in the CSR but also verifies the requested IPs and domain names. This would prevent
a malicious actor that has access to a kubelet client certificate to create
CSRs requesting serving certificates for any IP or domain name.</p><h2 id="kubeconfig-additional-users">Generating kubeconfig files for additional users</h2><p>During cluster creation, <code>kubeadm init</code> signs the certificate in the <code>super-admin.conf</code>
to have <code>Subject: O = system:masters, CN = kubernetes-super-admin</code>.
<a href="/docs/reference/access-authn-authz/rbac/#user-facing-roles"><code>system:masters</code></a>
is a break-glass, super user group that bypasses the authorization layer (for example,
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>). The file <code>admin.conf</code> is also created
by kubeadm on control plane nodes and it contains a certificate with
<code>Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin</code>. <code>kubeadm:cluster-admins</code>
is a group logically belonging to kubeadm. If your cluster uses RBAC
(the kubeadm default), the <code>kubeadm:cluster-admins</code> group is bound to the
<a href="/docs/reference/access-authn-authz/rbac/#user-facing-roles"><code>cluster-admin</code></a> ClusterRole.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Avoid sharing the <code>super-admin.conf</code> or <code>admin.conf</code> files. Instead, create least
privileged access even for people who work as administrators and use that least
privilege alternative for anything other than break-glass (emergency) access.</div><p>You can use the <a href="/docs/reference/setup-tools/kubeadm/kubeadm-kubeconfig/"><code>kubeadm kubeconfig user</code></a>
command to generate kubeconfig files for additional users.
The command accepts a mixture of command line flags and
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/">kubeadm configuration</a> options.
The generated kubeconfig will be written to stdout and can be piped to a file using
<code>kubeadm kubeconfig user ... &gt; somefile.conf</code>.</p><p>Example configuration file that can be used with <code>--config</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># example.yaml</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Will be used as the target "cluster" in the kubeconfig</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">clusterName</span>:<span style="color:#bbb"> </span><span style="color:#b44">"kubernetes"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Will be used as the "server" (IP or DNS name) of this cluster in the kubeconfig</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">controlPlaneEndpoint</span>:<span style="color:#bbb"> </span><span style="color:#b44">"some-dns-address:6443"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># The cluster CA key and certificate will be loaded from this local directory</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">certificatesDir</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/etc/kubernetes/pki"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Make sure that these settings match the desired target cluster settings.
To see the settings of an existing cluster use:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get cm kubeadm-config -n kube-system -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">"{.data.ClusterConfiguration}"</span>
</span></span></code></pre></div><p>The following example will generate a kubeconfig file with credentials valid for 24 hours
for a new user <code>johndoe</code> that is part of the <code>appdevs</code> group:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubeadm kubeconfig user --config example.yaml --org appdevs --client-name johndoe --validity-period 24h
</span></span></code></pre></div><p>The following example will generate a kubeconfig file with administrator credentials valid for 1 week:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubeadm kubeconfig user --config example.yaml --client-name admin --validity-period 168h
</span></span></code></pre></div><h2 id="signing-csr">Signing certificate signing requests (CSR) generated by kubeadm</h2><p>You can create certificate signing requests with <code>kubeadm certs generate-csr</code>.
Calling this command will generate <code>.csr</code> / <code>.key</code> file pairs for regular
certificates. For certificates embedded in kubeconfig files, the command will
generate a <code>.csr</code> / <code>.conf</code> pair where the key is already embedded in the <code>.conf</code> file.</p><p>A CSR file contains all relevant information for a CA to sign a certificate.
kubeadm uses a
<a href="/docs/setup/best-practices/certificates/#all-certificates">well defined specification</a>
for all its certificates and CSRs.</p><p>The default certificate directory is <code>/etc/kubernetes/pki</code>, while the default
directory for kubeconfig files is <code>/etc/kubernetes</code>. These defaults can be
overridden with the flags <code>--cert-dir</code> and <code>--kubeconfig-dir</code>, respectively.</p><p>To pass custom options to <code>kubeadm certs generate-csr</code> use the <code>--config</code> flag,
which accepts a <a href="/docs/reference/config-api/kubeadm-config.v1beta4/">kubeadm configuration</a>
file, similarly to commands such as <code>kubeadm init</code>. Any specification such
as extra SANs and custom IP addresses must be stored in the same configuration
file and used for all relevant kubeadm commands by passing it as <code>--config</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>This guide uses the default Kubernetes directory <code>/etc/kubernetes</code>, which requires
a super user. If you are following this guide and are using directories that you can
write to (typically, this means running <code>kubeadm</code> with <code>--cert-dir</code> and <code>--kubeconfig-dir</code>)
then you can omit the <code>sudo</code> command.</p><p>You must then copy the files that you produced over to within the <code>/etc/kubernetes</code>
directory so that <code>kubeadm init</code> or <code>kubeadm join</code> will find them.</p></div><h3 id="preparing-ca-and-service-account-files">Preparing CA and service account files</h3><p>On the primary control plane node, where <code>kubeadm init</code> will be executed, call the following
commands:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm init phase certs ca
</span></span><span style="display:flex"><span>sudo kubeadm init phase certs etcd-ca
</span></span><span style="display:flex"><span>sudo kubeadm init phase certs front-proxy-ca
</span></span><span style="display:flex"><span>sudo kubeadm init phase certs sa
</span></span></code></pre></div><p>This will populate the folders <code>/etc/kubernetes/pki</code> and <code>/etc/kubernetes/pki/etcd</code>
with all self-signed CA files (certificates and keys) and service account (public and
private keys) that kubeadm needs for a control plane node.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you are using an external CA, you must generate the same files out of band and manually
copy them to the primary control plane node in <code>/etc/kubernetes</code>.</p><p>Once all CSRs are signed, you can delete the root CA key (<code>ca.key</code>) as noted in the
<a href="#external-ca-mode">External CA mode</a> section.</p></div><p>For secondary control plane nodes (<code>kubeadm join --control-plane</code>) there is no need to call
the above commands. Depending on how you setup the
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/">High Availability</a>
cluster, you either have to manually copy the same files from the primary
control plane node, or use the automated <code>--upload-certs</code> functionality of <code>kubeadm init</code>.</p><h3 id="generate-csrs">Generate CSRs</h3><p>The <code>kubeadm certs generate-csr</code> command generates CSRs for all known certificates
managed by kubeadm. Once the command is done you must manually delete <code>.csr</code>, <code>.conf</code>
or <code>.key</code> files that you don't need.</p><h4 id="considerations-kubelet-conf">Considerations for kubelet.conf</h4><p>This section applies to both control plane and worker nodes.</p><p>If you have deleted the <code>ca.key</code> file from control plane nodes
(<a href="#external-ca-mode">External CA mode</a>), the active kube-controller-manager in
this cluster will not be able to sign kubelet client certificates. If no external
method for signing these certificates exists in your setup (such as an
<a href="#set-up-a-signer">external signer</a>), you could manually sign the <code>kubelet.conf.csr</code>
as explained in this guide.</p><p>Note that this also means that the automatic
<a href="/docs/tasks/tls/certificate-rotation/#enabling-client-certificate-rotation">kubelet client certificate rotation</a>
will be disabled. If so, close to certificate expiration, you must generate
a new <code>kubelet.conf.csr</code>, sign the certificate, embed it in <code>kubelet.conf</code>
and restart the kubelet.</p><p>If this does not apply to your setup, you can skip processing the <code>kubelet.conf.csr</code>
on secondary control plane and on workers nodes (all nodes that call <code>kubeadm join ...</code>).
That is because the active kube-controller-manager will be responsible
for signing new kubelet client certificates.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must process the <code>kubelet.conf.csr</code> file on the primary control plane node
(the host where you originally ran <code>kubeadm init</code>). This is because <code>kubeadm</code>
considers that as the node that bootstraps the cluster, and a pre-populated
<code>kubelet.conf</code> is needed.</div><h4 id="control-plane-nodes">Control plane nodes</h4><p>Execute the following command on primary (<code>kubeadm init</code>) and secondary
(<code>kubeadm join --control-plane</code>) control plane nodes to generate all CSR files:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm certs generate-csr
</span></span></code></pre></div><p>If external etcd is to be used, follow the
<a href="/docs/setup/production-environment/tools/kubeadm/high-availability/#external-etcd-nodes">External etcd with kubeadm</a>
guide to understand what CSR files are needed on the kubeadm and etcd nodes. Other
<code>.csr</code> and <code>.key</code> files under <code>/etc/kubernetes/pki/etcd</code> can be removed.</p><p>Based on the explanation in
<a href="#considerations-kubelet-conf">Considerations for kubelet.conf</a> keep or delete
the <code>kubelet.conf</code> and <code>kubelet.conf.csr</code> files.</p><h4 id="worker-nodes">Worker nodes</h4><p>Based on the explanation in
<a href="#considerations-kubelet-conf">Considerations for kubelet.conf</a>, optionally call:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm certs generate-csr
</span></span></code></pre></div><p>and keep only the <code>kubelet.conf</code> and <code>kubelet.conf.csr</code> files. Alternatively skip
the steps for worker nodes entirely.</p><h3 id="signing-csrs-for-all-certificates">Signing CSRs for all certificates</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you are using external CA and already have CA serial number files (<code>.srl</code>) for
<code>openssl</code>, you can copy such files to a kubeadm node where CSRs will be processed.
The <code>.srl</code> files to copy are <code>/etc/kubernetes/pki/ca.srl</code>,
<code>/etc/kubernetes/pki/front-proxy-ca.srl</code> and <code>/etc/kubernetes/pki/etcd/ca.srl</code>.
The files can be then moved to a new node where CSR files will be processed.</p><p>If a <code>.srl</code> file is missing for a CA on a node, the script below will generate a new SRL file
with a random starting serial number.</p><p>To read more about <code>.srl</code> files see the
<a href="https://www.openssl.org/docs/man3.0/man1/openssl-x509.html"><code>openssl</code></a>
documentation for the <code>--CAserial</code> flag.</p></div><p>Repeat this step for all nodes that have CSR files.</p><p>Write the following script in the <code>/etc/kubernetes</code> directory, navigate to the directory
and execute the script. The script will generate certificates for all CSR files that are
present in the <code>/etc/kubernetes</code> tree.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080">#!/bin/bash
</span></span></span><span style="display:flex"><span><span style="color:#080"/>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Set certificate expiration time in days</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">DAYS</span><span style="color:#666">=</span><span style="color:#666">365</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Process all CSR files except those for front-proxy and etcd</span>
</span></span><span style="display:flex"><span>find ./ -name <span style="color:#b44">"*.csr"</span> | grep -v <span style="color:#b44">"pki/etcd"</span> | grep -v <span style="color:#b44">"front-proxy"</span> | <span style="color:#a2f;font-weight:700">while</span> <span style="color:#a2f">read</span> -r FILE;
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f">echo</span> <span style="color:#b44">"* Processing </span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44"> ..."</span>
</span></span><span style="display:flex"><span>    <span style="color:#b8860b">FILE</span><span style="color:#666">=</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span>%.*<span style="color:#b68;font-weight:700">}</span> <span style="color:#080;font-style:italic"># Trim the extension</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">if</span> <span style="color:#666">[</span> -f <span style="color:#b44">"./pki/ca.srl"</span> <span style="color:#666">]</span>; <span style="color:#a2f;font-weight:700">then</span>
</span></span><span style="display:flex"><span>        <span style="color:#b8860b">SERIAL_FLAG</span><span style="color:#666">=</span><span style="color:#b44">"-CAserial ./pki/ca.srl"</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">else</span>
</span></span><span style="display:flex"><span>        <span style="color:#b8860b">SERIAL_FLAG</span><span style="color:#666">=</span><span style="color:#b44">"-CAcreateserial"</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">fi</span>
</span></span><span style="display:flex"><span>    openssl x509 -req -days <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">DAYS</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> -CA ./pki/ca.crt -CAkey ./pki/ca.key <span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">SERIAL_FLAG</span><span style="color:#b68;font-weight:700">}</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>        -in <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">.csr"</span> -out <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">.crt"</span>
</span></span><span style="display:flex"><span>    sleep <span style="color:#666">2</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Process all etcd CSRs</span>
</span></span><span style="display:flex"><span>find ./pki/etcd -name <span style="color:#b44">"*.csr"</span> | <span style="color:#a2f;font-weight:700">while</span> <span style="color:#a2f">read</span> -r FILE;
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f">echo</span> <span style="color:#b44">"* Processing </span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44"> ..."</span>
</span></span><span style="display:flex"><span>    <span style="color:#b8860b">FILE</span><span style="color:#666">=</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span>%.*<span style="color:#b68;font-weight:700">}</span> <span style="color:#080;font-style:italic"># Trim the extension</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">if</span> <span style="color:#666">[</span> -f <span style="color:#b44">"./pki/etcd/ca.srl"</span> <span style="color:#666">]</span>; <span style="color:#a2f;font-weight:700">then</span>
</span></span><span style="display:flex"><span>        <span style="color:#b8860b">SERIAL_FLAG</span><span style="color:#666">=</span>-CAserial ./pki/etcd/ca.srl
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">else</span>
</span></span><span style="display:flex"><span>        <span style="color:#b8860b">SERIAL_FLAG</span><span style="color:#666">=</span>-CAcreateserial
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">fi</span>
</span></span><span style="display:flex"><span>    openssl x509 -req -days <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">DAYS</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> -CA ./pki/etcd/ca.crt -CAkey ./pki/etcd/ca.key <span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">SERIAL_FLAG</span><span style="color:#b68;font-weight:700">}</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>        -in <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">.csr"</span> -out <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">.crt"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Process front-proxy CSRs</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b44">"* Processing ./pki/front-proxy-client.csr ..."</span>
</span></span><span style="display:flex"><span>openssl x509 -req -days <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">DAYS</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> -CA ./pki/front-proxy-ca.crt -CAkey ./pki/front-proxy-ca.key -CAcreateserial <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -in ./pki/front-proxy-client.csr -out ./pki/front-proxy-client.crt
</span></span></code></pre></div><h3 id="embedding-certificates-in-kubeconfig-files">Embedding certificates in kubeconfig files</h3><p>Repeat this step for all nodes that have CSR files.</p><p>Write the following script in the <code>/etc/kubernetes</code> directory, navigate to the directory
and execute the script. The script will take the <code>.crt</code> files that were signed for
kubeconfig files from CSRs in the previous step and will embed them in the kubeconfig files.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080">#!/bin/bash
</span></span></span><span style="display:flex"><span><span style="color:#080"/>
</span></span><span style="display:flex"><span><span style="color:#b8860b">CLUSTER</span><span style="color:#666">=</span>kubernetes
</span></span><span style="display:flex"><span>find ./ -name <span style="color:#b44">"*.conf"</span> | <span style="color:#a2f;font-weight:700">while</span> <span style="color:#a2f">read</span> -r FILE;
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f">echo</span> <span style="color:#b44">"* Processing </span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44"> ..."</span>
</span></span><span style="display:flex"><span>    <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> kubectl config set-cluster <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">CLUSTER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> --certificate-authority ./pki/ca.crt --embed-certs
</span></span><span style="display:flex"><span>    <span style="color:#b8860b">USER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span><span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.users[0].name}'</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>    <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> kubectl config set-credentials <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> --client-certificate <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILE</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">.crt"</span> --embed-certs
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span></code></pre></div><h3 id="post-csr-cleanup">Performing cleanup</h3><p>Perform this step on all nodes that have CSR files.</p><p>Write the following script in the <code>/etc/kubernetes</code> directory, navigate to the directory
and execute the script.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080">#!/bin/bash
</span></span></span><span style="display:flex"><span><span style="color:#080"/>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Cleanup CSR files</span>
</span></span><span style="display:flex"><span>rm -f ./*.csr ./pki/*.csr ./pki/etcd/*.csr <span style="color:#080;font-style:italic"># Clean all CSR files</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Cleanup CRT files that were already embedded in kubeconfig files</span>
</span></span><span style="display:flex"><span>rm -f ./*.crt
</span></span></code></pre></div><p>Optionally, move <code>.srl</code> files to the next node to be processed.</p><p>Optionally, if using external CA remove the <code>/etc/kubernetes/pki/ca.key</code> file,
as explained in the <a href="#external-ca-mode">External CA node</a> section.</p><h3 id="kubeadm-node-initialization">kubeadm node initialization</h3><p>Once CSR files have been signed and required certificates are in place on the hosts
you want to use as nodes, you can use the commands <code>kubeadm init</code> and <code>kubeadm join</code>
to create a Kubernetes cluster from these nodes. During <code>init</code> and <code>join</code>, kubeadm
uses existing certificates, encryption keys and kubeconfig files that it finds in the
<code>/etc/kubernetes</code> tree on the host's local filesystem.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Limit Storage Consumption</h1><p>This example demonstrates how to limit the amount of storage consumed in a namespace.</p><p>The following resources are used in the demonstration: <a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a>,
<a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">LimitRange</a>,
and <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a>.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></li></ul><h2 id="scenario-limiting-storage-consumption">Scenario: Limiting Storage Consumption</h2><p>The cluster-admin is operating a cluster on behalf of a user population and the admin wants to control
how much storage a single namespace can consume in order to control cost.</p><p>The admin would like to limit:</p><ol><li>The number of persistent volume claims in a namespace</li><li>The amount of storage each claim can request</li><li>The amount of cumulative storage the namespace can have</li></ol><h2 id="limitrange-to-limit-requests-for-storage">LimitRange to limit requests for storage</h2><p>Adding a <code>LimitRange</code> to a namespace enforces storage request sizes to a minimum and maximum. Storage is requested
via <code>PersistentVolumeClaim</code>. The admission controller that enforces limit ranges will reject any PVC that is above or below
the values set by the admin.</p><p>In this example, a PVC requesting 10Gi of storage would be rejected because it exceeds the 2Gi max.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>storagelimits<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">max</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">min</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span></code></pre></div><p>Minimum storage requests are used when the underlying storage provider requires certain minimums. For example,
AWS EBS volumes have a 1Gi minimum requirement.</p><h2 id="resourcequota-to-limit-pvc-count-and-cumulative-storage-capacity">ResourceQuota to limit PVC count and cumulative storage capacity</h2><p>Admins can limit the number of PVCs in a namespace as well as the cumulative capacity of those PVCs. New PVCs that exceed
either maximum value will be rejected.</p><p>In this example, a 6th PVC in the namespace would be rejected because it exceeds the maximum count of 5. Alternatively,
a 5Gi maximum quota when combined with the 2Gi max limit above, cannot have 3 PVCs where each has 2Gi. That would be 6Gi requested
for a namespace capped at 5Gi.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>storagequota<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">"5"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests.storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">"5Gi"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="summary">Summary</h2><p>A limit range can put a ceiling on how much storage is requested while a resource quota can effectively cap the storage
consumed by a namespace through claim counts and cumulative storage capacity. The allows a cluster-admin to plan their
cluster's storage budget without risk of any one project going over their allotment.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Reserve Compute Resources for System Daemons</h1><p>Kubernetes nodes can be scheduled to <code>Capacity</code>. Pods can consume all the
available capacity on a node by default. This is an issue because nodes
typically run quite a few system daemons that power the OS and Kubernetes
itself. Unless resources are set aside for these system daemons, pods and system
daemons compete for resources and lead to resource starvation issues on the
node.</p><p>The <code>kubelet</code> exposes a feature named 'Node Allocatable' that helps to reserve
compute resources for system daemons. Kubernetes recommends cluster
administrators to configure 'Node Allocatable' based on their workload density
on each node.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You can configure below kubelet <a href="/docs/reference/config-api/kubelet-config.v1beta1/">configuration settings</a>
using the <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><h2 id="node-allocatable">Node Allocatable</h2><p><img alt="node capacity" src="/images/docs/node-capacity.svg"/></p><p>'Allocatable' on a Kubernetes node is defined as the amount of compute resources
that are available for pods. The scheduler does not over-subscribe
'Allocatable'. 'CPU', 'memory' and 'ephemeral-storage' are supported as of now.</p><p>Node Allocatable is exposed as part of <code>v1.Node</code> object in the API and as part
of <code>kubectl describe node</code> in the CLI.</p><p>Resources can be reserved for two categories of system daemons in the <code>kubelet</code>.</p><h3 id="enabling-qos-and-pod-level-cgroups">Enabling QoS and Pod level cgroups</h3><p>To properly enforce node allocatable constraints on the node, you must
enable the new cgroup hierarchy via the <code>cgroupsPerQOS</code> setting. This setting is
enabled by default. When enabled, the <code>kubelet</code> will parent all end-user pods
under a cgroup hierarchy managed by the <code>kubelet</code>.</p><h3 id="configuring-a-cgroup-driver">Configuring a cgroup driver</h3><p>The <code>kubelet</code> supports manipulation of the cgroup hierarchy on
the host using a cgroup driver. The driver is configured via the <code>cgroupDriver</code> setting.</p><p>The supported values are the following:</p><ul><li><code>cgroupfs</code> is the default driver that performs direct manipulation of the
cgroup filesystem on the host in order to manage cgroup sandboxes.</li><li><code>systemd</code> is an alternative driver that manages cgroup sandboxes using
transient slices for resources that are supported by that init system.</li></ul><p>Depending on the configuration of the associated container runtime,
operators may have to choose a particular cgroup driver to ensure
proper system behavior. For example, if operators use the <code>systemd</code>
cgroup driver provided by the <code>containerd</code> runtime, the <code>kubelet</code> must
be configured to use the <code>systemd</code> cgroup driver.</p><h3 id="kube-reserved">Kube Reserved</h3><ul><li><strong>KubeletConfiguration Setting</strong>: <code>kubeReserved: {}</code>. Example value <code>{cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}</code></li><li><strong>KubeletConfiguration Setting</strong>: <code>kubeReservedCgroup: ""</code></li></ul><p><code>kubeReserved</code> is meant to capture resource reservation for kubernetes system
daemons like the <code>kubelet</code>, <code>container runtime</code>, etc.
It is not meant to reserve resources for system daemons that are run as pods.
<code>kubeReserved</code> is typically a function of <code>pod density</code> on the nodes.</p><p>In addition to <code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be
specified to reserve the specified number of process IDs for
kubernetes system daemons.</p><p>To optionally enforce <code>kubeReserved</code> on kubernetes system daemons, specify the parent
control group for kube daemons as the value for <code>kubeReservedCgroup</code> setting,
and <a href="#enforcing-node-allocatable">add <code>kube-reserved</code> to <code>enforceNodeAllocatable</code></a>.</p><p>It is recommended that the kubernetes system daemons are placed under a top
level control group (<code>runtime.slice</code> on systemd machines for example). Each
system daemon should ideally run within its own child control group. Refer to
<a href="https://git.k8s.io/design-proposals-archive/node/node-allocatable.md#recommended-cgroups-setup">the design proposal</a>
for more details on recommended control group hierarchy.</p><p>Note that Kubelet <strong>does not</strong> create <code>kubeReservedCgroup</code> if it doesn't
exist. The kubelet will fail to start if an invalid cgroup is specified. With <code>systemd</code>
cgroup driver, you should follow a specific pattern for the name of the cgroup you
define: the name should be the value you set for <code>kubeReservedCgroup</code>,
with <code>.slice</code> appended.</p><h3 id="system-reserved">System Reserved</h3><ul><li><strong>KubeletConfiguration Setting</strong>: <code>systemReserved: {}</code>. Example value <code>{cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}</code></li><li><strong>KubeletConfiguration Setting</strong>: <code>systemReservedCgroup: ""</code></li></ul><p><code>systemReserved</code> is meant to capture resource reservation for OS system daemons
like <code>sshd</code>, <code>udev</code>, etc. <code>systemReserved</code> should reserve <code>memory</code> for the
<code>kernel</code> too since <code>kernel</code> memory is not accounted to pods in Kubernetes at this time.
Reserving resources for user login sessions is also recommended (<code>user.slice</code> in
systemd world).</p><p>In addition to <code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be
specified to reserve the specified number of process IDs for OS system
daemons.</p><p>To optionally enforce <code>systemReserved</code> on system daemons, specify the parent
control group for OS system daemons as the value for <code>systemReservedCgroup</code> setting,
and <a href="#enforcing-node-allocatable">add <code>system-reserved</code> to <code>enforceNodeAllocatable</code></a>.</p><p>It is recommended that the OS system daemons are placed under a top level
control group (<code>system.slice</code> on systemd machines for example).</p><p>Note that <code>kubelet</code> <strong>does not</strong> create <code>systemReservedCgroup</code> if it doesn't
exist. <code>kubelet</code> will fail if an invalid cgroup is specified. With <code>systemd</code>
cgroup driver, you should follow a specific pattern for the name of the cgroup you
define: the name should be the value you set for <code>systemReservedCgroup</code>,
with <code>.slice</code> appended.</p><h3 id="explicitly-reserved-cpu-list">Explicitly Reserved CPU List</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p><strong>KubeletConfiguration Setting</strong>: <code>reservedSystemCPUs:</code>. Example value <code>0-3</code></p><p><code>reservedSystemCPUs</code> is meant to define an explicit CPU set for OS system daemons and
kubernetes system daemons. <code>reservedSystemCPUs</code> is for systems that do not intend to
define separate top level cgroups for OS system daemons and kubernetes system daemons
with regard to cpuset resource.
If the Kubelet <strong>does not</strong> have <code>kubeReservedCgroup</code> and <code>systemReservedCgroup</code>,
the explicit cpuset provided by <code>reservedSystemCPUs</code> will take precedence over the CPUs
defined by <code>kubeReservedCgroup</code> and <code>systemReservedCgroup</code> options.</p><p>This option is specifically designed for Telco/NFV use cases where uncontrolled
interrupts/timers may impact the workload performance. you can use this option
to define the explicit cpuset for the system/kubernetes daemons as well as the
interrupts/timers, so the rest CPUs on the system can be used exclusively for
workloads, with less impact from uncontrolled interrupts/timers. To move the
system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset
defined by this option, other mechanism outside Kubernetes should be used.
For example: in Centos, you can do this using the tuned toolset.</p><h3 id="eviction-thresholds">Eviction Thresholds</h3><p><strong>KubeletConfiguration Setting</strong>: <code>evictionHard: {memory.available: "100Mi", nodefs.available: "10%", nodefs.inodesFree: "5%", imagefs.available: "15%"}</code>. Example value: <code>{memory.available: "&lt;500Mi"}</code></p><p>Memory pressure at the node level leads to System OOMs which affects the entire
node and all pods running on it. Nodes can go offline temporarily until memory
has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet
provides <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">out of resource</a>
management. Evictions are
supported for <code>memory</code> and <code>ephemeral-storage</code> only. By reserving some memory via
<code>evictionHard</code> setting, the <code>kubelet</code> attempts to evict pods whenever memory
availability on the node drops below the reserved value. Hypothetically, if
system daemons did not exist on a node, pods cannot use more than <code>capacity - eviction-hard</code>. For this reason, resources reserved for evictions are not
available for pods.</p><h3 id="enforcing-node-allocatable">Enforcing Node Allocatable</h3><p><strong>KubeletConfiguration setting</strong>: <code>enforceNodeAllocatable: [pods]</code>. Example value: <code>[pods,system-reserved,kube-reserved]</code></p><p>The scheduler treats 'Allocatable' as the available <code>capacity</code> for pods.</p><p><code>kubelet</code> enforce 'Allocatable' across pods by default. Enforcement is performed
by evicting pods whenever the overall usage across all pods exceeds
'Allocatable'. More details on eviction policy can be found
on the <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">node pressure eviction</a>
page. This enforcement is controlled by
specifying <code>pods</code> value to the KubeletConfiguration setting <code>enforceNodeAllocatable</code>.</p><p>Optionally, <code>kubelet</code> can be made to enforce <code>kubeReserved</code> and
<code>systemReserved</code> by specifying <code>kube-reserved</code> &amp; <code>system-reserved</code> values in
the same setting. Note that to enforce <code>kubeReserved</code> or <code>systemReserved</code>,
<code>kubeReservedCgroup</code> or <code>systemReservedCgroup</code> needs to be specified
respectively.</p><h2 id="general-guidelines">General Guidelines</h2><p>System daemons are expected to be treated similar to
<a href="/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed">Guaranteed pods</a>.
System daemons can burst within their bounding control groups and this behavior needs
to be managed as part of kubernetes deployments. For example, <code>kubelet</code> should
have its own control group and share <code>kubeReserved</code> resources with the
container runtime. However, Kubelet cannot burst and use up all available Node
resources if <code>kubeReserved</code> is enforced.</p><p>Be extra careful while enforcing <code>systemReserved</code> reservation since it can lead
to critical system services being CPU starved, OOM killed, or unable
to fork on the node. The
recommendation is to enforce <code>systemReserved</code> only if a user has profiled their
nodes exhaustively to come up with precise estimates and is confident in their
ability to recover if any process in that group is oom-killed.</p><ul><li>To begin with enforce 'Allocatable' on <code>pods</code>.</li><li>Once adequate monitoring and alerting is in place to track kube system
daemons, attempt to enforce <code>kubeReserved</code> based on usage heuristics.</li><li>If absolutely necessary, enforce <code>systemReserved</code> over time.</li></ul><p>The resource requirements of kube system daemons may grow over time as more and
more features are added. Over time, kubernetes project will attempt to bring
down utilization of node system daemons, but that is not a priority as of now.
So expect a drop in <code>Allocatable</code> capacity in future releases.</p><h2 id="example-scenario">Example Scenario</h2><p>Here is an example to illustrate Node Allocatable computation:</p><ul><li>Node has <code>32Gi</code> of <code>memory</code>, <code>16 CPUs</code> and <code>100Gi</code> of <code>Storage</code></li><li><code>kubeReserved</code> is set to <code>{cpu: 1000m, memory: 2Gi, ephemeral-storage: 1Gi}</code></li><li><code>systemReserved</code> is set to <code>{cpu: 500m, memory: 1Gi, ephemeral-storage: 1Gi}</code></li><li><code>evictionHard</code> is set to <code>{memory.available: "&lt;500Mi", nodefs.available: "&lt;10%"}</code></li></ul><p>Under this scenario, 'Allocatable' will be 14.5 CPUs, 28.5Gi of memory and
<code>88Gi</code> of local storage.
Scheduler ensures that the total memory <code>requests</code> across all pods on this node does
not exceed 28.5Gi and storage doesn't exceed 88Gi.
Kubelet evicts pods whenever the overall memory usage across pods exceeds 28.5Gi,
or if overall disk usage exceeds 88Gi. If all processes on the node consume as
much CPU as they can, pods together cannot consume more than 14.5 CPUs.</p><p>If <code>kubeReserved</code> and/or <code>systemReserved</code> is not enforced and system daemons
exceed their reservation, <code>kubelet</code> evicts pods whenever the overall node memory
usage is higher than 31.5Gi or <code>storage</code> is greater than 90Gi.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Accessing Clusters</h1><p>This topic discusses multiple ways to interact with clusters.</p><h2 id="accessing-for-the-first-time-with-kubectl">Accessing for the first time with kubectl</h2><p>When accessing the Kubernetes API for the first time, we suggest using the
Kubernetes CLI, <code>kubectl</code>.</p><p>To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a <a href="/docs/setup/">Getting started guide</a>,
or someone else set up the cluster and provided you with credentials and a location.</p><p>Check the location and credentials that kubectl knows about with this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config view
</span></span></code></pre></div><p>Many of the <a href="/docs/reference/kubectl/quick-reference/">examples</a> provide an introduction to using
<code>kubectl</code>, and complete documentation is found in the
<a href="/docs/reference/kubectl/">kubectl reference</a>.</p><h2 id="directly-accessing-the-rest-api">Directly accessing the REST API</h2><p>Kubectl handles locating and authenticating to the apiserver.
If you want to directly access the REST API with an http client like
curl or wget, or a browser, there are several ways to locate and authenticate:</p><ul><li>Run kubectl in proxy mode.<ul><li>Recommended approach.</li><li>Uses stored apiserver location.</li><li>Verifies identity of apiserver using self-signed cert. No MITM possible.</li><li>Authenticates to apiserver.</li><li>In future, may do intelligent client-side load-balancing and failover.</li></ul></li><li>Provide the location and credentials directly to the http client.<ul><li>Alternate approach.</li><li>Works with some types of client code that are confused by using a proxy.</li><li>Need to import a root cert into your browser to protect against MITM.</li></ul></li></ul><h3 id="using-kubectl-proxy">Using kubectl proxy</h3><p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the apiserver and authenticating.
Run it like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span></code></pre></div><p>See <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl proxy</a> for more details.</p><p>Then you can explore the API with curl, wget, or a browser, replacing localhost
with [::1] for IPv6, like so:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl http://localhost:8080/api/
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"APIVersions"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"versions"</span>: [
</span></span><span style="display:flex"><span>    <span style="color:#b44">"v1"</span>
</span></span><span style="display:flex"><span>  ],
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"serverAddressByClientCIDRs"</span>: [
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"clientCIDR"</span>: <span style="color:#b44">"0.0.0.0/0"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"serverAddress"</span>: <span style="color:#b44">"10.0.1.149:443"</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  ]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><h3 id="without-kubectl-proxy">Without kubectl proxy</h3><p>Use <code>kubectl apply</code> and <code>kubectl describe secret...</code> to create a token for the default service account with grep/cut:</p><p>First, create the Secret, requesting a token for the default ServiceAccount:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF
</span></span></span><span style="display:flex"><span><span style="color:#b44">apiVersion: v1
</span></span></span><span style="display:flex"><span><span style="color:#b44">kind: Secret
</span></span></span><span style="display:flex"><span><span style="color:#b44">metadata:
</span></span></span><span style="display:flex"><span><span style="color:#b44">  name: default-token
</span></span></span><span style="display:flex"><span><span style="color:#b44">  annotations:
</span></span></span><span style="display:flex"><span><span style="color:#b44">    kubernetes.io/service-account.name: default
</span></span></span><span style="display:flex"><span><span style="color:#b44">type: kubernetes.io/service-account-token
</span></span></span><span style="display:flex"><span><span style="color:#b44">EOF</span>
</span></span></code></pre></div><p>Next, wait for the token controller to populate the Secret with a token:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">while</span> ! kubectl describe secret default-token | grep -E <span style="color:#b44">'^token'</span> &gt;/dev/null; <span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>  <span style="color:#a2f">echo</span> <span style="color:#b44">"waiting for token..."</span> &gt;&amp;<span style="color:#666">2</span>
</span></span><span style="display:flex"><span>  sleep <span style="color:#666">1</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span></code></pre></div><p>Capture and use the generated token:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl config view --minify | grep server | cut -f 2- -d <span style="color:#b44">":"</span> | tr -d <span style="color:#b44">" "</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl describe secret default-token | grep -E <span style="color:#b44">'^token'</span> | cut -f2 -d<span style="color:#b44">':'</span> | tr -d <span style="color:#b44">" "</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>curl <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">"Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">"</span> --insecure
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"APIVersions"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"versions"</span>: [
</span></span><span style="display:flex"><span>    <span style="color:#b44">"v1"</span>
</span></span><span style="display:flex"><span>  ],
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"serverAddressByClientCIDRs"</span>: [
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"clientCIDR"</span>: <span style="color:#b44">"0.0.0.0/0"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"serverAddress"</span>: <span style="color:#b44">"10.0.1.149:443"</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  ]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>Using <code>jsonpath</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl config view --minify -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.clusters[0].cluster.server}'</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl get secret default-token -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.data.token}'</span> | base64 --decode<span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>curl <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">"Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">"</span> --insecure
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"APIVersions"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"versions"</span>: [
</span></span><span style="display:flex"><span>    <span style="color:#b44">"v1"</span>
</span></span><span style="display:flex"><span>  ],
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"serverAddressByClientCIDRs"</span>: [
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"clientCIDR"</span>: <span style="color:#b44">"0.0.0.0/0"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"serverAddress"</span>: <span style="color:#b44">"10.0.1.149:443"</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  ]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>The above examples use the <code>--insecure</code> flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
<code>~/.kube</code> directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.</p><p>On some clusters, the apiserver does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. <a href="/docs/concepts/security/controlling-access/">Controlling Access to the API</a>
describes how a cluster admin can configure this.</p><h2 id="programmatic-access-to-the-api">Programmatic access to the API</h2><p>Kubernetes officially supports <a href="#go-client">Go</a> and <a href="#python-client">Python</a>
client libraries.</p><h3 id="go-client">Go client</h3><ul><li>To get the library, run the following command: <code>go get k8s.io/client-go@kubernetes-&lt;kubernetes-version-number&gt;</code>,
see <a href="https://github.com/kubernetes/client-go/blob/master/INSTALL.md#for-the-casual-user">INSTALL.md</a>
for detailed installation instructions. See
<a href="https://github.com/kubernetes/client-go#compatibility-matrix">https://github.com/kubernetes/client-go</a>
to see which versions are supported.</li><li>Write an application atop of the client-go clients. Note that client-go defines its own API objects,
so if needed, please import API definitions from client-go rather than from the main repository,
e.g., <code>import "k8s.io/client-go/kubernetes"</code> is correct.</li></ul><p>The Go client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the apiserver. See this
<a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">example</a>.</p><p>If the application is deployed as a Pod in the cluster, please refer to the <a href="#accessing-the-api-from-a-pod">next section</a>.</p><h3 id="python-client">Python client</h3><p>To use <a href="https://github.com/kubernetes-client/python">Python client</a>, run the following command:
<code>pip install kubernetes</code>. See <a href="https://github.com/kubernetes-client/python">Python Client Library page</a>
for more installation options.</p><p>The Python client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the apiserver. See this
<a href="https://github.com/kubernetes-client/python/tree/master/examples">example</a>.</p><h3 id="other-languages">Other languages</h3><p>There are <a href="/docs/reference/using-api/client-libraries/">client libraries</a> for accessing the API from other languages.
See documentation for other libraries for how they authenticate.</p><h2 id="accessing-the-api-from-a-pod">Accessing the API from a Pod</h2><p>When accessing the API from a pod, locating and authenticating
to the API server are somewhat different.</p><p>Please check <a href="/docs/tasks/run-application/access-api-from-pod/">Accessing the API from within a Pod</a>
for more details.</p><h2 id="accessing-services-running-on-the-cluster">Accessing services running on the cluster</h2><p>The previous section describes how to connect to the Kubernetes API server.
For information about connecting to other services running on a Kubernetes cluster, see
<a href="/docs/tasks/access-application-cluster/access-cluster-services/">Access Cluster Services</a>.</p><h2 id="requesting-redirects">Requesting redirects</h2><p>The redirect capabilities have been deprecated and removed. Please use a proxy (see below) instead.</p><h2 id="so-many-proxies">So many proxies</h2><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li><p>The <a href="#directly-accessing-the-rest-api">kubectl proxy</a>:</p><ul><li>runs on a user's desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li><p>The <a href="/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services">apiserver proxy</a>:</p><ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li><p>The <a href="/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>:</p><ul><li>runs on each node</li><li>proxies UDP and TCP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is only used to reach services</li></ul></li><li><p>A Proxy/Load-balancer in front of apiserver(s):</p><ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li><p>Cloud Load Balancers on external services:</p><ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <code>LoadBalancer</code></li><li>use UDP/TCP only</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin
will typically ensure that the latter types are set up correctly.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Check whether dockershim removal affects you</h1><p>The <code>dockershim</code> component of Kubernetes allows the use of Docker as a Kubernetes's
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>.
Kubernetes' built-in <code>dockershim</code> component was removed in release v1.24.</p><p>This page explains how your cluster could be using Docker as a container runtime,
provides details on the role that <code>dockershim</code> plays when in use, and shows steps
you can take to check whether any workloads could be affected by <code>dockershim</code> removal.</p><h2 id="find-docker-dependencies">Finding if your app has a dependencies on Docker</h2><p>If you are using Docker for building your application containers, you can still
run these containers on any container runtime. This use of Docker does not count
as a dependency on Docker as a container runtime.</p><p>When alternative container runtime is used, executing Docker commands may either
not work or yield unexpected output. This is how you can find whether you have a
dependency on Docker:</p><ol><li>Make sure no privileged Pods execute Docker commands (like <code>docker ps</code>),
restart the Docker service (commands such as <code>systemctl restart docker.service</code>),
or modify Docker-specific files such as <code>/etc/docker/daemon.json</code>.</li><li>Check for any private registries or image mirror settings in the Docker
configuration file (like <code>/etc/docker/daemon.json</code>). Those typically need to
be reconfigured for another container runtime.</li><li>Check that scripts and apps running on nodes outside of your Kubernetes
infrastructure do not execute Docker commands. It might be:<ul><li>SSH to nodes to troubleshoot;</li><li>Node startup scripts;</li><li>Monitoring and security agents installed on nodes directly.</li></ul></li><li>Third-party tools that perform above mentioned privileged operations. See
<a href="/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">Migrating telemetry and security agents from dockershim</a>
for more information.</li><li>Make sure there are no indirect dependencies on dockershim behavior.
This is an edge case and unlikely to affect your application. Some tooling may be configured
to react to Docker-specific behaviors, for example, raise alert on specific metrics or search for
a specific log message as part of troubleshooting instructions.
If you have such tooling configured, test the behavior on a test
cluster before migration.</li></ol><h2 id="role-of-dockershim">Dependency on Docker explained</h2><p>A <a href="/docs/concepts/containers/#container-runtimes">container runtime</a> is software that can
execute the containers that make up a Kubernetes pod. Kubernetes is responsible for orchestration
and scheduling of Pods; on each node, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a>
uses the container runtime interface as an abstraction so that you can use any compatible
container runtime.</p><p>In its earliest releases, Kubernetes offered compatibility with one container runtime: Docker.
Later in the Kubernetes project's history, cluster operators wanted to adopt additional container runtimes.
The CRI was designed to allow this kind of flexibility - and the kubelet began supporting CRI. However,
because Docker existed before the CRI specification was invented, the Kubernetes project created an
adapter component, <code>dockershim</code>. The dockershim adapter allows the kubelet to interact with Docker as
if Docker were a CRI compatible runtime.</p><p>You can read about it in <a href="/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/">Kubernetes Containerd integration goes GA</a> blog post.</p><p><img alt="Dockershim vs. CRI with Containerd" src="/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png"/></p><p>Switching to Containerd as a container runtime eliminates the middleman. All the
same containers can be run by container runtimes like Containerd as before. But
now, since containers schedule directly with the container runtime, they are not visible to Docker.
So any Docker tooling or fancy UI you might have used
before to check on these containers is no longer available.</p><p>You cannot get container information using <code>docker ps</code> or <code>docker inspect</code>
commands. As you cannot list containers, you cannot get logs, stop containers,
or execute something inside a container using <code>docker exec</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you're running workloads via Kubernetes, the best way to stop a container is through
the Kubernetes API rather than directly through the container runtime (this advice applies
for all container runtimes, not only Docker).</div><p>You can still pull images or build them using <code>docker build</code> command. But images
built or pulled by Docker would not be visible to container runtime and
Kubernetes. They needed to be pushed to some registry to allow them to be used
by Kubernetes.</p><h2 id="known-issues">Known issues</h2><h3 id="some-filesystem-metrics-are-missing-and-the-metrics-format-is-different">Some filesystem metrics are missing and the metrics format is different</h3><p>The Kubelet <code>/metrics/cadvisor</code> endpoint provides Prometheus metrics,
as documented in <a href="/docs/concepts/cluster-administration/system-metrics/">Metrics for Kubernetes system components</a>.
If you install a metrics collector that depends on that endpoint, you might see the following issues:</p><ul><li>The metrics format on the Docker node is <code>k8s_&lt;container-name&gt;_&lt;pod-name&gt;_&lt;namespace&gt;_&lt;pod-uid&gt;_&lt;restart-count&gt;</code>
but the format on other runtime is different. For example, on containerd node it is <code>&lt;container-id&gt;</code>.</li><li>Some filesystem metrics are missing, as follows:<pre tabindex="0"><code>container_fs_inodes_free
container_fs_inodes_total
container_fs_io_current
container_fs_io_time_seconds_total
container_fs_io_time_weighted_seconds_total
container_fs_limit_bytes
container_fs_read_seconds_total
container_fs_reads_merged_total
container_fs_sector_reads_total
container_fs_sector_writes_total
container_fs_usage_bytes
container_fs_write_seconds_total
container_fs_writes_merged_total
</code></pre></li></ul><h4 id="workaround">Workaround</h4><p>You can mitigate this issue by using <a href="https://github.com/google/cadvisor">cAdvisor</a> as a standalone daemonset.</p><ol><li>Find the latest <a href="https://github.com/google/cadvisor/releases">cAdvisor release</a>
with the name pattern <code>vX.Y.Z-containerd-cri</code> (for example, <code>v0.42.0-containerd-cri</code>).</li><li>Follow the steps in <a href="https://github.com/google/cadvisor/tree/master/deploy/kubernetes">cAdvisor Kubernetes Daemonset</a> to create the daemonset.</li><li>Point the installed metrics collector to use the cAdvisor <code>/metrics</code> endpoint
which provides the full set of
<a href="https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md">Prometheus container metrics</a>.</li></ol><p>Alternatives:</p><ul><li>Use alternative third party metrics collection solution.</li><li>Collect metrics from the Kubelet summary API that is served at <code>/stats/summary</code>.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim</a> to understand your next steps</li><li>Read the <a href="/blog/2020/12/02/dockershim-faq/">dockershim deprecation FAQ</a> article for more information.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Verify Signed Kubernetes Artifacts</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [beta]</code></div><h2 id="before-you-begin">Before you begin</h2><p>You will need to have the following tools installed:</p><ul><li><code>cosign</code> (<a href="https://docs.sigstore.dev/cosign/system_config/installation/">install guide</a>)</li><li><code>curl</code> (often provided by your operating system)</li><li><code>jq</code> (<a href="https://jqlang.github.io/jq/download/">download jq</a>)</li></ul><h2 id="verifying-binary-signatures">Verifying binary signatures</h2><p>The Kubernetes release process signs all binary artifacts (tarballs, SPDX files,
standalone binaries) by using cosign's keyless signing. To verify a particular
binary, retrieve it together with its signature and certificate:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#b8860b">URL</span><span style="color:#666">=</span>https://dl.k8s.io/release/v1.34.0/bin/linux/amd64
</span></span><span style="display:flex"><span><span style="color:#b8860b">BINARY</span><span style="color:#666">=</span>kubectl
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">FILES</span><span style="color:#666">=(</span>
</span></span><span style="display:flex"><span>    <span style="color:#b44">"</span><span style="color:#b8860b">$BINARY</span><span style="color:#b44">"</span>
</span></span><span style="display:flex"><span>    <span style="color:#b44">"</span><span style="color:#b8860b">$BINARY</span><span style="color:#b44">.sig"</span>
</span></span><span style="display:flex"><span>    <span style="color:#b44">"</span><span style="color:#b8860b">$BINARY</span><span style="color:#b44">.cert"</span>
</span></span><span style="display:flex"><span><span style="color:#666">)</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">for</span> FILE in <span style="color:#b44">"</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">FILES</span>[@]<span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span>; <span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>    curl -sSfL --retry <span style="color:#666">3</span> --retry-delay <span style="color:#666">3</span> <span style="color:#b44">"</span><span style="color:#b8860b">$URL</span><span style="color:#b44">/</span><span style="color:#b8860b">$FILE</span><span style="color:#b44">"</span> -o <span style="color:#b44">"</span><span style="color:#b8860b">$FILE</span><span style="color:#b44">"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span></code></pre></div><p>Then verify the blob by using <code>cosign verify-blob</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>cosign verify-blob <span style="color:#b44">"</span><span style="color:#b8860b">$BINARY</span><span style="color:#b44">"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --signature <span style="color:#b44">"</span><span style="color:#b8860b">$BINARY</span><span style="color:#b44">"</span>.sig <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --certificate <span style="color:#b44">"</span><span style="color:#b8860b">$BINARY</span><span style="color:#b44">"</span>.cert <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --certificate-oidc-issuer https://accounts.google.com
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Cosign 2.0 requires the <code>--certificate-identity</code> and <code>--certificate-oidc-issuer</code> options.</p><p>To learn more about keyless signing, please refer to <a href="https://docs.sigstore.dev/cosign/signing/overview/">Keyless Signatures</a>.</p><p>Previous versions of Cosign required that you set <code>COSIGN_EXPERIMENTAL=1</code>.</p><p>For additional information, please refer to the <a href="https://blog.sigstore.dev/cosign-2-0-released/">sigstore Blog</a></p></div><h2 id="verifying-image-signatures">Verifying image signatures</h2><p>For a complete list of images that are signed please refer
to <a href="/releases/download/">Releases</a>.</p><p>Pick one image from this list and verify its signature using
the <code>cosign verify</code> command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>cosign verify registry.k8s.io/kube-apiserver-amd64:v1.34.0 <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --certificate-oidc-issuer https://accounts.google.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  | jq .
</span></span></code></pre></div><h3 id="verifying-images-for-all-control-plane-components">Verifying images for all control plane components</h3><p>To verify all signed control plane images for the latest stable version
(v1.34.0), please run the following commands:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -Ls <span style="color:#b44">"https://sbom.k8s.io/</span><span style="color:#a2f;font-weight:700">$(</span>curl -Ls https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:700">)</span><span style="color:#b44">/release"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  | grep <span style="color:#b44">"SPDXID: SPDXRef-Package-registry.k8s.io"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  | grep -v sha256 | cut -d- -f3- | sed <span style="color:#b44">'s/-/\//'</span> | sed <span style="color:#b44">'s/-v1/:v1/'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  | sort &gt; images.txt
</span></span><span style="display:flex"><span><span style="color:#b8860b">input</span><span style="color:#666">=</span>images.txt
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">while</span> <span style="color:#b8860b">IFS</span><span style="color:#666">=</span> <span style="color:#a2f">read</span> -r image
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>  cosign verify <span style="color:#b44">"</span><span style="color:#b8860b">$image</span><span style="color:#b44">"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    --certificate-oidc-issuer https://accounts.google.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    | jq .
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span> &lt; <span style="color:#b44">"</span><span style="color:#b8860b">$input</span><span style="color:#b44">"</span>
</span></span></code></pre></div><p>Once you have verified an image, you can specify the image by its digest in your Pod
manifests as per this example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">registry-url/image-name@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2
</span></span></span></code></pre></div><p>For more information, please refer
to the <a href="/docs/concepts/containers/images/#image-pull-policy">Image Pull Policy</a>
section.</p><h2 id="verifying-image-signatures-with-admission-controller">Verifying Image Signatures with Admission Controller</h2><p>For non-control plane images (for example
<a href="https://github.com/kubernetes/kubernetes/blob/master/test/conformance/image/README.md">conformance image</a>),
signatures can also be verified at deploy time using
<a href="https://docs.sigstore.dev/policy-controller/overview">sigstore policy-controller</a>
admission controller.</p><p>Here are some helpful resources to get started with <code>policy-controller</code>:</p><ul><li><a href="https://github.com/sigstore/helm-charts/tree/main/charts/policy-controller">Installation</a></li><li><a href="https://github.com/sigstore/policy-controller/tree/main/config">Configuration Options</a></li></ul><h2 id="verify-the-software-bill-of-materials">Verify the Software Bill Of Materials</h2><p>You can verify the Kubernetes Software Bill of Materials (SBOM) by using the
sigstore certificate and signature, or the corresponding SHA files:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Retrieve the latest available Kubernetes release version</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">VERSION</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>curl -Ls https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Verify the SHA512 sum</span>
</span></span><span style="display:flex"><span>curl -Ls <span style="color:#b44">"https://sbom.k8s.io/</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">/release"</span> -o <span style="color:#b44">"</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b44">"</span><span style="color:#a2f;font-weight:700">$(</span>curl -Ls <span style="color:#b44">"https://sbom.k8s.io/</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">/release.sha512"</span><span style="color:#a2f;font-weight:700">)</span><span style="color:#b44"> </span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx"</span> | sha512sum --check
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Verify the SHA256 sum</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b44">"</span><span style="color:#a2f;font-weight:700">$(</span>curl -Ls <span style="color:#b44">"https://sbom.k8s.io/</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">/release.sha256"</span><span style="color:#a2f;font-weight:700">)</span><span style="color:#b44"> </span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx"</span> | sha256sum --check
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Retrieve sigstore signature and certificate</span>
</span></span><span style="display:flex"><span>curl -Ls <span style="color:#b44">"https://sbom.k8s.io/</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">/release.sig"</span> -o <span style="color:#b44">"</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx.sig"</span>
</span></span><span style="display:flex"><span>curl -Ls <span style="color:#b44">"https://sbom.k8s.io/</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">/release.cert"</span> -o <span style="color:#b44">"</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx.cert"</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Verify the sigstore signature</span>
</span></span><span style="display:flex"><span>cosign verify-blob <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    --certificate <span style="color:#b44">"</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx.cert"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    --signature <span style="color:#b44">"</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx.sig"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    --certificate-oidc-issuer https://accounts.google.com <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    <span style="color:#b44">"</span><span style="color:#b8860b">$VERSION</span><span style="color:#b44">.spdx"</span>
</span></span></code></pre></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Using NodeLocal DNSCache in Kubernetes Clusters</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>This page provides an overview of NodeLocal DNSCache feature in Kubernetes.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="introduction">Introduction</h2><p>NodeLocal DNSCache improves Cluster DNS performance by running a DNS caching agent
on cluster nodes as a DaemonSet. In today's architecture, Pods in 'ClusterFirst' DNS mode
reach out to a kube-dns <code>serviceIP</code> for DNS queries. This is translated to a
kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy.
With this new architecture, Pods will reach out to the DNS caching agent
running on the same node, thereby avoiding iptables DNAT rules and connection tracking.
The local caching agent will query kube-dns service for cache misses of cluster
hostnames ("<code>cluster.local</code>" suffix by default).</p><h2 id="motivation">Motivation</h2><ul><li><p>With the current DNS architecture, it is possible that Pods with the highest DNS QPS
have to reach out to a different node, if there is no local kube-dns/CoreDNS instance.
Having a local cache will help improve the latency in such scenarios.</p></li><li><p>Skipping iptables DNAT and connection tracking will help reduce
<a href="https://github.com/kubernetes/kubernetes/issues/56903">conntrack races</a>
and avoid UDP DNS entries filling up conntrack table.</p></li><li><p>Connections from the local caching agent to kube-dns service can be upgraded to TCP.
TCP conntrack entries will be removed on connection close in contrast with
UDP entries that have to timeout
(<a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt">default</a>
<code>nf_conntrack_udp_timeout</code> is 30 seconds)</p></li><li><p>Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to
dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout).
Since the nodelocal cache listens for UDP DNS queries, applications don't need to be changed.</p></li><li><p>Metrics &amp; visibility into DNS requests at a node level.</p></li><li><p>Negative caching can be re-enabled, thereby reducing the number of queries for the kube-dns service.</p></li></ul><h2 id="architecture-diagram">Architecture Diagram</h2><p>This is the path followed by DNS Queries after NodeLocal DNSCache is enabled:</p><figure class="diagram-medium"><img src="/images/docs/nodelocaldns.svg" alt="NodeLocal DNSCache flow"/><figcaption><h4>Nodelocal DNSCache flow</h4><p>This image shows how NodeLocal DNSCache handles DNS queries.</p></figcaption></figure><h2 id="configuration">Configuration</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The local listen IP address for NodeLocal DNSCache can be any address that
can be guaranteed to not collide with any existing IP in your cluster.
It's recommended to use an address with a local scope, for example,
from the 'link-local' range '169.254.0.0/16' for IPv4 or from the
'Unique Local Address' range in IPv6 'fd00::/8'.</div><p>This feature can be enabled using the following steps:</p><ul><li><p>Prepare a manifest similar to the sample
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"><code>nodelocaldns.yaml</code></a>
and save it as <code>nodelocaldns.yaml</code>.</p></li><li><p>If using IPv6, the CoreDNS configuration file needs to enclose all the IPv6 addresses
into square brackets if used in 'IP:Port' format.
If you are using the sample manifest from the previous point, this will require you to modify
<a href="https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70">the configuration line L70</a>
like this: "<code>health [__PILLAR__LOCAL__DNS__]:8080</code>"</p></li><li><p>Substitute the variables in the manifest with the right values:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">kubedns</span><span style="color:#666">=</span><span style="color:#b44">`</span>kubectl get svc kube-dns -n kube-system -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">={</span>.spec.clusterIP<span style="color:#666">}</span><span style="color:#b44">`</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">domain</span><span style="color:#666">=</span>&lt;cluster-domain&gt;
</span></span><span style="display:flex"><span><span style="color:#b8860b">localdns</span><span style="color:#666">=</span>&lt;node-local-address&gt;
</span></span></code></pre></div><p><code>&lt;cluster-domain&gt;</code> is "<code>cluster.local</code>" by default. <code>&lt;node-local-address&gt;</code> is the
local listen IP address chosen for NodeLocal DNSCache.</p><ul><li><p>If kube-proxy is running in IPTABLES mode:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>sed -i <span style="color:#b44">"s/__PILLAR__LOCAL__DNS__/</span><span style="color:#b8860b">$localdns</span><span style="color:#b44">/g; s/__PILLAR__DNS__DOMAIN__/</span><span style="color:#b8860b">$domain</span><span style="color:#b44">/g; s/__PILLAR__DNS__SERVER__/</span><span style="color:#b8860b">$kubedns</span><span style="color:#b44">/g"</span> nodelocaldns.yaml
</span></span></code></pre></div><p><code>__PILLAR__CLUSTER__DNS__</code> and <code>__PILLAR__UPSTREAM__SERVERS__</code> will be populated by
the <code>node-local-dns</code> pods.
In this mode, the <code>node-local-dns</code> pods listen on both the kube-dns service IP
as well as <code>&lt;node-local-address&gt;</code>, so pods can look up DNS records using either IP address.</p></li><li><p>If kube-proxy is running in IPVS mode:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>sed -i <span style="color:#b44">"s/__PILLAR__LOCAL__DNS__/</span><span style="color:#b8860b">$localdns</span><span style="color:#b44">/g; s/__PILLAR__DNS__DOMAIN__/</span><span style="color:#b8860b">$domain</span><span style="color:#b44">/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/</span><span style="color:#b8860b">$kubedns</span><span style="color:#b44">/g"</span> nodelocaldns.yaml
</span></span></code></pre></div><p>In this mode, the <code>node-local-dns</code> pods listen only on <code>&lt;node-local-address&gt;</code>.
The <code>node-local-dns</code> interface cannot bind the kube-dns cluster IP since the
interface used for IPVS loadbalancing already uses this address.
<code>__PILLAR__UPSTREAM__SERVERS__</code> will be populated by the node-local-dns pods.</p></li></ul></li><li><p>Run <code>kubectl create -f nodelocaldns.yaml</code></p></li><li><p>If using kube-proxy in IPVS mode, <code>--cluster-dns</code> flag to kubelet needs to be modified
to use <code>&lt;node-local-address&gt;</code> that NodeLocal DNSCache is listening on.
Otherwise, there is no need to modify the value of the <code>--cluster-dns</code> flag,
since NodeLocal DNSCache listens on both the kube-dns service IP as well as
<code>&lt;node-local-address&gt;</code>.</p></li></ul><p>Once enabled, the <code>node-local-dns</code> Pods will run in the <code>kube-system</code> namespace
on each of the cluster nodes. This Pod runs <a href="https://github.com/coredns/coredns">CoreDNS</a>
in cache mode, so all CoreDNS metrics exposed by the different plugins will
be available on a per-node basis.</p><p>You can disable this feature by removing the DaemonSet, using <code>kubectl delete -f &lt;manifest&gt;</code>.
You should also revert any changes you made to the kubelet configuration.</p><h2 id="stubdomains-and-upstream-server-configuration">StubDomains and Upstream server Configuration</h2><p>StubDomains and upstream servers specified in the <code>kube-dns</code> ConfigMap in the <code>kube-system</code> namespace
are automatically picked up by <code>node-local-dns</code> pods. The ConfigMap contents need to follow the format
shown in <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/#example-1">the example</a>.
The <code>node-local-dns</code> ConfigMap can also be modified directly with the stubDomain configuration
in the Corefile format. Some cloud providers might not allow modifying <code>node-local-dns</code> ConfigMap directly.
In those cases, the <code>kube-dns</code> ConfigMap can be updated.</p><h2 id="setting-memory-limits">Setting memory limits</h2><p>The <code>node-local-dns</code> Pods use memory for storing cache entries and processing queries.
Since they do not watch Kubernetes objects, the cluster size or the number of Services / EndpointSlices do not directly affect memory usage. Memory usage is influenced by the DNS query pattern.
From <a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">CoreDNS docs</a>,</p><blockquote><p>The default cache size is 10000 entries, which uses about 30 MB when completely filled.</p></blockquote><p>This would be the memory usage for each server block (if the cache gets completely filled).
Memory usage can be reduced by specifying smaller cache sizes.</p><p>The number of concurrent queries is linked to the memory demand, because each extra
goroutine used for handling a query requires an amount of memory. You can set an upper limit
using the <code>max_concurrent</code> option in the forward plugin.</p><p>If a <code>node-local-dns</code> Pod attempts to use more memory than is available (because of total system
resources, or because of a configured
<a href="/docs/concepts/configuration/manage-resources-containers/">resource limit</a>), the operating system
may shut down that pod's container.
If this happens, the container that is terminated (“OOMKilled”) does not clean up the custom
packet filtering rules that it previously added during startup.
The <code>node-local-dns</code> container should get restarted (since managed as part of a DaemonSet), but this
will lead to a brief DNS downtime each time that the container fails: the packet filtering rules direct
DNS queries to a local Pod that is unhealthy.</p><p>You can determine a suitable memory limit by running node-local-dns pods without a limit and
measuring the peak usage. You can also set up and use a
<a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VerticalPodAutoscaler</a>
in <em>recommender mode</em>, and then check its recommendations.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use Cascading Deletion in a Cluster</h1><p>This page shows you how to specify the type of
<a href="/docs/concepts/architecture/garbage-collection/#cascading-deletion">cascading deletion</a>
to use in your cluster during <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/garbage-collection/" target="_blank" aria-label="garbage collection">garbage collection</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You also need to <a href="/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">create a sample Deployment</a>
to experiment with the different types of cascading deletion. You will need to
recreate the Deployment for each type.</p><h2 id="check-owner-references-on-your-pods">Check owner references on your pods</h2><p>Check that the <code>ownerReferences</code> field is present on your pods:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output has an <code>ownerReferences</code> field similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ownerReferences</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">blockOwnerDeletion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">controller</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-deployment-6b474476c4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">uid</span>:<span style="color:#bbb"> </span>4fdcd81c-bd5d-41f7-97af-3a3b759af9a7<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="use-foreground-cascading-deletion">Use foreground cascading deletion</h2><p>By default, Kubernetes uses <a href="/docs/concepts/architecture/garbage-collection/#background-deletion">background cascading deletion</a>
to delete dependents of an object. You can switch to foreground cascading deletion
using either <code>kubectl</code> or the Kubernetes API, depending on the Kubernetes
version your cluster runs.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>You can delete objects using foreground cascading deletion using <code>kubectl</code> or the
Kubernetes API.</p><p><strong>Using kubectl</strong></p><p>Run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>foreground
</span></span></code></pre></div><p><strong>Using the Kubernetes API</strong></p><ol><li><p>Start a local proxy session:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span></code></pre></div></li><li><p>Use <code>curl</code> to trigger deletion:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -d <span style="color:#b44">'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -H <span style="color:#b44">"Content-Type: application/json"</span>
</span></span></code></pre></div><p>The output contains a <code>foregroundDeletion</code> <a class="glossary-tooltip" title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/finalizers/" target="_blank" aria-label="finalizer">finalizer</a>
like this:</p><pre tabindex="0"><code>"kind": "Deployment",
"apiVersion": "apps/v1",
"metadata": {
    "name": "nginx-deployment",
    "namespace": "default",
    "uid": "d1ce1b02-cae8-4288-8a53-30e84d8fa505",
    "resourceVersion": "1363097",
    "creationTimestamp": "2021-07-08T20:24:37Z",
    "deletionTimestamp": "2021-07-08T20:27:39Z",
    "finalizers": [
      "foregroundDeletion"
    ]
    ...
</code></pre></li></ol><h2 id="use-background-cascading-deletion">Use background cascading deletion</h2><ol><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">Create a sample Deployment</a>.</li><li>Use either <code>kubectl</code> or the Kubernetes API to delete the Deployment,
depending on the Kubernetes version your cluster runs.<p>To check the version, enter <code>kubectl version</code>.</p></li></ol><p>You can delete objects using background cascading deletion using <code>kubectl</code>
or the Kubernetes API.</p><p>Kubernetes uses background cascading deletion by default, and does so
even if you run the following commands without the <code>--cascade</code> flag or the
<code>propagationPolicy</code> argument.</p><p><strong>Using kubectl</strong></p><p>Run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>background
</span></span></code></pre></div><p><strong>Using the Kubernetes API</strong></p><ol><li><p>Start a local proxy session:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span></code></pre></div></li><li><p>Use <code>curl</code> to trigger deletion:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -d <span style="color:#b44">'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Background"}'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -H <span style="color:#b44">"Content-Type: application/json"</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>"kind": "Status",
"apiVersion": "v1",
...
"status": "Success",
"details": {
    "name": "nginx-deployment",
    "group": "apps",
    "kind": "deployments",
    "uid": "cc9eefb9-2d49-4445-b1c1-d261c9396456"
}
</code></pre></li></ol><h2 id="set-orphan-deletion-policy">Delete owner objects and orphan dependents</h2><p>By default, when you tell Kubernetes to delete an object, the
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> also deletes
dependent objects. You can make Kubernetes <em>orphan</em> these dependents using
<code>kubectl</code> or the Kubernetes API, depending on the Kubernetes version your
cluster runs.<p>To check the version, enter <code>kubectl version</code>.</p></p><p><strong>Using kubectl</strong></p><p>Run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>orphan
</span></span></code></pre></div><p><strong>Using the Kubernetes API</strong></p><ol><li><p>Start a local proxy session:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span></code></pre></div></li><li><p>Use <code>curl</code> to trigger deletion:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -d <span style="color:#b44">'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -H <span style="color:#b44">"Content-Type: application/json"</span>
</span></span></code></pre></div><p>The output contains <code>orphan</code> in the <code>finalizers</code> field, similar to this:</p><pre tabindex="0"><code>"kind": "Deployment",
"apiVersion": "apps/v1",
"namespace": "default",
"uid": "6f577034-42a0-479d-be21-78018c466f1f",
"creationTimestamp": "2021-07-09T16:46:37Z",
"deletionTimestamp": "2021-07-09T16:47:08Z",
"deletionGracePeriodSeconds": 0,
"finalizers": [
  "orphan"
],
...
</code></pre></li></ol><p>You can check that the Pods managed by the Deployment are still running:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/overview/working-with-objects/owners-dependents/">owners and dependents</a> in Kubernetes.</li><li>Learn about Kubernetes <a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a>.</li><li>Learn about <a href="/docs/concepts/architecture/garbage-collection/">garbage collection</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Connect a Frontend to a Backend Using Services</h1><p>This task shows how to create a <em>frontend</em> and a <em>backend</em> microservice. The backend
microservice is a hello greeter. The frontend exposes the backend using nginx and a
Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> object.</p><h2 id="objectives">Objectives</h2><ul><li>Create and run a sample <code>hello</code> backend microservice using a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a> object.</li><li>Use a Service object to send traffic to the backend microservice's multiple replicas.</li><li>Create and run a <code>nginx</code> frontend microservice, also using a Deployment object.</li><li>Configure the frontend microservice to send traffic to the backend microservice.</li><li>Use a Service object of <code>type=LoadBalancer</code> to expose the frontend microservice
outside the cluster.</li></ul><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>This task uses
<a href="/docs/tasks/access-application-cluster/create-external-load-balancer/">Services with external load balancers</a>, which
require a supported environment. If your environment does not support this, you can use a Service of type
<a href="/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> instead.</p><h2 id="creating-the-backend-using-a-deployment">Creating the backend using a Deployment</h2><p>The backend is a simple hello greeter microservice. Here is the configuration
file for the backend Deployment:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/backend-deployment.yaml" download="service/access/backend-deployment.yaml"><code>service/access/backend-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-access-backend-deployment-yaml&quot;)" title="Copy service/access/backend-deployment.yaml to clipboard"/></div><div class="includecode" id="service-access-backend-deployment-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">"gcr.io/google-samples/hello-go-gke:1.0"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span></span></span></code></pre></div></div></div><p>Create the backend Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/service/access/backend-deployment.yaml
</span></span></code></pre></div><p>View information about the backend Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe deployment backend
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                           backend
Namespace:                      default
CreationTimestamp:              Mon, 24 Oct 2016 14:21:02 -0700
Labels:                         app=hello
                                tier=backend
                                track=stable
Annotations:                    deployment.kubernetes.io/revision=1
Selector:                       app=hello,tier=backend,track=stable
Replicas:                       3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:                   RollingUpdate
MinReadySeconds:                0
RollingUpdateStrategy:          1 max unavailable, 1 max surge
Pod Template:
  Labels:       app=hello
                tier=backend
                track=stable
  Containers:
   hello:
    Image:              "gcr.io/google-samples/hello-go-gke:1.0"
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
OldReplicaSets:                 &lt;none&gt;
NewReplicaSet:                  hello-3621623197 (3/3 replicas created)
Events:
...
</code></pre><h2 id="creating-the-hello-service-object">Creating the <code>hello</code> Service object</h2><p>The key to sending requests from a frontend to a backend is the backend
Service. A Service creates a persistent IP address and DNS name entry
so that the backend microservice can always be reached. A Service uses
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels/" target="_blank" aria-label="selectors">selectors</a> to find
the Pods that it routes traffic to.</p><p>First, explore the Service configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/backend-service.yaml" download="service/access/backend-service.yaml"><code>service/access/backend-service.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-access-backend-service-yaml&quot;)" title="Copy service/access/backend-service.yaml to clipboard"/></div><div class="includecode" id="service-access-backend-service-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Service, named <code>hello</code> routes
traffic to Pods that have the labels <code>app: hello</code> and <code>tier: backend</code>.</p><p>Create the backend Service:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/service/access/backend-service.yaml
</span></span></code></pre></div><p>At this point, you have a <code>backend</code> Deployment running three replicas of your <code>hello</code>
application, and you have a Service that can route traffic to them. However, this
service is neither available nor resolvable outside the cluster.</p><h2 id="creating-the-frontend">Creating the frontend</h2><p>Now that you have your backend running, you can create a frontend that is accessible
outside the cluster, and connects to the backend by proxying requests to it.</p><p>The frontend sends requests to the backend worker Pods by using the DNS name
given to the backend Service. The DNS name is <code>hello</code>, which is the value
of the <code>name</code> field in the <code>examples/service/access/backend-service.yaml</code>
configuration file.</p><p>The Pods in the frontend Deployment run a nginx image that is configured
to proxy requests to the <code>hello</code> backend Service. Here is the nginx configuration file:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/frontend-nginx.conf" download="service/access/frontend-nginx.conf"><code>service/access/frontend-nginx.conf</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-access-frontend-nginx-conf&quot;)" title="Copy service/access/frontend-nginx.conf to clipboard"/></div><div class="includecode" id="service-access-frontend-nginx-conf"><pre tabindex="0"><code class="language-conf" data-lang="conf"># The identifier Backend is internal to nginx, and used to name this specific upstream
upstream Backend {
    # hello is the internal DNS name used by the backend Service inside Kubernetes
    server hello;
}
<p>server {
listen 80;</p>
<pre><code>location / {
    # The following statement will proxy traffic to the upstream named Backend
    proxy_pass http://Backend;
}
</code></pre><p>}</p></code></pre></div></div><p>Similar to the backend, the frontend has a Deployment and a Service. An important
difference to notice between the backend and frontend services, is that the
configuration for the frontend Service has <code>type: LoadBalancer</code>, which means that
the Service uses a load balancer provisioned by your cloud provider and will be
accessible from outside the cluster.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/frontend-service.yaml" download="service/access/frontend-service.yaml"><code>service/access/frontend-service.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-access-frontend-service-yaml&quot;)" title="Copy service/access/frontend-service.yaml to clipboard"/></div><div class="includecode" id="service-access-frontend-service-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span><span style="color:#b44">"TCP"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span></span></span></code></pre></div></div></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/frontend-deployment.yaml" download="service/access/frontend-deployment.yaml"><code>service/access/frontend-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-access-frontend-deployment-yaml&quot;)" title="Copy service/access/frontend-deployment.yaml to clipboard"/></div><div class="includecode" id="service-access-frontend-deployment-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">"gcr.io/google-samples/hello-frontend:1.0"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">lifecycle</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">preStop</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">exec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"/usr/sbin/nginx"</span>,<span style="color:#b44">"-s"</span>,<span style="color:#b44">"quit"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span></span></span></code></pre></div></div></div><p>Create the frontend Deployment and Service:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/service/access/frontend-deployment.yaml
</span></span><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/service/access/frontend-service.yaml
</span></span></code></pre></div><p>The output verifies that both resources were created:</p><pre tabindex="0"><code>deployment.apps/frontend created
service/frontend created
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The nginx configuration is baked into the
<a href="/examples/service/access/Dockerfile">container image</a>. A better way to do this would
be to use a
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>,
so that you can change the configuration more easily.</div><h2 id="interact-with-the-frontend-service">Interact with the frontend Service</h2><p>Once you've created a Service of type LoadBalancer, you can use this
command to find the external IP:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get service frontend --watch
</span></span></code></pre></div><p>This displays the configuration for the <code>frontend</code> Service and watches for
changes. Initially, the external IP is listed as <code>&lt;pending&gt;</code>:</p><pre tabindex="0"><code>NAME       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)  AGE
frontend   LoadBalancer   10.51.252.116   &lt;pending&gt;     80/TCP   10s
</code></pre><p>As soon as an external IP is provisioned, however, the configuration updates
to include the new IP under the <code>EXTERNAL-IP</code> heading:</p><pre tabindex="0"><code>NAME       TYPE           CLUSTER-IP      EXTERNAL-IP        PORT(S)  AGE
frontend   LoadBalancer   10.51.252.116   XXX.XXX.XXX.XXX    80/TCP   1m
</code></pre><p>That IP can now be used to interact with the <code>frontend</code> service from outside the
cluster.</p><h2 id="send-traffic-through-the-frontend">Send traffic through the frontend</h2><p>The frontend and backend are now connected. You can hit the endpoint
by using the curl command on the external IP of your frontend Service.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl http://<span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">EXTERNAL_IP</span><span style="color:#b68;font-weight:700">}</span> <span style="color:#080;font-style:italic"># replace this with the EXTERNAL-IP you saw earlier</span>
</span></span></code></pre></div><p>The output shows the message generated by the backend:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{<span style="color:green;font-weight:700">"message"</span>:<span style="color:#b44">"Hello"</span>}
</span></span></code></pre></div><h2 id="cleaning-up">Cleaning up</h2><p>To delete the Services, enter this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete services frontend backend
</span></span></code></pre></div><p>To delete the Deployments, the ReplicaSets and the Pods that are running the backend and frontend applications, enter this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment frontend backend
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/services-networking/service/">Services</a></li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a></li><li>Learn more about <a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Service and Pods</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Memory and CPU Quotas for a Namespace</h1><div class="lead">Define overall memory and CPU resource limits for a namespace.</div><p>This page shows how to set quotas for the total amount memory and CPU that
can be used by all Pods running in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>.
You specify quotas in a
<a href="/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/">ResourceQuota</a>
object.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>Each node in your cluster must have at least 1 GiB of memory.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace quota-mem-cpu-example
</span></span></code></pre></div><h2 id="create-a-resourcequota">Create a ResourceQuota</h2><p>Here is a manifest for an example ResourceQuota:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-mem-cpu.yaml" download="admin/resource/quota-mem-cpu.yaml"><code>admin/resource/quota-mem-cpu.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-mem-cpu-yaml&quot;)" title="Copy admin/resource/quota-mem-cpu.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-mem-cpu-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mem-cpu-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests.cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests.memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">limits.cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">limits.memory</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>View detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get resourcequota mem-cpu-demo --namespace<span style="color:#666">=</span>quota-mem-cpu-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:</p><ul><li>For every Pod in the namespace, each container must have a memory request, memory limit, cpu request, and cpu limit.</li><li>The memory request total for all Pods in that namespace must not exceed 1 GiB.</li><li>The memory limit total for all Pods in that namespace must not exceed 2 GiB.</li><li>The CPU request total for all Pods in that namespace must not exceed 1 cpu.</li><li>The CPU limit total for all Pods in that namespace must not exceed 2 cpu.</li></ul><p>See <a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>
to learn what Kubernetes means by “1 CPU”.</p><h2 id="create-a-pod">Create a Pod</h2><p>Here is a manifest for an example Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-mem-cpu-pod.yaml" download="admin/resource/quota-mem-cpu-pod.yaml"><code>admin/resource/quota-mem-cpu-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-mem-cpu-pod-yaml&quot;)" title="Copy admin/resource/quota-mem-cpu-pod.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-mem-cpu-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"600Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"400m"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>Verify that the Pod is running and that its (only) container is healthy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod quota-mem-cpu-demo --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>Once again, view detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get resourcequota mem-cpu-demo --namespace<span style="color:#666">=</span>quota-mem-cpu-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output shows the quota along with how much of the quota has been used.
You can see that the memory and CPU requests and limits for your Pod do not
exceed the quota.</p><pre tabindex="0"><code>status:
  hard:
    limits.cpu: "2"
    limits.memory: 2Gi
    requests.cpu: "1"
    requests.memory: 1Gi
  used:
    limits.cpu: 800m
    limits.memory: 800Mi
    requests.cpu: 400m
    requests.memory: 600Mi
</code></pre><p>If you have the <code>jq</code> tool, you can also query (using <a href="/docs/reference/kubectl/jsonpath/">JSONPath</a>)
for just the <code>used</code> values, <strong>and</strong> pretty-print that that of the output. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get resourcequota mem-cpu-demo --namespace<span style="color:#666">=</span>quota-mem-cpu-example -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{ .status.used }'</span> | jq .
</span></span></code></pre></div><h2 id="attempt-to-create-a-second-pod">Attempt to create a second Pod</h2><p>Here is a manifest for a second Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-mem-cpu-pod-2.yaml" download="admin/resource/quota-mem-cpu-pod-2.yaml"><code>admin/resource/quota-mem-cpu-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-mem-cpu-pod-2-yaml&quot;)" title="Copy admin/resource/quota-mem-cpu-pod-2.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-mem-cpu-pod-2-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-2-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"800m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"700Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"400m"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In the manifest, you can see that the Pod has a memory request of 700 MiB.
Notice that the sum of the used memory request and this new memory
request exceeds the memory request quota: 600 MiB + 700 MiB &gt; 1 GiB.</p><p>Attempt to create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</span></span></code></pre></div><p>The second Pod does not get created. The output shows that creating the second Pod
would cause the memory request total to exceed the memory request quota.</p><pre tabindex="0"><code>Error from server (Forbidden): error when creating "examples/admin/resource/quota-mem-cpu-pod-2.yaml":
pods "quota-mem-cpu-demo-2" is forbidden: exceeded quota: mem-cpu-demo,
requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi
</code></pre><h2 id="discussion">Discussion</h2><p>As you have seen in this exercise, you can use a ResourceQuota to restrict
the memory request total for all Pods running in a namespace.
You can also restrict the totals for memory limit, cpu request, and cpu limit.</p><p>Instead of managing total resource use within a namespace, you might want to restrict
individual Pods, or the containers in those Pods. To achieve that kind of limiting, use a
<a href="/docs/concepts/policy/limit-range/">LimitRange</a>.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace quota-mem-cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Using sysctls in a Kubernetes Cluster</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>This document describes how to configure and use kernel parameters within a
Kubernetes cluster using the <a class="glossary-tooltip" title="An interface for getting and setting Unix kernel parameters" data-toggle="tooltip" data-placement="top" href="/docs/tasks/administer-cluster/sysctl-cluster/" target="_blank" aria-label="sysctl">sysctl</a>
interface.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Starting from Kubernetes version 1.23, the kubelet supports the use of either <code>/</code> or <code>.</code>
as separators for sysctl names.
Starting from Kubernetes version 1.25, setting Sysctls for a Pod supports setting sysctls with slashes.
For example, you can represent the same sysctl name as <code>kernel.shm_rmid_forced</code> using a
period as the separator, or as <code>kernel/shm_rmid_forced</code> using a slash as a separator.
For more sysctl parameter conversion method details, please refer to
the page <a href="https://man7.org/linux/man-pages/man5/sysctl.d.5.html">sysctl.d(5)</a> from
the Linux man-pages project.</div><h2 id="before-you-begin">Before you begin</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>sysctl</code> is a Linux-specific command-line tool used to configure various kernel parameters
and it is not available on non-Linux operating systems.</div><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>For some steps, you also need to be able to reconfigure the command line
options for the kubelets running on your cluster.</p><h2 id="listing-all-sysctl-parameters">Listing all Sysctl Parameters</h2><p>In Linux, the sysctl interface allows an administrator to modify kernel
parameters at runtime. Parameters are available via the <code>/proc/sys/</code> virtual
process file system. The parameters cover various subsystems such as:</p><ul><li>kernel (common prefix: <code>kernel.</code>)</li><li>networking (common prefix: <code>net.</code>)</li><li>virtual memory (common prefix: <code>vm.</code>)</li><li>MDADM (common prefix: <code>dev.</code>)</li><li>More subsystems are described in <a href="https://www.kernel.org/doc/Documentation/sysctl/README">Kernel docs</a>.</li></ul><p>To get a list of all parameters, you can run</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo sysctl -a
</span></span></code></pre></div><h2 id="safe-and-unsafe-sysctls">Safe and Unsafe Sysctls</h2><p>Kubernetes classes sysctls as either <em>safe</em> or <em>unsafe</em>. In addition to proper
namespacing, a <em>safe</em> sysctl must be properly <em>isolated</em> between pods on the
same node. This means that setting a <em>safe</em> sysctl for one pod</p><ul><li>must not have any influence on any other pod on the node</li><li>must not allow to harm the node's health</li><li>must not allow to gain CPU or memory resources outside of the resource limits
of a pod.</li></ul><p>By far, most of the <em>namespaced</em> sysctls are not necessarily considered <em>safe</em>.
The following sysctls are supported in the <em>safe</em> set:</p><ul><li><code>kernel.shm_rmid_forced</code>;</li><li><code>net.ipv4.ip_local_port_range</code>;</li><li><code>net.ipv4.tcp_syncookies</code>;</li><li><code>net.ipv4.ping_group_range</code> (since Kubernetes 1.18);</li><li><code>net.ipv4.ip_unprivileged_port_start</code> (since Kubernetes 1.22);</li><li><code>net.ipv4.ip_local_reserved_ports</code> (since Kubernetes 1.27, needs kernel 3.16+);</li><li><code>net.ipv4.tcp_keepalive_time</code> (since Kubernetes 1.29, needs kernel 4.5+);</li><li><code>net.ipv4.tcp_fin_timeout</code> (since Kubernetes 1.29, needs kernel 4.6+);</li><li><code>net.ipv4.tcp_keepalive_intvl</code> (since Kubernetes 1.29, needs kernel 4.5+);</li><li><code>net.ipv4.tcp_keepalive_probes</code> (since Kubernetes 1.29, needs kernel 4.5+).</li><li><code>net.ipv4.tcp_rmem</code> (since Kubernetes 1.32, needs kernel 4.15+).</li><li><code>net.ipv4.tcp_wmem</code> (since Kubernetes 1.32, needs kernel 4.15+).</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>There are some exceptions to the set of safe sysctls:</p><ul><li>The <code>net.*</code> sysctls are not allowed with host networking enabled.</li><li>The <code>net.ipv4.tcp_syncookies</code> sysctl is not namespaced on Linux kernel version 4.5 or lower.</li></ul></div><p>This list will be extended in future Kubernetes versions when the kubelet
supports better isolation mechanisms.</p><h3 id="enabling-unsafe-sysctls">Enabling Unsafe Sysctls</h3><p>All <em>safe</em> sysctls are enabled by default.</p><p>All <em>unsafe</em> sysctls are disabled by default and must be allowed manually by the
cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be
scheduled, but will fail to launch.</p><p>With the warning above in mind, the cluster admin can allow certain <em>unsafe</em>
sysctls for very special situations such as high-performance or real-time
application tuning. <em>Unsafe</em> sysctls are enabled on a node-by-node basis with a
flag of the kubelet; for example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubelet --allowed-unsafe-sysctls <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  <span style="color:#b44">'kernel.msg*,net.core.somaxconn'</span> ...
</span></span></code></pre></div><p>For <a class="glossary-tooltip" title="A tool for running Kubernetes locally." data-toggle="tooltip" data-placement="top" href="/docs/tasks/tools/#minikube" target="_blank" aria-label="Minikube">Minikube</a>, this can be done via the <code>extra-config</code> flag:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>minikube start --extra-config<span style="color:#666">=</span><span style="color:#b44">"kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn"</span>...
</span></span></code></pre></div><p>Only <em>namespaced</em> sysctls can be enabled this way.</p><h2 id="setting-sysctls-for-a-pod">Setting Sysctls for a Pod</h2><p>A number of sysctls are <em>namespaced</em> in today's Linux kernels. This means that
they can be set independently for each pod on a node. Only namespaced sysctls
are configurable via the pod securityContext within Kubernetes.</p><p>The following sysctls are known to be namespaced. This list could change
in future versions of the Linux kernel.</p><ul><li><code>kernel.shm*</code>,</li><li><code>kernel.msg*</code>,</li><li><code>kernel.sem</code>,</li><li><code>fs.mqueue.*</code>,</li><li>Those <code>net.*</code> that can be set in container networking namespace. However,
there are exceptions (e.g., <code>net.netfilter.nf_conntrack_max</code> and
<code>net.netfilter.nf_conntrack_expect_max</code> can be set in container networking
namespace but are unnamespaced before Linux 5.12.2).</li></ul><p>Sysctls with no namespace are called <em>node-level</em> sysctls. If you need to set
them, you must manually configure them on each node's operating system, or by
using a DaemonSet with privileged containers.</p><p>Use the pod securityContext to configure namespaced sysctls. The securityContext
applies to all containers in the same pod.</p><p>This example uses the pod securityContext to set a safe sysctl
<code>kernel.shm_rmid_forced</code> and two unsafe sysctls <code>net.core.somaxconn</code> and
<code>kernel.msgmax</code>. There is no distinction between <em>safe</em> and <em>unsafe</em> sysctls in
the specification.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Only modify sysctl parameters after you understand their effects, to avoid
destabilizing your operating system.</div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>sysctl-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">securityContext</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">sysctls</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kernel.shm_rmid_forced<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>net.core.somaxconn<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1024"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kernel.msgmax<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"65536"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Due to their nature of being <em>unsafe</em>, the use of <em>unsafe</em> sysctls
is at-your-own-risk and can lead to severe problems like wrong behavior of
containers, resource shortage or complete breakage of a node.</div><p>It is good practice to consider nodes with special sysctl settings as
<em>tainted</em> within a cluster, and only schedule pods onto them which need those
sysctl settings. It is suggested to use the Kubernetes <a href="/docs/reference/generated/kubectl/kubectl-commands/#taint"><em>taints and toleration</em>
feature</a> to implement this.</p><p>A pod with the <em>unsafe</em> sysctls will fail to launch on any node which has not
enabled those two <em>unsafe</em> sysctls explicitly. As with <em>node-level</em> sysctls it
is recommended to use
<a href="/docs/reference/generated/kubectl/kubectl-commands/#taint"><em>taints and toleration</em> feature</a> or
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taints on nodes</a>
to schedule those pods onto the right nodes.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Operating etcd clusters for Kubernetes</h1><p><p>etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p></p><p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a
<a href="/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">back up</a> plan
for the data.</p><p>You can find in-depth information about etcd in the official <a href="https://etcd.io/docs/">documentation</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>Before you follow steps in this page to deploy, manage, back up or restore etcd,
you need to understand the typical expectations for operating an etcd cluster.
Refer to the <a href="https://etcd.io/docs/">etcd documentation</a> for more context.</p><p>Key details include:</p><ul><li><p>The minimum recommended etcd versions to run in production are <code>3.4.22+</code> and <code>3.5.6+</code>.</p></li><li><p>etcd is a leader-based distributed system. Ensure that the leader
periodically send heartbeats on time to all followers to keep the cluster
stable.</p></li><li><p>You should run etcd as a cluster with an odd number of members.</p></li><li><p>Aim to ensure that no resource starvation occurs.</p><p>Performance and stability of the cluster is sensitive to network and disk
I/O. Any resource starvation can lead to heartbeat timeout, causing instability
of the cluster. An unstable etcd indicates that no leader is elected. Under
such circumstances, a cluster cannot make any changes to its current state,
which implies no new pods can be scheduled.</p></li></ul><h3 id="resource-requirements-for-etcd">Resource requirements for etcd</h3><p>Operating etcd with limited resources is suitable only for testing purposes.
For deploying in production, advanced hardware configuration is required.
Before deploying etcd in production, see
<a href="https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations">resource requirement reference</a>.</p><p>Keeping etcd clusters stable is critical to the stability of Kubernetes
clusters. Therefore, run etcd clusters on dedicated machines or isolated
environments for <a href="https://etcd.io/docs/current/op-guide/hardware/">guaranteed resource requirements</a>.</p><h3 id="tools">Tools</h3><p>Depending on which specific outcome you're working on, you will need the <code>etcdctl</code> tool or the
<code>etcdutl</code> tool (you may need both).</p><h2 id="understanding-etcdctl-and-etcdutl">Understanding etcdctl and etcdutl</h2><p><code>etcdctl</code> and <code>etcdutl</code> are command-line tools used to interact with etcd clusters, but they serve different purposes:</p><ul><li><p><code>etcdctl</code>: This is the primary command-line client for interacting with etcd over a
network. It is used for day-to-day operations such as managing keys and values,
administering the cluster, checking health, and more.</p></li><li><p><code>etcdutl</code>: This is an administration utility designed to operate directly on etcd data
files, including migrating data between etcd versions, defragmenting the database,
restoring snapshots, and validating data consistency. For network operations, <code>etcdctl</code>
should be used.</p></li></ul><p>For more information on <code>etcdutl</code>, you can refer to the <a href="https://etcd.io/docs/v3.5/op-guide/recovery/">etcd recovery documentation</a>.</p><h2 id="starting-etcd-clusters">Starting etcd clusters</h2><p>This section covers starting a single-node and multi-node etcd cluster.</p><p>This guide assumes that <code>etcd</code> is already installed.</p><h3 id="single-node-etcd-cluster">Single-node etcd cluster</h3><p>Use a single-node etcd cluster only for testing purposes.</p><ol><li><p>Run the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379 <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379
</span></span></code></pre></div></li><li><p>Start the Kubernetes API server with the flag
<code>--etcd-servers=$PRIVATE_IP:2379</code>.</p><p>Make sure <code>PRIVATE_IP</code> is set to your etcd client IP.</p></li></ol><h3 id="multi-node-etcd-cluster">Multi-node etcd cluster</h3><p>For durability and high availability, run etcd as a multi-node cluster in
production and back it up periodically. A five-member cluster is recommended
in production. For more information, see
<a href="https://etcd.io/docs/current/faq/#what-is-failure-tolerance">FAQ documentation</a>.</p><p>As you're using Kubernetes, you have the option to run etcd as a container inside
one or more Pods. The <code>kubeadm</code> tool sets up etcd
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static pods">static pods</a> by default, or
you can deploy a
<a href="/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">separate cluster</a>
and instruct kubeadm to use that etcd cluster as the control plane's backing store.</p><p>You configure an etcd cluster either by static member information or by dynamic
discovery. For more information on clustering, see
<a href="https://etcd.io/docs/current/op-guide/clustering/">etcd clustering documentation</a>.</p><p>For an example, consider a five-member etcd cluster running with the following
client URLs: <code>http://$IP1:2379</code>, <code>http://$IP2:2379</code>, <code>http://$IP3:2379</code>,
<code>http://$IP4:2379</code>, and <code>http://$IP5:2379</code>. To start a Kubernetes API server:</p><ol><li><p>Run the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379 --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379
</span></span></code></pre></div></li><li><p>Start the Kubernetes API servers with the flag
<code>--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379</code>.</p><p>Make sure the <code>IP&lt;n&gt;</code> variables are set to your client IP addresses.</p></li></ol><h3 id="multi-node-etcd-cluster-with-load-balancer">Multi-node etcd cluster with load balancer</h3><p>To run a load balancing etcd cluster:</p><ol><li>Set up an etcd cluster.</li><li>Configure a load balancer in front of the etcd cluster.
For example, let the address of the load balancer be <code>$LB</code>.</li><li>Start Kubernetes API Servers with the flag <code>--etcd-servers=$LB:2379</code>.</li></ol><h2 id="securing-etcd-clusters">Securing etcd clusters</h2><p>Access to etcd is equivalent to root permission in the cluster so ideally only
the API server should have access to it. Considering the sensitivity of the
data, it is recommended to grant permission to only those nodes that require
access to etcd clusters.</p><p>To secure etcd, either set up firewall rules or use the security features
provided by etcd. etcd security features depend on x509 Public Key
Infrastructure (PKI). To begin, establish secure communication channels by
generating a key and certificate pair. For example, use key pairs <code>peer.key</code>
and <code>peer.cert</code> for securing communication between etcd members, and
<code>client.key</code> and <code>client.cert</code> for securing communication between etcd and its
clients. See the <a href="https://github.com/coreos/etcd/tree/master/hack/tls-setup">example scripts</a>
provided by the etcd project to generate key pairs and CA files for client
authentication.</p><h3 id="securing-communication">Securing communication</h3><p>To configure etcd with secure peer communication, specify flags
<code>--peer-key-file=peer.key</code> and <code>--peer-cert-file=peer.cert</code>, and use HTTPS as
the URL schema.</p><p>Similarly, to configure etcd with secure client communication, specify flags
<code>--key=k8sclient.key</code> and <code>--cert=k8sclient.cert</code>, and use HTTPS as
the URL schema. Here is an example on a client command that uses secure
communication:</p><pre tabindex="0"><code>ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
</code></pre><h3 id="limiting-access-of-etcd-clusters">Limiting access of etcd clusters</h3><p>After configuring secure communication, restrict the access of the etcd cluster to
only the Kubernetes API servers using TLS authentication.</p><p>For example, consider key pairs <code>k8sclient.key</code> and <code>k8sclient.cert</code> that are
trusted by the CA <code>etcd.ca</code>. When etcd is configured with <code>--client-cert-auth</code>
along with TLS, it verifies the certificates from clients by using system CAs
or the CA passed in by <code>--trusted-ca-file</code> flag. Specifying flags
<code>--client-cert-auth=true</code> and <code>--trusted-ca-file=etcd.ca</code> will restrict the
access to clients with the certificate <code>k8sclient.cert</code>.</p><p>Once etcd is configured correctly, only clients with valid certificates can
access it. To give Kubernetes API servers the access, configure them with the
flags <code>--etcd-certfile=k8sclient.cert</code>, <code>--etcd-keyfile=k8sclient.key</code> and
<code>--etcd-cafile=ca.cert</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>etcd authentication is not planned for Kubernetes.</div><h2 id="replacing-a-failed-etcd-member">Replacing a failed etcd member</h2><p>etcd cluster achieves high availability by tolerating minor member failures.
However, to improve the overall health of the cluster, replace failed members
immediately. When multiple members fail, replace them one by one. Replacing a
failed member involves two steps: removing the failed member and adding a new
member.</p><p>Though etcd keeps unique member IDs internally, it is recommended to use a
unique name for each member to avoid human errors. For example, consider a
three-member etcd cluster. Let the URLs be, <code>member1=http://10.0.0.1</code>,
<code>member2=http://10.0.0.2</code>, and <code>member3=http://10.0.0.3</code>. When <code>member1</code> fails,
replace it with <code>member4=http://10.0.0.4</code>.</p><ol><li><p>Get the member ID of the failed <code>member1</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdctl --endpoints<span style="color:#666">=</span>http://10.0.0.2,http://10.0.0.3 member list
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
</span></span></span><span style="display:flex"><span><span style="color:#888">91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
</span></span></span><span style="display:flex"><span><span style="color:#888">fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</span></span></span></code></pre></div></li><li><p>Do either of the following:</p><ol><li>If each Kubernetes API server is configured to communicate with all etcd
members, remove the failed member from the <code>--etcd-servers</code> flag, then
restart each Kubernetes API server.</li><li>If each Kubernetes API server communicates with a single etcd member,
then stop the Kubernetes API server that communicates with the failed
etcd.</li></ol></li><li><p>Stop the etcd server on the broken node. It is possible that other
clients besides the Kubernetes API server are causing traffic to etcd
and it is desirable to stop all traffic to prevent writes to the data
directory.</p></li><li><p>Remove the failed member:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdctl member remove 8211f1d0f64f3269
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Removed member 8211f1d0f64f3269 from cluster
</span></span></span></code></pre></div></li><li><p>Add the new member:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdctl member add member4 --peer-urls<span style="color:#666">=</span>http://10.0.0.4:2380
</span></span></code></pre></div><p>The following message is displayed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</span></span></span></code></pre></div></li><li><p>Start the newly added member on a machine with the IP <code>10.0.0.4</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_NAME</span><span style="color:#666">=</span><span style="color:#b44">"member4"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER</span><span style="color:#666">=</span><span style="color:#b44">"member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER_STATE</span><span style="color:#666">=</span>existing
</span></span><span style="display:flex"><span>etcd <span style="color:#666">[</span>flags<span style="color:#666">]</span>
</span></span></code></pre></div></li><li><p>Do either of the following:</p><ol><li>If each Kubernetes API server is configured to communicate with all etcd
members, add the newly added member to the <code>--etcd-servers</code> flag, then
restart each Kubernetes API server.</li><li>If each Kubernetes API server communicates with a single etcd member,
start the Kubernetes API server that was stopped in step 2. Then
configure Kubernetes API server clients to again route requests to the
Kubernetes API server that was stopped. This can often be done by
configuring a load balancer.</li></ol></li></ol><p>For more information on cluster reconfiguration, see
<a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd reconfiguration documentation</a>.</p><h2 id="backing-up-an-etcd-cluster">Backing up an etcd cluster</h2><p>All Kubernetes objects are stored in etcd. Periodically backing up the etcd
cluster data is important to recover Kubernetes clusters under disaster
scenarios, such as losing all control plane nodes. The snapshot file contains
all the Kubernetes state and critical information. In order to keep the
sensitive Kubernetes data safe, encrypt the snapshot files.</p><p>Backing up an etcd cluster can be accomplished in two ways: etcd built-in
snapshot and volume snapshot.</p><h3 id="built-in-snapshot">Built-in snapshot</h3><p>etcd supports built-in snapshot. A snapshot may either be created from a live
member with the <code>etcdctl snapshot save</code> command or by copying the
<code>member/snap/db</code> file from an etcd
<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">data directory</a>
that is not currently used by an etcd process. Creating the snapshot will
not affect the performance of the member.</p><p>Below is an example for creating a snapshot of the keyspace served by
<code>$ENDPOINT</code> to the file <code>snapshot.db</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints <span style="color:#b8860b">$ENDPOINT</span> snapshot save snapshot.db
</span></span></code></pre></div><p>Verify the snapshot:</p><ul class="nav nav-tabs" id="etcd-verify-snapshot" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#etcd-verify-snapshot-0" role="tab" aria-controls="etcd-verify-snapshot-0" aria-selected="true">Use etcdutl</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#etcd-verify-snapshot-1" role="tab" aria-controls="etcd-verify-snapshot-1">Use etcdctl (Deprecated)</a></li></ul><div class="tab-content" id="etcd-verify-snapshot"><div id="etcd-verify-snapshot-0" class="tab-pane show active" role="tabpanel" aria-labelledby="etcd-verify-snapshot-0"><p><p>The below example depicts the usage of the <code>etcdutl</code> tool for verifying a snapshot:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdutl --write-out<span style="color:#666">=</span>table snapshot status snapshot.db 
</span></span></code></pre></div><p>This should generate an output resembling the example provided below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">| fe01cf57 |       10 |          7 | 2.1 MB     |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span></code></pre></div></p></div><div id="etcd-verify-snapshot-1" class="tab-pane" role="tabpanel" aria-labelledby="etcd-verify-snapshot-1"><p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The usage of <code>etcdctl snapshot status</code> has been <strong>deprecated</strong> since etcd v3.5.x and is slated for removal from etcd v3.6.
It is recommended to utilize <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a> instead.</div><p>The below example depicts the usage of the <code>etcdctl</code> tool for verifying a snapshot:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span>
</span></span><span style="display:flex"><span>etcdctl --write-out<span style="color:#666">=</span>table snapshot status snapshot.db
</span></span></code></pre></div><p>This should generate an output resembling the example provided below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Deprecated: Use `etcdutl snapshot status` instead.
</span></span></span><span style="display:flex"><span><span style="color:#888"/><span>
</span></span></span><span style="display:flex"><span><span/><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span><span style="display:flex"><span><span style="color:#888">| fe01cf57 |       10 |          7 | 2.1 MB     |
</span></span></span><span style="display:flex"><span><span style="color:#888">+----------+----------+------------+------------+
</span></span></span></code></pre></div></p></div></div><h3 id="volume-snapshot">Volume snapshot</h3><p>If etcd is running on a storage volume that supports backup, such as Amazon
Elastic Block Store, back up etcd data by creating a snapshot of the storage
volume.</p><h3 id="snapshot-using-etcdctl-options">Snapshot using etcdctl options</h3><p>We can also create the snapshot using various options given by etcdctl. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl -h 
</span></span></code></pre></div><p>will list various options available from etcdctl. For example, you can create a snapshot by specifying
the endpoint, certificates and key as shown below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints<span style="color:#666">=</span>https://127.0.0.1:2379 <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --cacert<span style="color:#666">=</span>&lt;trusted-ca-file&gt; --cert<span style="color:#666">=</span>&lt;cert-file&gt; --key<span style="color:#666">=</span>&lt;key-file&gt; <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  snapshot save &lt;backup-file-location&gt;
</span></span></code></pre></div><p>where <code>trusted-ca-file</code>, <code>cert-file</code> and <code>key-file</code> can be obtained from the description of the etcd Pod.</p><h2 id="scaling-out-etcd-clusters">Scaling out etcd clusters</h2><p>Scaling out etcd clusters increases availability by trading off performance.
Scaling does not increase cluster performance nor capability. A general rule
is not to scale out or in etcd clusters. Do not configure any auto scaling
groups for etcd clusters. It is strongly recommended to always run a static
five-member etcd cluster for production Kubernetes clusters at any officially
supported scale.</p><p>A reasonable scaling is to upgrade a three-member cluster to a five-member
one, when more reliability is desired. See
<a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd reconfiguration documentation</a>
for information on how to add members into an existing cluster.</p><h2 id="restoring-an-etcd-cluster">Restoring an etcd cluster</h2><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>If any API servers are running in your cluster, you should not attempt to
restore instances of etcd. Instead, follow these steps to restore etcd:</p><ul><li>stop <em>all</em> API server instances</li><li>restore state in all etcd instances</li><li>restart all API server instances</li></ul><p>The Kubernetes project also recommends restarting Kubernetes components (<code>kube-scheduler</code>,
<code>kube-controller-manager</code>, <code>kubelet</code>) to ensure that they don't rely on some
stale data. In practice the restore takes a bit of time. During the
restoration, critical components will lose leader lock and restart themselves.</p></div><p>etcd supports restoring from snapshots that are taken from an etcd process of
the <a href="https://semver.org/">major.minor</a> version. Restoring a version from a
different patch version of etcd is also supported. A restore operation is
employed to recover the data of a failed cluster.</p><p>Before starting the restore operation, a snapshot file must be present. It can
either be a snapshot file from a previous backup operation, or from a remaining
<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">data directory</a>.</p><ul class="nav nav-tabs" id="etcd-restore" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#etcd-restore-0" role="tab" aria-controls="etcd-restore-0" aria-selected="true">Use etcdutl</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#etcd-restore-1" role="tab" aria-controls="etcd-restore-1">Use etcdctl (Deprecated)</a></li></ul><div class="tab-content" id="etcd-restore"><div id="etcd-restore-0" class="tab-pane show active" role="tabpanel" aria-labelledby="etcd-restore-0"><p><p>When restoring the cluster using <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a>,
use the <code>--data-dir</code> option to specify to which folder the cluster should be restored:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>etcdutl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</span></span></code></pre></div><p>where <code>&lt;data-dir-location&gt;</code> is a directory that will be created during the restore process.</p></p></div><div id="etcd-restore-1" class="tab-pane" role="tabpanel" aria-labelledby="etcd-restore-1"><p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The usage of <code>etcdctl</code> for restoring has been <strong>deprecated</strong> since etcd v3.5.x and is slated for removal from etcd v3.6.
It is recommended to utilize <a href="https://github.com/etcd-io/etcd/blob/main/etcdutl/README.md"><code>etcdutl</code></a> instead.</div><p>The below example depicts the usage of the <code>etcdctl</code> tool for the restore operation:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span>
</span></span><span style="display:flex"><span>etcdctl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</span></span></code></pre></div><p>If <code>&lt;data-dir-location&gt;</code> is the same folder as before, delete it and stop the etcd process before restoring the cluster.
Otherwise, change etcd configuration and restart the etcd process after restoration to have it use the new data directory:
first change <code>/etc/kubernetes/manifests/etcd.yaml</code>'s <code>volumes.hostPath.path</code> for <code>name: etcd-data</code> to <code>&lt;data-dir-location&gt;</code>,
then execute <code>kubectl -n kube-system delete pod &lt;name-of-etcd-pod&gt;</code> or <code>systemctl restart kubelet.service</code> (or both).</p></p></div></div><p>For more information and examples on restoring a cluster from a snapshot file, see
<a href="https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster">etcd disaster recovery documentation</a>.</p><p>If the access URLs of the restored cluster are changed from the previous
cluster, the Kubernetes API server must be reconfigured accordingly. In this
case, restart Kubernetes API servers with the flag
<code>--etcd-servers=$NEW_ETCD_CLUSTER</code> instead of the flag
<code>--etcd-servers=$OLD_ETCD_CLUSTER</code>. Replace <code>$NEW_ETCD_CLUSTER</code> and
<code>$OLD_ETCD_CLUSTER</code> with the respective IP addresses. If a load balancer is
used in front of an etcd cluster, you might need to update the load balancer
instead.</p><p>If the majority of etcd members have permanently failed, the etcd cluster is
considered failed. In this scenario, Kubernetes cannot make any changes to its
current state. Although the scheduled pods might continue to run, no new pods
can be scheduled. In such cases, recover the etcd cluster and potentially
reconfigure Kubernetes API servers to fix the issue.</p><h2 id="upgrading-etcd-clusters">Upgrading etcd clusters</h2><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Before you start an upgrade, back up your etcd cluster first.</div><p>For details on etcd upgrade, refer to the <a href="https://etcd.io/docs/latest/upgrades/">etcd upgrades</a> documentation.</p><h2 id="maintaining-etcd-clusters">Maintaining etcd clusters</h2><p>For more details on etcd maintenance, please refer to the <a href="https://etcd.io/docs/latest/op-guide/maintenance/">etcd maintenance</a> documentation.</p><h3 id="cluster-defragmentation">Cluster defragmentation</h3><div class="alert alert-secondary callout third-party-content" role="alert">🛇 This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>Defragmentation is an expensive operation, so it should be executed as infrequently
as possible. On the other hand, it's also necessary to make sure any etcd member
will not exceed the storage quota. The Kubernetes project recommends that when
you perform defragmentation, you use a tool such as <a href="https://github.com/ahrtr/etcd-defrag">etcd-defrag</a>.</p><p>You can also run the defragmentation tool as a Kubernetes CronJob, to make sure that
defragmentation happens regularly. See <a href="https://github.com/ahrtr/etcd-defrag/blob/main/doc/etcd-defrag-cronjob.yaml"><code>etcd-defrag-cronjob.yaml</code></a>
for details.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Upgrade A Cluster</h1><p>This page provides an overview of the steps you should follow to upgrade a
Kubernetes cluster.</p><p>The Kubernetes project recommends upgrading to the latest patch releases promptly, and
to ensure that you are running a supported minor release of Kubernetes.
Following this recommendation helps you to stay secure.</p><p>The way that you upgrade a cluster depends on how you initially deployed it
and on any subsequent changes.</p><p>At a high level, the steps you perform are:</p><ul><li>Upgrade the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a></li><li>Upgrade the nodes in your cluster</li><li>Upgrade clients such as <a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/kubectl/" target="_blank" aria-label="kubectl">kubectl</a></li><li>Adjust manifests and other resources based on the API changes that accompany the
new Kubernetes version</li></ul><h2 id="before-you-begin">Before you begin</h2><p>You must have an existing cluster. This page is about upgrading from Kubernetes
1.33 to Kubernetes 1.34. If your cluster
is not currently running Kubernetes 1.33 then please check
the documentation for the version of Kubernetes that you plan to upgrade to.</p><h2 id="upgrade-approaches">Upgrade approaches</h2><h3 id="upgrade-kubeadm">kubeadm</h3><p>If your cluster was deployed using the <code>kubeadm</code> tool, refer to
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a>
for detailed information on how to upgrade the cluster.</p><p>Once you have upgraded the cluster, remember to
<a href="/docs/tasks/tools/">install the latest version of <code>kubectl</code></a>.</p><h3 id="manual-deployments">Manual deployments</h3><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>These steps do not account for third-party extensions such as network and storage
plugins.</div><p>You should manually update the control plane following this sequence:</p><ul><li>etcd (all instances)</li><li>kube-apiserver (all control plane hosts)</li><li>kube-controller-manager</li><li>kube-scheduler</li><li>cloud controller manager, if you use one</li></ul><p>At this point you should
<a href="/docs/tasks/tools/">install the latest version of <code>kubectl</code></a>.</p><p>For each node in your cluster, <a href="/docs/tasks/administer-cluster/safely-drain-node/">drain</a>
that node and then either replace it with a new node that uses the 1.34
kubelet, or upgrade the kubelet on that node and bring the node back into service.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Draining nodes before upgrading kubelet ensures that pods are re-admitted and containers are
re-created, which may be necessary to resolve some security issues or other important bugs.</div><h3 id="upgrade-other">Other deployments</h3><p>Refer to the documentation for your cluster deployment tool to learn the recommended set
up steps for maintenance.</p><h2 id="post-upgrade-tasks">Post-upgrade tasks</h2><h3 id="switch-your-cluster-s-storage-api-version">Switch your cluster's storage API version</h3><p>The objects that are serialized into etcd for a cluster's internal
representation of the Kubernetes resources active in the cluster are
written using a particular version of the API.</p><p>When the supported API changes, these objects may need to be rewritten
in the newer API. Failure to do this will eventually result in resources
that are no longer decodable or usable by the Kubernetes API server.</p><p>For each affected object, fetch it using the latest supported API and then
write it back also using the latest supported API.</p><h3 id="update-manifests">Update manifests</h3><p>Upgrading to a new Kubernetes version can provide new APIs.</p><p>You can use <code>kubectl convert</code> command to convert manifests between different API versions.
For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl convert -f pod.yaml --output-version v1
</span></span></code></pre></div><p>The <code>kubectl</code> tool replaces the contents of <code>pod.yaml</code> with a manifest that sets <code>kind</code> to
Pod (unchanged), but with a revised <code>apiVersion</code>.</p><h3 id="device-plugins">Device Plugins</h3><p>If your cluster is running device plugins and the node needs to be upgraded to a Kubernetes
release with a newer device plugin API version, device plugins must be upgraded to support
both version before the node is upgraded in order to guarantee that device allocations
continue to complete successfully during the upgrade.</p><p>Refer to <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#api-compatibility">API compatibility</a> and <a href="/docs/reference/node/device-plugin-api-versions/">Kubelet Device Manager API Versions</a> for more details.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Communicate Between Containers in the Same Pod Using a Shared Volume</h1><p>This page shows how to use a Volume to communicate between two Containers running
in the same Pod. See also how to allow processes to communicate by
<a href="/docs/tasks/configure-pod-container/share-process-namespace/">sharing process namespace</a>
between containers.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="creating-a-pod-that-runs-two-containers">Creating a Pod that runs two Containers</h2><p>In this exercise, you create a Pod that runs two Containers. The two containers
share a Volume that they can use to communicate. Here is the configuration file
for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/two-container-pod.yaml" download="pods/two-container-pod.yaml"><code>pods/two-container-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-two-container-pod-yaml&quot;)" title="Copy pods/two-container-pod.yaml to clipboard"/></div><div class="includecode" id="pods-two-container-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>two-containers<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>debian-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/pod-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"/bin/sh"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"-c"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"echo Hello from the debian container &gt; /pod-data/index.html"</span>]<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In the configuration file, you can see that the Pod has a Volume named
<code>shared-data</code>.</p><p>The first container listed in the configuration file runs an nginx server. The
mount path for the shared Volume is <code>/usr/share/nginx/html</code>.
The second container is based on the debian image, and has a mount path of
<code>/pod-data</code>. The second container runs the following command and then terminates.</p><pre><code>echo Hello from the debian container &gt; /pod-data/index.html
</code></pre><p>Notice that the second container writes the <code>index.html</code> file in the root
directory of the nginx server.</p><p>Create the Pod and the two Containers:</p><pre><code>kubectl apply -f https://k8s.io/examples/pods/two-container-pod.yaml
</code></pre><p>View information about the Pod and the Containers:</p><pre><code>kubectl get pod two-containers --output=yaml
</code></pre><p>Here is a portion of the output:</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  ...
  name: two-containers
  namespace: default
  ...
spec:
  ...
  containerStatuses:

  - containerID: docker://c1d8abd1 ...
    image: debian
    ...
    lastState:
      terminated:
        ...
    name: debian-container
    ...

  - containerID: docker://96c1ff2c5bb ...
    image: nginx
    ...
    name: nginx-container
    ...
    state:
      running:
    ...
</code></pre><p>You can see that the debian Container has terminated, and the nginx Container
is still running.</p><p>Get a shell to nginx Container:</p><pre><code>kubectl exec -it two-containers -c nginx-container -- /bin/bash
</code></pre><p>In your shell, verify that nginx is running:</p><pre><code>root@two-containers:/# apt-get update
root@two-containers:/# apt-get install curl procps
root@two-containers:/# ps aux
</code></pre><p>The output is similar to this:</p><pre><code>USER       PID  ...  STAT START   TIME COMMAND
root         1  ...  Ss   21:12   0:00 nginx: master process nginx -g daemon off;
</code></pre><p>Recall that the debian Container created the <code>index.html</code> file in the nginx root
directory. Use <code>curl</code> to send a GET request to the nginx server:</p><pre tabindex="0"><code>root@two-containers:/# curl localhost
</code></pre><p>The output shows that nginx serves a web page written by the debian container:</p><pre tabindex="0"><code>Hello from the debian container
</code></pre><h2 id="discussion">Discussion</h2><p>The primary reason that Pods can have multiple containers is to support
helper applications that assist a primary application. Typical examples of
helper applications are data pullers, data pushers, and proxies.
Helper and primary applications often need to communicate with each other.
Typically this is done through a shared filesystem, as shown in this exercise,
or through the loopback network interface, localhost. An example of this pattern is a
web server along with a helper program that polls a Git repository for new updates.</p><p>The Volume in this exercise provides a way for Containers to communicate during
the life of the Pod. If the Pod is deleted and recreated, any data stored in
the shared Volume is lost.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Learn more about <a href="/blog/2015/06/the-distributed-system-toolkit-patterns/">patterns for composite containers</a>.</p></li><li><p>Learn about <a href="https://www.slideshare.net/Docker/slideshare-burns">composite containers for modular architecture</a>.</p></li><li><p>See <a href="/docs/tasks/configure-pod-container/configure-volume-storage/">Configuring a Pod to Use a Volume for Storage</a>.</p></li><li><p>See <a href="/docs/tasks/configure-pod-container/share-process-namespace/">Configure a Pod to share process namespace between containers in a Pod</a></p></li><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#volume-v1-core">Volume</a>.</p></li><li><p>See <a href="/docs/reference/generated/kubernetes-api/v1.34/#pod-v1-core">Pod</a>.</p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Safely Drain a Node</h1><p>This page shows how to safely drain a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a>,
optionally respecting the PodDisruptionBudget you have defined.</p><h2 id="before-you-begin">Before you begin</h2><p>This task assumes that you have met the following prerequisites:</p><ol><li>You do not require your applications to be highly available during the
node drain, or</li><li>You have read about the <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> concept,
and have <a href="/docs/tasks/run-application/configure-pdb/">configured PodDisruptionBudgets</a> for
applications that need them.</li></ol><h2 id="configure-poddisruptionbudget">(Optional) Configure a disruption budget</h2><p>To ensure that your workloads remain available during maintenance, you can
configure a <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>.</p><p>If availability is important for any applications that run or could run on the node(s)
that you are draining, <a href="/docs/tasks/run-application/configure-pdb/">configure a PodDisruptionBudgets</a>
first and then continue following this guide.</p><p>It is recommended to set <code>AlwaysAllow</code> <a href="/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a>
to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.
The default behavior is to wait for the application pods to become <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>
before the drain can proceed.</p><h2 id="use-kubectl-drain-to-remove-a-node-from-service">Use <code>kubectl drain</code> to remove a node from service</h2><p>You can use <code>kubectl drain</code> to safely evict all of your pods from a
node before you perform maintenance on the node (e.g. kernel upgrade,
hardware maintenance, etc.). Safe evictions allow the pod's containers
to <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">gracefully terminate</a>
and will respect the PodDisruptionBudgets you have specified.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>By default <code>kubectl drain</code> ignores certain system pods on the node
that cannot be killed; see
the <a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a>
documentation for more details.</div><p>When <code>kubectl drain</code> returns successfully, that indicates that all of
the pods (except the ones excluded as described in the previous paragraph)
have been safely evicted (respecting the desired graceful termination period,
and respecting the PodDisruptionBudget you have defined). It is then safe to
bring down the node by powering down its physical machine or, if running on a
cloud platform, deleting its virtual machine.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If any new Pods tolerate the <code>node.kubernetes.io/unschedulable</code> taint, then those Pods
might be scheduled to the node you have drained. Avoid tolerating that taint other than
for DaemonSets.</p><p>If you or another API user directly set the <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#nodename"><code>nodeName</code></a>
field for a Pod (bypassing the scheduler), then the Pod is bound to the specified node
and will run there, even though you have drained that node and marked it unschedulable.</p></div><p>First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster with</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get nodes
</span></span></code></pre></div><p>Next, tell Kubernetes to drain the node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl drain --ignore-daemonsets &lt;node name&gt;
</span></span></code></pre></div><p>If there are pods managed by a DaemonSet, you will need to specify
<code>--ignore-daemonsets</code> with <code>kubectl</code> to successfully drain the node. The <code>kubectl drain</code> subcommand on its own does not actually drain
a node of its DaemonSet pods:
the DaemonSet controller (part of the control plane) immediately replaces missing Pods with
new equivalent Pods. The DaemonSet controller also creates Pods that ignore unschedulable
taints, which allows the new Pods to launch onto a node that you are draining.</p><p>Once it returns (without giving an error), you can power down the node
(or equivalently, if on a cloud platform, delete the virtual machine backing the node).
If you leave the node in the cluster during the maintenance operation, you need to run</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl uncordon &lt;node name&gt;
</span></span></code></pre></div><p>afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.</p><h2 id="draining-multiple-nodes-in-parallel">Draining multiple nodes in parallel</h2><p>The <code>kubectl drain</code> command should only be issued to a single node at a
time. However, you can run multiple <code>kubectl drain</code> commands for
different nodes in parallel, in different terminals or in the
background. Multiple drain commands running concurrently will still
respect the PodDisruptionBudget you specify.</p><p>For example, if you have a StatefulSet with three replicas and have
set a PodDisruptionBudget for that set specifying <code>minAvailable: 2</code>,
<code>kubectl drain</code> only evicts a pod from the StatefulSet if all three
replicas pods are <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>;
if then you issue multiple drain commands in parallel,
Kubernetes respects the PodDisruptionBudget and ensures that
only 1 (calculated as <code>replicas - minAvailable</code>) Pod is unavailable
at any given time. Any drains that would cause the number of <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>
replicas to fall below the specified budget are blocked.</p><h2 id="eviction-api">The Eviction API</h2><p>If you prefer not to use <a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a> (such as
to avoid calling to an external command, or to get finer control over the pod
eviction process), you can also programmatically cause evictions using the
eviction API.</p><p>For more information, see <a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated eviction</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow steps to protect your application by <a href="/docs/tasks/run-application/configure-pdb/">configuring a Pod Disruption Budget</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Share a Cluster with Namespaces</h1><p>This page shows how to view, work in, and delete <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespaces">namespaces</a>.
The page also shows how to use Kubernetes namespaces to subdivide your cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Have an <a href="/docs/setup/">existing Kubernetes cluster</a>.</li><li>You have a basic understanding of Kubernetes <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>,
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Services">Services</a>, and
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployments">Deployments</a>.</li></ul><h2 id="viewing-namespaces">Viewing namespaces</h2><p>List the current namespaces in a cluster using:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get namespaces
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME              STATUS   AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">default           Active   11d
</span></span></span><span style="display:flex"><span><span style="color:#888">kube-node-lease   Active   11d
</span></span></span><span style="display:flex"><span><span style="color:#888">kube-public       Active   11d
</span></span></span><span style="display:flex"><span><span style="color:#888">kube-system       Active   11d
</span></span></span></code></pre></div><p>Kubernetes starts with four initial namespaces:</p><ul><li><code>default</code> The default namespace for objects with no other namespace</li><li><code>kube-node-lease</code> This namespace holds <a href="/docs/concepts/architecture/leases/">Lease</a> objects associated with each node. Node leases allow the kubelet to send <a href="/docs/concepts/architecture/nodes/#heartbeats">heartbeats</a> so that the control plane can detect node failure.</li><li><code>kube-public</code> This namespace is created automatically and is readable by all users
(including those not authenticated). This namespace is mostly reserved for cluster usage,
in case that some resources should be visible and readable publicly throughout the whole cluster.
The public aspect of this namespace is only a convention, not a requirement.</li><li><code>kube-system</code> The namespace for objects created by the Kubernetes system</li></ul><p>You can also get the summary of a specific namespace using:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get namespaces &lt;name&gt;
</span></span></code></pre></div><p>Or you can get detailed information with:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe namespaces &lt;name&gt;
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Name:           default
</span></span></span><span style="display:flex"><span><span style="color:#888">Labels:         &lt;none&gt;
</span></span></span><span style="display:flex"><span><span style="color:#888">Annotations:    &lt;none&gt;
</span></span></span><span style="display:flex"><span><span style="color:#888">Status:         Active
</span></span></span><span style="display:flex"><span><span style="color:#888"/><span>
</span></span></span><span style="display:flex"><span><span/><span style="color:#888">No resource quota.
</span></span></span><span style="display:flex"><span><span style="color:#888"/><span>
</span></span></span><span style="display:flex"><span><span/><span style="color:#888">Resource Limits
</span></span></span><span style="display:flex"><span><span style="color:#888"> Type       Resource    Min Max Default
</span></span></span><span style="display:flex"><span><span style="color:#888"> ----               --------    --- --- ---
</span></span></span><span style="display:flex"><span><span style="color:#888"> Container          cpu         -   -   100m
</span></span></span></code></pre></div><p>Note that these details show both resource quota (if present) as well as resource limit ranges.</p><p>Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators
to define <em>Hard</em> resource usage limits that a Namespace may consume.</p><p>A limit range defines min/max constraints on the amount of resources a single entity can consume in
a Namespace.</p><p>See <a href="https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md">Admission control: Limit Range</a></p><p>A namespace can be in one of two phases:</p><ul><li><code>Active</code> the namespace is in use</li><li><code>Terminating</code> the namespace is being deleted, and can not be used for new objects</li></ul><p>For more details, see <a href="/docs/reference/kubernetes-api/cluster-resources/namespace-v1/">Namespace</a>
in the API reference.</p><h2 id="creating-a-new-namespace">Creating a new namespace</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Avoid creating namespace with prefix <code>kube-</code>, since it is reserved for Kubernetes system namespaces.</div><p>Create a new YAML file called <code>my-namespace.yaml</code> with the contents:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Namespace<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>&lt;insert-namespace-name-here&gt;<span style="color:#bbb">
</span></span></span></code></pre></div><p>Then run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create -f ./my-namespace.yaml
</span></span></code></pre></div><p>Alternatively, you can create namespace using below command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace &lt;insert-namespace-name-here&gt;
</span></span></code></pre></div><p>The name of your namespace must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>There's an optional field <code>finalizers</code>, which allows observables to purge resources whenever the
namespace is deleted. Keep in mind that if you specify a nonexistent finalizer, the namespace will
be created but will get stuck in the <code>Terminating</code> state if the user tries to delete it.</p><p>More information on <code>finalizers</code> can be found in the namespace
<a href="https://git.k8s.io/design-proposals-archive/architecture/namespaces.md#finalizers">design doc</a>.</p><h2 id="deleting-a-namespace">Deleting a namespace</h2><p>Delete a namespace with</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespaces &lt;insert-some-namespace-name&gt;
</span></span></code></pre></div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>This deletes <em>everything</em> under the namespace!</div><p>This delete is asynchronous, so for a time you will see the namespace in the <code>Terminating</code> state.</p><h2 id="subdividing-your-cluster-using-kubernetes-namespaces">Subdividing your cluster using Kubernetes namespaces</h2><p>By default, a Kubernetes cluster will instantiate a default namespace when provisioning the
cluster to hold the default set of Pods, Services, and Deployments used by the cluster.</p><p>Assuming you have a fresh cluster, you can introspect the available namespaces by doing the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get namespaces
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME      STATUS    AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">default   Active    13m
</span></span></span></code></pre></div><h3 id="create-new-namespaces">Create new namespaces</h3><p>For this exercise, we will create two additional Kubernetes namespaces to hold our content.</p><p>In a scenario where an organization is using a shared Kubernetes cluster for development and
production use cases:</p><ul><li><p>The development team would like to maintain a space in the cluster where they can get a view on
the list of Pods, Services, and Deployments they use to build and run their application.
In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify
resources are relaxed to enable agile development.</p></li><li><p>The operations team would like to maintain a space in the cluster where they can enforce strict
procedures on who can or cannot manipulate the set of Pods, Services, and Deployments that run
the production site.</p></li></ul><p>One pattern this organization could follow is to partition the Kubernetes cluster into two
namespaces: <code>development</code> and <code>production</code>. Let's create two new namespaces to hold our work.</p><p>Create the <code>development</code> namespace using kubectl:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</span></span></code></pre></div><p>And then let's create the <code>production</code> namespace using kubectl:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</span></span></code></pre></div><p>To be sure things are right, list all of the namespaces in our cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get namespaces --show-labels
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME          STATUS    AGE       LABELS
</span></span></span><span style="display:flex"><span><span style="color:#888">default       Active    32m       &lt;none&gt;
</span></span></span><span style="display:flex"><span><span style="color:#888">development   Active    29s       name=development
</span></span></span><span style="display:flex"><span><span style="color:#888">production    Active    23s       name=production
</span></span></span></code></pre></div><h3 id="create-pods-in-each-namespace">Create pods in each namespace</h3><p>A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.
Users interacting with one namespace do not see the content in another namespace.
To demonstrate this, let's spin up a simple Deployment and Pods in the <code>development</code> namespace.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create deployment snowflake <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --image<span style="color:#666">=</span>registry.k8s.io/serve_hostname <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  -n<span style="color:#666">=</span>development --replicas<span style="color:#666">=</span><span style="color:#666">2</span>
</span></span></code></pre></div><p>We have created a deployment whose replica size is 2 that is running the pod called <code>snowflake</code>
with a basic container that serves the hostname.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment -n<span style="color:#666">=</span>development
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME         READY   UP-TO-DATE   AVAILABLE   AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">snowflake    2/2     2            2           2m
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>snowflake -n<span style="color:#666">=</span>development
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME                         READY     STATUS    RESTARTS   AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">snowflake-3968820950-9dgr8   1/1       Running   0          2m
</span></span></span><span style="display:flex"><span><span style="color:#888">snowflake-3968820950-vgc4n   1/1       Running   0          2m
</span></span></span></code></pre></div><p>And this is great, developers are able to do what they want, and they do not have to worry about
affecting content in the <code>production</code> namespace.</p><p>Let's switch to the <code>production</code> namespace and show how resources in one namespace are hidden from
the other. The <code>production</code> namespace should be empty, and the following commands should return nothing.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment -n<span style="color:#666">=</span>production
</span></span><span style="display:flex"><span>kubectl get pods -n<span style="color:#666">=</span>production
</span></span></code></pre></div><p>Production likes to run cattle, so let's create some cattle pods.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create deployment cattle --image<span style="color:#666">=</span>registry.k8s.io/serve_hostname -n<span style="color:#666">=</span>production
</span></span><span style="display:flex"><span>kubectl scale deployment cattle --replicas<span style="color:#666">=</span><span style="color:#666">5</span> -n<span style="color:#666">=</span>production
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>kubectl get deployment -n<span style="color:#666">=</span>production
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME         READY   UP-TO-DATE   AVAILABLE   AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">cattle       5/5     5            5           10s
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cattle -n<span style="color:#666">=</span>production
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME                      READY     STATUS    RESTARTS   AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">cattle-2263376956-41xy6   1/1       Running   0          34s
</span></span></span><span style="display:flex"><span><span style="color:#888">cattle-2263376956-kw466   1/1       Running   0          34s
</span></span></span><span style="display:flex"><span><span style="color:#888">cattle-2263376956-n4v97   1/1       Running   0          34s
</span></span></span><span style="display:flex"><span><span style="color:#888">cattle-2263376956-p5p3i   1/1       Running   0          34s
</span></span></span><span style="display:flex"><span><span style="color:#888">cattle-2263376956-sxpth   1/1       Running   0          34s
</span></span></span></code></pre></div><p>At this point, it should be clear that the resources users create in one namespace are hidden from
the other namespace.</p><p>As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.</p><h2 id="understanding-the-motivation-for-using-namespaces">Understanding the motivation for using namespaces</h2><p>A single cluster should be able to satisfy the needs of multiple users or groups of users
(henceforth in this document a <em>user community</em>).</p><p>Kubernetes <em>namespaces</em> help different projects, teams, or customers to share a Kubernetes cluster.</p><p>It does this by providing the following:</p><ol><li>A scope for <a href="/docs/concepts/overview/working-with-objects/names/">names</a>.</li><li>A mechanism to attach authorization and policy to a subsection of the cluster.</li></ol><p>Use of multiple namespaces is optional.</p><p>Each user community wants to be able to work in isolation from other communities.
Each user community has its own:</p><ol><li>resources (pods, services, replication controllers, etc.)</li><li>policies (who can or cannot perform actions in their community)</li><li>constraints (this community is allowed this much quota, etc.)</li></ol><p>A cluster operator may create a Namespace for each unique user community.</p><p>The Namespace provides a unique scope for:</p><ol><li>named resources (to avoid basic naming collisions)</li><li>delegated management authority to trusted users</li><li>ability to limit community resource consumption</li></ol><p>Use cases include:</p><ol><li>As a cluster operator, I want to support multiple user communities on a single cluster.</li><li>As a cluster operator, I want to delegate authority to partitions of the cluster to trusted
users in those communities.</li><li>As a cluster operator, I want to limit the amount of resources each community can consume in
order to limit the impact to other communities using the cluster.</li><li>As a cluster user, I want to interact with resources that are pertinent to my user community in
isolation of what other user communities are doing on the cluster.</li></ol><h2 id="understanding-namespaces-and-dns">Understanding namespaces and DNS</h2><p>When you create a <a href="/docs/concepts/services-networking/service/">Service</a>, it creates a corresponding
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS entry</a>.
This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>, which means
that if a container uses <code>&lt;service-name&gt;</code> it will resolve to the service which
is local to a namespace. This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production. If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference">setting the namespace preference</a>.</li><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request">setting the namespace for a request</a></li><li>See <a href="https://git.k8s.io/design-proposals-archive/architecture/namespaces.md">namespaces design</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Generate Certificates Manually</h1><p>When using client certificate authentication, you can generate certificates
manually through <a href="https://github.com/OpenVPN/easy-rsa"><code>easyrsa</code></a>, <a href="https://github.com/openssl/openssl"><code>openssl</code></a> or <a href="https://github.com/cloudflare/cfssl"><code>cfssl</code></a>.</p><h3 id="easyrsa">easyrsa</h3><p><strong>easyrsa</strong> can manually generate certificates for your cluster.</p><ol><li><p>Download, unpack, and initialize the patched version of <code>easyrsa3</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -LO https://dl.k8s.io/easy-rsa/easy-rsa.tar.gz
</span></span><span style="display:flex"><span>tar xzf easy-rsa.tar.gz
</span></span><span style="display:flex"><span><span style="color:#a2f">cd</span> easy-rsa-master/easyrsa3
</span></span><span style="display:flex"><span>./easyrsa init-pki
</span></span></code></pre></div></li><li><p>Generate a new certificate authority (CA). <code>--batch</code> sets automatic mode;
<code>--req-cn</code> specifies the Common Name (CN) for the CA's new root certificate.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>./easyrsa --batch <span style="color:#b44">"--req-cn=</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">MASTER_IP</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">@`date +%s`"</span> build-ca nopass
</span></span></code></pre></div></li><li><p>Generate server certificate and key.</p><p>The argument <code>--subject-alt-name</code> sets the possible IPs and DNS names the API server will
be accessed with. The <code>MASTER_CLUSTER_IP</code> is usually the first IP from the service CIDR
that is specified as the <code>--service-cluster-ip-range</code> argument for both the API server and
the controller manager component. The argument <code>--days</code> is used to set the number of days
after which the certificate expires.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>./easyrsa --subject-alt-name<span style="color:#666">=</span><span style="color:#b44">"IP:</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">MASTER_IP</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">,"</span><span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/><span style="color:#b44">"IP:</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">MASTER_CLUSTER_IP</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">,"</span><span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/><span style="color:#b44">"DNS:kubernetes,"</span><span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/><span style="color:#b44">"DNS:kubernetes.default,"</span><span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/><span style="color:#b44">"DNS:kubernetes.default.svc,"</span><span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/><span style="color:#b44">"DNS:kubernetes.default.svc.cluster,"</span><span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/><span style="color:#b44">"DNS:kubernetes.default.svc.cluster.local"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>--days<span style="color:#666">=</span><span style="color:#666">10000</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>build-server-full server nopass
</span></span></code></pre></div></li><li><p>Copy <code>pki/ca.crt</code>, <code>pki/issued/server.crt</code>, and <code>pki/private/server.key</code> to your directory.</p></li><li><p>Fill in and add the following parameters into the API server start parameters:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>--client-ca-file<span style="color:#666">=</span>/yourdirectory/ca.crt
</span></span><span style="display:flex"><span>--tls-cert-file<span style="color:#666">=</span>/yourdirectory/server.crt
</span></span><span style="display:flex"><span>--tls-private-key-file<span style="color:#666">=</span>/yourdirectory/server.key
</span></span></code></pre></div></li></ol><h3 id="openssl">openssl</h3><p><strong>openssl</strong> can manually generate certificates for your cluster.</p><ol><li><p>Generate a ca.key with 2048bit:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl genrsa -out ca.key <span style="color:#666">2048</span>
</span></span></code></pre></div></li><li><p>According to the ca.key generate a ca.crt (use <code>-days</code> to set the certificate effective time):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl req -x509 -new -nodes -key ca.key -subj <span style="color:#b44">"/CN=</span><span style="color:#b68;font-weight:700">${</span><span style="color:#b8860b">MASTER_IP</span><span style="color:#b68;font-weight:700">}</span><span style="color:#b44">"</span> -days <span style="color:#666">10000</span> -out ca.crt
</span></span></code></pre></div></li><li><p>Generate a server.key with 2048bit:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl genrsa -out server.key <span style="color:#666">2048</span>
</span></span></code></pre></div></li><li><p>Create a config file for generating a Certificate Signing Request (CSR).</p><p>Be sure to substitute the values marked with angle brackets (e.g. <code>&lt;MASTER_IP&gt;</code>)
with real values before saving this to a file (e.g. <code>csr.conf</code>).
Note that the value for <code>MASTER_CLUSTER_IP</code> is the service cluster IP for the
API server as described in previous subsection.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">[ req ]</span>
</span></span><span style="display:flex"><span><span style="color:#b44">default_bits</span> <span style="color:#666">=</span> <span style="color:#b44">2048</span>
</span></span><span style="display:flex"><span><span style="color:#b44">prompt</span> <span style="color:#666">=</span> <span style="color:#b44">no</span>
</span></span><span style="display:flex"><span><span style="color:#b44">default_md</span> <span style="color:#666">=</span> <span style="color:#b44">sha256</span>
</span></span><span style="display:flex"><span><span style="color:#b44">req_extensions</span> <span style="color:#666">=</span> <span style="color:#b44">req_ext</span>
</span></span><span style="display:flex"><span><span style="color:#b44">distinguished_name</span> <span style="color:#666">=</span> <span style="color:#b44">dn</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">[ dn ]</span>
</span></span><span style="display:flex"><span><span style="color:#b44">C</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;country&gt;</span>
</span></span><span style="display:flex"><span><span style="color:#b44">ST</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;state&gt;</span>
</span></span><span style="display:flex"><span><span style="color:#b44">L</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;city&gt;</span>
</span></span><span style="display:flex"><span><span style="color:#b44">O</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;organization&gt;</span>
</span></span><span style="display:flex"><span><span style="color:#b44">OU</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;organization unit&gt;</span>
</span></span><span style="display:flex"><span><span style="color:#b44">CN</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;MASTER_IP&gt;</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">[ req_ext ]</span>
</span></span><span style="display:flex"><span><span style="color:#b44">subjectAltName</span> <span style="color:#666">=</span> <span style="color:#b44">@alt_names</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">[ alt_names ]</span>
</span></span><span style="display:flex"><span><span style="color:#b44">DNS.1</span> <span style="color:#666">=</span> <span style="color:#b44">kubernetes</span>
</span></span><span style="display:flex"><span><span style="color:#b44">DNS.2</span> <span style="color:#666">=</span> <span style="color:#b44">kubernetes.default</span>
</span></span><span style="display:flex"><span><span style="color:#b44">DNS.3</span> <span style="color:#666">=</span> <span style="color:#b44">kubernetes.default.svc</span>
</span></span><span style="display:flex"><span><span style="color:#b44">DNS.4</span> <span style="color:#666">=</span> <span style="color:#b44">kubernetes.default.svc.cluster</span>
</span></span><span style="display:flex"><span><span style="color:#b44">DNS.5</span> <span style="color:#666">=</span> <span style="color:#b44">kubernetes.default.svc.cluster.local</span>
</span></span><span style="display:flex"><span><span style="color:#b44">IP.1</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;MASTER_IP&gt;</span>
</span></span><span style="display:flex"><span><span style="color:#b44">IP.2</span> <span style="color:#666">=</span> <span style="color:#b44">&lt;MASTER_CLUSTER_IP&gt;</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">[ v3_ext ]</span>
</span></span><span style="display:flex"><span><span style="color:#b44">authorityKeyIdentifier</span><span style="color:#666">=</span><span style="color:#b44">keyid,issuer:always</span>
</span></span><span style="display:flex"><span><span style="color:#b44">basicConstraints</span><span style="color:#666">=</span><span style="color:#b44">CA:FALSE</span>
</span></span><span style="display:flex"><span><span style="color:#b44">keyUsage</span><span style="color:#666">=</span><span style="color:#b44">keyEncipherment,dataEncipherment</span>
</span></span><span style="display:flex"><span><span style="color:#b44">extendedKeyUsage</span><span style="color:#666">=</span><span style="color:#b44">serverAuth,clientAuth</span>
</span></span><span style="display:flex"><span><span style="color:#b44">subjectAltName</span><span style="color:#666">=</span><span style="color:#b44">@alt_names</span>
</span></span></code></pre></div></li><li><p>Generate the certificate signing request based on the config file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl req -new -key server.key -out server.csr -config csr.conf
</span></span></code></pre></div></li><li><p>Generate the server certificate using the ca.key, ca.crt and server.csr:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -CAcreateserial -out server.crt -days <span style="color:#666">10000</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>    -extensions v3_ext -extfile csr.conf -sha256
</span></span></code></pre></div></li><li><p>View the certificate signing request:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl req  -noout -text -in ./server.csr
</span></span></code></pre></div></li><li><p>View the certificate:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>openssl x509  -noout -text -in ./server.crt
</span></span></code></pre></div></li></ol><p>Finally, add the same parameters into the API server start parameters.</p><h3 id="cfssl">cfssl</h3><p><strong>cfssl</strong> is another tool for certificate generation.</p><ol><li><p>Download, unpack and prepare the command line tools as shown below.</p><p>Note that you may need to adapt the sample commands based on the hardware
architecture and cfssl version you are using.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
</span></span><span style="display:flex"><span>chmod +x cfssl
</span></span><span style="display:flex"><span>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
</span></span><span style="display:flex"><span>chmod +x cfssljson
</span></span><span style="display:flex"><span>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
</span></span><span style="display:flex"><span>chmod +x cfssl-certinfo
</span></span></code></pre></div></li><li><p>Create a directory to hold the artifacts and initialize cfssl:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>mkdir cert
</span></span><span style="display:flex"><span><span style="color:#a2f">cd</span> cert
</span></span><span style="display:flex"><span>../cfssl print-defaults config &gt; config.json
</span></span><span style="display:flex"><span>../cfssl print-defaults csr &gt; csr.json
</span></span></code></pre></div></li><li><p>Create a JSON config file for generating the CA file, for example, <code>ca-config.json</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"signing"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"default"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"expiry"</span>: <span style="color:#b44">"8760h"</span>
</span></span><span style="display:flex"><span>    },
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"profiles"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"kubernetes"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"usages"</span>: [
</span></span><span style="display:flex"><span>          <span style="color:#b44">"signing"</span>,
</span></span><span style="display:flex"><span>          <span style="color:#b44">"key encipherment"</span>,
</span></span><span style="display:flex"><span>          <span style="color:#b44">"server auth"</span>,
</span></span><span style="display:flex"><span>          <span style="color:#b44">"client auth"</span>
</span></span><span style="display:flex"><span>        ],
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"expiry"</span>: <span style="color:#b44">"8760h"</span>
</span></span><span style="display:flex"><span>      }
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></li><li><p>Create a JSON config file for CA certificate signing request (CSR), for example,
<code>ca-csr.json</code>. Be sure to replace the values marked with angle brackets with
real values you want to use.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"CN"</span>: <span style="color:#b44">"kubernetes"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"key"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"algo"</span>: <span style="color:#b44">"rsa"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"size"</span>: <span style="color:#666">2048</span>
</span></span><span style="display:flex"><span>  },
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"names"</span>:[{
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"C"</span>: <span style="color:#b44">"&lt;country&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"ST"</span>: <span style="color:#b44">"&lt;state&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"L"</span>: <span style="color:#b44">"&lt;city&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"O"</span>: <span style="color:#b44">"&lt;organization&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"OU"</span>: <span style="color:#b44">"&lt;organization unit&gt;"</span>
</span></span><span style="display:flex"><span>  }]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></li><li><p>Generate CA key (<code>ca-key.pem</code>) and certificate (<code>ca.pem</code>):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</span></span></code></pre></div></li><li><p>Create a JSON config file for generating keys and certificates for the API
server, for example, <code>server-csr.json</code>. Be sure to replace the values in angle brackets with
real values you want to use. The <code>&lt;MASTER_CLUSTER_IP&gt;</code> is the service cluster
IP for the API server as described in previous subsection.
The sample below also assumes that you are using <code>cluster.local</code> as the default
DNS domain name.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"CN"</span>: <span style="color:#b44">"kubernetes"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"hosts"</span>: [
</span></span><span style="display:flex"><span>    <span style="color:#b44">"127.0.0.1"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"&lt;MASTER_IP&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"&lt;MASTER_CLUSTER_IP&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"kubernetes"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"kubernetes.default"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"kubernetes.default.svc"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"kubernetes.default.svc.cluster"</span>,
</span></span><span style="display:flex"><span>    <span style="color:#b44">"kubernetes.default.svc.cluster.local"</span>
</span></span><span style="display:flex"><span>  ],
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"key"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"algo"</span>: <span style="color:#b44">"rsa"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"size"</span>: <span style="color:#666">2048</span>
</span></span><span style="display:flex"><span>  },
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"names"</span>: [{
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"C"</span>: <span style="color:#b44">"&lt;country&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"ST"</span>: <span style="color:#b44">"&lt;state&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"L"</span>: <span style="color:#b44">"&lt;city&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"O"</span>: <span style="color:#b44">"&lt;organization&gt;"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"OU"</span>: <span style="color:#b44">"&lt;organization unit&gt;"</span>
</span></span><span style="display:flex"><span>  }]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></li><li><p>Generate the key and certificate for the API server, which are by default
saved into file <code>server-key.pem</code> and <code>server.pem</code> respectively:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>../cfssl gencert -ca<span style="color:#666">=</span>ca.pem -ca-key<span style="color:#666">=</span>ca-key.pem <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>     --config<span style="color:#666">=</span>ca-config.json -profile<span style="color:#666">=</span>kubernetes <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>     server-csr.json | ../cfssljson -bare server
</span></span></code></pre></div></li></ol><h2 id="distributing-self-signed-ca-certificate">Distributing Self-Signed CA Certificate</h2><p>A client node may refuse to recognize a self-signed CA certificate as valid.
For a non-production deployment, or for a deployment that runs behind a company
firewall, you can distribute a self-signed CA certificate to all clients and
refresh the local list for valid certificates.</p><p>On each client, perform the following operations:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
</span></span><span style="display:flex"><span>sudo update-ca-certificates
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id="certificates-api">Certificates API</h2><p>You can use the <code>certificates.k8s.io</code> API to provision
x509 certificates to use for authentication as documented
in the <a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Managing TLS in a cluster</a>
task page.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Upgrading Windows nodes</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [beta]</code></div><p>This page explains how to upgrade a Windows node created with kubeadm.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have shell access to all the nodes, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial
on a cluster with at least two nodes that are not acting as control plane hosts.</p>Your Kubernetes server must be at or later than version 1.17.<p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>Familiarize yourself with <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">the process for upgrading the rest of your kubeadm
cluster</a>. You will want to
upgrade the control plane nodes before upgrading your Windows nodes.</li></ul><h2 id="upgrading-worker-nodes">Upgrading worker nodes</h2><h3 id="upgrade-kubeadm">Upgrade kubeadm</h3><ol><li><p>From the Windows node, upgrade kubeadm:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace 1.34.0 with your desired version</span>
</span></span><span style="display:flex"><span>curl.exe -Lo &lt;<span style="color:#a2f">path-to</span>-kubeadm.exe&gt;  <span style="color:#b44">"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kubeadm.exe"</span>
</span></span></code></pre></div></li></ol><h3 id="drain-the-node">Drain the node</h3><ol><li><p>From a machine with access to the Kubernetes API,
prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
</span></span><span style="display:flex"><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><p>You should see output similar to this:</p><pre tabindex="0"><code>node/ip-172-31-85-18 cordoned
node/ip-172-31-85-18 drained
</code></pre></li></ol><h3 id="upgrade-the-kubelet-configuration">Upgrade the kubelet configuration</h3><ol><li><p>From the Windows node, call the following command to sync new kubelet configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span>kubeadm upgrade node
</span></span></code></pre></div></li></ol><h3 id="upgrade-kubelet-and-kube-proxy">Upgrade kubelet and kube-proxy</h3><ol><li><p>From the Windows node, upgrade and restart the kubelet:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#a2f">stop-service</span> kubelet
</span></span><span style="display:flex"><span>curl.exe -Lo &lt;<span style="color:#a2f">path-to</span>-kubelet.exe&gt; <span style="color:#b44">"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kubelet.exe"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">restart-service</span> kubelet
</span></span></code></pre></div></li><li><p>From the Windows node, upgrade and restart the kube-proxy.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#a2f">stop-service</span> <span style="color:#a2f">kube-proxy</span>
</span></span><span style="display:flex"><span>curl.exe -Lo &lt;<span style="color:#a2f">path-to</span>-kube-proxy.exe&gt; <span style="color:#b44">"https://dl.k8s.io/v1.34.0/bin/windows/amd64/kube-proxy.exe"</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">restart-service</span> <span style="color:#a2f">kube-proxy</span>
</span></span></code></pre></div></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you are running kube-proxy in a HostProcess container within a Pod, and not as a Windows Service,
you can upgrade kube-proxy by applying a newer version of your kube-proxy manifests.</div><h3 id="uncordon-the-node">Uncordon the node</h3><ol><li><p>From a machine with access to the Kubernetes API,
bring the node back online by marking it schedulable:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace &lt;node-to-drain&gt; with the name of your node</span>
</span></span><span style="display:flex"><span>kubectl uncordon &lt;node-to-drain&gt;
</span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrade Linux nodes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Deploy and Access the Kubernetes Dashboard</h1><div class="lead">Deploy the web UI (Kubernetes Dashboard) and access it.</div><p>Dashboard is a web-based Kubernetes user interface.
You can use Dashboard to deploy containerized applications to a Kubernetes cluster,
troubleshoot your containerized application, and manage the cluster resources.
You can use Dashboard to get an overview of applications running on your cluster,
as well as for creating or modifying individual Kubernetes resources
(such as Deployments, Jobs, DaemonSets, etc).
For example, you can scale a Deployment, initiate a rolling update, restart a pod
or deploy new applications using a deploy wizard.</p><p>Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.</p><p><img alt="Kubernetes Dashboard UI" src="/images/docs/ui-dashboard.png"/></p><h2 id="deploying-the-dashboard-ui">Deploying the Dashboard UI</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes Dashboard supports only Helm-based installation currently as it is faster
and gives us better control over all dependencies required by Dashboard to run.</div><p>The Dashboard UI is not deployed by default. To deploy it, run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Add kubernetes-dashboard repository</span>
</span></span><span style="display:flex"><span>helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart</span>
</span></span><span style="display:flex"><span>helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
</span></span></code></pre></div><h2 id="accessing-the-dashboard-ui">Accessing the Dashboard UI</h2><p>To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default.
Currently, Dashboard only supports logging in with a Bearer Token.
To create a token for this demo, you can follow our guide on
<a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">creating a sample user</a>.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>The sample user created in the tutorial will have administrative privileges and is for educational purposes only.</div><h3 id="command-line-proxy">Command line proxy</h3><p>You can enable access to the Dashboard using the <code>kubectl</code> command-line tool,
by running the following command:</p><pre tabindex="0"><code>kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
</code></pre><p>Kubectl will make Dashboard available at <a href="https://localhost:8443">https://localhost:8443</a>.</p><p>The UI can <em>only</em> be accessed from the machine where the command is executed. See <code>kubectl port-forward --help</code> for more options.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The kubeconfig authentication method does <strong>not</strong> support external identity providers
or X.509 certificate-based authentication.</div><h2 id="welcome-view">Welcome view</h2><p>When you access Dashboard on an empty cluster, you'll see the welcome page.
This page contains a link to this document as well as a button to deploy your first application.
In addition, you can view which system applications are running by default in the <code>kube-system</code>
<a href="/docs/tasks/administer-cluster/namespaces/">namespace</a> of your cluster, for example the Dashboard itself.</p><p><img alt="Kubernetes Dashboard welcome page" src="/images/docs/ui-dashboard-zerostate.png"/></p><h2 id="deploying-containerized-applications">Deploying containerized applications</h2><p>Dashboard lets you create and deploy a containerized application as a Deployment and optional Service with a simple wizard.
You can either manually specify application details, or upload a YAML or JSON <em>manifest</em> file containing application configuration.</p><p>Click the <strong>CREATE</strong> button in the upper right corner of any page to begin.</p><h3 id="specifying-application-details">Specifying application details</h3><p>The deploy wizard expects that you provide the following information:</p><ul><li><p><strong>App name</strong> (mandatory): Name for your application.
A <a href="/docs/concepts/overview/working-with-objects/labels/">label</a> with the name will be
added to the Deployment and Service, if any, that will be deployed.</p><p>The application name must be unique within the selected Kubernetes <a href="/docs/tasks/administer-cluster/namespaces/">namespace</a>.
It must start with a lowercase character, and end with a lowercase character or a number,
and contain only lowercase letters, numbers and dashes (-). It is limited to 24 characters.
Leading and trailing spaces are ignored.</p></li><li><p><strong>Container image</strong> (mandatory):
The URL of a public Docker <a href="/docs/concepts/containers/images/">container image</a> on any registry,
or a private image (commonly hosted on the Google Container Registry or Docker Hub).
The container image specification must end with a colon.</p></li><li><p><strong>Number of pods</strong> (mandatory): The target number of Pods you want your application to be deployed in.
The value must be a positive integer.</p><p>A <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> will be created to
maintain the desired number of Pods across your cluster.</p></li><li><p><strong>Service</strong> (optional): For some parts of your application (e.g. frontends) you may want to expose a
<a href="/docs/concepts/services-networking/service/">Service</a> onto an external,
maybe public IP address outside of your cluster (external Service).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>For external Services, you may need to open up one or more ports to do so.</div><p>Other Services that are only visible from inside the cluster are called internal Services.</p><p>Irrespective of the Service type, if you choose to create a Service and your container listens
on a port (incoming), you need to specify two ports.
The Service will be created mapping the port (incoming) to the target port seen by the container.
This Service will route to your deployed Pods. Supported protocols are TCP and UDP.
The internal DNS name for this Service will be the value you specified as application name above.</p></li></ul><p>If needed, you can expand the <strong>Advanced options</strong> section where you can specify more settings:</p><ul><li><p><strong>Description</strong>: The text you enter here will be added as an
<a href="/docs/concepts/overview/working-with-objects/annotations/">annotation</a>
to the Deployment and displayed in the application's details.</p></li><li><p><strong>Labels</strong>: Default <a href="/docs/concepts/overview/working-with-objects/labels/">labels</a> to be used
for your application are application name and version.
You can specify additional labels to be applied to the Deployment, Service (if any), and Pods,
such as release, environment, tier, partition, and release track.</p><p>Example:</p><pre tabindex="0"><code class="language-conf" data-lang="conf">release=1.0
tier=frontend
environment=pod
track=stable
</code></pre></li><li><p><strong>Namespace</strong>: Kubernetes supports multiple virtual clusters backed by the same physical cluster.
These virtual clusters are called <a href="/docs/tasks/administer-cluster/namespaces/">namespaces</a>.
They let you partition resources into logically named groups.</p><p>Dashboard offers all available namespaces in a dropdown list, and allows you to create a new namespace.
The namespace name may contain a maximum of 63 alphanumeric characters and dashes (-) but can not contain capital letters.
Namespace names should not consist of only numbers.
If the name is set as a number, such as 10, the pod will be put in the default namespace.</p><p>In case the creation of the namespace is successful, it is selected by default.
If the creation fails, the first namespace is selected.</p></li><li><p><strong>Image Pull Secret</strong>:
In case the specified Docker container image is private, it may require
<a href="/docs/concepts/configuration/secret/">pull secret</a> credentials.</p><p>Dashboard offers all available secrets in a dropdown list, and allows you to create a new secret.
The secret name must follow the DNS domain name syntax, for example <code>new.image-pull.secret</code>.
The content of a secret must be base64-encoded and specified in a
<a href="/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"><code>.dockercfg</code></a> file.
The secret name may consist of a maximum of 253 characters.</p><p>In case the creation of the image pull secret is successful, it is selected by default. If the creation fails, no secret is applied.</p></li><li><p><strong>CPU requirement (cores)</strong> and <strong>Memory requirement (MiB)</strong>:
You can specify the minimum <a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">resource limits</a>
for the container. By default, Pods run with unbounded CPU and memory limits.</p></li><li><p><strong>Run command</strong> and <strong>Run command arguments</strong>:
By default, your containers run the specified Docker image's default
<a href="/docs/tasks/inject-data-application/define-command-argument-container/">entrypoint command</a>.
You can use the command options and arguments to override the default.</p></li><li><p><strong>Run as privileged</strong>: This setting determines whether processes in
<a href="/docs/concepts/workloads/pods/#privileged-mode-for-containers">privileged containers</a>
are equivalent to processes running as root on the host.
Privileged containers can make use of capabilities like manipulating the network stack and accessing devices.</p></li><li><p><strong>Environment variables</strong>: Kubernetes exposes Services through
<a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a>.
You can compose environment variable or pass arguments to your commands using the values of environment variables.
They can be used in applications to find a Service.
Values can reference other variables using the <code>$(VAR_NAME)</code> syntax.</p></li></ul><h3 id="uploading-a-yaml-or-json-file">Uploading a YAML or JSON file</h3><p>Kubernetes supports declarative configuration.
In this style, all configuration is stored in manifests (YAML or JSON configuration files).
The manifests use Kubernetes <a href="/docs/concepts/overview/kubernetes-api/">API</a> resource schemas.</p><p>As an alternative to specifying application details in the deploy wizard,
you can define your application in one or more manifests, and upload the files using Dashboard.</p><h2 id="using-dashboard">Using Dashboard</h2><p>Following sections describe views of the Kubernetes Dashboard UI; what they provide and how can they be used.</p><h3 id="navigation">Navigation</h3><p>When there are Kubernetes objects defined in the cluster, Dashboard shows them in the initial view.
By default only objects from the <em>default</em> namespace are shown and
this can be changed using the namespace selector located in the navigation menu.</p><p>Dashboard shows most Kubernetes object kinds and groups them in a few menu categories.</p><h4 id="admin-overview">Admin overview</h4><p>For cluster and namespace administrators, Dashboard lists Nodes, Namespaces and PersistentVolumes and has detail views for them.
Node list view contains CPU and memory usage metrics aggregated across all Nodes.
The details view shows the metrics for a Node, its specification, status,
allocated resources, events and pods running on the node.</p><h4 id="workloads">Workloads</h4><p>Shows all applications running in the selected namespace.
The view lists applications by workload kind (for example: Deployments, ReplicaSets, StatefulSets).
Each workload kind can be viewed separately.
The lists summarize actionable information about the workloads,
such as the number of ready pods for a ReplicaSet or current memory usage for a Pod.</p><p>Detail views for workloads show status and specification information and
surface relationships between objects.
For example, Pods that ReplicaSet is controlling or new ReplicaSets and HorizontalPodAutoscalers for Deployments.</p><h4 id="services">Services</h4><p>Shows Kubernetes resources that allow for exposing services to external world and
discovering them within a cluster.
For that reason, Service and Ingress views show Pods targeted by them,
internal endpoints for cluster connections and external endpoints for external users.</p><h4 id="storage">Storage</h4><p>Storage view shows PersistentVolumeClaim resources which are used by applications for storing data.</p><h4 id="config-maps-and-secrets">ConfigMaps and Secrets</h4><p>Shows all Kubernetes resources that are used for live configuration of applications running in clusters.
The view allows for editing and managing config objects and displays secrets hidden by default.</p><h4 id="logs-viewer">Logs viewer</h4><p>Pod lists and detail pages link to a logs viewer that is built into Dashboard.
The viewer allows for drilling down logs from containers belonging to a single Pod.</p><p><img alt="Logs viewer" src="/images/docs/ui-dashboard-logs-view.png"/></p><h2 id="what-s-next">What's next</h2><p>For more information, see the
<a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard project page</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Find Out What Container Runtime is Used on a Node</h1><p>This page outlines steps to find out what <a href="/docs/setup/production-environment/container-runtimes/">container runtime</a>
the nodes in your cluster use.</p><p>Depending on the way you run your cluster, the container runtime for the nodes may
have been pre-configured or you need to configure it. If you're using a managed
Kubernetes service, there might be vendor-specific ways to check what container runtime is
configured for the nodes. The method described on this page should work whenever
the execution of <code>kubectl</code> is allowed.</p><h2 id="before-you-begin">Before you begin</h2><p>Install and configure <code>kubectl</code>. See <a href="/docs/tasks/tools/#kubectl">Install Tools</a> section for details.</p><h2 id="find-out-the-container-runtime-used-on-a-node">Find out the container runtime used on a Node</h2><p>Use <code>kubectl</code> to fetch and show node information:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get nodes -o wide
</span></span></code></pre></div><p>The output is similar to the following. The column <code>CONTAINER-RUNTIME</code> outputs
the runtime and its version.</p><p>For Docker Engine, the output is similar to this:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME         STATUS   VERSION    CONTAINER-RUNTIME
node-1       Ready    v1.16.15   docker://19.3.1
node-2       Ready    v1.16.15   docker://19.3.1
node-3       Ready    v1.16.15   docker://19.3.1
</code></pre><p>If your runtime shows as Docker Engine, you still might not be affected by the
removal of dockershim in Kubernetes v1.24.
<a href="#which-endpoint">Check the runtime endpoint</a> to see if you use dockershim.
If you don't use dockershim, you aren't affected.</p><p>For containerd, the output is similar to this:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME         STATUS   VERSION   CONTAINER-RUNTIME
node-1       Ready    v1.19.6   containerd://1.4.1
node-2       Ready    v1.19.6   containerd://1.4.1
node-3       Ready    v1.19.6   containerd://1.4.1
</code></pre><p>Find out more information about container runtimes
on <a href="/docs/setup/production-environment/container-runtimes/">Container Runtimes</a>
page.</p><h2 id="which-endpoint">Find out what container runtime endpoint you use</h2><p>The container runtime talks to the kubelet over a Unix socket using the <a href="/docs/concepts/architecture/cri/">CRI
protocol</a>, which is based on the gRPC
framework. The kubelet acts as a client, and the runtime acts as the server.
In some cases, you might find it useful to know which socket your nodes use. For
example, with the removal of dockershim in Kubernetes v1.24 and later, you might
want to know whether you use Docker Engine with dockershim.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you currently use Docker Engine in your nodes with <code>cri-dockerd</code>, you aren't
affected by the dockershim removal.</div><p>You can check which socket you use by checking the kubelet configuration on your
nodes.</p><ol><li><p>Read the starting commands for the kubelet process:</p><pre tabindex="0"><code>tr \\0 ' ' &lt; /proc/"$(pgrep kubelet)"/cmdline
</code></pre><p>If you don't have <code>tr</code> or <code>pgrep</code>, check the command line for the kubelet
process manually.</p></li><li><p>In the output, look for the <code>--container-runtime</code> flag and the
<code>--container-runtime-endpoint</code> flag.</p><ul><li>If your nodes use Kubernetes v1.23 and earlier and these flags aren't
present or if the <code>--container-runtime</code> flag is not <code>remote</code>,
you use the dockershim socket with Docker Engine. The <code>--container-runtime</code> command line
argument is not available in Kubernetes v1.27 and later.</li><li>If the <code>--container-runtime-endpoint</code> flag is present, check the socket
name to find out which runtime you use. For example,
<code>unix:///run/containerd/containerd.sock</code> is the containerd endpoint.</li></ul></li></ol><p>If you want to change the Container Runtime on a Node from Docker Engine to containerd,
you can find out more information on <a href="/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">migrating from Docker Engine to containerd</a>,
or, if you want to continue using Docker Engine in Kubernetes v1.24 and later, migrate to a
CRI-compatible adapter like <a href="https://github.com/Mirantis/cri-dockerd"><code>cri-dockerd</code></a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Adding Linux worker nodes</h1><p>This page explains how to add Linux worker nodes to a kubeadm cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>Each joining worker node has installed the required components from
<a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installing kubeadm</a>, such as,
kubeadm, the kubelet and a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>.</li><li>A running kubeadm cluster created by <code>kubeadm init</code> and following the steps
in the document <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>.</li><li>You need superuser access to the node.</li></ul><h2 id="adding-linux-worker-nodes">Adding Linux worker nodes</h2><p>To add new Linux worker nodes to your cluster do the following for each machine:</p><ol><li>Connect to the machine by using SSH or another method.</li><li>Run the command that was output by <code>kubeadm init</code>. For example:</li></ol><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>sudo kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><h3 id="additional-information-for-kubeadm-join">Additional information for kubeadm join</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To specify an IPv6 tuple for <code>&lt;control-plane-host&gt;:&lt;control-plane-port&gt;</code>, IPv6 address must be enclosed in square brackets, for example: <code>[2001:db8::101]:2073</code>.</div><p>If you do not have the token, you can get it by running the following command on the control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on a control plane node</span>
</span></span><span style="display:flex"><span>sudo kubeadm token list
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style="display:flex"><span><span style="color:#888">8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                                   signing          token generated by     bootstrappers:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                                                    'kubeadm init'.        kubeadm:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                                                                           default-node-token
</span></span></span></code></pre></div><p>By default, node join tokens expire after 24 hours. If you are joining a node to the cluster after the
current token has expired, you can create a new token by running the following command on the
control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on a control plane node</span>
</span></span><span style="display:flex"><span>sudo kubeadm token create
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>To print a kubeadm join command while also generating a new token you can use:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>sudo kubeadm token create --print-join-command
</span></span></code></pre></div><p>If you don't have the value of <code>--discovery-token-ca-cert-hash</code>, you can get it by running the
following commands on the control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on a control plane node</span>
</span></span><span style="display:flex"><span>sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey  | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   openssl dgst -sha256 -hex | sed <span style="color:#b44">'s/^.* //'</span>
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><p>The output of the <code>kubeadm join</code> command should look something like:</p><pre tabindex="0"><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><p>A few seconds later, you should notice this node in the output from <code>kubectl get nodes</code>.
(for example, run <code>kubectl</code> on a control plane node).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run
on the first control plane node. To provide higher availability, please rebalance the CoreDNS Pods
with <code>kubectl -n kube-system rollout restart deployment coredns</code> after at least one new node is joined.</div><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/">add Windows worker nodes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Romana for NetworkPolicy</h1><p>This page shows how to use Romana for NetworkPolicy.</p><h2 id="before-you-begin">Before you begin</h2><p>Complete steps 1, 2, and 3 of the <a href="/docs/reference/setup-tools/kubeadm/">kubeadm getting started guide</a>.</p><h2 id="installing-romana-with-kubeadm">Installing Romana with kubeadm</h2><p>Follow the <a href="https://github.com/romana/romana/tree/master/containerize">containerized installation guide</a> for kubeadm.</p><h2 id="applying-network-policies">Applying network policies</h2><p>To apply network policies use one of the following:</p><ul><li><a href="https://github.com/romana/romana/wiki/Romana-policies">Romana network policies</a>.<ul><li><a href="https://github.com/romana/core/blob/master/doc/policy.md">Example of Romana network policy</a>.</li></ul></li><li>The NetworkPolicy API.</li></ul><h2 id="what-s-next">What's next</h2><p>Once you have installed Romana, you can follow the
<a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
to try out Kubernetes NetworkPolicy.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Changing the Container Runtime on a Node from Docker Engine to containerd</h1><p>This task outlines the steps needed to update your container runtime to containerd from Docker. It
is applicable for cluster operators running Kubernetes 1.23 or earlier. This also covers an
example scenario for migrating from dockershim to containerd. Alternative container runtimes
can be picked from this <a href="/docs/setup/production-environment/container-runtimes/">page</a>.</p><h2 id="before-you-begin">Before you begin</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Install containerd. For more information see
<a href="https://containerd.io/docs/getting-started/">containerd's installation documentation</a>
and for specific prerequisite follow
<a href="/docs/setup/production-environment/container-runtimes/#containerd">the containerd guide</a>.</p><h2 id="drain-the-node">Drain the node</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><p>Replace <code>&lt;node-to-drain&gt;</code> with the name of your node you are draining.</p><h2 id="stop-the-docker-daemon">Stop the Docker daemon</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>systemctl stop kubelet
</span></span><span style="display:flex"><span>systemctl disable docker.service --now
</span></span></code></pre></div><h2 id="install-containerd">Install Containerd</h2><p>Follow the <a href="/docs/setup/production-environment/container-runtimes/#containerd">guide</a>
for detailed steps to install containerd.</p><ul class="nav nav-tabs" id="tab-cri-containerd-installation" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-cri-containerd-installation-0" role="tab" aria-controls="tab-cri-containerd-installation-0" aria-selected="true">Linux</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-containerd-installation-1" role="tab" aria-controls="tab-cri-containerd-installation-1">Windows (PowerShell)</a></li></ul><div class="tab-content" id="tab-cri-containerd-installation"><div id="tab-cri-containerd-installation-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-cri-containerd-installation-0"><p><ol><li><p>Install the <code>containerd.io</code> package from the official Docker repositories.
Instructions for setting up the Docker repository for your respective Linux distribution and
installing the <code>containerd.io</code> package can be found at
<a href="https://github.com/containerd/containerd/blob/main/docs/getting-started.md">Getting started with containerd</a>.</p></li><li><p>Configure containerd:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo mkdir -p /etc/containerd
</span></span><span style="display:flex"><span>containerd config default | sudo tee /etc/containerd/config.toml
</span></span></code></pre></div></li><li><p>Restart containerd:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo systemctl restart containerd
</span></span></code></pre></div></li></ol></p></div><div id="tab-cri-containerd-installation-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-containerd-installation-1"><p><p>Start a Powershell session, set <code>$Version</code> to the desired version (ex: <code>$Version="1.4.3"</code>), and
then run the following commands:</p><ol><li><p>Download containerd:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span>curl.exe -L https<span>:</span>//github.com/containerd/containerd/releases/download/v<span style="color:#b8860b">$Version</span>/containerd-<span style="color:#b8860b">$Version</span>-windows-amd64.tar.gz -o <span style="color:#a2f">containerd-windows</span>-amd64.tar.gz
</span></span><span style="display:flex"><span>tar.exe xvf .\<span style="color:#a2f">containerd-windows</span>-amd64.tar.gz
</span></span></code></pre></div></li><li><p>Extract and configure:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#a2f">Copy-Item</span> -Path <span style="color:#b44">".\bin\"</span> -Destination <span style="color:#b44">"</span><span style="color:#b8860b">$Env:ProgramFiles</span><span style="color:#b44">\containerd"</span> -Recurse -Force
</span></span><span style="display:flex"><span><span style="color:#a2f">cd </span><span style="color:#b8860b">$Env:ProgramFiles</span>\containerd\
</span></span><span style="display:flex"><span>.\containerd.exe config <span style="color:#a2f;font-weight:700">default</span> | <span style="color:#a2f">Out-File</span> config.toml -Encoding ascii
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Review the configuration. Depending on setup you may want to adjust:</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># - the sandbox_image (Kubernetes pause image)</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># - cni bin_dir and conf_dir locations</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">Get-Content</span> config.toml
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># (Optional - but highly recommended) Exclude containerd from Windows Defender Scans</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">Add-MpPreference</span> -ExclusionProcess <span style="color:#b44">"</span><span style="color:#b8860b">$Env:ProgramFiles</span><span style="color:#b44">\containerd\containerd.exe"</span>
</span></span></code></pre></div></li><li><p>Start containerd:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span>.\containerd.exe --register-service
</span></span><span style="display:flex"><span><span style="color:#a2f">Start-Service</span> containerd
</span></span></code></pre></div></li></ol></p></div></div><h2 id="configure-the-kubelet-to-use-containerd-as-its-container-runtime">Configure the kubelet to use containerd as its container runtime</h2><p>Edit the file <code>/var/lib/kubelet/kubeadm-flags.env</code> and add the containerd runtime to the flags;
<code>--container-runtime-endpoint=unix:///run/containerd/containerd.sock</code>.</p><p>Users using kubeadm should be aware that the kubeadm tool stores the host's CRI socket in the</p><p><code>/var/lib/kubelet/instance-config.yaml</code> file on each node. You can create this <code>/var/lib/kubelet/instance-config.yaml</code> file on the node.</p><p>The <code>/var/lib/kubelet/instance-config.yaml</code> file allows setting the <code>containerRuntimeEndpoint</code> parameter.</p><p>You can set this parameter's value to the path of your chosen CRI socket (for example <code>unix:///run/containerd/containerd.sock</code>).</p><h2 id="restart-the-kubelet">Restart the kubelet</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>systemctl start kubelet
</span></span></code></pre></div><h2 id="verify-that-the-node-is-healthy">Verify that the node is healthy</h2><p>Run <code>kubectl get nodes -o wide</code> and containerd appears as the runtime for the node we just changed.</p><h2 id="remove-docker-engine">Remove Docker Engine</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>If the node appears healthy, remove Docker.</p><ul class="nav nav-tabs" id="tab-remove-docker-engine" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-remove-docker-engine-0" role="tab" aria-controls="tab-remove-docker-engine-0" aria-selected="true">CentOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-remove-docker-engine-1" role="tab" aria-controls="tab-remove-docker-engine-1">Debian</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-remove-docker-engine-2" role="tab" aria-controls="tab-remove-docker-engine-2">Fedora</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-remove-docker-engine-3" role="tab" aria-controls="tab-remove-docker-engine-3">Ubuntu</a></li></ul><div class="tab-content" id="tab-remove-docker-engine"><div id="tab-remove-docker-engine-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-remove-docker-engine-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo yum remove docker-ce docker-ce-cli
</span></span></code></pre></div></p></div><div id="tab-remove-docker-engine-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-remove-docker-engine-1"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo apt-get purge docker-ce docker-ce-cli
</span></span></code></pre></div></p></div><div id="tab-remove-docker-engine-2" class="tab-pane" role="tabpanel" aria-labelledby="tab-remove-docker-engine-2"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo dnf remove docker-ce docker-ce-cli
</span></span></code></pre></div></p></div><div id="tab-remove-docker-engine-3" class="tab-pane" role="tabpanel" aria-labelledby="tab-remove-docker-engine-3"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo apt-get purge docker-ce docker-ce-cli
</span></span></code></pre></div></p></div></div><p>The preceding commands don't remove images, containers, volumes, or customized configuration files on your host.
To delete them, follow Docker's instructions to <a href="https://docs.docker.com/engine/install/ubuntu/#uninstall-docker-engine">Uninstall Docker Engine</a>.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Docker's instructions for uninstalling Docker Engine create a risk of deleting containerd. Be careful when executing commands.</div><h2 id="uncordon-the-node">Uncordon the node</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl uncordon &lt;node-to-uncordon&gt;
</span></span></code></pre></div><p>Replace <code>&lt;node-to-uncordon&gt;</code> with the name of your node you previously drained.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">IP Masquerade Agent User Guide</h1><p>This page shows how to configure and enable the <code>ip-masq-agent</code>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="ip-masquerade-agent-user-guide">IP Masquerade Agent User Guide</h2><p>The <code>ip-masq-agent</code> configures iptables rules to hide a pod's IP address behind the cluster
node's IP address. This is typically done when sending traffic to destinations outside the
cluster's pod <a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR</a> range.</p><h3 id="key-terms">Key Terms</h3><ul><li><strong>NAT (Network Address Translation)</strong>:
Is a method of remapping one IP address to another by modifying either the source and/or
destination address information in the IP header. Typically performed by a device doing IP routing.</li><li><strong>Masquerading</strong>:
A form of NAT that is typically used to perform a many to one address translation, where
multiple source IP addresses are masked behind a single address, which is typically the
device doing the IP routing. In Kubernetes this is the Node's IP address.</li><li><strong>CIDR (Classless Inter-Domain Routing)</strong>:
Based on the variable-length subnet masking, allows specifying arbitrary-length prefixes.
CIDR introduced a new method of representation for IP addresses, now commonly known as
<strong>CIDR notation</strong>, in which an address or routing prefix is written with a suffix indicating
the number of bits of the prefix, such as 192.168.2.0/24.</li><li><strong>Link Local</strong>:
A link-local address is a network address that is valid only for communications within the
network segment or the broadcast domain that the host is connected to. Link-local addresses
for IPv4 are defined in the address block 169.254.0.0/16 in CIDR notation.</li></ul><p>The ip-masq-agent configures iptables rules to handle masquerading node/pod IP addresses when
sending traffic to destinations outside the cluster node's IP and the Cluster IP range. This
essentially hides pod IP addresses behind the cluster node's IP address. In some environments,
traffic to "external" addresses must come from a known machine address. For example, in Google
Cloud, any traffic to the internet must come from a VM's IP. When containers are used, as in
Google Kubernetes Engine, the Pod IP will be rejected for egress. To avoid this, we must hide
the Pod IP behind the VM's own IP address - generally known as "masquerade". By default, the
agent is configured to treat the three private IP ranges specified by
<a href="https://tools.ietf.org/html/rfc1918">RFC 1918</a> as non-masquerade
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR</a>.
These ranges are <code>10.0.0.0/8</code>, <code>172.16.0.0/12</code>, and <code>192.168.0.0/16</code>.
The agent will also treat link-local (169.254.0.0/16) as a non-masquerade CIDR by default.
The agent is configured to reload its configuration from the location
<em>/etc/config/ip-masq-agent</em> every 60 seconds, which is also configurable.</p><p><img alt="masq/non-masq example" src="/images/docs/ip-masq.png"/></p><p>The agent configuration file must be written in YAML or JSON syntax, and may contain three
optional keys:</p><ul><li><code>nonMasqueradeCIDRs</code>: A list of strings in
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing">CIDR</a> notation that specify
the non-masquerade ranges.</li><li><code>masqLinkLocal</code>: A Boolean (true/false) which indicates whether to masquerade traffic to the
link local prefix <code>169.254.0.0/16</code>. False by default.</li><li><code>resyncInterval</code>: A time interval at which the agent attempts to reload config from disk.
For example: '30s', where 's' means seconds, 'ms' means milliseconds.</li></ul><p>Traffic to 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16 ranges will NOT be masqueraded. Any
other traffic (assumed to be internet) will be masqueraded. An example of a local destination
from a pod could be its Node's IP address as well as another node's address or one of the IP
addresses in Cluster's IP range. Any other traffic will be masqueraded by default. The
below entries show the default set of rules that are applied by the ip-masq-agent:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>iptables -t nat -L IP-MASQ-AGENT
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><p>By default, in GCE/Google Kubernetes Engine, if network policy is enabled or
you are using a cluster CIDR not in the 10.0.0.0/8 range, the <code>ip-masq-agent</code>
will run in your cluster. If you are running in another environment,
you can add the <code>ip-masq-agent</code> <a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>
to your cluster.</p><h2 id="create-an-ip-masq-agent">Create an ip-masq-agent</h2><p>To create an ip-masq-agent, run the following kubectl command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</span></span></code></pre></div><p>You must also apply the appropriate node label to any nodes in your cluster that you want the
agent to run on.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready<span style="color:#666">=</span><span style="color:#a2f">true</span>
</span></span></code></pre></div><p>More information can be found in the ip-masq-agent documentation <a href="https://github.com/kubernetes-sigs/ip-masq-agent">here</a>.</p><p>In most cases, the default set of rules should be sufficient; however, if this is not the case
for your cluster, you can create and apply a
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> to customize the IP
ranges that are affected. For example, to allow
only 10.0.0.0/8 to be considered by the ip-masq-agent, you can create the following
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> in a file called
"config".</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>It is important that the file is called config since, by default, that will be used as the key
for lookup by the <code>ip-masq-agent</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">nonMasqueradeCIDRs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:#666">10.0.0.0</span>/8<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resyncInterval</span>:<span style="color:#bbb"> </span>60s<span style="color:#bbb">
</span></span></span></code></pre></div></div><p>Run the following command to add the configmap to your cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create configmap ip-masq-agent --from-file<span style="color:#666">=</span>config --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>This will update a file located at <code>/etc/config/ip-masq-agent</code> which is periodically checked
every <code>resyncInterval</code> and applied to the cluster node.
After the resync interval has expired, you should see the iptables rules reflect your changes:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>iptables -t nat -L IP-MASQ-AGENT
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><p>By default, the link local range (169.254.0.0/16) is also handled by the ip-masq agent, which
sets up the appropriate iptables rules. To have the ip-masq-agent ignore link local, you can
set <code>masqLinkLocal</code> to true in the ConfigMap.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">nonMasqueradeCIDRs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:#666">10.0.0.0</span>/8<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resyncInterval</span>:<span style="color:#bbb"> </span>60s<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">masqLinkLocal</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Quotas for API Objects</h1><p>This page shows how to configure quotas for API objects, including
PersistentVolumeClaims and Services. A quota restricts the number of
objects, of a particular type, that can be created in a namespace.
You specify quotas in a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#resourcequota-v1-core">ResourceQuota</a>
object.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace quota-object-example
</span></span></code></pre></div><h2 id="create-a-resourcequota">Create a ResourceQuota</h2><p>Here is the configuration file for a ResourceQuota object:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-objects.yaml" download="admin/resource/quota-objects.yaml"><code>admin/resource/quota-objects.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-objects-yaml&quot;)" title="Copy admin/resource/quota-objects.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-objects-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>object-quota-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace<span style="color:#666">=</span>quota-object-example
</span></span></code></pre></div><p>View detailed information about the ResourceQuota:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get resourcequota object-quota-demo --namespace<span style="color:#666">=</span>quota-object-example --output<span style="color:#666">=</span>yaml
</span></span></code></pre></div><p>The output shows that in the quota-object-example namespace, there can be at most
one PersistentVolumeClaim, at most two Services of type LoadBalancer, and no Services
of type NodePort.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">used</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="create-a-persistentvolumeclaim">Create a PersistentVolumeClaim</h2><p>Here is the configuration file for a PersistentVolumeClaim object:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-objects-pvc.yaml" download="admin/resource/quota-objects-pvc.yaml"><code>admin/resource/quota-objects-pvc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-objects-pvc-yaml&quot;)" title="Copy admin/resource/quota-objects-pvc.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-objects-pvc-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pvc-quota-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>3Gi<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace<span style="color:#666">=</span>quota-object-example
</span></span></code></pre></div><p>Verify that the PersistentVolumeClaim was created:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get persistentvolumeclaims --namespace<span style="color:#666">=</span>quota-object-example
</span></span></code></pre></div><p>The output shows that the PersistentVolumeClaim exists and has status Pending:</p><pre tabindex="0"><code>NAME             STATUS
pvc-quota-demo   Pending
</code></pre><h2 id="attempt-to-create-a-second-persistentvolumeclaim">Attempt to create a second PersistentVolumeClaim</h2><p>Here is the configuration file for a second PersistentVolumeClaim:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/quota-objects-pvc-2.yaml" download="admin/resource/quota-objects-pvc-2.yaml"><code>admin/resource/quota-objects-pvc-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-quota-objects-pvc-2-yaml&quot;)" title="Copy admin/resource/quota-objects-pvc-2.yaml to clipboard"/></div><div class="includecode" id="admin-resource-quota-objects-pvc-2-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pvc-quota-demo-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>4Gi<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Attempt to create the second PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace<span style="color:#666">=</span>quota-object-example
</span></span></code></pre></div><p>The output shows that the second PersistentVolumeClaim was not created,
because it would have exceeded the quota for the namespace.</p><pre tabindex="0"><code>persistentvolumeclaims "pvc-quota-demo-2" is forbidden:
exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1,
used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1
</code></pre><h2 id="notes">Notes</h2><p>These are the strings used to identify API resources that can be constrained
by quotas:</p><table><tr><th>String</th><th>API Object</th></tr><tr><td>"pods"</td><td>Pod</td></tr><tr><td>"services"</td><td>Service</td></tr><tr><td>"replicationcontrollers"</td><td>ReplicationController</td></tr><tr><td>"resourcequotas"</td><td>ResourceQuota</td></tr><tr><td>"secrets"</td><td>Secret</td></tr><tr><td>"configmaps"</td><td>ConfigMap</td></tr><tr><td>"persistentvolumeclaims"</td><td>PersistentVolumeClaim</td></tr><tr><td>"services.nodeports"</td><td>Service of type NodePort</td></tr><tr><td>"services.loadbalancers"</td><td>Service of type LoadBalancer</td></tr></table><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace quota-object-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1>Manage Memory, CPU, and API Resources</h1><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></h5><p>Define a default memory resource limit for a namespace, so that every new Pod in that namespace has a memory resource limit configured.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a></h5><p>Define a default CPU resource limits for a namespace, so that every new Pod in that namespace has a CPU resource limit configured.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></h5><p>Define a range of valid memory resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></h5><p>Define a range of valid CPU resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></h5><p>Define overall memory and CPU resource limits for a namespace.</p></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></h5><p>Restrict how many Pods you can create within a namespace.</p></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Securing a Cluster</h1><p>This document covers topics related to protecting a cluster from accidental or malicious access
and provides recommendations on overall security.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></li></ul><h2 id="controlling-access-to-the-kubernetes-api">Controlling access to the Kubernetes API</h2><p>As Kubernetes is entirely API-driven, controlling and limiting who can access the cluster and what actions
they are allowed to perform is the first line of defense.</p><h3 id="use-transport-layer-security-tls-for-all-api-traffic">Use Transport Layer Security (TLS) for all API traffic</h3><p>Kubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the
majority of installation methods will allow the necessary certificates to be created and distributed to
the cluster components. Note that some components and installation methods may enable local ports over
HTTP and administrators should familiarize themselves with the settings of each component to identify
potentially unsecured traffic.</p><h3 id="api-authentication">API Authentication</h3><p>Choose an authentication mechanism for the API servers to use that matches the common access patterns
when you install a cluster. For instance, small, single-user clusters may wish to use a simple certificate
or static Bearer token approach. Larger clusters may wish to integrate an existing OIDC or LDAP server that
allow users to be subdivided into groups.</p><p>All API clients must be authenticated, even those that are part of the infrastructure like nodes,
proxies, the scheduler, and volume plugins. These clients are typically <a href="/docs/reference/access-authn-authz/service-accounts-admin/">service accounts</a> or use x509 client certificates, and they are created automatically at cluster startup or are setup as part of the cluster installation.</p><p>Consult the <a href="/docs/reference/access-authn-authz/authentication/">authentication reference document</a> for more information.</p><h3 id="api-authorization">API Authorization</h3><p>Once authenticated, every API call is also expected to pass an authorization check. Kubernetes ships
an integrated <a href="/docs/reference/access-authn-authz/rbac/">Role-Based Access Control (RBAC)</a> component that matches an incoming user or group to a
set of permissions bundled into roles. These permissions combine verbs (get, create, delete) with
resources (pods, services, nodes) and can be namespace-scoped or cluster-scoped. A set of out-of-the-box
roles are provided that offer reasonable default separation of responsibility depending on what
actions a client might want to perform. It is recommended that you use the
<a href="/docs/reference/access-authn-authz/node/">Node</a> and
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> authorizers together, in combination with the
<a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction</a> admission plugin.</p><p>As with authentication, simple and broad roles may be appropriate for smaller clusters, but as
more users interact with the cluster, it may become necessary to separate teams into separate
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespaces">namespaces</a> with more limited roles.</p><p>With authorization, it is important to understand how updates on one object may cause actions in
other places. For instance, a user may not be able to create pods directly, but allowing them to
create a deployment, which creates pods on their behalf, will let them create those pods
indirectly. Likewise, deleting a node from the API will result in the pods scheduled to that node
being terminated and recreated on other nodes. The out-of-the box roles represent a balance
between flexibility and common use cases, but more limited roles should be carefully reviewed
to prevent accidental escalation. You can make roles specific to your use case if the out-of-box ones don't meet your needs.</p><p>Consult the <a href="/docs/reference/access-authn-authz/authorization/">authorization reference section</a> for more information.</p><h2 id="controlling-access-to-the-kubelet">Controlling access to the Kubelet</h2><p>Kubelets expose HTTPS endpoints which grant powerful control over the node and containers.
By default Kubelets allow unauthenticated access to this API.</p><p>Production clusters should enable Kubelet authentication and authorization.</p><p>Consult the <a href="/docs/reference/access-authn-authz/kubelet-authn-authz/">Kubelet authentication/authorization reference</a>
for more information.</p><h2 id="controlling-the-capabilities-of-a-workload-or-user-at-runtime">Controlling the capabilities of a workload or user at runtime</h2><p>Authorization in Kubernetes is intentionally high level, focused on coarse actions on resources.
More powerful controls exist as <strong>policies</strong> to limit by use case how those objects act on the
cluster, themselves, and other resources.</p><h3 id="limiting-resource-usage-on-a-cluster">Limiting resource usage on a cluster</h3><p><a href="/docs/concepts/policy/resource-quotas/">Resource quota</a> limits the number or capacity of
resources granted to a namespace. This is most often used to limit the amount of CPU, memory,
or persistent disk a namespace can allocate, but can also control how many pods, services, or
volumes exist in each namespace.</p><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Limit ranges</a> restrict the maximum or minimum size of some of the
resources above, to prevent users from requesting unreasonably high or low values for commonly
reserved resources like memory, or to provide default limits when none are specified.</p><h3 id="controlling-what-privileges-containers-run-with">Controlling what privileges containers run with</h3><p>A pod definition contains a <a href="/docs/tasks/configure-pod-container/security-context/">security context</a>
that allows it to request access to run as a specific Linux user on a node (like root),
access to run privileged or access the host network, and other controls that would otherwise
allow it to run unfettered on a hosting node.</p><p>You can configure <a href="/docs/concepts/security/pod-security-admission/">Pod security admission</a>
to enforce use of a particular <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standard</a>
in a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>, or to detect breaches.</p><p>Generally, most application workloads need limited access to host resources so they can
successfully run as a root process (uid 0) without access to host information. However,
considering the privileges associated with the root user, you should write application
containers to run as a non-root user. Similarly, administrators who wish to prevent
client applications from escaping their containers should apply the <strong>Baseline</strong>
or <strong>Restricted</strong> Pod Security Standard.</p><h3 id="preventing-containers-from-loading-unwanted-kernel-modules">Preventing containers from loading unwanted kernel modules</h3><p>The Linux kernel automatically loads kernel modules from disk if needed in certain
circumstances, such as when a piece of hardware is attached or a filesystem is mounted. Of
particular relevance to Kubernetes, even unprivileged processes can cause certain
network-protocol-related kernel modules to be loaded, just by creating a socket of the
appropriate type. This may allow an attacker to exploit a security hole in a kernel module
that the administrator assumed was not in use.</p><p>To prevent specific modules from being automatically loaded, you can uninstall them from
the node, or add rules to block them. On most Linux distributions, you can do that by
creating a file such as <code>/etc/modprobe.d/kubernetes-blacklist.conf</code> with contents like:</p><pre tabindex="0"><code># DCCP is unlikely to be needed, has had multiple serious
# vulnerabilities, and is not well-maintained.
blacklist dccp

# SCTP is not used in most Kubernetes clusters, and has also had
# vulnerabilities in the past.
blacklist sctp
</code></pre><p>To block module loading more generically, you can use a Linux Security Module (such as
SELinux) to completely deny the <code>module_request</code> permission to containers, preventing the
kernel from loading modules for containers under any circumstances. (Pods would still be
able to use modules that had been loaded manually, or modules that were loaded by the
kernel on behalf of some more-privileged process.)</p><h3 id="restricting-network-access">Restricting network access</h3><p>The <a href="/docs/tasks/administer-cluster/declare-network-policy/">network policies</a> for a namespace
allows application authors to restrict which pods in other namespaces may access pods and ports
within their namespaces. Many of the supported <a href="/docs/concepts/cluster-administration/networking/">Kubernetes networking providers</a>
now respect network policy.</p><p>Quota and limit ranges can also be used to control whether users may request node ports or
load-balanced services, which on many clusters can control whether those users applications
are visible outside of the cluster.</p><p>Additional protections may be available that control network rules on a per-plugin or per-
environment basis, such as per-node firewalls, physically separating cluster nodes to
prevent cross talk, or advanced networking policy.</p><h3 id="restricting-cloud-metadata-api-access">Restricting cloud metadata API access</h3><p>Cloud platforms (AWS, Azure, GCE, etc.) often expose metadata services locally to instances.
By default these APIs are accessible by pods running on an instance and can contain cloud
credentials for that node, or provisioning data such as kubelet credentials. These credentials
can be used to escalate within the cluster or to other cloud services under the same account.</p><p>When running Kubernetes on a cloud platform, limit permissions given to instance credentials, use
<a href="/docs/tasks/administer-cluster/declare-network-policy/">network policies</a> to restrict pod access
to the metadata API, and avoid using provisioning data to deliver secrets.</p><h3 id="controlling-which-nodes-pods-may-access">Controlling which nodes pods may access</h3><p>By default, there are no restrictions on which nodes may run a pod. Kubernetes offers a
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">rich set of policies for controlling placement of pods onto nodes</a>
and the <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taint-based pod placement and eviction</a>
that are available to end users. For many clusters use of these policies to separate workloads
can be a convention that authors adopt or enforce via tooling.</p><p>As an administrator, a beta admission plugin <code>PodNodeSelector</code> can be used to force pods
within a namespace to default or require a specific node selector, and if end users cannot
alter namespaces, this can strongly limit the placement of all of the pods in a specific workload.</p><h2 id="protecting-cluster-components-from-compromise">Protecting cluster components from compromise</h2><p>This section describes some common patterns for protecting clusters from compromise.</p><h3 id="restrict-access-to-etcd">Restrict access to etcd</h3><p>Write access to the etcd backend for the API is equivalent to gaining root on the entire cluster,
and read access can be used to escalate fairly quickly. Administrators should always use strong
credentials from the API servers to their etcd server, such as mutual auth via TLS client certificates,
and it is often recommended to isolate the etcd servers behind a firewall that only the API servers
may access.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Allowing other components within the cluster to access the master etcd instance with
read or write access to the full keyspace is equivalent to granting cluster-admin access. Using
separate etcd instances for non-master components or using etcd ACLs to restrict read and write
access to a subset of the keyspace is strongly recommended.</div><h3 id="enable-audit-logging">Enable audit logging</h3><p>The <a href="/docs/tasks/debug/debug-cluster/audit/">audit logger</a> is a beta feature that records actions taken by the
API for later analysis in the event of a compromise. It is recommended to enable audit logging
and archive the audit file on a secure server.</p><h3 id="restrict-access-to-alpha-or-beta-features">Restrict access to alpha or beta features</h3><p>Alpha and beta Kubernetes features are in active development and may have limitations or bugs
that result in security vulnerabilities. Always assess the value an alpha or beta feature may
provide against the possible risk to your security posture. When in doubt, disable features you
do not use.</p><h3 id="rotate-infrastructure-credentials-frequently">Rotate infrastructure credentials frequently</h3><p>The shorter the lifetime of a secret or credential the harder it is for an attacker to make
use of that credential. Set short lifetimes on certificates and automate their rotation. Use
an authentication provider that can control how long issued tokens are available and use short
lifetimes where possible. If you use service-account tokens in external integrations, plan to
rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap
token used for setting up nodes should be revoked or its authorization removed.</p><h3 id="review-third-party-integrations-before-enabling-them">Review third party integrations before enabling them</h3><p>Many third party integrations to Kubernetes may alter the security profile of your cluster. When
enabling an integration, always review the permissions that an extension requests before granting
it access. For example, many security integrations may request access to view all secrets on
your cluster which is effectively making that component a cluster admin. When in doubt,
restrict the integration to functioning in a single namespace if possible.</p><p>Components that create pods may also be unexpectedly powerful if they can do so inside namespaces
like the <code>kube-system</code> namespace, because those pods can gain access to service account secrets
or run with elevated permissions if those service accounts are granted access to permissive
<a href="/docs/concepts/security/pod-security-policy/">PodSecurityPolicies</a>.</p><p>If you use <a href="/docs/concepts/security/pod-security-admission/">Pod Security admission</a> and allow
any component to create Pods within a namespace that permits privileged Pods, those Pods may
be able to escape their containers and use this widened access to elevate their privileges.</p><p>You should not allow untrusted components to create Pods in any system namespace (those with
names that start with <code>kube-</code>) nor in any namespace where that access grant allows the possibility
of privilege escalation.</p><h3 id="encrypt-secrets-at-rest">Encrypt secrets at rest</h3><p>In general, the etcd database will contain any information accessible via the Kubernetes API
and may grant an attacker significant visibility into the state of your cluster. Always encrypt
your backups using a well reviewed backup and encryption solution, and consider using full disk
encryption where possible.</p><p>Kubernetes supports optional <a href="/docs/tasks/administer-cluster/encrypt-data/">encryption at rest</a> for information in the Kubernetes API.
This lets you ensure that when Kubernetes stores data for objects (for example, <code>Secret</code> or
<code>ConfigMap</code> objects), the API server writes an encrypted representation of the object.
That encryption means that even someone who has access to etcd backup data is unable
to view the content of those objects.
In Kubernetes 1.34 you can also encrypt custom resources;
encryption-at-rest for extension APIs defined in CustomResourceDefinitions was added to
Kubernetes as part of the v1.26 release.</p><h3 id="receiving-alerts-for-security-updates-and-reporting-vulnerabilities">Receiving alerts for security updates and reporting vulnerabilities</h3><p>Join the <a href="https://groups.google.com/forum/#!forum/kubernetes-announce">kubernetes-announce</a>
group for emails about security announcements. See the
<a href="/docs/reference/issues-security/security/">security reporting</a>
page for more on how to report vulnerabilities.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/security/security-checklist/">Security Checklist</a> for additional information on Kubernetes security guidance.</li><li><a href="/docs/reference/node/seccomp/">Seccomp Node Reference</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Create an External Load Balancer</h1><p>This page shows how to create an external load balancer.</p><p>When creating a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>, you have
the option of automatically creating a cloud load balancer. This provides an
externally-accessible IP address that sends traffic to the correct port on your cluster
nodes,
<em>provided your cluster runs in a supported environment and is configured with
the correct cloud load balancer provider package</em>.</p><p>You can also use an <a class="glossary-tooltip" title="An API object that manages external access to the services in a cluster, typically HTTP." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/ingress/" target="_blank" aria-label="Ingress">Ingress</a> in place of Service.
For more information, check the <a href="/docs/concepts/services-networking/ingress/">Ingress</a>
documentation.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your cluster must be running in a cloud or other environment that already has support
for configuring external load balancers.</p><h2 id="create-a-service">Create a Service</h2><h3 id="create-a-service-from-a-manifest">Create a Service from a manifest</h3><p>To create an external load balancer, add the following line to your
Service manifest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span></code></pre></div><p>Your manifest might then look like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8765</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="create-a-service-using-kubectl">Create a Service using kubectl</h3><p>You can alternatively create the service with the <code>kubectl expose</code> command and
its <code>--type=LoadBalancer</code> flag:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl expose deployment example --port<span style="color:#666">=</span><span style="color:#666">8765</span> --target-port<span style="color:#666">=</span><span style="color:#666">9376</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>        --name<span style="color:#666">=</span>example-service --type<span style="color:#666">=</span>LoadBalancer
</span></span></code></pre></div><p>This command creates a new Service using the same selectors as the referenced
resource (in the case of the example above, a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a> named <code>example</code>).</p><p>For more information, including optional flags, refer to the
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose"><code>kubectl expose</code> reference</a>.</p><h2 id="finding-your-ip-address">Finding your IP address</h2><p>You can find the IP address created for your service by getting the service
information through <code>kubectl</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl describe services example-service
</span></span></code></pre></div><p>which should produce output similar to:</p><pre tabindex="0"><code>Name:                     example-service
Namespace:                default
Labels:                   app=example
Annotations:              &lt;none&gt;
Selector:                 app=example
Type:                     LoadBalancer
IP Families:              &lt;none&gt;
IP:                       10.3.22.96
IPs:                      10.3.22.96
LoadBalancer Ingress:     192.0.2.89
Port:                     &lt;unset&gt;  8765/TCP
TargetPort:               9376/TCP
NodePort:                 &lt;unset&gt;  30593/TCP
Endpoints:                172.17.0.3:9376
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre><p>The load balancer's IP address is listed next to <code>LoadBalancer Ingress</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you are running your service on Minikube, you can find the assigned IP address and port with:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>minikube service example-service --url
</span></span></code></pre></div></div><h2 id="preserving-the-client-source-ip">Preserving the client source IP</h2><p>By default, the source IP seen in the target container is <em>not the original
source IP</em> of the client. To enable preservation of the client IP, the following
fields can be configured in the <code>.spec</code> of the Service:</p><ul><li><code>.spec.externalTrafficPolicy</code> - denotes if this Service desires to route
external traffic to node-local or cluster-wide endpoints. There are two available
options: <code>Cluster</code> (default) and <code>Local</code>. <code>Cluster</code> obscures the client source
IP and may cause a second hop to another node, but should have good overall
load-spreading. <code>Local</code> preserves the client source IP and avoids a second hop
for LoadBalancer and NodePort type Services, but risks potentially imbalanced
traffic spreading.</li><li><code>.spec.healthCheckNodePort</code> - specifies the health check node port
(numeric port number) for the service. If you don't specify
<code>healthCheckNodePort</code>, the service controller allocates a port from your
cluster's NodePort range.<br/>You can configure that range by setting an API server command line option,
<code>--service-node-port-range</code>. The Service will use the user-specified
<code>healthCheckNodePort</code> value if you specify it, provided that the
Service <code>type</code> is set to LoadBalancer and <code>externalTrafficPolicy</code> is set
to <code>Local</code>.</li></ul><p>Setting <code>externalTrafficPolicy</code> to Local in the Service manifest
activates this feature. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8765</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">externalTrafficPolicy</span>:<span style="color:#bbb"> </span>Local<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="caveats-and-limitations-when-preserving-source-ips">Caveats and limitations when preserving source IPs</h3><p>Load balancing services from some cloud providers do not let you configure different weights for each target.</p><p>With each target weighted equally in terms of sending traffic to Nodes, external
traffic is not equally load balanced across different Pods. The external load balancer
is unaware of the number of Pods on each node that are used as a target.</p><p>Where <code>NumServicePods &lt;&lt; NumNodes</code> or <code>NumServicePods &gt;&gt; NumNodes</code>, a fairly close-to-equal
distribution will be seen, even without weights.</p><p>Internal pod to pod traffic should behave similar to ClusterIP services, with equal probability across all pods.</p><h2 id="garbage-collecting-load-balancers">Garbage collecting load balancers</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>In usual case, the correlating load balancer resources in cloud provider should
be cleaned up soon after a LoadBalancer type Service is deleted. But it is known
that there are various corner cases where cloud resources are orphaned after the
associated Service is deleted. Finalizer Protection for Service LoadBalancers was
introduced to prevent this from happening. By using finalizers, a Service resource
will never be deleted until the correlating load balancer resources are also deleted.</p><p>Specifically, if a Service has <code>type</code> LoadBalancer, the service controller will attach
a finalizer named <code>service.kubernetes.io/load-balancer-cleanup</code>.
The finalizer will only be removed after the load balancer resource is cleaned up.
This prevents dangling load balancer resources even in corner cases such as the
service controller crashing.</p><h2 id="external-load-balancer-providers">External load balancer providers</h2><p>It is important to note that the datapath for this functionality is provided by a load balancer external to the Kubernetes cluster.</p><p>When the Service <code>type</code> is set to LoadBalancer, Kubernetes provides functionality equivalent to <code>type</code> equals ClusterIP to pods
within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the nodes
hosting the relevant Kubernetes pods. The Kubernetes control plane automates the creation of the external load balancer,
health checks (if needed), and packet filtering rules (if needed). Once the cloud provider allocates an IP address for the load
balancer, the control plane looks up that external IP address and populates it into the Service object.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li><li>Read about <a href="/docs/concepts/services-networking/service/">Service</a></li><li>Read about <a href="/docs/concepts/services-networking/ingress/">Ingress</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Control Topology Management Policies on a node</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>An increasing number of systems leverage a combination of CPUs and hardware accelerators to
support latency-critical execution and high-throughput parallel computation. These include
workloads in fields such as telecommunications, scientific computing, machine learning, financial
services and data analytics. Such hybrid systems comprise a high performance environment.</p><p>In order to extract the best performance, optimizations related to CPU isolation, memory and
device locality are required. However, in Kubernetes, these optimizations are handled by a
disjoint set of components.</p><p><em>Topology Manager</em> is a kubelet component that aims to coordinate the set of components that are
responsible for these optimizations.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.18.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="how-topology-manager-works">How topology manager works</h2><p>Prior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make
resource allocation decisions independently of each other. This can result in undesirable
allocations on multiple-socketed systems, and performance/latency sensitive applications will suffer
due to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and
devices being allocated from different NUMA Nodes, thus incurring additional latency.</p><p>The Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet
components can make topology aligned resource allocation choices.</p><p>The Topology Manager provides an interface for components, called <em>Hint Providers</em>, to send and
receive topology information. The Topology Manager has a set of node level policies which are
explained below.</p><p>The Topology Manager receives topology information from the <em>Hint Providers</em> as a bitmask denoting
NUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform
a set of operations on the hints provided and converge on the hint determined by the policy to
give the optimal result. If an undesirable hint is stored, the preferred field for the hint will be
set to false. In the current policies preferred is the narrowest preferred mask.
The selected hint is stored as part of the Topology Manager. Depending on the policy configured,
the pod can be accepted or rejected from the node based on the selected hint.
The hint is then stored in the Topology Manager for use by the <em>Hint Providers</em> when making the
resource allocation decisions.</p><p>The flow can be seen in the following diagram.</p><p><img alt="topology_manager_flow" src="/images/docs/topology-manager-flow.png"/></p><h2 id="windows-support">Windows Support</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>The Topology Manager support can be enabled on Windows by using the <code>WindowsCPUAndMemoryAffinity</code> feature gate and
it requires support in the container runtime.</p><h2 id="topology-manager-scopes-and-policies">Topology manager scopes and policies</h2><p>The Topology Manager currently:</p><ul><li>aligns Pods of all QoS classes.</li><li>aligns the requested resources that Hint Provider provides topology hints for.</li></ul><p>If these conditions are met, the Topology Manager will align the requested resources.</p><p>In order to customize how this alignment is carried out, the Topology Manager provides two
distinct options: <code>scope</code> and <code>policy</code>.</p><p>The <code>scope</code> defines the granularity at which you would like resource alignment to be performed,
for example, at the <code>pod</code> or <code>container</code> level. And the <code>policy</code> defines the actual policy used to
carry out the alignment, for example, <code>best-effort</code>, <code>restricted</code>, and <code>single-numa-node</code>.
Details on the various <code>scopes</code> and <code>policies</code> available today can be found below.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be
enabled and proper CPU Manager policy should be configured on a Node.
See <a href="/docs/tasks/administer-cluster/cpu-management-policies/">Control CPU Management Policies on the Node</a>.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory
Manager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to
<a href="/docs/tasks/administer-cluster/memory-manager/">Memory Manager</a> documentation.</div><h2 id="topology-manager-scopes">Topology manager scopes</h2><p>The Topology Manager can deal with the alignment of resources in a couple of distinct scopes:</p><ul><li><code>container</code> (default)</li><li><code>pod</code></li></ul><p>Either option can be selected at a time of the kubelet startup, by setting the
<code>topologyManagerScope</code> in the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><h3 id="container-scope"><code>container</code> scope</h3><p>The <code>container</code> scope is used by default. You can also explicitly set the
<code>topologyManagerScope</code> to <code>container</code> in the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><p>Within this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,
for each container (in a pod) a separate alignment is computed. In other words, there is no notion
of grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,
the Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.</p><p>The notion of grouping the containers was endorsed and implemented on purpose in the following
scope, for example the <code>pod</code> scope.</p><h3 id="pod-scope"><code>pod</code> scope</h3><p>To select the <code>pod</code> scope, set <code>topologyManagerScope</code> in the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a> to <code>pod</code>.</p><p>This scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the
Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)
to either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the
alignments produced by the Topology Manager on different occasions:</p><ul><li>all containers can be and are allocated to a single NUMA node;</li><li>all containers can be and are allocated to a shared set of NUMA nodes.</li></ul><p>The total amount of particular resource demanded for the entire pod is calculated according to
<a href="/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers">effective requests/limits</a>
formula, and thus, this total value is equal to the maximum of:</p><ul><li>the sum of all app container requests,</li><li>the maximum of init container requests,</li></ul><p>for a resource.</p><p>Using the <code>pod</code> scope in tandem with <code>single-numa-node</code> Topology Manager policy is specifically
valuable for workloads that are latency sensitive or for high-throughput applications that perform
IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA
node; hence, the inter-NUMA communication overhead can be eliminated for that pod.</p><p>In the case of <code>single-numa-node</code> policy, a pod is accepted only if a suitable set of NUMA nodes
is present among possible allocations. Reconsider the example above:</p><ul><li>a set containing only a single NUMA node - it leads to pod being admitted,</li><li>whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one
NUMA node, two or more NUMA nodes are required to satisfy the allocation).</li></ul><p>To recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology
Manager policy, which either leads to the rejection or admission of the pod.</p><h2 id="topology-manager-policies">Topology manager policies</h2><p>The Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,
<code>--topology-manager-policy</code>. There are four supported policies:</p><ul><li><code>none</code> (default)</li><li><code>best-effort</code></li><li><code>restricted</code></li><li><code>single-numa-node</code></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If the Topology Manager is configured with the <strong>pod</strong> scope, the container, which is considered by
the policy, is reflecting requirements of the entire pod, and thus each container from the pod
will result with <strong>the same</strong> topology alignment decision.</div><h3 id="policy-none"><code>none</code> policy</h3><p>This is the default policy and does not perform any topology alignment.</p><h3 id="policy-best-effort"><code>best-effort</code> policy</h3><p>For each container in a Pod, the kubelet, with <code>best-effort</code> topology management policy, calls
each Hint Provider to discover their resource availability. Using this information, the Topology
Manager stores the preferred NUMA Node affinity for that container. If the affinity is not
preferred, the Topology Manager will store this and admit the pod to the node anyway.</p><p>The <em>Hint Providers</em> can then use this information when making the
resource allocation decision.</p><h3 id="policy-restricted"><code>restricted</code> policy</h3><p>For each container in a Pod, the kubelet, with <code>restricted</code> topology management policy, calls each
Hint Provider to discover their resource availability. Using this information, the Topology
Manager stores the preferred NUMA Node affinity for that container. If the affinity is not
preferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a
<code>Terminated</code> state with a pod admission failure.</p><p>Once the pod is in a <code>Terminated</code> state, the Kubernetes scheduler will <strong>not</strong> attempt to
reschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of
the pod. An external control loop could be also implemented to trigger a redeployment of pods that
have the <code>Topology Affinity</code> error.</p><p>If the pod is admitted, the <em>Hint Providers</em> can then use this information when making the
resource allocation decision.</p><h3 id="policy-single-numa-node"><code>single-numa-node</code> policy</h3><p>For each container in a Pod, the kubelet, with <code>single-numa-node</code> topology management policy,
calls each Hint Provider to discover their resource availability. Using this information, the
Topology Manager determines if a single NUMA Node affinity is possible. If it is, Topology
Manager will store this and the <em>Hint Providers</em> can then use this information when making the
resource allocation decision. If, however, this is not possible then the Topology Manager will
reject the pod from the node. This will result in a pod in a <code>Terminated</code> state with a pod
admission failure.</p><p>Once the pod is in a <code>Terminated</code> state, the Kubernetes scheduler will <strong>not</strong> attempt to
reschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of
the Pod. An external control loop could be also implemented to trigger a redeployment of pods
that have the <code>Topology Affinity</code> error.</p><h2 id="topology-manager-policy-options">Topology manager policy options</h2><p>Support for the Topology Manager policy options requires <code>TopologyManagerPolicyOptions</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> to be enabled
(it is enabled by default).</p><p>You can toggle groups of options on and off based upon their maturity level using the following feature gates:</p><ul><li><code>TopologyManagerPolicyBetaOptions</code> default enabled. Enable to show beta-level options.</li><li><code>TopologyManagerPolicyAlphaOptions</code> default disabled. Enable to show alpha-level options.</li></ul><p>You will still have to enable each option using the <code>TopologyManagerPolicyOptions</code> kubelet option.</p><h3 id="policy-option-prefer-closest-numa-nodes"><code>prefer-closest-numa-nodes</code></h3><p>The <code>prefer-closest-numa-nodes</code> option is GA since Kubernetes 1.32. In Kubernetes 1.34
this policy option is visible by default provided that the <code>TopologyManagerPolicyOptions</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled.</p><p>The Topology Manager is not aware by default of NUMA distances, and does not take them into account when making
Pod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,
and can cause significant performance degradation in latency-critical execution and high-throughput applications
if the Topology Manager decides to align resources on non-adjacent NUMA nodes.</p><p>If you specify the <code>prefer-closest-numa-nodes</code> policy option, the <code>best-effort</code> and <code>restricted</code>
policies favor sets of NUMA nodes with shorter distance between them when making admission decisions.</p><p>You can enable this option by adding <code>prefer-closest-numa-nodes=true</code> to the Topology Manager policy options.</p><p>By default (without this option), the Topology Manager aligns resources on either a single NUMA node or,
in the case where more than one NUMA node is required, using the minimum number of NUMA nodes.</p><h3 id="policy-option-max-allowable-numa-nodes"><code>max-allowable-numa-nodes</code> (beta)</h3><p>The <code>max-allowable-numa-nodes</code> option is beta since Kubernetes 1.31. In Kubernetes 1.34,
this policy option is visible by default provided that the <code>TopologyManagerPolicyOptions</code> and
<code>TopologyManagerPolicyBetaOptions</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>
are enabled.</p><p>The time to admit a pod is tied to the number of NUMA nodes on the physical machine.
By default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where
more than 8 NUMA nodes are detected.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you select the <code>max-allowable-numa-nodes</code> policy option, nodes with more than 8 NUMA nodes can
be allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact
of using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that
lack of data, using this policy option with Kubernetes 1.34 is <strong>not</strong> recommended and is
at your own risk.</div><p>You can enable this option by adding <code>max-allowable-numa-nodes=true</code> to the Topology Manager policy options.</p><p>Setting a value of <code>max-allowable-numa-nodes</code> does not (in and of itself) affect the
latency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.
Future, potential improvements to Kubernetes may improve Pod admission performance and the high
latency that happens as the number of NUMA nodes increases.</p><h2 id="pod-interactions-with-topology-manager-policies">Pod interactions with topology manager policies</h2><p>Consider the containers in the following Pod manifest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because no resource <code>requests</code> or <code>limits</code> are specified.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This pod runs in the <code>Burstable</code> QoS class because requests are less than limits.</p><p>If the selected policy is anything other than <code>none</code>, the Topology Manager would consider these Pod
specifications. The Topology Manager would consult the Hint Providers to get topology hints.
In the case of the <code>static</code>, the CPU Manager policy would return default topology hint, because
these Pods do not explicitly request CPU resources.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This pod with integer CPU request runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal
to <code>limits</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"300m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"300m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This pod with sharing CPU request runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal
to <code>limits</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/deviceA</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/deviceB</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/deviceA</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/deviceB</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This pod runs in the <code>BestEffort</code> QoS class because there are no CPU and memory requests.</p><p>The Topology Manager would consider the above pods. The Topology Manager would consult the Hint
Providers, which are CPU and Device Manager to get topology hints for the pods.</p><p>In the case of the <code>Guaranteed</code> pod with integer CPU request, the <code>static</code> CPU Manager policy
would return topology hints relating to the exclusive CPU and the Device Manager would send back
hints for the requested device.</p><p>In the case of the <code>Guaranteed</code> pod with sharing CPU request, the <code>static</code> CPU Manager policy
would return default topology hint as there is no exclusive CPU request and the Device Manager
would send back hints for the requested device.</p><p>In the above two cases of the <code>Guaranteed</code> pod, the <code>none</code> CPU Manager policy would return default
topology hint.</p><p>In the case of the <code>BestEffort</code> pod, the <code>static</code> CPU Manager policy would send back the default
topology hint as there is no CPU request and the Device Manager would send back the hints for each
of the requested devices.</p><p>Using this information the Topology Manager calculates the optimal hint for the pod and stores
this information, which will be used by the Hint Providers when they are making their resource
assignments.</p><h2 id="known-limitations">Known limitations</h2><ol><li><p>The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes,
there will be a state explosion when trying to enumerate the possible NUMA affinities and
generating their hints. See <a href="#policy-option-max-allowable-numa-nodes"><code>max-allowable-numa-nodes</code></a>
(beta) for more options.</p></li><li><p>The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail
on the node due to the Topology Manager.</p></li></ol></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Enable Or Disable A Kubernetes API</h1><p>This page shows how to enable or disable an API version from your cluster's
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>.</p><p>Specific API versions can be turned on or off by passing <code>--runtime-config=api/&lt;version&gt;</code> as a
command line argument to the API server. The values for this argument are a comma-separated
list of API versions. Later values override earlier values.</p><p>The <code>runtime-config</code> command line argument also supports 2 special keys:</p><ul><li><code>api/all</code>, representing all known APIs</li><li><code>api/legacy</code>, representing only legacy APIs. Legacy APIs are any APIs that have been
explicitly <a href="/docs/reference/using-api/deprecation-policy/">deprecated</a>.</li></ul><p>For example, to turn off all API versions except v1, pass <code>--runtime-config=api/all=false,api/v1=true</code>
to the <code>kube-apiserver</code>.</p><h2 id="what-s-next">What's next</h2><p>Read the <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">full documentation</a>
for the <code>kube-apiserver</code> component.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Declare Network Policy</h1><p>This document helps you get started using the Kubernetes <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy API</a> to declare network policies that govern how pods communicate with each other.</p><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.8.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>Make sure you've configured a network provider with network policy support. There are a number of network providers that support NetworkPolicy, including:</p><ul><li><a href="/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/">Antrea</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/">Calico</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">Cilium</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">Kube-router</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">Romana</a></li><li><a href="/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">Weave Net</a></li></ul><h2 id="create-an-nginx-deployment-and-expose-it-via-a-service">Create an <code>nginx</code> deployment and expose it via a service</h2><p>To see how Kubernetes network policy works, start off by creating an <code>nginx</code> Deployment.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl create deployment nginx --image=nginx
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/nginx created
</code></pre><p>Expose the Deployment through a Service called <code>nginx</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl expose deployment nginx --port=80
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">service/nginx exposed
</code></pre><p>The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named <code>nginx</code>. The <code>nginx</code> Pod and Deployment are found in the <code>default</code> namespace.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl get svc,pod
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/kubernetes          10.100.0.1    &lt;none&gt;        443/TCP    46m
service/nginx               10.100.0.16   &lt;none&gt;        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
pod/nginx-701339712-e0qfq   1/1           Running       0          35s
</code></pre><h2 id="test-the-service-by-accessing-it-from-another-pod">Test the service by accessing it from another Pod</h2><p>You should be able to access the new <code>nginx</code> service from other Pods. To access the <code>nginx</code> Service from another Pod in the <code>default</code> namespace, start a busybox container:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl run busybox --rm -ti --image=busybox -- /bin/sh
</span></span></span></code></pre></div><p>In your shell, run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre><h2 id="limit-access-to-the-nginx-service">Limit access to the <code>nginx</code> service</h2><p>To limit the access to the <code>nginx</code> service so that only Pods with the label <code>access: true</code> can query it, create a NetworkPolicy object as follows:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/nginx-policy.yaml" download="service/networking/nginx-policy.yaml"><code>service/networking/nginx-policy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-nginx-policy-yaml&quot;)" title="Copy service/networking/nginx-policy.yaml to clipboard"/></div><div class="includecode" id="service-networking-nginx-policy-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>access-nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">from</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">access</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The name of a NetworkPolicy object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>NetworkPolicy includes a <code>podSelector</code> which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label <code>app=nginx</code>. The label was automatically added to the Pod in the <code>nginx</code> Deployment. An empty <code>podSelector</code> selects all pods in the namespace.</div><h2 id="assign-the-policy-to-the-service">Assign the policy to the service</h2><p>Use kubectl to create a NetworkPolicy from the above <code>nginx-policy.yaml</code> file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml
</span></span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">networkpolicy.networking.k8s.io/access-nginx created
</code></pre><h2 id="test-access-to-the-service-when-access-label-is-not-defined">Test access to the service when access label is not defined</h2><p>When you attempt to access the <code>nginx</code> Service from a Pod without the correct labels, the request times out:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl run busybox --rm -ti --image=busybox -- /bin/sh
</span></span></span></code></pre></div><p>In your shell, run the command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
wget: download timed out
</code></pre><h2 id="define-access-label-and-test-again">Define access label and test again</h2><p>You can create a Pod with the correct labels to see that the request is allowed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">kubectl run busybox --rm -ti --labels="access=true" --image=busybox -- /bin/sh
</span></span></span></code></pre></div><p>In your shell, run the command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Reconfiguring a kubeadm cluster</h1><p>kubeadm does not support automated ways of reconfiguring components that
were deployed on managed nodes. One way of automating this would be
by using a custom <a href="/docs/concepts/extend-kubernetes/operator/">operator</a>.</p><p>To modify the components configuration you must manually edit associated cluster
objects and files on disk.</p><p>This guide shows the correct sequence of steps that need to be performed
to achieve kubeadm cluster reconfiguration.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need a cluster that was deployed using kubeadm</li><li>Have administrator credentials (<code>/etc/kubernetes/admin.conf</code>) and network connectivity
to a running kube-apiserver in the cluster from a host that has kubectl installed</li><li>Have a text editor installed on all hosts</li></ul><h2 id="reconfiguring-the-cluster">Reconfiguring the cluster</h2><p>kubeadm writes a set of cluster wide component configuration options in
ConfigMaps and other objects. These objects must be manually edited. The command <code>kubectl edit</code>
can be used for that.</p><p>The <code>kubectl edit</code> command will open a text editor where you can edit and save the object directly.</p><p>You can use the environment variables <code>KUBECONFIG</code> and <code>KUBE_EDITOR</code> to specify the location of
the kubectl consumed kubeconfig file and preferred text editor.</p><p>For example:</p><pre tabindex="0"><code>KUBECONFIG=/etc/kubernetes/admin.conf KUBE_EDITOR=nano kubectl edit &lt;parameters&gt;
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Upon saving any changes to these cluster objects, components running on nodes may not be
automatically updated. The steps below instruct you on how to perform that manually.</div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Component configuration in ConfigMaps is stored as unstructured data (YAML string).
This means that validation will not be performed upon updating the contents of a ConfigMap.
You have to be careful to follow the documented API format for a particular
component configuration and avoid introducing typos and YAML indentation mistakes.</div><h3 id="applying-cluster-configuration-changes">Applying cluster configuration changes</h3><h4 id="updating-the-clusterconfiguration">Updating the <code>ClusterConfiguration</code></h4><p>During cluster creation and upgrade, kubeadm writes its
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/"><code>ClusterConfiguration</code></a>
in a ConfigMap called <code>kubeadm-config</code> in the <code>kube-system</code> namespace.</p><p>To change a particular option in the <code>ClusterConfiguration</code> you can edit the ConfigMap with this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit cm -n kube-system kubeadm-config
</span></span></code></pre></div><p>The configuration is located under the <code>data.ClusterConfiguration</code> key.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>ClusterConfiguration</code> includes a variety of options that affect the configuration of individual
components such as kube-apiserver, kube-scheduler, kube-controller-manager, CoreDNS, etcd and kube-proxy.
Changes to the configuration must be reflected on node components manually.</div><h4 id="reflecting-clusterconfiguration-changes-on-control-plane-nodes">Reflecting <code>ClusterConfiguration</code> changes on control plane nodes</h4><p>kubeadm manages the control plane components as static Pod manifests located in
the directory <code>/etc/kubernetes/manifests</code>.
Any changes to the <code>ClusterConfiguration</code> under the <code>apiServer</code>, <code>controllerManager</code>, <code>scheduler</code> or <code>etcd</code>
keys must be reflected in the associated files in the manifests directory on a control plane node.</p><p>Such changes may include:</p><ul><li><code>extraArgs</code> - requires updating the list of flags passed to a component container</li><li><code>extraVolumes</code> - requires updating the volume mounts for a component container</li><li><code>*SANs</code> - requires writing new certificates with updated Subject Alternative Names</li></ul><p>Before proceeding with these changes, make sure you have backed up the directory <code>/etc/kubernetes/</code>.</p><p>To write new certificates you can use:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubeadm init phase certs &lt;component-name&gt; --config &lt;config-file&gt;
</span></span></code></pre></div><p>To write new manifest files in <code>/etc/kubernetes/manifests</code> you can use:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># For Kubernetes control plane components</span>
</span></span><span style="display:flex"><span>kubeadm init phase control-plane &lt;component-name&gt; --config &lt;config-file&gt;
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># For local etcd</span>
</span></span><span style="display:flex"><span>kubeadm init phase etcd <span style="color:#a2f">local</span> --config &lt;config-file&gt;
</span></span></code></pre></div><p>The <code>&lt;config-file&gt;</code> contents must match the updated <code>ClusterConfiguration</code>.
The <code>&lt;component-name&gt;</code> value must be a name of a Kubernetes control plane component (<code>apiserver</code>, <code>controller-manager</code> or <code>scheduler</code>).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Updating a file in <code>/etc/kubernetes/manifests</code> will tell the kubelet to restart the static Pod for the corresponding component.
Try doing these changes one node at a time to leave the cluster without downtime.</div><h3 id="applying-kubelet-configuration-changes">Applying kubelet configuration changes</h3><h4 id="updating-the-kubeletconfiguration">Updating the <code>KubeletConfiguration</code></h4><p>During cluster creation and upgrade, kubeadm writes its
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
in a ConfigMap called <code>kubelet-config</code> in the <code>kube-system</code> namespace.</p><p>You can edit the ConfigMap with this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit cm -n kube-system kubelet-config
</span></span></code></pre></div><p>The configuration is located under the <code>data.kubelet</code> key.</p><h4 id="reflecting-the-kubelet-changes">Reflecting the kubelet changes</h4><p>To reflect the change on kubeadm nodes you must do the following:</p><ul><li>Log in to a kubeadm node</li><li>Run <code>kubeadm upgrade node phase kubelet-config</code> to download the latest <code>kubelet-config</code>
ConfigMap contents into the local file <code>/var/lib/kubelet/config.yaml</code></li><li>Edit the file <code>/var/lib/kubelet/kubeadm-flags.env</code> to apply additional configuration with
flags</li><li>Restart the kubelet service with <code>systemctl restart kubelet</code></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Do these changes one node at a time to allow workloads to be rescheduled properly.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>During <code>kubeadm upgrade</code>, kubeadm downloads the <code>KubeletConfiguration</code> from the
<code>kubelet-config</code> ConfigMap and overwrite the contents of <code>/var/lib/kubelet/config.yaml</code>.
This means that node local configuration must be applied either by flags in
<code>/var/lib/kubelet/kubeadm-flags.env</code> or by manually updating the contents of
<code>/var/lib/kubelet/config.yaml</code> after <code>kubeadm upgrade</code>, and then restarting the kubelet.</div><h3 id="applying-kube-proxy-configuration-changes">Applying kube-proxy configuration changes</h3><h4 id="updating-the-kubeproxyconfiguration">Updating the <code>KubeProxyConfiguration</code></h4><p>During cluster creation and upgrade, kubeadm writes its
<a href="/docs/reference/config-api/kube-proxy-config.v1alpha1/"><code>KubeProxyConfiguration</code></a>
in a ConfigMap in the <code>kube-system</code> namespace called <code>kube-proxy</code>.</p><p>This ConfigMap is used by the <code>kube-proxy</code> DaemonSet in the <code>kube-system</code> namespace.</p><p>To change a particular option in the <code>KubeProxyConfiguration</code>, you can edit the ConfigMap with this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit cm -n kube-system kube-proxy
</span></span></code></pre></div><p>The configuration is located under the <code>data.config.conf</code> key.</p><h4 id="reflecting-the-kube-proxy-changes">Reflecting the kube-proxy changes</h4><p>Once the <code>kube-proxy</code> ConfigMap is updated, you can restart all kube-proxy Pods:</p><p>Delete the Pods with:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete po -n kube-system -l k8s-app<span style="color:#666">=</span>kube-proxy
</span></span></code></pre></div><p>New Pods that use the updated ConfigMap will be created.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Because kubeadm deploys kube-proxy as a DaemonSet, node specific configuration is unsupported.</div><h3 id="applying-coredns-configuration-changes">Applying CoreDNS configuration changes</h3><h4 id="updating-the-coredns-deployment-and-service">Updating the CoreDNS Deployment and Service</h4><p>kubeadm deploys CoreDNS as a Deployment called <code>coredns</code> and with a Service <code>kube-dns</code>,
both in the <code>kube-system</code> namespace.</p><p>To update any of the CoreDNS settings, you can edit the Deployment and
Service objects:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit deployment -n kube-system coredns
</span></span><span style="display:flex"><span>kubectl edit service -n kube-system kube-dns
</span></span></code></pre></div><h4 id="reflecting-the-coredns-changes">Reflecting the CoreDNS changes</h4><p>Once the CoreDNS changes are applied you can restart the CoreDNS deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout restart deployment -n kube-system coredns
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>kubeadm does not allow CoreDNS configuration during cluster creation and upgrade.
This means that if you execute <code>kubeadm upgrade apply</code>, your changes to the CoreDNS
objects will be lost and must be reapplied.</div><h2 id="persisting-the-reconfiguration">Persisting the reconfiguration</h2><p>During the execution of <code>kubeadm upgrade</code> on a managed node, kubeadm might overwrite configuration
that was applied after the cluster was created (reconfiguration).</p><h3 id="persisting-node-object-reconfiguration">Persisting Node object reconfiguration</h3><p>kubeadm writes Labels, Taints, CRI socket and other information on the Node object for a particular
Kubernetes node. To change any of the contents of this Node object you can use:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit no &lt;node-name&gt;
</span></span></code></pre></div><p>During <code>kubeadm upgrade</code> the contents of such a Node might get overwritten.
If you would like to persist your modifications to the Node object after upgrade,
you can prepare a <a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">kubectl patch</a>
and apply it to the Node object:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch no &lt;node-name&gt; --patch-file &lt;patch-file&gt;
</span></span></code></pre></div><h4 id="persisting-control-plane-component-reconfiguration">Persisting control plane component reconfiguration</h4><p>The main source of control plane configuration is the <code>ClusterConfiguration</code>
object stored in the cluster. To extend the static Pod manifests configuration,
<a href="/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches">patches</a> can be used.</p><p>These patch files must remain as files on the control plane nodes to ensure that
they can be used by the <code>kubeadm upgrade ... --patches &lt;directory&gt;</code>.</p><p>If reconfiguration is done to the <code>ClusterConfiguration</code> and static Pod manifests on disk,
the set of node specific patches must be updated accordingly.</p><h4 id="persisting-kubelet-reconfiguration">Persisting kubelet reconfiguration</h4><p>Any changes to the <code>KubeletConfiguration</code> stored in <code>/var/lib/kubelet/config.yaml</code> will be overwritten on
<code>kubeadm upgrade</code> by downloading the contents of the cluster wide <code>kubelet-config</code> ConfigMap.
To persist kubelet node specific configuration either the file <code>/var/lib/kubelet/config.yaml</code>
has to be updated manually post-upgrade or the file <code>/var/lib/kubelet/kubeadm-flags.env</code> can include flags.
The kubelet flags override the associated <code>KubeletConfiguration</code> options, but note that
some of the flags are deprecated.</p><p>A kubelet restart will be required after changing <code>/var/lib/kubelet/config.yaml</code> or
<code>/var/lib/kubelet/kubeadm-flags.env</code>.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a></li><li><a href="/docs/setup/production-environment/tools/kubeadm/control-plane-flags/">Customizing components with the kubeadm API</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">Certificate management with kubeadm</a></li><li><a href="/docs/reference/setup-tools/kubeadm/">Find more about kubeadm set-up</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Guaranteed Scheduling For Critical Add-On Pods</h1><p>Kubernetes core components such as the API server, scheduler, and controller-manager run on a control plane node. However, add-ons must run on a regular cluster node.
Some of these add-ons are critical to a fully functional cluster, such as metrics-server, DNS, and UI.
A cluster may stop working properly if a critical add-on is evicted (either manually or as a side effect of another operation like upgrade)
and becomes pending (for example when the cluster is highly utilized and either there are other pending pods that schedule into the space
vacated by the evicted critical add-on pod or the amount of resources available on the node changed for some other reason).</p><p>Note that marking a pod as critical is not meant to prevent evictions entirely; it only prevents the pod from becoming permanently unavailable.
A static pod marked as critical can't be evicted. However, non-static pods marked as critical are always rescheduled.</p><h3 id="marking-pod-as-critical">Marking pod as critical</h3><p>To mark a Pod as critical, set priorityClassName for that Pod to <code>system-cluster-critical</code> or <code>system-node-critical</code>. <code>system-node-critical</code> is the highest available priority, even higher than <code>system-cluster-critical</code>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Upgrading kubeadm clusters</h1><p>This page explains how to upgrade a Kubernetes cluster created with kubeadm from version
1.33.x to version 1.34.x, and from version
1.34.x to 1.34.y (where <code>y &gt; x</code>). Skipping MINOR versions
when upgrading is unsupported. For more details, please visit <a href="/releases/version-skew-policy/">Version Skew Policy</a>.</p><p>To see information about upgrading clusters created using older versions of kubeadm,
please refer to following pages instead:</p><ul><li><a href="https://v1-33.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.32 to 1.33</a></li><li><a href="https://v1-32.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.31 to 1.32</a></li><li><a href="https://v1-31.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.30 to 1.31</a></li><li><a href="https://v1-30.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading a kubeadm cluster from 1.29 to 1.30</a></li></ul><p>The Kubernetes project recommends upgrading to the latest patch releases promptly, and
to ensure that you are running a supported minor release of Kubernetes.
Following this recommendation helps you to stay secure.</p><p>The upgrade workflow at high level is the following:</p><ol><li>Upgrade a primary control plane node.</li><li>Upgrade additional control plane nodes.</li><li>Upgrade worker nodes.</li></ol><h2 id="before-you-begin">Before you begin</h2><ul><li>Make sure you read the <a href="https://git.k8s.io/kubernetes/CHANGELOG">release notes</a> carefully.</li><li>The cluster should use a static control plane and etcd pods or external etcd.</li><li>Make sure to back up any important components, such as app-level state stored in a database.
<code>kubeadm upgrade</code> does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.</li><li><a href="https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux">Swap must be disabled</a>.</li></ul><h3 id="additional-information">Additional information</h3><ul><li>The instructions below outline when to drain each node during the upgrade process.
If you are performing a <strong>minor</strong> version upgrade for any kubelet, you <strong>must</strong>
first drain the node (or nodes) that you are upgrading. In the case of control plane nodes,
they could be running CoreDNS Pods or other critical workloads. For more information see
<a href="/docs/tasks/administer-cluster/safely-drain-node/">Draining nodes</a>.</li><li>The Kubernetes project recommends that you match your kubelet and kubeadm versions.
You can instead use a version of kubelet that is older than kubeadm, provided it is within the
range of supported versions.
For more details, please visit <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#kubeadm-s-skew-against-the-kubelet">kubeadm's skew against the kubelet</a>.</li><li>All containers are restarted after upgrade, because the container spec hash value is changed.</li><li>To verify that the kubelet service has successfully restarted after the kubelet has been upgraded,
you can execute <code>systemctl status kubelet</code> or view the service logs with <code>journalctl -xeu kubelet</code>.</li><li><code>kubeadm upgrade</code> supports <code>--config</code> with a
<a href="/docs/reference/config-api/kubeadm-config.v1beta4/"><code>UpgradeConfiguration</code> API type</a> which can
be used to configure the upgrade process.</li><li><code>kubeadm upgrade</code> does not support reconfiguration of an existing cluster. Follow the steps in
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/">Reconfiguring a kubeadm cluster</a> instead.</li></ul><h3 id="considerations-when-upgrading-etcd">Considerations when upgrading etcd</h3><p>Because the <code>kube-apiserver</code> static pod is running at all times (even if you
have drained the node), when you perform a kubeadm upgrade which includes an
etcd upgrade, in-flight requests to the server will stall while the new etcd
static pod is restarting. As a workaround, it is possible to actively stop the
<code>kube-apiserver</code> process a few seconds before starting the <code>kubeadm upgrade apply</code> command. This permits to complete in-flight requests and close existing
connections, and minimizes the consequence of the etcd downtime. This can be
done as follows on control plane nodes:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>killall -s SIGTERM kube-apiserver <span style="color:#080;font-style:italic"># trigger a graceful kube-apiserver shutdown</span>
</span></span><span style="display:flex"><span>sleep <span style="color:#666">20</span> <span style="color:#080;font-style:italic"># wait a little bit to permit completing in-flight requests</span>
</span></span><span style="display:flex"><span>kubeadm upgrade ... <span style="color:#080;font-style:italic"># execute a kubeadm upgrade command</span>
</span></span></code></pre></div><h2 id="changing-the-package-repository">Changing the package repository</h2><p>If you're using the community-owned package repositories (<code>pkgs.k8s.io</code>), you need to
enable the package repository for the desired Kubernetes minor release. This is explained in
<a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing the Kubernetes package repository</a>
document.</p><div class="alert alert-secondary callout note" role="alert"><strong>Note:</strong> The legacy package repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) have been
<a href="/blog/2023/08/31/legacy-package-repository-deprecation/">deprecated and frozen starting from September 13, 2023</a>.
<strong>Using the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">new package repositories hosted at <code>pkgs.k8s.io</code></a>
is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.</strong>
The deprecated legacy repositories, and their contents, might be removed at any time in the future and without
a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.</div><h2 id="determine-which-version-to-upgrade-to">Determine which version to upgrade to</h2><p>Find the latest patch release for Kubernetes 1.34 using the OS package manager:</p><ul class="nav nav-tabs" id="k8s-install-versions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-versions-0" role="tab" aria-controls="k8s-install-versions-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-versions-1" role="tab" aria-controls="k8s-install-versions-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-versions"><div id="k8s-install-versions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-versions-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Find the latest 1.34 version in the list.</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># It should look like 1.34.x-*, where x is the latest patch.</span>
</span></span><span style="display:flex"><span>sudo apt update
</span></span><span style="display:flex"><span>sudo apt-cache madison kubeadm
</span></span></code></pre></div></p></div><div id="k8s-install-versions-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-versions-1"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Find the latest 1.34 version in the list.</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># It should look like 1.34.x-*, where x is the latest patch.</span>
</span></span><span style="display:flex"><span>sudo yum list --showduplicates kubeadm --disableexcludes<span style="color:#666">=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Find the latest 1.34 version in the list.</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># It should look like 1.34.x-*, where x is the latest patch.</span>
</span></span><span style="display:flex"><span>sudo yum list --showduplicates kubeadm --setopt<span style="color:#666">=</span><span style="color:#b8860b">disable_excludes</span><span style="color:#666">=</span>kubernetes
</span></span></code></pre></div></p></div></div><p>If you don't see the version you expect to upgrade to, <a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/#verifying-if-the-kubernetes-package-repositories-are-used">verify if the Kubernetes package repositories are used.</a></p><h2 id="upgrading-control-plane-nodes">Upgrading control plane nodes</h2><p>The upgrade procedure on control plane nodes should be executed one node at a time.
Pick a control plane node that you wish to upgrade first. It must have the <code>/etc/kubernetes/admin.conf</code> file.</p><h3 id="call-kubeadm-upgrade">Call "kubeadm upgrade"</h3><p><strong>For the first control plane node</strong></p><ol><li><p>Upgrade kubeadm:</p><ul class="nav nav-tabs" id="k8s-install-kubeadm-first-cp" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubeadm-first-cp-0" role="tab" aria-controls="k8s-install-kubeadm-first-cp-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubeadm-first-cp-1" role="tab" aria-controls="k8s-install-kubeadm-first-cp-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-kubeadm-first-cp"><div id="k8s-install-kubeadm-first-cp-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubeadm-first-cp-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo apt-mark unhold kubeadm <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-get update <span style="color:#666">&amp;&amp;</span> sudo apt-get install -y <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span><span style="color:#b44">'1.34.x-*'</span> <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-mark hold kubeadm
</span></span></code></pre></div></p></div><div id="k8s-install-kubeadm-first-cp-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubeadm-first-cp-1"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubeadm-<span style="color:#b44">'1.34.x-*'</span> --disableexcludes<span style="color:#666">=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubeadm-<span style="color:#b44">'1.34.x-*'</span> --setopt<span style="color:#666">=</span><span style="color:#b8860b">disable_excludes</span><span style="color:#666">=</span>kubernetes
</span></span></code></pre></div></p></div></div></li><li><p>Verify that the download works and has the expected version:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubeadm version
</span></span></code></pre></div></li><li><p>Verify the upgrade plan:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm upgrade plan
</span></span></code></pre></div><p>This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
It also shows a table with the component config version states.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>kubeadm upgrade</code> also automatically renews the certificates that it manages on this node.
To opt-out of certificate renewal the flag <code>--certificate-renewal=false</code> can be used.
For more information see the <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">certificate management guide</a>.</div></li><li><p>Choose a version to upgrade to, and run the appropriate command. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x with the patch version you picked for this upgrade</span>
</span></span><span style="display:flex"><span>sudo kubeadm upgrade apply v1.34.x
</span></span></code></pre></div><p>Once the command finishes you should see:</p><pre tabindex="0"><code>[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.34.x". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>For versions earlier than v1.28, kubeadm defaulted to a mode that upgrades the addons (including CoreDNS and kube-proxy)
immediately during <code>kubeadm upgrade apply</code>, regardless of whether there are other control plane instances that have not
been upgraded. This may cause compatibility problems. Since v1.28, kubeadm defaults to a mode that checks whether all
the control plane instances have been upgraded before starting to upgrade the addons. You must perform control plane
instances upgrade sequentially or at least ensure that the last control plane instance upgrade is not started until all
the other control plane instances have been upgraded completely, and the addons upgrade will be performed after the last
control plane instance is upgraded.</div></li><li><p>Manually upgrade your CNI provider plugin.</p><p>Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow.
Check the <a href="/docs/concepts/cluster-administration/addons/">addons</a> page to
find your CNI provider and see whether additional upgrade steps are required.</p><p>This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.</p></li></ol><p><strong>For the other control plane nodes</strong></p><p>Same as the first control plane node but use:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm upgrade node
</span></span></code></pre></div><p>instead of:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm upgrade apply
</span></span></code></pre></div><p>Also calling <code>kubeadm upgrade plan</code> and upgrading the CNI provider plugin is no longer needed.</p><h3 id="drain-the-node">Drain the node</h3><p>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
</span></span><span style="display:flex"><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><h3 id="upgrade-kubelet-and-kubectl">Upgrade kubelet and kubectl</h3><ol><li><p>Upgrade the kubelet and kubectl:</p><ul class="nav nav-tabs" id="k8s-install-kubelet" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubelet-0" role="tab" aria-controls="k8s-install-kubelet-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubelet-1" role="tab" aria-controls="k8s-install-kubelet-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-kubelet"><div id="k8s-install-kubelet-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubelet-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo apt-mark unhold kubelet kubectl <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-get update <span style="color:#666">&amp;&amp;</span> sudo apt-get install -y <span style="color:#b8860b">kubelet</span><span style="color:#666">=</span><span style="color:#b44">'1.34.x-*'</span> <span style="color:#b8860b">kubectl</span><span style="color:#666">=</span><span style="color:#b44">'1.34.x-*'</span> <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-mark hold kubelet kubectl
</span></span></code></pre></div></p></div><div id="k8s-install-kubelet-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubelet-1"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubelet-<span style="color:#b44">'1.34.x-*'</span> kubectl-<span style="color:#b44">'1.34.x-*'</span> --disableexcludes<span style="color:#666">=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubelet-<span style="color:#b44">'1.34.x-*'</span> kubectl-<span style="color:#b44">'1.34.x-*'</span> --setopt<span style="color:#666">=</span><span style="color:#b8860b">disable_excludes</span><span style="color:#666">=</span>kubernetes
</span></span></code></pre></div></p></div></div></li><li><p>Restart the kubelet:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo systemctl daemon-reload
</span></span><span style="display:flex"><span>sudo systemctl restart kubelet
</span></span></code></pre></div></li></ol><h3 id="uncordon-the-node">Uncordon the node</h3><p>Bring the node back online by marking it schedulable:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace &lt;node-to-uncordon&gt; with the name of your node</span>
</span></span><span style="display:flex"><span>kubectl uncordon &lt;node-to-uncordon&gt;
</span></span></code></pre></div><h2 id="upgrade-worker-nodes">Upgrade worker nodes</h2><p>The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time,
without compromising the minimum required capacity for running your workloads.</p><p>The following pages show how to upgrade Linux and Windows worker nodes:</p><ul><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrade Linux nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrade Windows nodes</a></li></ul><h2 id="verify-the-status-of-the-cluster">Verify the status of the cluster</h2><p>After the kubelet is upgraded on all nodes verify that all nodes are available again by running
the following command from anywhere kubectl can access the cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get nodes
</span></span></code></pre></div><p>The <code>STATUS</code> column should show <code>Ready</code> for all your nodes, and the version number should be updated.</p><h2 id="recovering-from-a-failure-state">Recovering from a failure state</h2><p>If <code>kubeadm upgrade</code> fails and does not roll back, for example because of an unexpected shutdown during execution, you can run <code>kubeadm upgrade</code> again.
This command is idempotent and eventually makes sure that the actual state is the desired state you declare.</p><p>To recover from a bad state, you can also run <code>sudo kubeadm upgrade apply --force</code> without changing the version that your cluster is running.</p><p>During upgrade kubeadm writes the following backup folders under <code>/etc/kubernetes/tmp</code>:</p><ul><li><code>kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;</code></li><li><code>kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code></li></ul><p><code>kubeadm-backup-etcd</code> contains a backup of the local etcd member data for this control plane Node.
In case of an etcd upgrade failure and if the automatic rollback does not work, the contents of this folder
can be manually restored in <code>/var/lib/etcd</code>. In case external etcd is used this backup folder will be empty.</p><p><code>kubeadm-backup-manifests</code> contains a backup of the static Pod manifest files for this control plane Node.
In case of a upgrade failure and if the automatic rollback does not work, the contents of this folder can be
manually restored in <code>/etc/kubernetes/manifests</code>. If for some reason there is no difference between a pre-upgrade
and post-upgrade manifest file for a certain component, a backup file for it will not be written.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>After the cluster upgrade using kubeadm, the backup directory <code>/etc/kubernetes/tmp</code> will remain and
these backup files will need to be cleared manually.</div><h2 id="how-it-works">How it works</h2><p><code>kubeadm upgrade apply</code> does the following:</p><ul><li>Checks that your cluster is in an upgradeable state:<ul><li>The API server is reachable</li><li>All nodes are in the <code>Ready</code> state</li><li>The control plane is healthy</li></ul></li><li>Enforces the version skew policies.</li><li>Makes sure the control plane images are available or available to pull to the machine.</li><li>Generates replacements and/or uses user supplied overwrites if component configs require version upgrades.</li><li>Upgrades the control plane components or rollbacks if any of them fails to come up.</li><li>Applies the new <code>CoreDNS</code> and <code>kube-proxy</code> manifests and makes sure that all necessary RBAC rules are created.</li><li>Creates new certificate and key files of the API server and backs up old files if they're about to expire in 180 days.</li></ul><p><code>kubeadm upgrade node</code> does the following on additional control plane nodes:</p><ul><li>Fetches the kubeadm <code>ClusterConfiguration</code> from the cluster.</li><li>Optionally backups the kube-apiserver certificate.</li><li>Upgrades the static Pod manifests for the control plane components.</li><li>Upgrades the kubelet configuration for this node.</li></ul><p><code>kubeadm upgrade node</code> does the following on worker nodes:</p><ul><li>Fetches the kubeadm <code>ClusterConfiguration</code> from the cluster.</li><li>Upgrades the kubelet configuration for this node.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Weave Net for NetworkPolicy</h1><p>This page shows how to use Weave Net for NetworkPolicy.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster. Follow the
<a href="/docs/reference/setup-tools/kubeadm/">kubeadm getting started guide</a> to bootstrap one.</p><h2 id="install-the-weave-net-addon">Install the Weave Net addon</h2><p>Follow the <a href="https://github.com/weaveworks/weave/blob/master/site/kubernetes/kube-addon.md#-installation">Integrating Kubernetes via the Addon</a> guide.</p><p>The Weave Net addon for Kubernetes comes with a
<a href="https://github.com/weaveworks/weave/blob/master/site/kubernetes/kube-addon.md#network-policy">Network Policy Controller</a>
that automatically monitors Kubernetes for any NetworkPolicy annotations on all
namespaces and configures <code>iptables</code> rules to allow or block traffic as directed by the policies.</p><h2 id="test-the-installation">Test the installation</h2><p>Verify that the weave works.</p><p>Enter the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods -n kube-system -o wide
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
weave-net-1t1qg                         2/2       Running   0          9d        192.168.2.10    worknode3
weave-net-231d7                         2/2       Running   1          7d        10.2.0.17       worknodegpu
weave-net-7nmwt                         2/2       Running   3          9d        192.168.2.131   masternode
weave-net-pmw8w                         2/2       Running   0          9d        192.168.2.216   worknode2
</code></pre><p>Each Node has a weave Pod, and all Pods are <code>Running</code> and <code>2/2 READY</code>. (<code>2/2</code> means that each Pod has <code>weave</code> and <code>weave-npc</code>.)</p><h2 id="what-s-next">What's next</h2><p>Once you have installed the Weave Net addon, you can follow the
<a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
to try out Kubernetes NetworkPolicy. If you have any question, contact us at
<a href="https://github.com/weaveworks/weave#getting-help">#weave-community on Slack or Weave User Group</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Migrating from dockershim</h1><p>This section presents information you need to know when migrating from
dockershim to other container runtimes.</p><p>Since the announcement of <a href="/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">dockershim deprecation</a>
in Kubernetes 1.20, there were questions on how this will affect various workloads and Kubernetes
installations. Our <a href="/blog/2022/02/17/dockershim-faq/">Dockershim Removal FAQ</a> is there to help you
to understand the problem better.</p><p>Dockershim was removed from Kubernetes with the release of v1.24.
If you use Docker Engine via dockershim as your container runtime and wish to upgrade to v1.24,
it is recommended that you either migrate to another runtime or find an alternative means to obtain Docker Engine support.
Check out the <a href="/docs/setup/production-environment/container-runtimes/">container runtimes</a>
section to know your options.</p><p>The version of Kubernetes with dockershim (1.23) is out of support and the v1.24
will run out of support <a href="/releases/#release-v1-24">soon</a>. Make sure to
<a href="https://github.com/kubernetes/kubernetes/issues">report issues</a> you encountered
with the migration so the issues can be fixed in a timely manner and your cluster would be
ready for dockershim removal. After v1.24 running out of support, you will need
to contact your Kubernetes provider for support or upgrade multiple versions at a time
if there are critical issues affecting your cluster.</p><p>Your cluster might have more than one kind of node, although this is not a common
configuration.</p><p>These tasks will help you to migrate:</p><ul><li><a href="/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether Dockershim removal affects you</a></li><li><a href="/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">Migrating telemetry and security agents from dockershim</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Check out <a href="/docs/setup/production-environment/container-runtimes/">container runtimes</a>
to understand your options for an alternative.</li><li>If you find a defect or other technical concern relating to migrating away from dockershim,
you can <a href="https://github.com/kubernetes/kubernetes/issues/new/choose">report an issue</a>
to the Kubernetes project.</li></ul><div class="section-index"/></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use Antrea for NetworkPolicy</h1><p>This page shows how to install and use Antrea CNI plugin on Kubernetes.
For background on Project Antrea, read the <a href="https://antrea.io/docs/">Introduction to Antrea</a>.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster. Follow the
<a href="/docs/reference/setup-tools/kubeadm/">kubeadm getting started guide</a> to bootstrap one.</p><h2 id="deploying-antrea-with-kubeadm">Deploying Antrea with kubeadm</h2><p>Follow <a href="https://github.com/vmware-tanzu/antrea/blob/main/docs/getting-started.md">Getting Started</a> guide to deploy Antrea for kubeadm.</p><h2 id="what-s-next">What's next</h2><p>Once your cluster is running, you can follow the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Debugging DNS Resolution</h1><p>This page provides hints on diagnosing DNS problems.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><br/>Your cluster must be configured to use the CoreDNS
<a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/cluster-administration/addons/" target="_blank" aria-label="addon">addon</a> or its precursor,
kube-dns.</p><p>Your Kubernetes server must be at or later than version v1.6.</p><p>To check the version, enter <code>kubectl version</code>.</p><h3 id="create-a-simple-pod-to-use-as-a-test-environment">Create a simple Pod to use as a test environment</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/dns/dnsutils.yaml" download="admin/dns/dnsutils.yaml"><code>admin/dns/dnsutils.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-dns-dnsutils-yaml&quot;)" title="Copy admin/dns/dnsutils.yaml to clipboard"/></div><div class="includecode" id="admin-dns-dnsutils-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/e2e-test-images/agnhost:2.39<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This example creates a pod in the <code>default</code> namespace. DNS name resolution for
services depends on the namespace of the pod. For more information, review
<a href="/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">DNS for Services and Pods</a>.</div><p>Use that manifest to create a Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
</span></span></code></pre></div><pre tabindex="0"><code>pod/dnsutils created
</code></pre><p>…and verify its status:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods dnsutils
</span></span></code></pre></div><pre tabindex="0"><code>NAME       READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          &lt;some-time&gt;
</code></pre><p>Once that Pod is running, you can exec <code>nslookup</code> in that environment.
If you see something like the following, DNS is working correctly.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
</code></pre><p>If the <code>nslookup</code> command fails, check the following:</p><h3 id="check-the-local-dns-configuration-first">Check the local DNS configuration first</h3><p>Take a look inside the resolv.conf file.
(See <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a> and
<a href="#known-issues">Known issues</a> below for more information)</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -ti dnsutils -- cat /etc/resolv.conf
</span></span></code></pre></div><p>Verify that the search path and name server are set up like the following
(note that search path may vary for different cloud providers):</p><pre tabindex="0"><code>search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
</code></pre><p>Errors such as the following indicate a problem with the CoreDNS (or kube-dns)
add-on or with associated Services:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
</code></pre><p>or</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
</code></pre><h3 id="check-if-the-dns-pod-is-running">Check if the DNS pod is running</h3><p>Use the <code>kubectl get pods</code> command to verify that the DNS pod is running.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>kube-dns
</span></span></code></pre></div><pre tabindex="0"><code>NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The value for label <code>k8s-app</code> is <code>kube-dns</code> for both CoreDNS and kube-dns deployments.</div><p>If you see that no CoreDNS Pod is running or that the Pod has failed/completed,
the DNS add-on may not be deployed by default in your current environment and you
will have to deploy it manually.</p><h3 id="check-for-errors-in-the-dns-pod">Check for errors in the DNS pod</h3><p>Use the <code>kubectl logs</code> command to see logs for the DNS containers.</p><p>For CoreDNS:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>kube-dns
</span></span></code></pre></div><p>Here is an example of a healthy CoreDNS log:</p><pre tabindex="0"><code>.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
</code></pre><p>See if there are any suspicious or unexpected messages in the logs.</p><h3 id="is-dns-service-up">Is DNS service up?</h3><p>Verify that the DNS service is up by using the <code>kubectl get service</code> command.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get svc --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><pre tabindex="0"><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h
...
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The service name is <code>kube-dns</code> for both CoreDNS and kube-dns deployments.</div><p>If you have created the Service or in the case it should be created by default
but it does not appear, see
<a href="/docs/tasks/debug/debug-application/debug-service/">debugging Services</a> for
more information.</p><h3 id="are-dns-endpoints-exposed">Are DNS endpoints exposed?</h3><p>You can verify that DNS endpoints are exposed by using the <code>kubectl get endpointslice</code>
command.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get endpointslice -l k8s.io/service-name<span style="color:#666">=</span>kube-dns --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><pre tabindex="0"><code>NAME             ADDRESSTYPE   PORTS   ENDPOINTS                  AGE
kube-dns-zxoja   IPv4          53      10.180.3.17,10.180.3.17    1h
</code></pre><p>If you do not see the endpoints, see the endpoints section in the
<a href="/docs/tasks/debug/debug-application/debug-service/">debugging Services</a> documentation.</p><h3 id="are-dns-queries-being-received-processed">Are DNS queries being received/processed?</h3><p>You can verify if queries are being received by CoreDNS by adding the <code>log</code> plugin to the CoreDNS configuration (aka Corefile).
The CoreDNS Corefile is held in a <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/configmap/" target="_blank" aria-label="ConfigMap">ConfigMap</a> named <code>coredns</code>. To edit it, use the command:</p><pre tabindex="0"><code>kubectl -n kube-system edit configmap coredns
</code></pre><p>Then add <code>log</code> in the Corefile section per the example below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    .:53 {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        log
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        errors
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        health
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">          pods insecure
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">          upstream
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">          fallthrough in-addr.arpa ip6.arpa
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        }
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        prometheus :9153
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        forward . /etc/resolv.conf
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        cache 30
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        loop
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        reload
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        loadbalance
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></span></span></code></pre></div><p>After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.</p><p>Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.</p><p>Here is an example of a query in the log:</p><pre tabindex="0"><code>.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 "A IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd,ra 106 0.000066649s
</code></pre><h3 id="does-coredns-have-sufficient-permissions">Does CoreDNS have sufficient permissions?</h3><p>CoreDNS must be able to list <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="service">service</a> and <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/endpoint-slices/" target="_blank" aria-label="endpointslice">endpointslice</a> related resources to properly resolve service names.</p><p>Sample error message:</p><pre tabindex="0"><code>2022-03-18T07:12:15.699431183Z [INFO] 10.96.144.227:52299 - 3686 "A IN serverproxy.contoso.net.cluster.local. udp 52 false 512" SERVFAIL qr,aa,rd 145 0.000091221s
</code></pre><p>First, get the current ClusterRole of <code>system:coredns</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe clusterrole system:coredns -n kube-system
</span></span></code></pre></div><p>Expected output:</p><pre tabindex="0"><code>PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  endpoints                        []                 []              [list watch]
  namespaces                       []                 []              [list watch]
  pods                             []                 []              [list watch]
  services                         []                 []              [list watch]
  endpointslices.discovery.k8s.io  []                 []              [list watch]
</code></pre><p>If any permissions are missing, edit the ClusterRole to add them:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit clusterrole system:coredns -n kube-system
</span></span></code></pre></div><p>Example insertion of EndpointSlices permissions:</p><pre tabindex="0"><code>...
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
...
</code></pre><h3 id="are-you-in-the-right-namespace-for-the-service">Are you in the right namespace for the service?</h3><p>DNS queries that don't specify a namespace are limited to the pod's
namespace.</p><p>If the namespace of the pod and service differ, the DNS query must include
the namespace of the service.</p><p>This query is limited to the pod's namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;
</span></span></code></pre></div><p>This query specifies the namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;.&lt;namespace&gt;
</span></span></code></pre></div><p>To learn more about name resolution, see
<a href="/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">DNS for Services and Pods</a>.</p><h2 id="known-issues">Known issues</h2><p>Some Linux distributions (e.g. Ubuntu) use a local DNS resolver by default (systemd-resolved).
Systemd-resolved moves and replaces <code>/etc/resolv.conf</code> with a stub file that can cause a fatal forwarding
loop when resolving names in upstream servers. This can be fixed manually by using kubelet's <code>--resolv-conf</code> flag
to point to the correct <code>resolv.conf</code> (With <code>systemd-resolved</code>, this is <code>/run/systemd/resolve/resolv.conf</code>).
kubeadm automatically detects <code>systemd-resolved</code>, and adjusts the kubelet flags accordingly.</p><p>Kubernetes installs do not configure the nodes' <code>resolv.conf</code> files to use the
cluster DNS by default, because that process is inherently distribution-specific.
This should probably be implemented eventually.</p><p>Linux's libc (a.k.a. glibc) has a limit for the DNS <code>nameserver</code> records to 3 by
default and Kubernetes needs to consume 1 <code>nameserver</code> record. This means that
if a local installation already uses 3 <code>nameserver</code>s, some of those entries will
be lost. To work around this limit, the node can run <code>dnsmasq</code>, which will
provide more <code>nameserver</code> entries. You can also use kubelet's <code>--resolv-conf</code>
flag.</p><p>If you are using Alpine version 3.17 or earlier as your base image, DNS may not
work properly due to a design issue with Alpine.
Until musl version 1.24 didn't include TCP fallback to the DNS stub resolver meaning any DNS call above 512 bytes would fail.
Please upgrade your images to Alpine version 3.18 or above.</p><h2 id="what-s-next">What's next</h2><ul><li>See <a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscaling the DNS Service in a Cluster</a>.</li><li>Read <a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Customizing DNS Service</h1><p>This page explains how to configure your DNS
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod(s)">Pod(s)</a> and customize the
DNS resolution process in your cluster.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>Your cluster must be running the CoreDNS add-on.</p><p>Your Kubernetes server must be at or later than version v1.12.</p><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="introduction">Introduction</h2><p>DNS is a built-in Kubernetes service launched automatically
using the <em>addon manager</em> <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/addon-manager/README.md">cluster add-on</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The CoreDNS Service is named <code>kube-dns</code> in the <code>metadata.name</code> field.<br/>The intent is to ensure greater interoperability with workloads that relied on
the legacy <code>kube-dns</code> Service name to resolve addresses internal to the cluster.
Using a Service named <code>kube-dns</code> abstracts away the implementation detail of
which DNS provider is running behind that common name.</div><p>If you are running CoreDNS as a Deployment, it will typically be exposed as
a Kubernetes Service with a static IP address.
The kubelet passes DNS resolver information to each container with the
<code>--cluster-dns=&lt;dns-service-ip&gt;</code> flag.</p><p>DNS names also need domains. You configure the local domain in the kubelet
with the flag <code>--cluster-domain=&lt;default-local-domain&gt;</code>.</p><p>The DNS server supports forward lookups (A and AAAA records), port lookups (SRV records),
reverse IP address lookups (PTR records), and more. For more information, see
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a>.</p><p>If a Pod's <code>dnsPolicy</code> is set to <code>default</code>, it inherits the name resolution
configuration from the node that the Pod runs on. The Pod's DNS resolution
should behave the same as the node.
But see <a href="/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues">Known issues</a>.</p><p>If you don't want this, or if you want a different DNS config for pods, you can
use the kubelet's <code>--resolv-conf</code> flag. Set this flag to "" to prevent Pods from
inheriting DNS. Set it to a valid file path to specify a file other than
<code>/etc/resolv.conf</code> for DNS inheritance.</p><h2 id="coredns">CoreDNS</h2><p>CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS,
complying with the <a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS specifications</a>.</p><h3 id="coredns-configmap-options">CoreDNS ConfigMap options</h3><p>CoreDNS is a DNS server that is modular and pluggable, with plugins adding new functionalities.
The CoreDNS server can be configured by maintaining a <a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile</a>,
which is the CoreDNS configuration file. As a cluster administrator, you can modify the
<a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/configmap/" target="_blank" aria-label="ConfigMap">ConfigMap</a> for the CoreDNS Corefile to
change how DNS service discovery behaves for that cluster.</p><p>In Kubernetes, CoreDNS is installed with the following default Corefile configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    .:53 {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        errors
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        health {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            lameduck 5s
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        }
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        ready
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            pods insecure
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            fallthrough in-addr.arpa ip6.arpa
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            ttl 30
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        }
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        prometheus :9153
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        forward . /etc/resolv.conf
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        cache 30
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        loop
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        reload
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        loadbalance
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></span></span></code></pre></div><p>The Corefile configuration includes the following <a href="https://coredns.io/plugins/">plugins</a> of CoreDNS:</p><ul><li><a href="https://coredns.io/plugins/errors/">errors</a>: Errors are logged to stdout.</li><li><a href="https://coredns.io/plugins/health/">health</a>: Health of CoreDNS is reported to
<code>http://localhost:8080/health</code>. In this extended syntax <code>lameduck</code> will make the process
unhealthy then wait for 5 seconds before the process is shut down.</li><li><a href="https://coredns.io/plugins/ready/">ready</a>: An HTTP endpoint on port 8181 will return 200 OK,
when all plugins that are able to signal readiness have done so.</li><li><a href="https://coredns.io/plugins/kubernetes/">kubernetes</a>: CoreDNS will reply to DNS queries
based on IP of the Services and Pods. You can find <a href="https://coredns.io/plugins/kubernetes/">more details</a>
about this plugin on the CoreDNS website.<ul><li><code>ttl</code> allows you to set a custom TTL for responses. The default is 5 seconds.
The minimum TTL allowed is 0 seconds, and the maximum is capped at 3600 seconds.
Setting TTL to 0 will prevent records from being cached.</li><li>The <code>pods insecure</code> option is provided for backward compatibility with <code>kube-dns</code>.</li><li>You can use the <code>pods verified</code> option, which returns an A record only if there exists a pod
in the same namespace with a matching IP.</li><li>The <code>pods disabled</code> option can be used if you don't use pod records.</li></ul></li><li><a href="https://coredns.io/plugins/metrics/">prometheus</a>: Metrics of CoreDNS are available at
<code>http://localhost:9153/metrics</code> in the <a href="https://prometheus.io/">Prometheus</a> format
(also known as OpenMetrics).</li><li><a href="https://coredns.io/plugins/forward/">forward</a>: Any queries that are not within the Kubernetes
cluster domain are forwarded to predefined resolvers (/etc/resolv.conf).</li><li><a href="https://coredns.io/plugins/cache/">cache</a>: This enables a frontend cache.</li><li><a href="https://coredns.io/plugins/loop/">loop</a>: Detects simple forwarding loops and
halts the CoreDNS process if a loop is found.</li><li><a href="https://coredns.io/plugins/reload">reload</a>: Allows automatic reload of a changed Corefile.
After you edit the ConfigMap configuration, allow two minutes for your changes to take effect.</li><li><a href="https://coredns.io/plugins/loadbalance">loadbalance</a>: This is a round-robin DNS loadbalancer
that randomizes the order of A, AAAA, and MX records in the answer.</li></ul><p>You can modify the default CoreDNS behavior by modifying the ConfigMap.</p><h3 id="configuration-of-stub-domain-and-upstream-nameserver-using-coredns">Configuration of Stub-domain and upstream nameserver using CoreDNS</h3><p>CoreDNS has the ability to configure stub-domains and upstream nameservers
using the <a href="https://coredns.io/plugins/forward/">forward plugin</a>.</p><h4 id="example">Example</h4><p>If a cluster operator has a <a href="https://www.consul.io/">Consul</a> domain server located at "10.150.0.1",
and all Consul names have the suffix ".consul.local". To configure it in CoreDNS,
the cluster administrator creates the following stanza in the CoreDNS ConfigMap.</p><pre tabindex="0"><code>consul.local:53 {
    errors
    cache 30
    forward . 10.150.0.1
}
</code></pre><p>To explicitly force all non-cluster DNS lookups to go through a specific nameserver at 172.16.0.1,
point the <code>forward</code> to the nameserver instead of <code>/etc/resolv.conf</code></p><pre tabindex="0"><code>forward .  172.16.0.1
</code></pre><p>The final ConfigMap along with the default <code>Corefile</code> configuration looks like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    .:53 {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        errors
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        health
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">           pods insecure
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">           fallthrough in-addr.arpa ip6.arpa
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        }
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        prometheus :9153
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        forward . 172.16.0.1
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        cache 30
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        loop
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        reload
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        loadbalance
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    }
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    consul.local:53 {
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        errors
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        cache 30
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        forward . 10.150.0.1
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>CoreDNS does not support FQDNs for stub-domains and nameservers (eg: "ns.foo.com").
During translation, all FQDN nameservers will be omitted from the CoreDNS config.</div><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/docs/tasks/administer-cluster/dns-debugging-resolution/">Debugging DNS Resolution</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use Kube-router for NetworkPolicy</h1><p>This page shows how to use <a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a> for NetworkPolicy.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster running. If you do not already have a cluster, you can create one by using any of the cluster installers like Kops, Bootkube, Kubeadm etc.</p><h2 id="installing-kube-router-addon">Installing Kube-router addon</h2><p>The Kube-router Addon comes with a Network Policy Controller that watches Kubernetes API server for any NetworkPolicy and pods updated and configures iptables rules and ipsets to allow or block traffic as directed by the policies. Please follow the <a href="https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers">trying Kube-router with cluster installers</a> guide to install Kube-router addon.</p><h2 id="what-s-next">What's next</h2><p>Once you have installed the Kube-router addon, you can follow the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a> to try out Kubernetes NetworkPolicy.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Autoscale the DNS Service in a Cluster</h1><p>This page shows how to enable and configure autoscaling of the DNS service in
your Kubernetes cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p></li><li><p>This guide assumes your nodes use the AMD64 or Intel 64 CPU architecture.</p></li><li><p>Make sure <a href="/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS</a> is enabled.</p></li></ul><h2 id="determining-whether-dns-horizontal-autoscaling-is-already-enabled">Determine whether DNS horizontal autoscaling is already enabled</h2><p>List the <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployments">Deployments</a>
in your cluster in the kube-system <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The output is similar to this:</p><pre><code>NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
...
kube-dns-autoscaler    1/1     1            1           ...
...
</code></pre><p>If you see "kube-dns-autoscaler" in the output, DNS horizontal autoscaling is
already enabled, and you can skip to
<a href="#tuning-autoscaling-parameters">Tuning autoscaling parameters</a>.</p><h2 id="find-scaling-target">Get the name of your DNS Deployment</h2><p>List the DNS deployments in your cluster in the kube-system namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment -l k8s-app<span style="color:#666">=</span>kube-dns --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The output is similar to this:</p><pre><code>NAME      READY   UP-TO-DATE   AVAILABLE   AGE
...
coredns   2/2     2            2           ...
...
</code></pre><p>If you don't see a Deployment for DNS services, you can also look for it by name:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>and look for a deployment named <code>coredns</code> or <code>kube-dns</code>.</p><p>Your scale target is</p><pre><code>Deployment/&lt;your-deployment-name&gt;
</code></pre><p>where <code>&lt;your-deployment-name&gt;</code> is the name of your DNS Deployment. For example, if
the name of your Deployment for DNS is coredns, your scale target is Deployment/coredns.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label
<code>k8s-app=kube-dns</code> so that it can work in clusters that originally used
kube-dns.</div><h2 id="enablng-dns-horizontal-autoscaling">Enable DNS horizontal autoscaling</h2><p>In this section, you create a new Deployment. The Pods in the Deployment run a
container based on the <code>cluster-proportional-autoscaler-amd64</code> image.</p><p>Create a file named <code>dns-horizontal-autoscaler.yaml</code> with this content:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/dns/dns-horizontal-autoscaler.yaml" download="admin/dns/dns-horizontal-autoscaler.yaml"><code>admin/dns/dns-horizontal-autoscaler.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-dns-dns-horizontal-autoscaler-yaml&quot;)" title="Copy admin/dns/dns-horizontal-autoscaler.yaml to clipboard"/></div><div class="includecode" id="admin-dns-dns-horizontal-autoscaler-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>system:kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">""</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"nodes"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"list"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"watch"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">""</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"replicationcontrollers/scale"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"get"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"update"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"apps"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"deployments/scale"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"replicasets/scale"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"get"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"update"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Remove the configmaps rule once below issue is fixed:</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># kubernetes-incubator/cluster-proportional-autoscaler#16</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">""</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"configmaps"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"get"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"create"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>system:kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">subjects</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">roleRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>system:kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">priorityClassName</span>:<span style="color:#bbb"> </span>system-cluster-critical<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">securityContext</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">seccompProfile</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>RuntimeDefault<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">supplementalGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#666">65534</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">65534</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span>linux<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/cpa/cluster-proportional-autoscaler:1.8.4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"20m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"10Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- /cluster-proportional-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- --namespace=kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- --configmap=kube-dns-autoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- --target=&lt;SCALE_TARGET&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># If using small nodes, "nodesPerReplica" should dominate.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true,"includeUnschedulableNodes":true}}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- --logtostderr=true<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- --v=2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"CriticalAddonsOnly"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Exists"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">serviceAccountName</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In the file, replace <code>&lt;SCALE_TARGET&gt;</code> with your scale target.</p><p>Go to the directory that contains your configuration file, and enter this
command to create the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f dns-horizontal-autoscaler.yaml
</span></span></code></pre></div><p>The output of a successful command is:</p><pre><code>deployment.apps/kube-dns-autoscaler created
</code></pre><p>DNS horizontal autoscaling is now enabled.</p><h2 id="tuning-autoscaling-parameters">Tune DNS autoscaling parameters</h2><p>Verify that the kube-dns-autoscaler <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/configmap/" target="_blank" aria-label="ConfigMap">ConfigMap</a> exists:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get configmap --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The output is similar to this:</p><pre><code>NAME                  DATA      AGE
...
kube-dns-autoscaler   1         ...
...
</code></pre><p>Modify the data in the ConfigMap:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit configmap kube-dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>Look for this line:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">linear</span>:<span style="color:#bbb"> </span><span style="color:#b44">'{"coresPerReplica":256,"min":1,"nodesPerReplica":16}'</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Modify the fields according to your needs. The "min" field indicates the
minimal number of DNS backends. The actual number of backends is
calculated using this equation:</p><pre><code>replicas = max( ceil( cores × 1/coresPerReplica ) , ceil( nodes × 1/nodesPerReplica ) )
</code></pre><p>Note that the values of both <code>coresPerReplica</code> and <code>nodesPerReplica</code> are
floats.</p><p>The idea is that when a cluster is using nodes that have many cores,
<code>coresPerReplica</code> dominates. When a cluster is using nodes that have fewer
cores, <code>nodesPerReplica</code> dominates.</p><p>There are other supported scaling patterns. For details, see
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler</a>.</p><h2 id="disable-dns-horizontal-autoscaling">Disable DNS horizontal autoscaling</h2><p>There are a few options for tuning DNS horizontal autoscaling. Which option to
use depends on different conditions.</p><h3 id="option-1-scale-down-the-kube-dns-autoscaler-deployment-to-0-replicas">Option 1: Scale down the kube-dns-autoscaler deployment to 0 replicas</h3><p>This option works for all situations. Enter this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale deployment --replicas<span style="color:#666">=</span><span style="color:#666">0</span> kube-dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The output is:</p><pre><code>deployment.apps/kube-dns-autoscaler scaled
</code></pre><p>Verify that the replica count is zero:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The output displays 0 in the DESIRED and CURRENT columns:</p><pre><code>NAME                                  DESIRED   CURRENT   READY   AGE
...
kube-dns-autoscaler-6b59789fc8        0         0         0       ...
...
</code></pre><h3 id="option-2-delete-the-kube-dns-autoscaler-deployment">Option 2: Delete the kube-dns-autoscaler deployment</h3><p>This option works if kube-dns-autoscaler is under your own control, which means
no one will re-create it:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment kube-dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div><p>The output is:</p><pre><code>deployment.apps "kube-dns-autoscaler" deleted
</code></pre><h3 id="option-3-delete-the-kube-dns-autoscaler-manifest-file-from-the-master-node">Option 3: Delete the kube-dns-autoscaler manifest file from the master node</h3><p>This option works if kube-dns-autoscaler is under control of the (deprecated)
<a href="https://git.k8s.io/kubernetes/cluster/addons/README.md">Addon Manager</a>,
and you have write access to the master node.</p><p>Sign in to the master node and delete the corresponding manifest file.
The common path for this kube-dns-autoscaler is:</p><pre><code>/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre><p>After the manifest file is deleted, the Addon Manager will delete the
kube-dns-autoscaler Deployment.</p><h2 id="understanding-how-dns-horizontal-autoscaling-works">Understanding how DNS horizontal autoscaling works</h2><ul><li><p>The cluster-proportional-autoscaler application is deployed separately from
the DNS service.</p></li><li><p>An autoscaler Pod runs a client that polls the Kubernetes API server for the
number of nodes and cores in the cluster.</p></li><li><p>A desired replica count is calculated and applied to the DNS backends based on
the current schedulable nodes and cores and the given scaling parameters.</p></li><li><p>The scaling parameters and data points are provided via a ConfigMap to the
autoscaler, and it refreshes its parameters table every poll interval to be up
to date with the latest desired scaling parameters.</p></li><li><p>Changes to the scaling parameters are allowed without rebuilding or restarting
the autoscaler Pod.</p></li><li><p>The autoscaler provides a controller interface to support two control
patterns: <em>linear</em> and <em>ladder</em>.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">Guaranteed Scheduling For Critical Add-On Pods</a>.</li><li>Learn more about the
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">implementation of cluster-proportional-autoscaler</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Switching from Polling to CRI Event-based Updates to Container Status</h1><div class="feature-state-notice feature-alpha" title="Feature Gate: EventedPLEG"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [alpha]</code> (enabled by default: false)</div><p>This page shows how to migrate nodes to use event based updates for container status. The event-based
implementation reduces node resource consumption by the kubelet, compared to the legacy approach
that relies on polling.
You may know this feature as <em>evented Pod lifecycle event generator (PLEG)</em>. That's the name used
internally within the Kubernetes project for a key implementation detail.</p><p>The polling based approach is referred to as <em>generic PLEG</em>.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to run a version of Kubernetes that provides this feature.
Kubernetes v1.27 includes beta support for event-based container
status updates. The feature is beta but is <em>disabled</em> by default
because it requires support from the container runtime.</li><li>Your Kubernetes server must be at or later than version 1.26.<p>To check the version, enter <code>kubectl version</code>.</p>If you are running a different version of Kubernetes, check the documentation for that release.</li><li>The container runtime in use must support container lifecycle events.
The kubelet automatically switches back to the legacy generic PLEG
mechanism if the container runtime does not announce support for
container lifecycle events, even if you have this feature gate enabled.</li></ul><h2 id="why-switch-to-evented-pleg">Why switch to Evented PLEG?</h2><ul><li>The <em>Generic PLEG</em> incurs non-negligible overhead due to frequent polling of container statuses.</li><li>This overhead is exacerbated by Kubelet's parallelized polling of container states, thus limiting
its scalability and causing poor performance and reliability problems.</li><li>The goal of <em>Evented PLEG</em> is to reduce unnecessary work during inactivity
by replacing periodic polling.</li></ul><h2 id="switching-to-evented-pleg">Switching to Evented PLEG</h2><ol><li><p>Start the Kubelet with the <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
<code>EventedPLEG</code> enabled. You can manage the kubelet feature gates editing the kubelet
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">config file</a> and restarting the kubelet service.
You need to do this on each node where you are using this feature.</p></li><li><p>Make sure the node is <a href="/docs/tasks/administer-cluster/safely-drain-node/">drained</a> before proceeding.</p></li><li><p>Start the container runtime with the container event generation enabled.</p><ul class="nav nav-tabs" id="tab-with-code" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-with-code-0" role="tab" aria-controls="tab-with-code-0" aria-selected="true">Containerd</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-with-code-1" role="tab" aria-controls="tab-with-code-1">CRI-O</a></li></ul><div class="tab-content" id="tab-with-code"><div id="tab-with-code-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-with-code-0"><p><p>Version 1.7+</p></p></div><div id="tab-with-code-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-with-code-1"><p><p>Version 1.26+</p><p>Check if the CRI-O is already configured to emit CRI events by verifying the configuration,</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>crio config | grep enable_pod_events
</span></span></code></pre></div><p>If it is enabled, the output should be similar to the following:</p><pre tabindex="0"><code class="language-none" data-lang="none">enable_pod_events = true
</code></pre><p>To enable it, start the CRI-O daemon with the flag <code>--enable-pod-events=true</code> or
use a dropin config with the following lines:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml"><span style="display:flex"><span>[crio.runtime]
</span></span><span style="display:flex"><span>enable_pod_events<span>:</span> <span style="color:#a2f;font-weight:700">true</span>
</span></span></code></pre></div></p></div></div>Your Kubernetes server must be at or later than version 1.26.<p>To check the version, enter <code>kubectl version</code>.</p></li><li><p>Verify that the kubelet is using event-based container stage change monitoring.
To check, look for the term <code>EventedPLEG</code> in the kubelet logs.</p><p>The output should be similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">I0314 11:10:13.909915 1105457 feature_gate.go:249] feature gates: &amp;{map[EventedPLEG:true]}
</span></span></span></code></pre></div><p>If you have set <code>--v</code> to 4 and above, you might see more entries that indicate
that the kubelet is using event-based container state monitoring.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">I0314 11:12:42.009542 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=3b2c6172-b112-447a-ba96-94e7022912dc
</span></span></span><span style="display:flex"><span><span style="color:#888">I0314 11:12:44.623326 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=b3fba5ea-a8c5-4b76-8f43-481e17e8ec40
</span></span></span><span style="display:flex"><span><span style="color:#888">I0314 11:12:44.714564 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=b3fba5ea-a8c5-4b76-8f43-481e17e8ec40
</span></span></span></code></pre></div></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the design in the Kubernetes Enhancement Proposal (KEP):
<a href="https://github.com/kubernetes/enhancements/blob/5b258a990adabc2ffdc9d84581ea6ed696f7ce6c/keps/sig-node/3386-kubelet-evented-pleg/README.md">Kubelet Evented PLEG for Better Performance</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Upgrading Linux nodes</h1><p>This page explains how to upgrade a Linux Worker Nodes created with kubeadm.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have shell access to all the nodes, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial
on a cluster with at least two nodes that are not acting as control plane hosts.</p><p>To check the version, enter <code>kubectl version</code>.</p></p><ul><li>Familiarize yourself with <a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">the process for upgrading the rest of your kubeadm
cluster</a>. You will want to
upgrade the control plane nodes before upgrading your Linux Worker nodes.</li></ul><h2 id="changing-the-package-repository">Changing the package repository</h2><p>If you're using the community-owned package repositories (<code>pkgs.k8s.io</code>), you need to
enable the package repository for the desired Kubernetes minor release. This is explained in
<a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing the Kubernetes package repository</a>
document.</p><div class="alert alert-secondary callout note" role="alert"><strong>Note:</strong> The legacy package repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) have been
<a href="/blog/2023/08/31/legacy-package-repository-deprecation/">deprecated and frozen starting from September 13, 2023</a>.
<strong>Using the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">new package repositories hosted at <code>pkgs.k8s.io</code></a>
is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.</strong>
The deprecated legacy repositories, and their contents, might be removed at any time in the future and without
a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.</div><h2 id="upgrading-worker-nodes">Upgrading worker nodes</h2><h3 id="upgrade-kubeadm">Upgrade kubeadm</h3><p>Upgrade kubeadm:</p><ul class="nav nav-tabs" id="k8s-install-kubeadm-worker-nodes" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubeadm-worker-nodes-0" role="tab" aria-controls="k8s-install-kubeadm-worker-nodes-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubeadm-worker-nodes-1" role="tab" aria-controls="k8s-install-kubeadm-worker-nodes-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-install-kubeadm-worker-nodes"><div id="k8s-install-kubeadm-worker-nodes-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubeadm-worker-nodes-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo apt-mark unhold kubeadm <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-get update <span style="color:#666">&amp;&amp;</span> sudo apt-get install -y <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span><span style="color:#b44">'1.34.x-*'</span> <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-mark hold kubeadm
</span></span></code></pre></div></p></div><div id="k8s-install-kubeadm-worker-nodes-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubeadm-worker-nodes-1"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubeadm-<span style="color:#b44">'1.34.x-*'</span> --disableexcludes<span style="color:#666">=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubeadm-<span style="color:#b44">'1.34.x-*'</span> --setopt<span style="color:#666">=</span><span style="color:#b8860b">disable_excludes</span><span style="color:#666">=</span>kubernetes
</span></span></code></pre></div></p></div></div><h3 id="call-kubeadm-upgrade">Call "kubeadm upgrade"</h3><p>For worker nodes this upgrades the local kubelet configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo kubeadm upgrade node
</span></span></code></pre></div><h3 id="drain-the-node">Drain the node</h3><p>Prepare the node for maintenance by marking it unschedulable and evicting the workloads:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># execute this command on a control plane node</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace &lt;node-to-drain&gt; with the name of your node you are draining</span>
</span></span><span style="display:flex"><span>kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</span></span></code></pre></div><h3 id="upgrade-kubelet-and-kubectl">Upgrade kubelet and kubectl</h3><ol><li><p>Upgrade the kubelet and kubectl:</p><ul class="nav nav-tabs" id="k8s-kubelet-and-kubectl" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-kubelet-and-kubectl-0" role="tab" aria-controls="k8s-kubelet-and-kubectl-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-kubelet-and-kubectl-1" role="tab" aria-controls="k8s-kubelet-and-kubectl-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-kubelet-and-kubectl"><div id="k8s-kubelet-and-kubectl-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-kubelet-and-kubectl-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo apt-mark unhold kubelet kubectl <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-get update <span style="color:#666">&amp;&amp;</span> sudo apt-get install -y <span style="color:#b8860b">kubelet</span><span style="color:#666">=</span><span style="color:#b44">'1.34.x-*'</span> <span style="color:#b8860b">kubectl</span><span style="color:#666">=</span><span style="color:#b44">'1.34.x-*'</span> <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sudo apt-mark hold kubelet kubectl
</span></span></code></pre></div></p></div><div id="k8s-kubelet-and-kubectl-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-kubelet-and-kubectl-1"><p><p>For systems with DNF:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubelet-<span style="color:#b44">'1.34.x-*'</span> kubectl-<span style="color:#b44">'1.34.x-*'</span> --disableexcludes<span style="color:#666">=</span>kubernetes
</span></span></code></pre></div><p>For systems with DNF5:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace x in 1.34.x-* with the latest patch version</span>
</span></span><span style="display:flex"><span>sudo yum install -y kubelet-<span style="color:#b44">'1.34.x-*'</span> kubectl-<span style="color:#b44">'1.34.x-*'</span> --setopt<span style="color:#666">=</span><span style="color:#b8860b">disable_excludes</span><span style="color:#666">=</span>kubernetes
</span></span></code></pre></div></p></div></div></li><li><p>Restart the kubelet:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo systemctl daemon-reload
</span></span><span style="display:flex"><span>sudo systemctl restart kubelet
</span></span></code></pre></div></li></ol><h3 id="uncordon-the-node">Uncordon the node</h3><p>Bring the node back online by marking it schedulable:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># execute this command on a control plane node</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># replace &lt;node-to-uncordon&gt; with the name of your node</span>
</span></span><span style="display:flex"><span>kubectl uncordon &lt;node-to-uncordon&gt;
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrade Windows nodes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use a Service to Access an Application in a Cluster</h1><p>This page shows how to create a Kubernetes Service object that external
clients can use to access an application running in a cluster. The Service
provides load balancing for an application that has two running instances.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><h2 id="objectives">Objectives</h2><ul><li>Run two instances of a Hello World application.</li><li>Create a Service object that exposes a node port.</li><li>Use the Service object to access the running application.</li></ul><h2 id="creating-a-service-for-an-application-running-in-two-pods">Creating a service for an application running in two pods</h2><p>Here is the configuration file for the application Deployment:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/access/hello-application.yaml" download="service/access/hello-application.yaml"><code>service/access/hello-application.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-access-hello-application-yaml&quot;)" title="Copy service/access/hello-application.yaml to clipboard"/></div><div class="includecode" id="service-access-hello-application-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello-world<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">run</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">run</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello-world<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><ol><li><p>Run a Hello World application in your cluster:
Create the application Deployment using the file above:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml
</span></span></code></pre></div><p>The preceding command creates a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>
and an associated
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank" aria-label="ReplicaSet">ReplicaSet</a>.
The ReplicaSet has two
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>
each of which runs the Hello World application.</p></li><li><p>Display information about the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployments hello-world
</span></span><span style="display:flex"><span>kubectl describe deployments hello-world
</span></span></code></pre></div></li><li><p>Display information about your ReplicaSet objects:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get replicasets
</span></span><span style="display:flex"><span>kubectl describe replicasets
</span></span></code></pre></div></li><li><p>Create a Service object that exposes the deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl expose deployment hello-world --type<span style="color:#666">=</span>NodePort --name<span style="color:#666">=</span>example-service
</span></span></code></pre></div></li><li><p>Display information about the Service:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe services example-service
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none" data-lang="none">Name:                   example-service
Namespace:              default
Labels:                 run=load-balancer-example
Annotations:            &lt;none&gt;
Selector:               run=load-balancer-example
Type:                   NodePort
IP:                     10.32.0.16
Port:                   &lt;unset&gt; 8080/TCP
TargetPort:             8080/TCP
NodePort:               &lt;unset&gt; 31496/TCP
Endpoints:              10.200.1.4:8080,10.200.2.5:8080
Session Affinity:       None
Events:                 &lt;none&gt;
</code></pre><p>Make a note of the NodePort value for the Service. For example,
in the preceding output, the NodePort value is 31496.</p></li><li><p>List the pods that are running the Hello World application:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --selector<span style="color:#666">=</span><span style="color:#b44">"run=load-balancer-example"</span> --output<span style="color:#666">=</span>wide
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME                           READY   STATUS    ...  IP           NODE
hello-world-2895499144-bsbk5   1/1     Running   ...  10.200.1.4   worker1
hello-world-2895499144-m1pwt   1/1     Running   ...  10.200.2.5   worker2
</code></pre></li><li><p>Get the public IP address of one of your nodes that is running
a Hello World pod. How you get this address depends on how you set
up your cluster. For example, if you are using Minikube, you can
see the node address by running <code>kubectl cluster-info</code>. If you are
using Google Compute Engine instances, you can use the
<code>gcloud compute instances list</code> command to see the public addresses of your
nodes.</p></li><li><p>On your chosen node, create a firewall rule that allows TCP traffic
on your node port. For example, if your Service has a NodePort value of
31568, create a firewall rule that allows TCP traffic on port 31568. Different
cloud providers offer different ways of configuring firewall rules.</p></li><li><p>Use the node address and node port to access the Hello World application:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl http://&lt;public-node-ip&gt;:&lt;node-port&gt;
</span></span></code></pre></div><p>where <code>&lt;public-node-ip&gt;</code> is the public IP address of your node,
and <code>&lt;node-port&gt;</code> is the NodePort value for your service. The
response to a successful request is a hello message:</p><pre tabindex="0"><code class="language-none" data-lang="none">Hello, world!
Version: 2.0.0
Hostname: hello-world-cdd4458f4-m47c8
</code></pre></li></ol><h2 id="using-a-service-configuration-file">Using a service configuration file</h2><p>As an alternative to using <code>kubectl expose</code>, you can use a
<a href="/docs/concepts/services-networking/service/">service configuration file</a>
to create a Service.</p><h2 id="cleaning-up">Cleaning up</h2><p>To delete the Service, enter this command:</p><pre><code>kubectl delete services example-service
</code></pre><p>To delete the Deployment, the ReplicaSet, and the Pods that are running
the Hello World application, enter this command:</p><pre><code>kubectl delete deployment hello-world
</code></pre><h2 id="what-s-next">What's next</h2><p>Follow the
<a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a>
tutorial.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Adding Windows worker nodes</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [beta]</code></div><p>This page explains how to add Windows worker nodes to a kubeadm cluster.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>A running <a href="https://www.microsoft.com/cloud-platform/windows-server-pricing">Windows Server 2022</a>
(or higher) instance with administrative access.</li><li>A running kubeadm cluster created by <code>kubeadm init</code> and following the steps
in the document <a href="/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a cluster with kubeadm</a>.</li></ul><h2 id="adding-windows-worker-nodes">Adding Windows worker nodes</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To facilitate the addition of Windows worker nodes to a cluster, PowerShell scripts from the repository
<a href="https://sigs.k8s.io/sig-windows-tools">https://sigs.k8s.io/sig-windows-tools</a> are used.</div><p>Do the following for each machine:</p><ol><li>Open a PowerShell session on the machine.</li><li>Make sure you are Administrator or a privileged user.</li></ol><p>Then proceed with the steps outlined below.</p><h3 id="install-containerd">Install containerd</h3><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>To install containerd, first run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell"><span style="display:flex"><span>curl.exe -LO https<span>:</span>//raw.githubusercontent.com/<span style="color:#a2f">kubernetes-sigs</span>/<span style="color:#a2f">sig-windows</span>-tools/master/hostprocess/<span style="color:#a2f">Install-Containerd</span>.ps1
</span></span></code></pre></div><p>Then run the following command, but first replace <code>CONTAINERD_VERSION</code> with a recent release
from the <a href="https://github.com/containerd/containerd/releases">containerd repository</a>.
The version must not have a <code>v</code> prefix. For example, use <code>1.7.22</code> instead of <code>v1.7.22</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell"><span style="display:flex"><span>.\<span style="color:#a2f">Install-Containerd</span>.ps1 -ContainerDVersion CONTAINERD_VERSION
</span></span></code></pre></div><ul><li>Adjust any other parameters for <code>Install-Containerd.ps1</code> such as <code>netAdapterName</code> as you need them.</li><li>Set <code>skipHypervisorSupportCheck</code> if your machine does not support Hyper-V and cannot host Hyper-V isolated
containers.</li><li>If you change the <code>Install-Containerd.ps1</code> optional parameters <code>CNIBinPath</code> and/or <code>CNIConfigPath</code> you will
need to configure the installed Windows CNI plugin with matching values.</li></ul><h3 id="install-kubeadm-and-kubelet">Install kubeadm and kubelet</h3><p>Run the following commands to install kubeadm and the kubelet:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell"><span style="display:flex"><span>curl.exe -LO https<span>:</span>//raw.githubusercontent.com/<span style="color:#a2f">kubernetes-sigs</span>/<span style="color:#a2f">sig-windows</span>-tools/master/hostprocess/PrepareNode.ps1
</span></span><span style="display:flex"><span>.\PrepareNode.ps1 -KubernetesVersion v1.34
</span></span></code></pre></div><ul><li>Adjust the parameter <code>KubernetesVersion</code> of <code>PrepareNode.ps1</code> if needed.</li></ul><h3 id="run-kubeadm-join">Run <code>kubeadm join</code></h3><p>Run the command that was output by <code>kubeadm init</code>. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><h4 id="additional-information-about-kubeadm-join">Additional information about kubeadm join</h4><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To specify an IPv6 tuple for <code>&lt;control-plane-host&gt;:&lt;control-plane-port&gt;</code>, IPv6 address must be enclosed in square brackets, for example: <code>[2001:db8::101]:2073</code>.</div><p>If you do not have the token, you can get it by running the following command on the control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on a control plane node</span>
</span></span><span style="display:flex"><span>sudo kubeadm token list
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style="display:flex"><span><span style="color:#888">8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                                   signing          token generated by     bootstrappers:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                                                    'kubeadm init'.        kubeadm:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                                                                           default-node-token
</span></span></span></code></pre></div><p>By default, node join tokens expire after 24 hours. If you are joining a node to the cluster after the
current token has expired, you can create a new token by running the following command on the
control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on a control plane node</span>
</span></span><span style="display:flex"><span>sudo kubeadm token create
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>If you don't have the value of <code>--discovery-token-ca-cert-hash</code>, you can get it by running the
following commands on the control plane node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey  | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   openssl dgst -sha256 -hex | sed <span style="color:#b44">'s/^.* //'</span>
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><p>The output of the <code>kubeadm join</code> command should look something like:</p><pre tabindex="0"><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><p>A few seconds later, you should notice this node in the output from <code>kubectl get nodes</code>.
(for example, run <code>kubectl</code> on a control plane node).</p><h3 id="network-configuration">Network configuration</h3><p>CNI setup on clusters mixed with Linux and Windows nodes requires more steps than just
running <code>kubectl apply</code> on a manifest file. Additionally, the CNI plugin running on control
plane nodes must be prepared to support the CNI plugin running on Windows worker nodes.</p><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Only a few CNI plugins currently support Windows. Below you can find individual setup instructions for them:</p><ul><li><a href="https://sigs.k8s.io/sig-windows-tools/guides/flannel.md">Flannel</a></li><li><a href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/windows-calico/">Calico</a></li></ul><h3 id="install-kubectl">Install kubectl for Windows (optional)</h3><p>See <a href="/docs/tasks/tools/install-kubectl-windows/">Install and Set Up kubectl on Windows</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/adding-linux-nodes/">add Linux worker nodes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1>Access Applications in a Cluster</h1><div class="lead">Configure load balancing, port forwarding, or setup firewall or DNS configurations to access applications in a cluster.</div><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/web-ui-dashboard/">Deploy and Access the Kubernetes Dashboard</a></h5><p>Deploy the web UI (Kubernetes Dashboard) and access it.</p></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/access-cluster/">Accessing Clusters</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Use Port Forwarding to Access Applications in a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/connecting-frontend-backend/">Connect a Frontend to a Backend Using Services</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/create-external-load-balancer/">Create an External Load Balancer</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/list-all-running-container-images/">List All Container Images Running in a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">Communicate Between Containers in the Same Pod Using a Shared Volume</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/configure-dns-cluster/">Configure DNS for a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/access-application-cluster/access-cluster-services/">Access Services Running on Clusters</a></h5><p/></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Running Kubernetes Node Components as a Non-root User</h1><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [alpha]</code></div><p>This document describes how to run Kubernetes Node components such as kubelet, CRI, OCI, and CNI
without root privileges, by using a <a class="glossary-tooltip" title="A Linux kernel feature to emulate superuser privilege for unprivileged users." data-toggle="tooltip" data-placement="top" href="https://man7.org/linux/man-pages/man7/user_namespaces.7.html" target="_blank" aria-label="user namespace">user namespace</a>.</p><p>This technique is also known as <em>rootless mode</em>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>This document describes how to run Kubernetes Node components (and hence pods) as a non-root user.</p><p>If you are just looking for how to run a pod as a non-root user, see <a href="/docs/tasks/configure-pod-container/security-context/">SecurityContext</a>.</p></div><h2 id="before-you-begin">Before you begin</h2><p>Your Kubernetes server must be at or later than version 1.22.</p><p>To check the version, enter <code>kubectl version</code>.</p><ul><li><a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">Enable Cgroup v2</a></li><li><a href="https://rootlesscontaine.rs/getting-started/common/login/">Enable systemd with user session</a></li><li><a href="https://rootlesscontaine.rs/getting-started/common/sysctl/">Configure several sysctl values, depending on host Linux distribution</a></li><li><a href="https://rootlesscontaine.rs/getting-started/common/subuid/">Ensure that your unprivileged user is listed in <code>/etc/subuid</code> and <code>/etc/subgid</code></a></li><li>Enable the <code>KubeletInUserNamespace</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a></li></ul><h2 id="running-kubernetes-inside-rootless-docker-podman">Running Kubernetes inside Rootless Docker/Podman</h2><h3 id="kind">kind</h3><p><a href="https://kind.sigs.k8s.io/">kind</a> supports running Kubernetes inside Rootless Docker or Rootless Podman.</p><p>See <a href="https://kind.sigs.k8s.io/docs/user/rootless/">Running kind with Rootless Docker</a>.</p><h3 id="minikube">minikube</h3><p><a href="https://minikube.sigs.k8s.io/">minikube</a> also supports running Kubernetes inside Rootless Docker or Rootless Podman.</p><p>See the Minikube documentation:</p><ul><li><a href="https://minikube.sigs.k8s.io/docs/drivers/docker/">Rootless Docker</a></li><li><a href="https://minikube.sigs.k8s.io/docs/drivers/podman/">Rootless Podman</a></li></ul><h2 id="running-kubernetes-inside-unprivileged-containers">Running Kubernetes inside Unprivileged Containers</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h3 id="sysbox">sysbox</h3><p><a href="https://github.com/nestybox/sysbox">Sysbox</a> is an open-source container runtime
(similar to "runc") that supports running system-level workloads such as Docker
and Kubernetes inside unprivileged containers isolated with the Linux user
namespace.</p><p>See <a href="https://github.com/nestybox/sysbox/blob/master/docs/quickstart/kind.md">Sysbox Quick Start Guide: Kubernetes-in-Docker</a> for more info.</p><p>Sysbox supports running Kubernetes inside unprivileged containers without
requiring Cgroup v2 and without the <code>KubeletInUserNamespace</code> feature gate. It
does this by exposing specially crafted <code>/proc</code> and <code>/sys</code> filesystems inside
the container plus several other advanced OS virtualization techniques.</p><h2 id="running-rootless-kubernetes-directly-on-a-host">Running Rootless Kubernetes directly on a host</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h3 id="k3s">K3s</h3><p><a href="https://k3s.io/">K3s</a> experimentally supports rootless mode.</p><p>See <a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">Running K3s with Rootless mode</a> for the usage.</p><h3 id="usernetes">Usernetes</h3><p><a href="https://github.com/rootless-containers/usernetes">Usernetes</a> is a reference distribution of Kubernetes that can be installed under <code>$HOME</code> directory without the root privilege.</p><p>Usernetes supports both containerd and CRI-O as CRI runtimes.
Usernetes supports multi-node clusters using Flannel (VXLAN).</p><p>See <a href="https://github.com/rootless-containers/usernetes">the Usernetes repo</a> for the usage.</p><h2 id="userns-the-hard-way">Manually deploy a node that runs the kubelet in a user namespace</h2><p>This section provides hints for running Kubernetes in a user namespace manually.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This section is intended to be read by developers of Kubernetes distributions, not by end users.</div><h3 id="creating-a-user-namespace">Creating a user namespace</h3><p>The first step is to create a <a class="glossary-tooltip" title="A Linux kernel feature to emulate superuser privilege for unprivileged users." data-toggle="tooltip" data-placement="top" href="https://man7.org/linux/man-pages/man7/user_namespaces.7.html" target="_blank" aria-label="user namespace">user namespace</a>.</p><p>If you are trying to run Kubernetes in a user-namespaced container such as
Rootless Docker/Podman or LXC/LXD, you are all set, and you can go to the next subsection.</p><p>Otherwise you have to create a user namespace by yourself, by calling <code>unshare(2)</code> with <code>CLONE_NEWUSER</code>.</p><p>A user namespace can be also unshared by using command line tools such as:</p><ul><li><a href="https://man7.org/linux/man-pages/man1/unshare.1.html"><code>unshare(1)</code></a></li><li><a href="https://github.com/rootless-containers/rootlesskit">RootlessKit</a></li><li><a href="https://github.com/giuseppe/become-root">become-root</a></li></ul><p>After unsharing the user namespace, you will also have to unshare other namespaces such as mount namespace.</p><p>You do <em>not</em> need to call <code>chroot()</code> nor <code>pivot_root()</code> after unsharing the mount namespace,
however, you have to mount writable filesystems on several directories <em>in</em> the namespace.</p><p>At least, the following directories need to be writable <em>in</em> the namespace (not <em>outside</em> the namespace):</p><ul><li><code>/etc</code></li><li><code>/run</code></li><li><code>/var/logs</code></li><li><code>/var/lib/kubelet</code></li><li><code>/var/lib/cni</code></li><li><code>/var/lib/containerd</code> (for containerd)</li><li><code>/var/lib/containers</code> (for CRI-O)</li></ul><h3 id="creating-a-delegated-cgroup-tree">Creating a delegated cgroup tree</h3><p>In addition to the user namespace, you also need to have a writable cgroup tree with cgroup v2.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes support for running Node components in user namespaces requires cgroup v2.
Cgroup v1 is not supported.</div><p>If you are trying to run Kubernetes in Rootless Docker/Podman or LXC/LXD on a systemd-based host, you are all set.</p><p>Otherwise you have to create a systemd unit with <code>Delegate=yes</code> property to delegate a cgroup tree with writable permission.</p><p>On your node, systemd must already be configured to allow delegation; for more details, see
<a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">cgroup v2</a> in the Rootless
Containers documentation.</p><h3 id="configuring-network">Configuring network</h3><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The network namespace of the Node components has to have a non-loopback interface, which can be for example configured with
<a href="https://github.com/rootless-containers/slirp4netns">slirp4netns</a>,
<a href="https://github.com/moby/vpnkit">VPNKit</a>, or
<a href="https://www.man7.org/linux/man-pages/man1/lxc-user-nic.1.html">lxc-user-nic(1)</a>.</p><p>The network namespaces of the Pods can be configured with regular CNI plugins.
For multi-node networking, Flannel (VXLAN, 8472/UDP) is known to work.</p><p>Ports such as the kubelet port (10250/TCP) and <code>NodePort</code> service ports have to be exposed from the Node network namespace to
the host with an external port forwarder, such as RootlessKit, slirp4netns, or
<a href="https://linux.die.net/man/1/socat">socat(1)</a>.</p><p>You can use the port forwarder from K3s.
See <a href="https://rancher.com/docs/k3s/latest/en/advanced/#known-issues-with-rootless-mode">Running K3s in Rootless Mode</a>
for more details.
The implementation can be found in <a href="https://github.com/k3s-io/k3s/blob/v1.22.3+k3s1/pkg/rootlessports/controller.go">the <code>pkg/rootlessports</code> package</a> of k3s.</p><h3 id="configuring-cri">Configuring CRI</h3><p>The kubelet relies on a container runtime. You should deploy a container runtime such as
containerd or CRI-O and ensure that it is running within the user namespace before the kubelet starts.</p><ul class="nav nav-tabs" id="cri" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#cri-0" role="tab" aria-controls="cri-0" aria-selected="true">containerd</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#cri-1" role="tab" aria-controls="cri-1">CRI-O</a></li></ul><div class="tab-content" id="cri"><div id="cri-0" class="tab-pane show active" role="tabpanel" aria-labelledby="cri-0"><p><p>Running CRI plugin of containerd in a user namespace is supported since containerd 1.4.</p><p>Running containerd within a user namespace requires the following configurations.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml"><span style="display:flex"><span>version = <span style="color:#666">2</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>[plugins.<span style="color:#b44">"io.containerd.grpc.v1.cri"</span>]
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Disable AppArmor</span>
</span></span><span style="display:flex"><span>  disable_apparmor = <span style="color:#a2f;font-weight:700">true</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Ignore an error during setting oom_score_adj</span>
</span></span><span style="display:flex"><span>  restrict_oom_score_adj = <span style="color:#a2f;font-weight:700">true</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Disable hugetlb cgroup v2 controller (because systemd does not support delegating hugetlb controller)</span>
</span></span><span style="display:flex"><span>  disable_hugetlb_controller = <span style="color:#a2f;font-weight:700">true</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>[plugins.<span style="color:#b44">"io.containerd.grpc.v1.cri"</span>.containerd]
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
</span></span><span style="display:flex"><span>  snapshotter = <span style="color:#b44">"fuse-overlayfs"</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>[plugins.<span style="color:#b44">"io.containerd.grpc.v1.cri"</span>.containerd.runtimes.runc.options]
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># We use cgroupfs that is delegated by systemd, so we do not use SystemdCgroup driver</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># (unless you run another systemd in the namespace)</span>
</span></span><span style="display:flex"><span>  SystemdCgroup = <span style="color:#a2f;font-weight:700">false</span>
</span></span></code></pre></div><p>The default path of the configuration file is <code>/etc/containerd/config.toml</code>.
The path can be specified with <code>containerd -c /path/to/containerd/config.toml</code>.</p></p></div><div id="cri-1" class="tab-pane" role="tabpanel" aria-labelledby="cri-1"><p><p>Running CRI-O in a user namespace is supported since CRI-O 1.22.</p><p>CRI-O requires an environment variable <code>_CRIO_ROOTLESS=1</code> to be set.</p><p>The following configurations are also recommended:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml"><span style="display:flex"><span>[crio]
</span></span><span style="display:flex"><span>  storage_driver = <span style="color:#b44">"overlay"</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
</span></span><span style="display:flex"><span>  storage_option = [<span style="color:#b44">"overlay.mount_program=/usr/local/bin/fuse-overlayfs"</span>]
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>[crio.runtime]
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># (unless you run another systemd in the namespace)</span>
</span></span><span style="display:flex"><span>  cgroup_manager = <span style="color:#b44">"cgroupfs"</span>
</span></span></code></pre></div><p>The default path of the configuration file is <code>/etc/crio/crio.conf</code>.
The path can be specified with <code>crio --config /path/to/crio/crio.conf</code>.</p></p></div></div><h3 id="configuring-kubelet">Configuring kubelet</h3><p>Running kubelet in a user namespace requires the following configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">featureGates</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">KubeletInUserNamespace</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># (unless you run another systemd in the namespace)</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">cgroupDriver</span>:<span style="color:#bbb"> </span><span style="color:#b44">"cgroupfs"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>When the <code>KubeletInUserNamespace</code> feature gate is enabled, the kubelet ignores errors
that may happen during setting the following sysctl values on the node.</p><ul><li><code>vm.overcommit_memory</code></li><li><code>vm.panic_on_oom</code></li><li><code>kernel.panic</code></li><li><code>kernel.panic_on_oops</code></li><li><code>kernel.keys.root_maxkeys</code></li><li><code>kernel.keys.root_maxbytes</code>.</li></ul><p>Within a user namespace, the kubelet also ignores any error raised from trying to open <code>/dev/kmsg</code>.
This feature gate also allows kube-proxy to ignore an error during setting <code>RLIMIT_NOFILE</code>.</p><p>The <code>KubeletInUserNamespace</code> feature gate was introduced in Kubernetes v1.22 with "alpha" status.</p><p>Running kubelet in a user namespace without using this feature gate is also possible
by mounting a specially crafted proc filesystem (as done by <a href="https://github.com/nestybox/sysbox">Sysbox</a>), but not officially supported.</p><h3 id="configuring-kube-proxy">Configuring kube-proxy</h3><p>Running kube-proxy in a user namespace requires the following configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeproxy.config.k8s.io/v1alpha1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeProxyConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">mode</span>:<span style="color:#bbb"> </span><span style="color:#b44">"iptables"</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># or "userspace"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">conntrack</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Skip setting sysctl value "net.netfilter.nf_conntrack_max"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">maxPerCore</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tcpEstablishedTimeout</span>:<span style="color:#bbb"> </span>0s<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tcpCloseWaitTimeout</span>:<span style="color:#bbb"> </span>0s<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="caveats">Caveats</h2><ul><li><p>Most of "non-local" volume drivers such as <code>nfs</code> and <code>iscsi</code> do not work.
Local volumes like <code>local</code>, <code>hostPath</code>, <code>emptyDir</code>, <code>configMap</code>, <code>secret</code>, and <code>downwardAPI</code> are known to work.</p></li><li><p>Some CNI plugins may not work. Flannel (VXLAN) is known to work.</p></li></ul><p>For more on this, see the <a href="https://rootlesscontaine.rs/caveats/">Caveats and Future work</a> page
on the rootlesscontaine.rs website.</p><h2 id="see-also">See Also</h2><ul><li><a href="https://rootlesscontaine.rs/">rootlesscontaine.rs</a></li><li><a href="https://www.slideshare.net/AkihiroSuda/kubecon-na-2020-containerd-rootless-containers-2020">Rootless Containers 2020 (KubeCon NA 2020)</a></li><li><a href="https://kind.sigs.k8s.io/docs/user/rootless/">Running kind with Rootless Docker</a></li><li><a href="https://github.com/rootless-containers/usernetes">Usernetes</a></li><li><a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">Running K3s with rootless mode</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2033-kubelet-in-userns-aka-rootless">KEP-2033: Kubelet-in-UserNS (aka Rootless mode)</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Control CPU Management Policies on the Node</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Kubernetes keeps many aspects of how pods execute on nodes abstracted
from the user. This is by design.  However, some workloads require
stronger guarantees in terms of latency and/or performance in order to operate
acceptably. The kubelet provides methods to enable more complex workload
placement policies while keeping the abstraction free from explicit placement
directives.</p><p>For detailed information on resource management, please refer to the
<a href="/docs/concepts/configuration/manage-resources-containers/">Resource Management for Pods and Containers</a>
documentation.</p><p>For detailed information on how the kubelet implements resource management, please refer to the
<a href="/docs/concepts/policy/node-resource-managers/">Node ResourceManagers</a> documentation.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.26.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>If you are running an older version of Kubernetes, please look at the documentation for the version you are actually running.</p><h2 id="configuring-cpu-management-policies">Configuring CPU management policies</h2><p>By default, the kubelet uses <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS quota</a>
to enforce pod CPU limits.  When the node runs many CPU-bound pods,
the workload can move to different CPU cores depending on
whether the pod is throttled and which CPU cores are available at
scheduling time. Many workloads are not sensitive to this migration and thus
work fine without any intervention.</p><p>However, in workloads where CPU cache affinity and scheduling latency
significantly affect workload performance, the kubelet allows alternative CPU
management policies to determine some placement preferences on the node.</p><h2 id="windows-support">Windows Support</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>CPU Manager support can be enabled on Windows by using the <code>WindowsCPUAndMemoryAffinity</code> feature gate
and it requires support in the container runtime.
Once the feature gate is enabled, follow the steps below to configure the <a href="#configuration">CPU manager policy</a>.</p><h2 id="configuration">Configuration</h2><p>The CPU Manager policy is set with the <code>--cpu-manager-policy</code> kubelet
flag or the <code>cpuManagerPolicy</code> field in <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration</a>.
There are two supported policies:</p><ul><li><a href="#none-policy"><code>none</code></a>: the default policy.</li><li><a href="#static-policy"><code>static</code></a>: allows pods with certain resource characteristics to be
granted increased CPU affinity and exclusivity on the node.</li></ul><p>The CPU manager periodically writes resource updates through the CRI in
order to reconcile in-memory CPU assignments with cgroupfs. The reconcile
frequency is set through a new Kubelet configuration value
<code>--cpu-manager-reconcile-period</code>. If not specified, it defaults to the same
duration as <code>--node-status-update-frequency</code>.</p><p>The behavior of the static policy can be fine-tuned using the <code>--cpu-manager-policy-options</code> flag.
The flag takes a comma-separated list of <code>key=value</code> policy options.
If you disable the <code>CPUManagerPolicyOptions</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
then you cannot fine-tune CPU manager policies. In that case, the CPU manager
operates only using its default settings.</p><p>In addition to the top-level <code>CPUManagerPolicyOptions</code> feature gate, the policy options are split
into two groups: alpha quality (hidden by default) and beta quality (visible by default).
The groups are guarded respectively by the <code>CPUManagerPolicyAlphaOptions</code>
and <code>CPUManagerPolicyBetaOptions</code> feature gates. Diverging from the Kubernetes standard, these
feature gates guard groups of options, because it would have been too cumbersome to add a feature
gate for each individual option.</p><h2 id="changing-the-cpu-manager-policy">Changing the CPU Manager Policy</h2><p>Since the CPU manager policy can only be applied when kubelet spawns new pods, simply changing from
"none" to "static" won't apply to existing pods. So in order to properly change the CPU manager
policy on a node, perform the following steps:</p><ol><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Drain</a> the node.</li><li>Stop kubelet.</li><li>Remove the old CPU manager state file. The path to this file is
<code>/var/lib/kubelet/cpu_manager_state</code> by default. This clears the state maintained by the
CPUManager so that the cpu-sets set up by the new policy won’t conflict with it.</li><li>Edit the kubelet configuration to change the CPU manager policy to the desired value.</li><li>Start kubelet.</li></ol><p>Repeat this process for every node that needs its CPU manager policy changed. Skipping this
process will result in kubelet crashlooping with the following error:</p><pre tabindex="0"><code>could not restore state from checkpoint: configured policy "static" differs from state checkpoint policy "none", please drain this node and delete the CPU manager checkpoint file "/var/lib/kubelet/cpu_manager_state" before restarting Kubelet
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>if the set of online CPUs changes on the node, the node must be drained and CPU manager manually reset by deleting the
state file <code>cpu_manager_state</code> in the kubelet root directory.</div><h3 id="none-policy-configuration"><code>none</code> policy configuration</h3><p>This policy has no extra configuration items.</p><h3 id="static-policy-configuration"><code>static</code> policy configuration</h3><p>This policy manages a shared pool of CPUs that initially contains all CPUs in the
node. The amount of exclusively allocatable CPUs is equal to the total
number of CPUs in the node minus any CPU reservations by the kubelet <code>--kube-reserved</code> or
<code>--system-reserved</code> options. From 1.17, the CPU reservation list can be specified
explicitly by kubelet <code>--reserved-cpus</code> option. The explicit CPU list specified by
<code>--reserved-cpus</code> takes precedence over the CPU reservation specified by
<code>--kube-reserved</code> and <code>--system-reserved</code>. CPUs reserved by these options are taken, in
integer quantity, from the initial shared pool in ascending order by physical
core ID.  This shared pool is the set of CPUs on which any containers in
<code>BestEffort</code> and <code>Burstable</code> pods run. Containers in <code>Guaranteed</code> pods with fractional
CPU <code>requests</code> also run on CPUs in the shared pool. Only containers that are
both part of a <code>Guaranteed</code> pod and have integer CPU <code>requests</code> are assigned
exclusive CPUs.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The kubelet requires a CPU reservation greater than zero be made
using either <code>--kube-reserved</code> and/or <code>--system-reserved</code> or <code>--reserved-cpus</code> when
the static policy is enabled. This is because zero CPU reservation would allow the shared
pool to become empty.</div><h3 id="cpu-policy-static--options">Static policy options</h3><p>You can toggle groups of options on and off based upon their maturity level
using the following feature gates:</p><ul><li><code>CPUManagerPolicyBetaOptions</code> default enabled. Disable to hide beta-level options.</li><li><code>CPUManagerPolicyAlphaOptions</code> default disabled. Enable to show alpha-level options.
You will still have to enable each option using the <code>CPUManagerPolicyOptions</code> kubelet option.</li></ul><p>The following policy options exist for the static <code>CPUManager</code> policy:</p><ul><li><code>full-pcpus-only</code> (GA, visible by default) (1.33 or higher)</li><li><code>distribute-cpus-across-numa</code> (beta, visible by default) (1.33 or higher)</li><li><code>align-by-socket</code> (alpha, hidden by default) (1.25 or higher)</li><li><code>distribute-cpus-across-cores</code> (alpha, hidden by default) (1.31 or higher)</li><li><code>strict-cpu-reservation</code> (beta, visible by default) (1.32 or higher)</li><li><code>prefer-align-cpus-by-uncorecache</code> (beta, visible by default) (1.34 or higher)</li></ul><p>The <code>full-pcpus-only</code> option can be enabled by adding <code>full-pcpus-only=true</code> to
the CPUManager policy options.
Likewise, the <code>distribute-cpus-across-numa</code> option can be enabled by adding
<code>distribute-cpus-across-numa=true</code> to the CPUManager policy options.
When both are set, they are "additive" in the sense that CPUs will be
distributed across NUMA nodes in chunks of full-pcpus rather than individual
cores.
The <code>align-by-socket</code> policy option can be enabled by adding <code>align-by-socket=true</code>
to the <code>CPUManager</code> policy options. It is also additive to the <code>full-pcpus-only</code>
and <code>distribute-cpus-across-numa</code> policy options.</p><p>The <code>distribute-cpus-across-cores</code> option can be enabled by adding
<code>distribute-cpus-across-cores=true</code> to the <code>CPUManager</code> policy options.
It cannot be used with <code>full-pcpus-only</code> or <code>distribute-cpus-across-numa</code> policy
options together at this moment.</p><p>The <code>strict-cpu-reservation</code> option can be enabled by adding <code>strict-cpu-reservation=true</code> to
the CPUManager policy options followed by removing the <code>/var/lib/kubelet/cpu_manager_state</code> file and restart kubelet.</p><p>The <code>prefer-align-cpus-by-uncorecache</code> option can be enabled by adding the
<code>prefer-align-cpus-by-uncorecache</code> to the <code>CPUManager</code> policy options. If
incompatible options are used, the kubelet will fail to start with the error
explained in the logs.</p><p>For mode detail about the behavior of the individual options you can configure, please refer to the
<a href="/docs/concepts/policy/node-resource-managers/">Node ResourceManagers</a> documentation.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure a kubelet image credential provider</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Starting from Kubernetes v1.20, the kubelet can dynamically retrieve credentials for a container image registry
using exec plugins. The kubelet and the exec plugin communicate through stdio (stdin, stdout, and stderr) using
Kubernetes versioned APIs. These plugins allow the kubelet to request credentials for a container registry dynamically
as opposed to storing static credentials on disk. For example, the plugin may talk to a local metadata server to retrieve
short-lived credentials for an image that is being pulled by the kubelet.</p><p>You may be interested in using this capability if any of the below are true:</p><ul><li>API calls to a cloud provider service are required to retrieve authentication information for a registry.</li><li>Credentials have short expiration times and requesting new credentials frequently is required.</li><li>Storing registry credentials on disk or in imagePullSecrets is not acceptable.</li></ul><p>This guide demonstrates how to configure the kubelet's image credential provider plugin mechanism.</p><h2 id="service-account-token-for-image-pulls">Service Account Token for Image Pulls</h2><div class="feature-state-notice feature-beta" title="Feature Gate: KubeletServiceAccountTokenForCredentialProviders"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>Starting from Kubernetes v1.33,
the kubelet can be configured to send a service account token
bound to the pod for which the image pull is being performed
to the credential provider plugin.</p><p>This allows the plugin to exchange the token for credentials
to access the image registry.</p><p>To enable this feature,
the <code>KubeletServiceAccountTokenForCredentialProviders</code> feature gate
must be enabled on the kubelet,
and the <code>tokenAttributes</code> field must be set
in the <code>CredentialProviderConfig</code> file for the plugin.</p><p>The <code>tokenAttributes</code> field contains information
about the service account token that will be passed to the plugin,
including the intended audience for the token
and whether the plugin requires the pod to have a service account.</p><p>Using service account token credentials can enable the following use-cases:</p><ul><li>Avoid needing a kubelet/node-based identity to pull images from a registry.</li><li>Allow workloads to pull images based on their own runtime identity
without long-lived/persisted secrets.</li></ul><h2 id="before-you-begin">Before you begin</h2><ul><li>You need a Kubernetes cluster with nodes that support kubelet credential
provider plugins. This support is available in Kubernetes 1.34;
Kubernetes v1.24 and v1.25 included this as a beta feature, enabled by default.</li><li>If you are configuring a credential provider plugin
that requires the service account token,
you need a Kubernetes cluster with nodes running Kubernetes v1.33 or later
and the <code>KubeletServiceAccountTokenForCredentialProviders</code> feature gate
enabled on the kubelet.</li><li>A working implementation of a credential provider exec plugin. You can build your own plugin or use one provided by cloud providers.</li></ul>Your Kubernetes server must be at or later than version v1.26.<p>To check the version, enter <code>kubectl version</code>.</p><h2 id="installing-plugins-on-nodes">Installing Plugins on Nodes</h2><p>A credential provider plugin is an executable binary that will be run by the kubelet. Ensure that the plugin binary exists on
every node in your cluster and stored in a known directory. The directory will be required later when configuring kubelet flags.</p><h2 id="configuring-the-kubelet">Configuring the Kubelet</h2><p>In order to use this feature, the kubelet expects two flags to be set:</p><ul><li><code>--image-credential-provider-config</code> - the path to the credential provider plugin config file.</li><li><code>--image-credential-provider-bin-dir</code> - the path to the directory where credential provider plugin binaries are located.</li></ul><h3 id="configure-a-kubelet-credential-provider">Configure a kubelet credential provider</h3><p>The configuration file passed into <code>--image-credential-provider-config</code> is read by the kubelet to determine which exec plugins
should be invoked for which container images. Here's an example configuration file you may end up using if you are using the
<a href="https://github.com/kubernetes/cloud-provider-aws/tree/master/cmd/ecr-credential-provider">ECR-based plugin</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>CredentialProviderConfig<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># providers is a list of credential provider helper plugins that will be enabled by the kubelet.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Multiple providers may match against a single image, in which case credentials</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># from all providers will be returned to the kubelet. If multiple providers are called</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># for a single image, the results are combined. If providers return overlapping</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># auth keys, the value from the provider earlier in this list is used.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># name is the required name of the credential provider. It must match the name of the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># provider executable as seen by the kubelet. The executable must be in the kubelet's</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># bin directory (set by the --image-credential-provider-bin-dir flag).</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ecr-credential-provider<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># matchImages is a required list of strings used to match against images in order to</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># determine if this provider should be invoked. If one of the strings matches the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># requested image from the kubelet, the plugin will be invoked and given a chance</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># to provide credentials. Images are expected to contain the registry domain</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># and URL path.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Each entry in matchImages is a pattern which can optionally contain a port and a path.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Globs can be used in the domain, but not in the port or the path. Globs are supported</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># as subdomains like '*.k8s.io' or 'k8s.*.io', and top-level-domains such as 'k8s.*'.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Matching partial subdomains like 'app*.k8s.io' is also supported. Each glob can only match</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># a single subdomain segment, so `*.io` does **not** match `*.k8s.io`.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># A match exists between an image and a matchImage when all of the below are true:</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - Both contain the same number of domain parts and each part matches.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - The URL path of an matchImages must be a prefix of the target image URL path.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - If the matchImages contains a port, then the port must match in the image as well.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Example values of matchImages:</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - 123456789.dkr.ecr.us-east-1.amazonaws.com</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - *.azurecr.io</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - gcr.io</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - *.*.registry.io</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - registry.io:8080/path</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchImages</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"*.dkr.ecr.*.amazonaws.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"*.dkr.ecr.*.amazonaws.com.cn"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"*.dkr.ecr-fips.*.amazonaws.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"*.dkr.ecr.us-iso-east-1.c2s.ic.gov"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># defaultCacheDuration is the default duration the plugin will cache credentials in-memory</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># if a cache duration is not provided in the plugin response. This field is required.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">defaultCacheDuration</span>:<span style="color:#bbb"> </span><span style="color:#b44">"12h"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Required input version of the exec CredentialProviderRequest. The returned CredentialProviderResponse</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># MUST use the same encoding version as the input. Current supported values are:</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - credentialprovider.kubelet.k8s.io/v1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>credentialprovider.kubelet.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Arguments to pass to the command when executing it.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># +optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># args:</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#   - --example-argument</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Env defines additional environment variables to expose to the process. These</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># are unioned with the host's environment, as well as variables client-go uses</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># to pass argument to the plugin.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># +optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>AWS_PROFILE<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>example_profile<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># tokenAttributes is the configuration for the service account token that will be passed to the plugin.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># The credential provider opts in to using service account tokens for image pull by setting this field.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># if this field is set without the `KubeletServiceAccountTokenForCredentialProviders` feature gate enabled, </span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># kubelet will fail to start with invalid configuration error.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># +optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tokenAttributes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># serviceAccountTokenAudience is the intended audience for the projected service account token.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># +required</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">serviceAccountTokenAudience</span>:<span style="color:#bbb"> </span><span style="color:#b44">"&lt;audience for the token&gt;"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># cacheType indicates the type of cache key use for caching the credentials returned by the plugin</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># when the service account token is used.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The most conservative option is to set this to "Token", which means the kubelet will cache</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># returned credentials on a per-token basis. This should be set if the returned credential's</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># lifetime is limited to the service account token's lifetime.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># If the plugin's credential retrieval logic depends only on the service account and not on</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># pod-specific claims, then the plugin can set this to "ServiceAccount". In this case, the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># kubelet will cache returned credentials on a per-serviceaccount basis. Use this when the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># returned credential is valid for all pods using the same service account.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># +required</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cacheType</span>:<span style="color:#bbb"> </span><span style="color:#b44">"&lt;Token or ServiceAccount&gt;"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># requireServiceAccount indicates whether the plugin requires the pod to have a service account.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># If set to true, kubelet will only invoke the plugin if the pod has a service account.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># If set to false, kubelet will invoke the plugin even if the pod does not have a service account</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># and will not include a token in the CredentialProviderRequest. This is useful for plugins</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># that are used to pull images for pods without service accounts (e.g., static pods).</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># +required</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requireServiceAccount</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># requiredServiceAccountAnnotationKeys is the list of annotation keys that the plugin is interested in</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># and that are required to be present in the service account.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The keys defined in this list will be extracted from the corresponding service account and passed</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># to the plugin as part of the CredentialProviderRequest. If any of the keys defined in this list</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># are not present in the service account, kubelet will not invoke the plugin and will return an error.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># This field is optional and may be empty. Plugins may use this field to extract additional information</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># required to fetch credentials or allow workloads to opt in to using service account tokens for image pull.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># If non-empty, requireServiceAccount must be set to true.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The keys defined in this list must be unique and not overlap with the keys defined in the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># optionalServiceAccountAnnotationKeys list.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># +optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredServiceAccountAnnotationKeys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"example.com/required-annotation-key-1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"example.com/required-annotation-key-2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># optionalServiceAccountAnnotationKeys is the list of annotation keys that the plugin is interested in</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># and that are optional to be present in the service account.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The keys defined in this list will be extracted from the corresponding service account and passed</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># to the plugin as part of the CredentialProviderRequest. The plugin is responsible for validating the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># existence of annotations and their values. This field is optional and may be empty.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Plugins may use this field to extract additional information required to fetch credentials.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The keys defined in this list must be unique and not overlap with the keys defined in the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># requiredServiceAccountAnnotationKeys list.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># +optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">optionalServiceAccountAnnotationKeys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"example.com/optional-annotation-key-1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"example.com/optional-annotation-key-2"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>providers</code> field is a list of enabled plugins used by the kubelet. Each entry has a few required fields:</p><ul><li><code>name</code>: the name of the plugin which MUST match the name of the executable binary that exists
in the directory passed into <code>--image-credential-provider-bin-dir</code>.</li><li><code>matchImages</code>: a list of strings used to match against images in order to determine
if this provider should be invoked. More on this below.</li><li><code>defaultCacheDuration</code>: the default duration the kubelet will cache credentials in-memory
if a cache duration was not specified by the plugin.</li><li><code>apiVersion</code>: the API version that the kubelet and the exec plugin will use when communicating.</li></ul><p>Each credential provider can also be given optional args and environment variables as well.
Consult the plugin implementors to determine what set of arguments and environment variables are required for a given plugin.</p><p>If you are using the KubeletServiceAccountTokenForCredentialProviders feature gate
and configuring the plugin to use the service account token
by setting the tokenAttributes field,
the following fields are required:</p><ul><li><code>serviceAccountTokenAudience</code>:
the intended audience for the projected service account token.
This cannot be the empty string.</li><li><code>cacheType</code>:
the type of cache key used for caching the credentials returned by the plugin
when the service account token is used.
The most conservative option is to set this to <code>Token</code>,
which means the kubelet will cache returned credentials
on a per-token basis.
This should be set if the returned credential's lifetime
is limited to the service account token's lifetime.
If the plugin's credential retrieval logic depends only on the service account
and not on pod-specific claims,
then the plugin can set this to <code>ServiceAccount</code>.
In this case, the kubelet will cache returned credentials
on a per-service account basis.
Use this when the returned credential is valid for all pods using the same service account.</li><li><code>requireServiceAccount</code>:
whether the plugin requires the pod to have a service account.<ul><li>If set to <code>true</code>, kubelet will only invoke the plugin
if the pod has a service account.</li><li>If set to <code>false</code>, kubelet will invoke the plugin
even if the pod does not have a service account
and will not include a token in the <code>CredentialProviderRequest</code>.</li></ul></li></ul><p>This is useful for plugins that are used
to pull images for pods without service accounts
(e.g., static pods).</p><h4 id="configure-image-matching">Configure image matching</h4><p>The <code>matchImages</code> field for each credential provider is used by the kubelet to determine whether a plugin should be invoked
for a given image that a Pod is using. Each entry in <code>matchImages</code> is an image pattern which can optionally contain a port and a path.
Globs can be used in the domain, but not in the port or the path. Globs are supported as subdomains like <code>*.k8s.io</code> or <code>k8s.*.io</code>,
and top-level domains such as <code>k8s.*</code>. Matching partial subdomains like <code>app*.k8s.io</code> is also supported. Each glob can only match
a single subdomain segment, so <code>*.io</code> does NOT match <code>*.k8s.io</code>.</p><p>A match exists between an image name and a <code>matchImage</code> entry when all of the below are true:</p><ul><li>Both contain the same number of domain parts and each part matches.</li><li>The URL path of match image must be a prefix of the target image URL path.</li><li>If the matchImages contains a port, then the port must match in the image as well.</li></ul><p>Some example values of <code>matchImages</code> patterns are:</p><ul><li><code>123456789.dkr.ecr.us-east-1.amazonaws.com</code></li><li><code>*.azurecr.io</code></li><li><code>gcr.io</code></li><li><code>*.*.registry.io</code></li><li><code>foo.registry.io:8080/path</code></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read the details about <code>CredentialProviderConfig</code> in the
<a href="/docs/reference/config-api/kubelet-config.v1/">kubelet configuration API (v1) reference</a>.</li><li>Read the <a href="/docs/reference/config-api/kubelet-credentialprovider.v1/">kubelet credential provider API reference (v1)</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Developing Cloud Controller Manager</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.11 [beta]</code></div><p><p>The cloud-controller-manager is a Kubernetes <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p><p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p><h2 id="background">Background</h2><p>Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the <code>cloud-controller-manager</code> binary allows cloud vendors to evolve independently from the core Kubernetes code.</p><p>The Kubernetes project provides skeleton cloud-controller-manager code with Go interfaces to allow you (or your cloud provider) to plug in your own implementations. This means that a cloud provider can implement a cloud-controller-manager by importing packages from Kubernetes core; each cloudprovider will register their own code by calling <code>cloudprovider.RegisterCloudProvider</code> to update a global variable of available cloud providers.</p><h2 id="developing">Developing</h2><h3 id="out-of-tree">Out of tree</h3><p>To build an out-of-tree cloud-controller-manager for your cloud:</p><ol><li>Create a go package with an implementation that satisfies <a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go">cloudprovider.Interface</a>.</li><li>Use <a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go"><code>main.go</code> in cloud-controller-manager</a> from Kubernetes core as a template for your <code>main.go</code>. As mentioned above, the only difference should be the cloud package that will be imported.</li><li>Import your cloud package in <code>main.go</code>, ensure your package has an <code>init</code> block to run <a href="https://github.com/kubernetes/cloud-provider/blob/master/plugins.go"><code>cloudprovider.RegisterCloudProvider</code></a>.</li></ol><p>Many cloud providers publish their controller manager code as open source. If you are creating
a new cloud-controller-manager from scratch, you could take an existing out-of-tree cloud
controller manager as your starting point.</p><h3 id="in-tree">In tree</h3><p>For in-tree cloud providers, you can run the in-tree cloud controller manager as a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/daemonset" target="_blank" aria-label="DaemonSet">DaemonSet</a> in your cluster. See <a href="/docs/tasks/administer-cluster/running-cloud-controller/">Cloud Controller Manager Administration</a> for more details.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Utilizing the NUMA-aware Memory Manager</h1><div class="feature-state-notice feature-stable" title="Feature Gate: MemoryManager"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The Kubernetes <em>Memory Manager</em> enables the feature of guaranteed memory (and hugepages)
allocation for pods in the <code>Guaranteed</code> <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/pod-qos/" target="_blank" aria-label="QoS class">QoS class</a>.</p><p>The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.
The Memory Manager feeds the central manager (<em>Topology Manager</em>) with these affinity hints.
Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.</p><p>Moreover, the Memory Manager ensures that the memory which a pod requests
is allocated from a minimum number of NUMA nodes.</p><p>The Memory Manager is only pertinent to Linux based hosts.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.32.<p>To check the version, enter <code>kubectl version</code>.</p></p><p>To align memory resources with other requested resources in a Pod spec:</p><ul><li>the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node.
See <a href="/docs/tasks/administer-cluster/cpu-management-policies/">control CPU Management Policies</a>;</li><li>the Topology Manager should be enabled and proper Topology Manager policy should be configured on a Node.
See <a href="/docs/tasks/administer-cluster/topology-manager/">control Topology Management Policies</a>.</li></ul><p>Starting from v1.22, the Memory Manager is enabled by default through <code>MemoryManager</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>Preceding v1.22, the <code>kubelet</code> must be started with the following flag:</p><p><code>--feature-gates=MemoryManager=true</code></p><p>in order to enable the Memory Manager feature.</p><h2 id="how-does-the-memory-manager-operate">How does the Memory Manager Operate?</h2><p>The Memory Manager currently offers the guaranteed memory (and hugepages) allocation
for Pods in Guaranteed QoS class.
To immediately put the Memory Manager into operation follow the guidelines in the section
<a href="#memory-manager-configuration">Memory Manager configuration</a>, and subsequently,
prepare and deploy a <code>Guaranteed</code> pod as illustrated in the section
<a href="#placing-a-pod-in-the-guaranteed-qos-class">Placing a Pod in the Guaranteed QoS class</a>.</p><p>The Memory Manager is a Hint Provider, and it provides topology hints for
the Topology Manager which then aligns the requested resources according to these topology hints.
On Linux, it also enforces <code>cgroups</code> (i.e. <code>cpuset.mems</code>) for pods.
The complete flow diagram concerning pod admission and deployment process is illustrated in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview">Memory Manager KEP: Design Overview</a> and below:</p><p><img alt="Memory Manager in the pod admission and deployment process" src="/images/docs/memory-manager-diagram.svg"/></p><p>During this process, the Memory Manager updates its internal counters stored in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Node Map and Memory Maps</a> to manage guaranteed memory allocation.</p><p>The Memory Manager updates the Node Map during the startup and runtime as follows.</p><h3 id="startup">Startup</h3><p>This occurs once a node administrator employs <code>--reserved-memory</code> (section
<a href="#reserved-memory-flag">Reserved memory flag</a>).
In this case, the Node Map becomes updated to reflect this reservation as illustrated in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples">Memory Manager KEP: Memory Maps at start-up (with examples)</a>.</p><p>The administrator must provide <code>--reserved-memory</code> flag when <code>Static</code> policy is configured.</p><h3 id="runtime">Runtime</h3><p>Reference <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples">Memory Manager KEP: Memory Maps at runtime (with examples)</a> illustrates
how a successful pod deployment affects the Node Map, and it also relates to
how potential Out-of-Memory (OOM) situations are handled further by Kubernetes or operating system.</p><p>Important topic in the context of Memory Manager operation is the management of NUMA groups.
Each time pod's memory request is in excess of single NUMA node capacity, the Memory Manager
attempts to create a group that comprises several NUMA nodes and features extend memory capacity.
The problem has been solved as elaborated in
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a>.
Also, reference <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a>
illustrates how the management of groups occurs.</p><h3 id="windows-support">Windows Support</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>Windows support can be enabled via the <code>WindowsCPUAndMemoryAffinity</code> feature gate
and it requires support in the container runtime.
Only the <a href="#policy-best-effort">BestEffort Policy</a> is supported on Windows.</p><h2 id="memory-manager-configuration">Memory Manager configuration</h2><p>Other Managers should be first pre-configured. Next, the Memory Manager feature should be enabled
and be run with <code>Static</code> policy (section <a href="#policy-static">Static policy</a>).
Optionally, some amount of memory can be reserved for system or kubelet processes to increase
node stability (section <a href="#reserved-memory-flag">Reserved memory flag</a>).</p><h3 id="policies">Policies</h3><p>Memory Manager supports two policies. You can select a policy via a <code>kubelet</code> flag <code>--memory-manager-policy</code>:</p><ul><li><code>None</code> (default)</li><li><code>Static</code> (Linux only)</li><li><code>BestEffort</code> (Windows Only)</li></ul><h4 id="policy-none">None policy</h4><p>This is the default policy and does not affect the memory allocation in any way.
It acts the same as if the Memory Manager is not present at all.</p><p>The <code>None</code> policy returns default topology hint. This special hint denotes that Hint Provider
(Memory Manager in this case) has no preference for NUMA affinity with any resource.</p><h4 id="policy-static">Static policy</h4><p>In the case of the <code>Guaranteed</code> pod, the <code>Static</code> Memory Manager policy returns topology hints
relating to the set of NUMA nodes where the memory can be guaranteed,
and reserves the memory through updating the internal <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">NodeMap</a> object.</p><p>In the case of the <code>BestEffort</code> or <code>Burstable</code> pod, the <code>Static</code> Memory Manager policy sends back
the default topology hint as there is no request for the guaranteed memory,
and does not reserve the memory in the internal <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">NodeMap</a> object.</p><p>This policy is only supported on Linux.</p><h4 id="policy-best-effort">BestEffort policy</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: WindowsCPUAndMemoryAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>This policy is only supported on Windows.</p><p>On Windows, NUMA node assignment works differently than Linux.
There is no mechanism to ensure that Memory access only comes from a specific NUMA node.
Instead the Windows scheduler will select the most optimal NUMA node based on the CPU(s) assignments.
It is possible that Windows might use other NUMA nodes if deemed optimal by the Windows scheduler.</p><p>The policy does track the amount of memory available and requested through the internal <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">NodeMap</a>.
The memory manager will make a best effort at ensuring that enough memory is available on
a NUMA node before making the assignment.<br/>This means that in most cases memory assignment should function as expected.</p><h3 id="reserved-memory-flag">Reserved memory flag</h3><p>The <a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Node Allocatable</a> mechanism
is commonly used by node administrators to reserve K8S node system resources for the kubelet
or operating system processes in order to enhance the node stability.
A dedicated set of flags can be used for this purpose to set the total amount of reserved memory
for a node. This pre-configured value is subsequently utilized to calculate
the real amount of node's "allocatable" memory available to pods.</p><p>The Kubernetes scheduler incorporates "allocatable" to optimise pod scheduling process.
The foregoing flags include <code>--kube-reserved</code>, <code>--system-reserved</code> and <code>--eviction-threshold</code>.
The sum of their values will account for the total amount of reserved memory.</p><p>A new <code>--reserved-memory</code> flag was added to Memory Manager to allow for this total reserved memory
to be split (by a node administrator) and accordingly reserved across many NUMA nodes.</p><p>The flag specifies a comma-separated list of memory reservations of different memory types per NUMA node.
Memory reservations across multiple NUMA nodes can be specified using semicolon as separator.
This parameter is only useful in the context of the Memory Manager feature.
The Memory Manager will not use this reserved memory for the allocation of container workloads.</p><p>For example, if you have a NUMA node "NUMA0" with <code>10Gi</code> of memory available, and
the <code>--reserved-memory</code> was specified to reserve <code>1Gi</code> of memory at "NUMA0",
the Memory Manager assumes that only <code>9Gi</code> is available for containers.</p><p>You can omit this parameter, however, you should be aware that the quantity of reserved memory
from all NUMA nodes should be equal to the quantity of memory specified by the
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Node Allocatable feature</a>.
If at least one node allocatable parameter is non-zero, you will need to specify
<code>--reserved-memory</code> for at least one NUMA node.
In fact, <code>eviction-hard</code> threshold value is equal to <code>100Mi</code> by default, so
if <code>Static</code> policy is used, <code>--reserved-memory</code> is obligatory.</p><p>Also, avoid the following configurations:</p><ol><li>duplicates, i.e. the same NUMA node or memory type, but with a different value;</li><li>setting zero limit for any of memory types;</li><li>NUMA node IDs that do not exist in the machine hardware;</li><li>memory type names different than <code>memory</code> or <code>hugepages-&lt;size&gt;</code>
(hugepages of particular <code>&lt;size&gt;</code> should also exist).</li></ol><p>Syntax:</p><p><code>--reserved-memory N:memory-type1=value1,memory-type2=value2,...</code></p><ul><li><code>N</code> (integer) - NUMA node index, e.g. <code>0</code></li><li><code>memory-type</code> (string) - represents memory type:<ul><li><code>memory</code> - conventional memory</li><li><code>hugepages-2Mi</code> or <code>hugepages-1Gi</code> - hugepages</li></ul></li><li><code>value</code> (string) - the quantity of reserved memory, e.g. <code>1Gi</code></li></ul><p>Example usage:</p><p><code>--reserved-memory 0:memory=1Gi,hugepages-1Gi=2Gi</code></p><p>or</p><p><code>--reserved-memory 0:memory=1Gi --reserved-memory 1:memory=2Gi</code></p><p>or</p><p><code>--reserved-memory '0:memory=1Gi;1:memory=2Gi'</code></p><p>When you specify values for <code>--reserved-memory</code> flag, you must comply with the setting that
you prior provided via Node Allocatable Feature flags.
That is, the following rule must be obeyed for each memory type:</p><p><code>sum(reserved-memory(i)) = kube-reserved + system-reserved + eviction-threshold</code>,</p><p>where <code>i</code> is an index of a NUMA node.</p><p>If you do not follow the formula above, the Memory Manager will show an error on startup.</p><p>In other words, the example above illustrates that for the conventional memory (<code>type=memory</code>),
we reserve <code>3Gi</code> in total, i.e.:</p><p><code>sum(reserved-memory(i)) = reserved-memory(0) + reserved-memory(1) = 1Gi + 2Gi = 3Gi</code></p><p>An example of kubelet command-line arguments relevant to the node Allocatable configuration:</p><ul><li><code>--kube-reserved=cpu=500m,memory=50Mi</code></li><li><code>--system-reserved=cpu=123m,memory=333Mi</code></li><li><code>--eviction-hard=memory.available&lt;500Mi</code></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The default hard eviction threshold is 100MiB, and <strong>not</strong> zero.
Remember to increase the quantity of memory that you reserve by setting <code>--reserved-memory</code>
by that hard eviction threshold. Otherwise, the kubelet will not start Memory Manager and
display an error.</div><p>Here is an example of a correct configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>--kube-reserved<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>4,memory<span style="color:#666">=</span>4Gi
</span></span><span style="display:flex"><span>--system-reserved<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>1,memory<span style="color:#666">=</span>1Gi
</span></span><span style="display:flex"><span>--memory-manager-policy<span style="color:#666">=</span>Static
</span></span><span style="display:flex"><span>--reserved-memory <span style="color:#b44">'0:memory=3Gi;1:memory=2148Mi'</span>
</span></span></code></pre></div><p>Prior to Kubernetes 1.32, you also need to add</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>--feature-gates<span style="color:#666">=</span><span style="color:#b8860b">MemoryManager</span><span style="color:#666">=</span><span style="color:#a2f">true</span>
</span></span></code></pre></div><p>Let us validate the configuration above:</p><ol><li><code>kube-reserved + system-reserved + eviction-hard(default) = reserved-memory(0) + reserved-memory(1)</code></li><li><code>4GiB + 1GiB + 100MiB = 3GiB + 2148MiB</code></li><li><code>5120MiB + 100MiB = 3072MiB + 2148MiB</code></li><li><code>5220MiB = 5220MiB</code> (which is correct)</li></ol><h2 id="placing-a-pod-in-the-guaranteed-qos-class">Placing a Pod in the Guaranteed QoS class</h2><p>If the selected policy is anything other than <code>None</code>, the Memory Manager identifies pods
that are in the <code>Guaranteed</code> QoS class.
The Memory Manager provides specific topology hints to the Topology Manager for each <code>Guaranteed</code> pod.
For pods in a QoS class other than <code>Guaranteed</code>, the Memory Manager provides default topology hints
to the Topology Manager.</p><p>The following excerpts from pod manifests assign a pod to the <code>Guaranteed</code> QoS class.</p><p>Pod with integer CPU(s) runs in the <code>Guaranteed</code> QoS class, when <code>requests</code> are equal to <code>limits</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Also, a pod sharing CPU(s) runs in the <code>Guaranteed</code> QoS class, when <code>requests</code> are equal to <code>limits</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"300m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"300m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Notice that both CPU and memory requests must be specified for a Pod to lend it to Guaranteed QoS class.</p><h2 id="troubleshooting">Troubleshooting</h2><p>The following means can be used to troubleshoot the reason why a pod could not be deployed or
became rejected at a node:</p><ul><li>pod status - indicates topology affinity errors</li><li>system logs - include valuable information for debugging, e.g., about generated hints</li><li>state file - the dump of internal state of the Memory Manager
(includes <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Node Map and Memory Maps</a>)</li><li>starting from v1.22, the <a href="#device-plugin-resource-api">device plugin resource API</a> can be used
to retrieve information about the memory reserved for containers</li></ul><h3 id="TopologyAffinityError">Pod status (TopologyAffinityError)</h3><p>This error typically occurs in the following situations:</p><ul><li>a node has not enough resources available to satisfy the pod's request</li><li>the pod's request is rejected due to particular Topology Manager policy constraints</li></ul><p>The error appears in the status of a pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME         READY   STATUS                  RESTARTS   AGE
guaranteed   0/1     TopologyAffinityError   0          113s
</code></pre><p>Use <code>kubectl describe pod &lt;id&gt;</code> or <code>kubectl get events</code> to obtain detailed error message:</p><pre tabindex="0"><code class="language-none" data-lang="none">Warning  TopologyAffinityError  10m   kubelet, dell8  Resources cannot be allocated with Topology locality
</code></pre><h3 id="system-logs">System logs</h3><p>Search system logs with respect to a particular pod.</p><p>The set of hints that Memory Manager generated for the pod can be found in the logs.
Also, the set of hints generated by CPU Manager should be present in the logs.</p><p>Topology Manager merges these hints to calculate a single best hint.
The best hint should be also present in the logs.</p><p>The best hint indicates where to allocate all the resources.
Topology Manager tests this hint against its current policy, and based on the verdict,
it either admits the pod to the node or rejects it.</p><p>Also, search the logs for occurrences associated with the Memory Manager,
e.g. to find out information about <code>cgroups</code> and <code>cpuset.mems</code> updates.</p><h3 id="examine-the-memory-manager-state-on-a-node">Examine the memory manager state on a node</h3><p>Let us first deploy a sample <code>Guaranteed</code> pod whose specification is as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>guaranteed<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>guaranteed<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>consumer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagePullPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>150Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>150Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#b44">"infinity"</span>]<span style="color:#bbb">
</span></span></span></code></pre></div><p>Next, let us log into the node where it was deployed and examine the state file in
<code>/var/lib/kubelet/memory_manager_state</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"policyName"</span>:<span style="color:#b44">"Static"</span>,
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"machineState"</span>:{
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"0"</span>:{
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"numberOfAssignments"</span>:<span style="color:#666">1</span>,
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"memoryMap"</span>:{
</span></span><span style="display:flex"><span>            <span style="color:green;font-weight:700">"hugepages-1Gi"</span>:{
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"total"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"systemReserved"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"allocatable"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"reserved"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"free"</span>:<span style="color:#666">0</span>
</span></span><span style="display:flex"><span>            },
</span></span><span style="display:flex"><span>            <span style="color:green;font-weight:700">"memory"</span>:{
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"total"</span>:<span style="color:#666">134987354112</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"systemReserved"</span>:<span style="color:#666">3221225472</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"allocatable"</span>:<span style="color:#666">131766128640</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"reserved"</span>:<span style="color:#666">131766128640</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"free"</span>:<span style="color:#666">0</span>
</span></span><span style="display:flex"><span>            }
</span></span><span style="display:flex"><span>         },
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"nodes"</span>:[
</span></span><span style="display:flex"><span>            <span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>            <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>         ]
</span></span><span style="display:flex"><span>      },
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"1"</span>:{
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"numberOfAssignments"</span>:<span style="color:#666">1</span>,
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"memoryMap"</span>:{
</span></span><span style="display:flex"><span>            <span style="color:green;font-weight:700">"hugepages-1Gi"</span>:{
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"total"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"systemReserved"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"allocatable"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"reserved"</span>:<span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"free"</span>:<span style="color:#666">0</span>
</span></span><span style="display:flex"><span>            },
</span></span><span style="display:flex"><span>            <span style="color:green;font-weight:700">"memory"</span>:{
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"total"</span>:<span style="color:#666">135286722560</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"systemReserved"</span>:<span style="color:#666">2252341248</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"allocatable"</span>:<span style="color:#666">133034381312</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"reserved"</span>:<span style="color:#666">29295144960</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"free"</span>:<span style="color:#666">103739236352</span>
</span></span><span style="display:flex"><span>            }
</span></span><span style="display:flex"><span>         },
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"nodes"</span>:[
</span></span><span style="display:flex"><span>            <span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>            <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>         ]
</span></span><span style="display:flex"><span>      }
</span></span><span style="display:flex"><span>   },
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"entries"</span>:{
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"fa9bdd38-6df9-4cf9-aa67-8c4814da37a8"</span>:{
</span></span><span style="display:flex"><span>         <span style="color:green;font-weight:700">"guaranteed"</span>:[
</span></span><span style="display:flex"><span>            {
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"numaAffinity"</span>:[
</span></span><span style="display:flex"><span>                  <span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>                  <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>               ],
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"type"</span>:<span style="color:#b44">"memory"</span>,
</span></span><span style="display:flex"><span>               <span style="color:green;font-weight:700">"size"</span>:<span style="color:#666">161061273600</span>
</span></span><span style="display:flex"><span>            }
</span></span><span style="display:flex"><span>         ]
</span></span><span style="display:flex"><span>      }
</span></span><span style="display:flex"><span>   },
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"checksum"</span>:<span style="color:#666">4142013182</span>
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>It can be deduced from the state file that the pod was pinned to both NUMA nodes, i.e.:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span><span style="color:#b44">"numaAffinity"</span><span>:</span>[
</span></span><span style="display:flex"><span>   <span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>   <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>]<span>,</span>
</span></span></code></pre></div><p>Pinned term means that pod's memory consumption is constrained (through <code>cgroups</code> configuration)
to these NUMA nodes.</p><p>This automatically implies that Memory Manager instantiated a new group that
comprises these two NUMA nodes, i.e. <code>0</code> and <code>1</code> indexed NUMA nodes.</p><p>Notice that the management of groups is handled in a relatively complex manner, and
further elaboration is provided in Memory Manager KEP in <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">this</a> and <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">this</a> sections.</p><p>In order to analyse memory resources available in a group,the corresponding entries from
NUMA nodes belonging to the group must be added up.</p><p>For example, the total amount of free "conventional" memory in the group can be computed
by adding up the free memory available at every NUMA node in the group,
i.e., in the <code>"memory"</code> section of NUMA node <code>0</code> (<code>"free":0</code>) and NUMA node <code>1</code> (<code>"free":103739236352</code>).
So, the total amount of free "conventional" memory in this group is equal to <code>0 + 103739236352</code> bytes.</p><p>The line <code>"systemReserved":3221225472</code> indicates that the administrator of this node reserved
<code>3221225472</code> bytes (i.e. <code>3Gi</code>) to serve kubelet and system processes at NUMA node <code>0</code>,
by using <code>--reserved-memory</code> flag.</p><h3 id="device-plugin-resource-api">Device plugin resource API</h3><p>The kubelet provides a <code>PodResourceLister</code> gRPC service to enable discovery of resources and associated metadata.
By using its <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#grpc-endpoint-list">List gRPC endpoint</a>,
information about reserved memory for each container can be retrieved, which is contained
in protobuf <code>ContainerMemory</code> message.
This information can be retrieved solely for pods in Guaranteed QoS class.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview">Memory Manager KEP: Design Overview</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples">Memory Manager KEP: Memory Maps at start-up (with examples)</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples">Memory Manager KEP: Memory Maps at runtime (with examples)</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Memory Manager KEP: The Concept of Node Map and Memory Maps</a></li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Overprovision Node Capacity For A Cluster</h1><p>This page guides you through configuring <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Node">Node</a>
overprovisioning in your Kubernetes cluster. Node overprovisioning is a strategy that proactively
reserves a portion of your cluster's compute resources. This reservation helps reduce the time
required to schedule new pods during scaling events, enhancing your cluster's responsiveness
to sudden spikes in traffic or workload demands.</p><p>By maintaining some unused capacity, you ensure that resources are immediately available when
new pods are created, preventing them from entering a pending state while the cluster scales up.</p><h2 id="before-you-begin">Before you begin</h2><ul><li>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with
your cluster.</li><li>You should already have a basic understanding of
<a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>,
Pod <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank" aria-label="priority">priority</a>,
and <a class="glossary-tooltip" title="A mapping from a class name to the scheduling priority that a Pod should have." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass" target="_blank" aria-label="PriorityClasses">PriorityClasses</a>.</li><li>Your cluster must be set up with an <a href="/docs/concepts/cluster-administration/cluster-autoscaling/">autoscaler</a>
that manages nodes based on demand.</li></ul><h2 id="create-a-priorityclass">Create a PriorityClass</h2><p>Begin by defining a PriorityClass for the placeholder Pods. First, create a PriorityClass with a
negative priority value, that you will shortly assign to the placeholder pods.
Later, you will set up a Deployment that uses this PriorityClass</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priorityclass/low-priority-class.yaml" download="priorityclass/low-priority-class.yaml"><code>priorityclass/low-priority-class.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;priorityclass-low-priority-class-yaml&quot;)" title="Copy priorityclass/low-priority-class.yaml to clipboard"/></div><div class="includecode" id="priorityclass-low-priority-class-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>scheduling.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>placeholder<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># these Pods represent placeholder capacity</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>-<span style="color:#666">1000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">globalDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Negative priority for placeholder pods to enable overprovisioning."</span></span></span></code></pre></div></div></div><p>Then create the PriorityClass:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/priorityclass/low-priority-class.yaml
</span></span></code></pre></div><p>You will next define a Deployment that uses the negative-priority PriorityClass and runs a minimal container.
When you add this to your cluster, Kubernetes runs those placeholder pods to reserve capacity. Any time there
is a capacity shortage, the control plane will pick one these placeholder pods as the first candidate to
<a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank" aria-label="preempt">preempt</a>.</p><h2 id="run-pods-that-request-node-capacity">Run Pods that request node capacity</h2><p>Review the sample manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/deployments/deployment-with-capacity-reservation.yaml" download="deployments/deployment-with-capacity-reservation.yaml"><code>deployments/deployment-with-capacity-reservation.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;deployments-deployment-with-capacity-reservation-yaml&quot;)" title="Copy deployments/deployment-with-capacity-reservation.yaml to clipboard"/></div><div class="includecode" id="deployments-deployment-with-capacity-reservation-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>capacity-reservation<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># You should decide what namespace to deploy this into</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>capacity-placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>capacity-placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">kubernetes.io/description</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Capacity reservation"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">priorityClassName</span>:<span style="color:#bbb"> </span>placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Try to place these overhead Pods on different nodes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:#080;font-style:italic"># if possible</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAntiAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">podAffinityTerm</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                  </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>capacity-placeholder<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/hostname<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.6<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"50m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"512Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"512Mi"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h3 id="pick-a-namespace-for-the-placeholder-pods">Pick a namespace for the placeholder pods</h3><p>You should select, or create, a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>
that the placeholder Pods will go into.</p><h3 id="create-the-placeholder-deployment">Create the placeholder deployment</h3><p>Create a Deployment based on that manifest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Change the namespace name "example"</span>
</span></span><span style="display:flex"><span>kubectl --namespace example apply -f https://k8s.io/examples/deployments/deployment-with-capacity-reservation.yaml
</span></span></code></pre></div><h2 id="adjust-placeholder-resource-requests">Adjust placeholder resource requests</h2><p>Configure the resource requests and limits for the placeholder pods to define the amount of overprovisioned resources you want to maintain. This reservation ensures that a specific amount of CPU and memory is kept available for new pods.</p><p>To edit the Deployment, modify the <code>resources</code> section in the Deployment manifest file
to set appropriate requests and limits. You can download that file locally and then edit it
with whichever text editor you prefer.</p><p>You can also edit the Deployment using kubectl:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit deployment capacity-reservation
</span></span></code></pre></div><p>For example, to reserve a total of a 0.5 CPU and 1GiB of memory across 5 placeholder pods,
define the resource requests and limits for a single placeholder pod as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100m"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="set-the-desired-replica-count">Set the desired replica count</h2><h3 id="calculate-the-total-reserved-resources">Calculate the total reserved resources</h3><p>For example, with 5 replicas each reserving 0.1 CPU and 200MiB of memory:<br/>Total CPU reserved: 5 × 0.1 = 0.5 (in the Pod specification, you'll write the quantity <code>500m</code>)<br/>Total memory reserved: 5 × 200MiB = 1GiB (in the Pod specification, you'll write <code>1 Gi</code>)</p><p>To scale the Deployment, adjust the number of replicas based on your cluster's size and expected workload:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale deployment capacity-reservation --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
</span></span></code></pre></div><p>Verify the scaling:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment capacity-reservation
</span></span></code></pre></div><p>The output should reflect the updated number of replicas:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
capacity-reservation   5/5     5            5           2m
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Some autoscalers, notably <a href="/docs/concepts/cluster-administration/cluster-autoscaling/#autoscaler-karpenter">Karpenter</a>,
treat preferred affinity rules as hard rules when considering node scaling.
If you use Karpenter or another node autoscaler that uses the same heuristic,
the replica count you set here also sets a minimum node count for your cluster.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClasses</a> and how they affect pod scheduling.</li><li>Explore <a href="/docs/concepts/cluster-administration/cluster-autoscaling/">node autoscaling</a> to dynamically adjust your cluster's size based on workload demands.</li><li>Understand <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod preemption</a>, a
key mechanism for Kubernetes to handle resource contention. The same page covers <em>eviction</em>,
which is less relevant to the placeholder Pod approach, but is also a mechanism for Kubernetes
to react when resources are contended.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Administration with kubeadm</h1><p>If you don't yet have a cluster, visit
<a href="/docs/setup/production-environment/tools/kubeadm/">bootstrapping clusters with <code>kubeadm</code></a>.</p><p>The tasks in this section are aimed at people administering an existing cluster:</p><div class="section-index"><ul><li><a href="/docs/tasks/administer-cluster/kubeadm/adding-linux-nodes/">Adding Linux worker nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/">Adding Windows worker nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrading Linux nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrading Windows nodes</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/">Configuring a cgroup driver</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">Certificate Management with kubeadm</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/">Reconfiguring a kubeadm cluster</a></li><li><a href="/docs/tasks/administer-cluster/kubeadm/change-package-repository/">Changing The Kubernetes Package Repository</a></li></ul></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use Port Forwarding to Access Applications in a Cluster</h1><p>This page shows how to use <code>kubectl port-forward</code> to connect to a MongoDB
server running in a Kubernetes cluster. This type of connection can be useful
for database debugging.</p><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.10.<p>To check the version, enter <code>kubectl version</code>.</p></li><li>Install <a href="https://www.mongodb.com/try/download/shell">MongoDB Shell</a>.</li></ul><h2 id="creating-mongodb-deployment-and-service">Creating MongoDB deployment and service</h2><ol><li><p>Create a Deployment that runs MongoDB:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-deployment.yaml
</span></span></code></pre></div><p>The output of a successful command verifies that the deployment was created:</p><pre tabindex="0"><code>deployment.apps/mongo created
</code></pre><p>View the pod status to check that it is ready:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><p>The output displays the pod created:</p><pre tabindex="0"><code>NAME                     READY   STATUS    RESTARTS   AGE
mongo-75f59d57f4-4nd6q   1/1     Running   0          2m4s
</code></pre><p>View the Deployment's status:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment
</span></span></code></pre></div><p>The output displays that the Deployment was created:</p><pre tabindex="0"><code>NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mongo   1/1     1            1           2m21s
</code></pre><p>The Deployment automatically manages a ReplicaSet.
View the ReplicaSet status using:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get replicaset
</span></span></code></pre></div><p>The output displays that the ReplicaSet was created:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY   AGE
mongo-75f59d57f4   1         1         1       3m12s
</code></pre></li><li><p>Create a Service to expose MongoDB on the network:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-service.yaml
</span></span></code></pre></div><p>The output of a successful command verifies that the Service was created:</p><pre tabindex="0"><code>service/mongo created
</code></pre><p>Check the Service created:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get service mongo
</span></span></code></pre></div><p>The output displays the service created:</p><pre tabindex="0"><code>NAME    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
mongo   ClusterIP   10.96.41.183   &lt;none&gt;        27017/TCP   11s
</code></pre></li><li><p>Verify that the MongoDB server is running in the Pod, and listening on port 27017:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Change mongo-75f59d57f4-4nd6q to the name of the Pod</span>
</span></span><span style="display:flex"><span>kubectl get pod mongo-75f59d57f4-4nd6q --template<span style="color:#666">=</span><span style="color:#b44">'{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}'</span>
</span></span></code></pre></div><p>The output displays the port for MongoDB in that Pod:</p><pre tabindex="0"><code>27017
</code></pre><p>27017 is the official TCP port for MongoDB.</p></li></ol><h2 id="forward-a-local-port-to-a-port-on-the-pod">Forward a local port to a port on the Pod</h2><ol><li><p><code>kubectl port-forward</code> allows using resource name, such as a pod name, to select a matching pod to port forward to.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Change mongo-75f59d57f4-4nd6q to the name of the Pod</span>
</span></span><span style="display:flex"><span>kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017
</span></span></code></pre></div><p>which is the same as</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl port-forward deployment/mongo 28015:27017
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl port-forward replicaset/mongo-75f59d57f4 28015:27017
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl port-forward service/mongo 28015:27017
</span></span></code></pre></div><p>Any of the above commands works. The output is similar to this:</p><pre tabindex="0"><code>Forwarding from 127.0.0.1:28015 -&gt; 27017
Forwarding from [::1]:28015 -&gt; 27017
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>kubectl port-forward</code> does not return. To continue with the exercises, you will need to open another terminal.</div></li><li><p>Start the MongoDB command line interface:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>mongosh --port <span style="color:#666">28015</span>
</span></span></code></pre></div></li><li><p>At the MongoDB command line prompt, enter the <code>ping</code> command:</p><pre tabindex="0"><code>db.runCommand( { ping: 1 } )
</code></pre><p>A successful ping request returns:</p><pre tabindex="0"><code>{ ok: 1 }
</code></pre></li></ol><h3 id="let-kubectl-choose-local-port">Optionally let <em>kubectl</em> choose the local port</h3><p>If you don't need a specific local port, you can let <code>kubectl</code> choose and allocate
the local port and thus relieve you from having to manage local port conflicts, with
the slightly simpler syntax:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl port-forward deployment/mongo :27017
</span></span></code></pre></div><p>The <code>kubectl</code> tool finds a local port number that is not in use (avoiding low ports numbers,
because these might be used by other applications). The output is similar to:</p><pre tabindex="0"><code>Forwarding from 127.0.0.1:63753 -&gt; 27017
Forwarding from [::1]:63753 -&gt; 27017
</code></pre><h2 id="discussion">Discussion</h2><p>Connections made to local port 28015 are forwarded to port 27017 of the Pod that
is running the MongoDB server. With this connection in place, you can use your
local workstation to debug the database that is running in the Pod.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>kubectl port-forward</code> is implemented for TCP ports only.
The support for UDP protocol is tracked in
<a href="https://github.com/kubernetes/kubernetes/issues/47862">issue 47862</a>.</div><h2 id="what-s-next">What's next</h2><p>Learn more about <a href="/docs/reference/generated/kubectl/kubectl-commands/#port-forward">kubectl port-forward</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Encrypting Confidential Data at Rest</h1><p>All of the APIs in Kubernetes that let you write persistent API resource data support
at-rest encryption. For example, you can enable at-rest encryption for
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secrets">Secrets</a>.
This at-rest encryption is additional to any system-level encryption for the
etcd cluster or for the filesystem(s) on hosts where you are running the
kube-apiserver.</p><p>This page shows how to enable and configure encryption of API data at rest.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>This task covers encryption for resource data stored using the
<a class="glossary-tooltip" title="The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/kubernetes-api/" target="_blank" aria-label="Kubernetes API">Kubernetes API</a>. For example, you can
encrypt Secret objects, including the key-value data they contain.</p><p>If you want to encrypt data in filesystems that are mounted into containers, you instead need
to either:</p><ul><li>use a storage integration that provides encrypted
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volumes">volumes</a></li><li>encrypt the data within your own application</li></ul></div><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>This task assumes that you are running the Kubernetes API server as a
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static pod">static pod</a> on each control
plane node.</p></li><li><p>Your cluster's control plane <strong>must</strong> use etcd v3.x (major version 3, any minor version).</p></li><li><p>To encrypt a custom resource, your cluster must be running Kubernetes v1.26 or newer.</p></li><li><p>To use a wildcard to match resources, your cluster must be running Kubernetes v1.27 or newer.</p></li></ul><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="determining-whether-encryption-at-rest-is-already-enabled">Determine whether encryption at rest is already enabled</h2><p>By default, the API server stores plain-text representations of resources into etcd, with
no at-rest encryption.</p><p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code>
that specifies a path to a configuration file. The contents of that file, if you specify one,
control how Kubernetes API data is encrypted in etcd.
If you are running the kube-apiserver without the <code>--encryption-provider-config</code> command line
argument, you do not have encryption at rest enabled. If you are running the kube-apiserver
with the <code>--encryption-provider-config</code> command line argument, and the file that it references
specifies the <code>identity</code> provider as the first encryption provider in the list, then you
do not have at-rest encryption enabled
(<strong>the default <code>identity</code> provider does not provide any confidentiality protection.</strong>)</p><p>If you are running the kube-apiserver
with the <code>--encryption-provider-config</code> command line argument, and the file that it references
specifies a provider other than <code>identity</code> as the first encryption provider in the list, then
you already have at-rest encryption enabled. However, that check does not tell you whether
a previous migration to encrypted storage has succeeded. If you are not sure, see
<a href="#ensure-all-secrets-are-encrypted">ensure all relevant data are encrypted</a>.</p><h2 id="understanding-the-encryption-at-rest-configuration">Understanding the encryption at rest configuration</h2><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex;background-color:#dfdfdf"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex;background-color:#dfdfdf"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># CAUTION: this is an example configuration.</span><span style="color:#bbb">
</span></span></span><span style="display:flex;background-color:#dfdfdf"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#          Do not use this for your own cluster!</span><span style="color:#bbb">
</span></span></span><span style="display:flex;background-color:#dfdfdf"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- configmaps<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- pandas.awesome.bears.example<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># a custom resource API</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># This configuration does not provide data confidentiality. The first</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># configured provider is specifying the "identity" mechanism, which</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># stores resources as plain text.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># plain text, in other words NO encryption</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aesgcm</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>dGhpcyBpcyBwYXNzd29yZA==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>dGhpcyBpcyBwYXNzd29yZA==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">secretbox</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- events<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># do not encrypt Events even though *.* is specified below</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">'*.apps'</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># wildcard match requires Kubernetes 1.27 or later</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">'*.*'</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># wildcard match requires Kubernetes 1.27 or later</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==</span></span></code></pre></div><p>Each <code>resources</code> array item is a separate config and contains a complete configuration. The
<code>resources.resources</code> field is an array of Kubernetes resource names (<code>resource</code> or <code>resource.group</code>)
that should be encrypted like Secrets, ConfigMaps, or other resources.</p><p>If custom resources are added to <code>EncryptionConfiguration</code> and the cluster version is 1.26 or newer,
any newly created custom resources mentioned in the <code>EncryptionConfiguration</code> will be encrypted.
Any custom resources that existed in etcd prior to that version and configuration will be unencrypted
until they are next written to storage. This is the same behavior as built-in resources.
See the <a href="#ensure-all-secrets-are-encrypted">Ensure all secrets are encrypted</a> section.</p><p>The <code>providers</code> array is an ordered list of the possible encryption providers to use for the APIs that you listed.
Each provider supports multiple keys - the keys are tried in order for decryption, and if the provider
is the first provider, the first key is used for encryption.</p><p>Only one provider type may be specified per entry (<code>identity</code> or <code>aescbc</code> may be provided,
but not both in the same item).
The first provider in the list is used to encrypt resources written into the storage. When reading
resources from storage, each provider that matches the stored data attempts in order to decrypt the
data. If no provider can read the stored data due to a mismatch in format or secret key, an error
is returned which prevents clients from accessing that resource.</p><p><code>EncryptionConfiguration</code> supports the use of wildcards to specify the resources that should be encrypted.
Use '<code>*.&lt;group&gt;</code>' to encrypt all resources within a group (for eg '<code>*.apps</code>' in above example) or '<code>*.*</code>'
to encrypt all resources. '<code>*.</code>' can be used to encrypt all resource in the core group. '<code>*.*</code>' will
encrypt all resources, even custom resources that are added after API server start.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Use of wildcards that overlap within the same resource list or across multiple entries are not allowed
since part of the configuration would be ineffective. The <code>resources</code> list's processing order and precedence
are determined by the order it's listed in the configuration.</div><p>If you have a wildcard covering resources and want to opt out of at-rest encryption for a particular kind
of resource, you achieve that by adding a separate <code>resources</code> array item with the name of the resource that
you want to exempt, followed by a <code>providers</code> array item where you specify the <code>identity</code> provider. You add
this item to the list so that it appears earlier than the configuration where you do specify encryption
(a provider that is not <code>identity</code>).</p><p>For example, if '<code>*.*</code>' is enabled and you want to opt out of encryption for Events and ConfigMaps, add a
new <strong>earlier</strong> item to the <code>resources</code>, followed by the providers array item with <code>identity</code> as the
provider. The more specific entry must come before the wildcard entry.</p><p>The new item would look similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- configmaps.<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># specifically from the core API group,</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                    </span><span style="color:#080;font-style:italic"># because of trailing "."</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- events<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># and then other entries in resources</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Ensure that the exemption is listed <em>before</em> the wildcard '<code>*.*</code>' item in the resources array
to give it precedence.</p><p>For more detailed information about the <code>EncryptionConfiguration</code> struct, please refer to the
<a href="/docs/reference/config-api/apiserver-config.v1/">encryption configuration API</a>.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>If any resource is not readable via the encryption configuration (because keys were changed),
and you cannot restore a working configuration, your only recourse is to delete that entry from
the underlying etcd directly.</p><p>Any calls to the Kubernetes API that attempt to read that resource will fail until it is deleted
or a valid decryption key is provided.</p></div><h3 id="providers">Available providers</h3><p>Before you configure encryption-at-rest for data in your cluster's Kubernetes API, you
need to select which provider(s) you will use.</p><p>The following table describes each available provider.</p><table class="complex-layout"><caption style="display:none">Providers for Kubernetes encryption at rest</caption><thead><tr><th>Name</th><th>Encryption</th><th>Strength</th><th>Speed</th><th>Key length</th></tr></thead><tbody id="encryption-providers-identity"><tr><th rowspan="2" scope="row"><tt>identity</tt></th><td><strong>None</strong></td><td>N/A</td><td>N/A</td><td>N/A</td></tr><tr><td colspan="4">Resources written as-is without encryption. When set as the first provider, the resource will be decrypted as new values are written. Existing encrypted resources are <strong>not</strong> automatically overwritten with the plaintext data.
The <tt>identity</tt> provider is the default if you do not specify otherwise.</td></tr></tbody><tbody id="encryption-providers-that-encrypt"><tr><th rowspan="2" scope="row"><tt>aescbc</tt></th><td>AES-CBC with <a href="https://datatracker.ietf.org/doc/html/rfc2315">PKCS#7</a> padding</td><td>Weak</td><td>Fast</td><td>16, 24, or 32-byte</td></tr><tr><td colspan="4">Not recommended due to CBC's vulnerability to padding oracle attacks. Key material accessible from control plane host.</td></tr><tr><th rowspan="2" scope="row"><tt>aesgcm</tt></th><td>AES-GCM with random nonce</td><td>Must be rotated every 200,000 writes</td><td>Fastest</td><td>16, 24, or 32-byte</td></tr><tr><td colspan="4">Not recommended for use except when an automated key rotation scheme is implemented. Key material accessible from control plane host.</td></tr><tr><th rowspan="2" scope="row"><tt>kms</tt> v1 <em>(deprecated since Kubernetes v1.28)</em></th><td>Uses envelope encryption scheme with DEK per resource.</td><td>Strongest</td><td>Slow (<em>compared to <tt>kms</tt> version 2</em>)</td><td>32-bytes</td></tr><tr><td colspan="4">Data is encrypted by data encryption keys (DEKs) using AES-GCM;
DEKs are encrypted by key encryption keys (KEKs) according to
configuration in Key Management Service (KMS).
Simple key rotation, with a new DEK generated for each encryption, and
KEK rotation controlled by the user.<br/>Read how to <a href="/docs/tasks/administer-cluster/kms-provider#configuring-the-kms-provider-kms-v1">configure the KMS V1 provider</a>.</td></tr><tr><th rowspan="2" scope="row"><tt>kms</tt> v2</th><td>Uses envelope encryption scheme with DEK per API server.</td><td>Strongest</td><td>Fast</td><td>32-bytes</td></tr><tr><td colspan="4">Data is encrypted by data encryption keys (DEKs) using AES-GCM; DEKs
are encrypted by key encryption keys (KEKs) according to configuration
in Key Management Service (KMS).
Kubernetes generates a new DEK per encryption from a secret seed.
The seed is rotated whenever the KEK is rotated.<br/>A good choice if using a third party tool for key management.
Available as stable from Kubernetes v1.29.<br/>Read how to <a href="/docs/tasks/administer-cluster/kms-provider#configuring-the-kms-provider-kms-v2">configure the KMS V2 provider</a>.</td></tr><tr><th rowspan="2" scope="row"><tt>secretbox</tt></th><td>XSalsa20 and Poly1305</td><td>Strong</td><td>Faster</td><td>32-byte</td></tr><tr><td colspan="4">Uses relatively new encryption technologies that may not be considered acceptable in environments that require high levels of review. Key material accessible from control plane host.</td></tr></tbody></table><p>The <code>identity</code> provider is the default if you do not specify otherwise. <strong>The <code>identity</code> provider does not
encrypt stored data and provides <em>no</em> additional confidentiality protection.</strong></p><h3 id="key-storage">Key storage</h3><h4 id="local-key-storage">Local key storage</h4><p>Encrypting secret data with a locally managed key protects against an etcd compromise, but it fails to
protect against a host compromise. Since the encryption keys are stored on the host in the
EncryptionConfiguration YAML file, a skilled attacker can access that file and extract the encryption
keys.</p><h4 id="kms-key-storage">Managed (KMS) key storage</h4><p>The KMS provider uses <em>envelope encryption</em>: Kubernetes encrypts resources using a data key, and then
encrypts that data key using the managed encryption service. Kubernetes generates a unique data key for
each resource. The API server stores an encrypted version of the data key in etcd alongside the ciphertext;
when reading the resource, the API server calls the managed encryption service and provides both the
ciphertext and the (encrypted) data key.
Within the managed encryption service, the provider use a <em>key encryption key</em> to decipher the data key,
deciphers the data key, and finally recovers the plain text. Communication between the control plane
and the KMS requires in-transit protection, such as TLS.</p><p>Using envelope encryption creates dependence on the key encryption key, which is not stored in Kubernetes.
In the KMS case, an attacker who intends to get unauthorised access to the plaintext
values would need to compromise etcd <strong>and</strong> the third-party KMS provider.</p><h3 id="protection-for-encryption-keys">Protection for encryption keys</h3><p>You should take appropriate measures to protect the confidential information that allows decryption,
whether that is a local encryption key, or an authentication token that allows the API server to
call KMS.</p><p>Even when you rely on a provider to manage the use and lifecycle of the main encryption key (or keys), you are still responsible
for making sure that access controls and other security measures for the managed encryption service are
appropriate for your security needs.</p><h2 id="encrypting-your-data">Encrypt your data</h2><h3 id="generate-key-no-kms">Generate the encryption key</h3><p>The following steps assume that you are not using KMS, and therefore the steps also
assume that you need to generate an encryption key. If you already have an encryption key,
skip to <a href="#write-an-encryption-configuration-file">Write an encryption configuration file</a>.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>Storing the raw encryption key in the EncryptionConfig only moderately improves your security posture,
compared to no encryption.</p><p>For additional secrecy, consider using the <code>kms</code> provider as this relies on keys held outside your
Kubernetes cluster. Implementations of <code>kms</code> can work with hardware security modules or with
encryption services managed by your cloud provider.</p><p>To learn about setting
up encryption at rest using KMS, see
<a href="/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a>.
The KMS provider plugin that you use may also come with additional specific documentation.</p></div><p>Start by generating a new encryption key, and then encode it using base64:</p><ul class="nav nav-tabs" id="generate-encryption-key" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#generate-encryption-key-0" role="tab" aria-controls="generate-encryption-key-0" aria-selected="true">Linux</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#generate-encryption-key-1" role="tab" aria-controls="generate-encryption-key-1">macOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#generate-encryption-key-2" role="tab" aria-controls="generate-encryption-key-2">Windows</a></li></ul><div class="tab-content" id="generate-encryption-key"><div id="generate-encryption-key-0" class="tab-pane show active" role="tabpanel" aria-labelledby="generate-encryption-key-0"><p><p>Generate a 32-byte random key and base64 encode it. You can use this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>head -c <span style="color:#666">32</span> /dev/urandom | base64
</span></span></code></pre></div><p>You can use <code>/dev/hwrng</code> instead of <code>/dev/urandom</code> if you want to
use your PC's built-in hardware entropy source. Not all Linux
devices provide a hardware random generator.</p></p></div><div id="generate-encryption-key-1" class="tab-pane" role="tabpanel" aria-labelledby="generate-encryption-key-1"><p><p>Generate a 32-byte random key and base64 encode it. You can use this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>head -c <span style="color:#666">32</span> /dev/urandom | base64
</span></span></code></pre></div></p></div><div id="generate-encryption-key-2" class="tab-pane" role="tabpanel" aria-labelledby="generate-encryption-key-2"><p><p>Generate a 32-byte random key and base64 encode it. You can use this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Do not run this in a session where you have set a random number</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># generator seed.</span>
</span></span><span style="display:flex"><span>[<span style="color:#800">Convert</span>]::ToBase64String((<span style="color:#666">1</span>.<span style="color:#666">.32</span>|%{[<span style="color:#800">byte</span>](<span style="color:#a2f">Get-Random</span> -Max <span style="color:#666">256</span>)}))
</span></span></code></pre></div></p></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Keep the encryption key confidential, including while you generate it and
ideally even after you are no longer actively using it.</div><h3 id="replicate-the-encryption-key">Replicate the encryption key</h3><p>Using a secure mechanism for file transfer, make a copy of that encryption key
available to every other control plane host.</p><p>At a minimum, use encryption in transit - for example, secure shell (SSH). For more
security, use asymmetric encryption between hosts, or change the approach you are using
so that you're relying on KMS encryption.</p><h3 id="write-an-encryption-configuration-file">Write an encryption configuration file</h3><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>The encryption configuration file may contain keys that can decrypt content in etcd.
If the configuration file contains any key material, you must properly
restrict permissions on all your control plane hosts so only the user
who runs the kube-apiserver can read this configuration.</div><p>Create a new encryption configuration file. The contents should be similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- configmaps<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- pandas.awesome.bears.example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:#080;font-style:italic"># See the following text for more details about the secret value</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># this fallback allows reading unencrypted secrets;</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                     </span><span style="color:#080;font-style:italic"># for example, during initial migration</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>To create a new encryption key (that does not use KMS), see
<a href="#generate-key-no-kms">Generate the encryption key</a>.</p><h3 id="use-the-new-encryption-configuration-file">Use the new encryption configuration file</h3><p>You will need to mount the new encryption config file to the <code>kube-apiserver</code> static pod. Here is an example on how to do that:</p><ol><li><p>Save the new encryption config file to <code>/etc/kubernetes/enc/enc.yaml</code> on the control-plane node.</p></li><li><p>Edit the manifest for the <code>kube-apiserver</code> static pod: <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> so that it is similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># This is a fragment of a manifest for a static Pod.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Check whether this is correct for your cluster and for your API server.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint</span>:<span style="color:#bbb"> </span><span style="color:#666">10.20.30.40</span>:<span style="color:#666">443</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>kube-apiserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>control-plane<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kube-apiserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- kube-apiserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- --encryption-provider-config=/etc/kubernetes/enc/enc.yaml <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>enc                          <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/enc     <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">                      </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>enc                            <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">                             </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/etc/kubernetes/enc          <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>DirectoryOrCreate            <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div></li><li><p>Restart your API server.</p></li></ol><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Your config file contains keys that can decrypt the contents in etcd, so you must properly restrict
permissions on your control-plane nodes so only the user who runs the <code>kube-apiserver</code> can read it.</div><p>You now have encryption in place for <strong>one</strong> control plane host. A typical
Kubernetes cluster has multiple control plane hosts, so there is more to do.</p><h3 id="api-server-config-update-more">Reconfigure other control plane hosts</h3><p>If you have multiple API servers in your cluster, you should deploy the
changes in turn to each API server.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>For cluster configurations with two or more control plane nodes, the encryption configuration
should be identical across each control plane node.</p><p>If there is a difference in the encryption provider configuration between control plane
nodes, this difference may mean that the kube-apiserver can't decrypt data.</p></div><p>When you are planning to update the encryption configuration of your cluster, plan this
so that the API servers in your control plane can always decrypt the stored data
(even part way through rolling out the change).</p><p>Make sure that you use the <strong>same</strong> encryption configuration on each
control plane host.</p><h3 id="verifying-that-data-is-encrypted">Verify that newly written data is encrypted</h3><p>Data is encrypted when written to etcd. After restarting your <code>kube-apiserver</code>, any newly
created or updated Secret (or other resource kinds configured in <code>EncryptionConfiguration</code>)
should be encrypted when stored.</p><p>To check this, you can use the <code>etcdctl</code> command line
program to retrieve the contents of your secret data.</p><p>This example shows how to check this for encrypting the Secret API.</p><ol><li><p>Create a new Secret called <code>secret1</code> in the <code>default</code> namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret generic secret1 -n default --from-literal<span style="color:#666">=</span><span style="color:#b8860b">mykey</span><span style="color:#666">=</span>mydata
</span></span></code></pre></div></li><li><p>Using the <code>etcdctl</code> command line tool, read that Secret out of etcd:</p><pre tabindex="0"><code>ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C
</code></pre><p>where <code>[...]</code> must be the additional arguments for connecting to the etcd server.</p><p>For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   --cacert<span style="color:#666">=</span>/etc/kubernetes/pki/etcd/ca.crt   <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   --cert<span style="color:#666">=</span>/etc/kubernetes/pki/etcd/server.crt <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   --key<span style="color:#666">=</span>/etc/kubernetes/pki/etcd/server.key  <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>   get /registry/secrets/default/secret1 | hexdump -C
</span></span></code></pre></div><p>The output is similar to this (abbreviated):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-hexdump" data-lang="hexdump"><span style="display:flex"><span><span style="color:#a0a000">00000000</span>  <span style="color:#666">2f</span> <span style="color:#666">72</span> <span style="color:#666">65</span> <span style="color:#666">67</span> <span style="color:#666">69</span> <span style="color:#666">73</span> <span style="color:#666">74</span> <span style="color:#666">72</span>  <span style="color:#666">79</span> <span style="color:#666">2f</span> <span style="color:#666">73</span> <span style="color:#666">65</span> <span style="color:#666">63</span> <span style="color:#666">72</span> <span style="color:#666">65</span> <span style="color:#666">74</span>  |<span style="color:#b44">/registry/secret</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000010</span>  <span style="color:#666">73</span> <span style="color:#666">2f</span> <span style="color:#666">64</span> <span style="color:#666">65</span> <span style="color:#666">66</span> <span style="color:#666">61</span> <span style="color:#666">75</span> <span style="color:#666">6c</span>  <span style="color:#666">74</span> <span style="color:#666">2f</span> <span style="color:#666">73</span> <span style="color:#666">65</span> <span style="color:#666">63</span> <span style="color:#666">72</span> <span style="color:#666">65</span> <span style="color:#666">74</span>  |<span style="color:#b44">s/default/secret</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000020</span>  <span style="color:#666">31</span> <span style="color:#666">0a</span> <span style="color:#666">6b</span> <span style="color:#666">38</span> <span style="color:#666">73</span> <span style="color:#666">3a</span> <span style="color:#666">65</span> <span style="color:#666">6e</span>  <span style="color:#666">63</span> <span style="color:#666">3a</span> <span style="color:#666">61</span> <span style="color:#666">65</span> <span style="color:#666">73</span> <span style="color:#666">63</span> <span style="color:#666">62</span> <span style="color:#666">63</span>  |<span style="color:#b44">1.k8s:enc:aescbc</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000030</span>  <span style="color:#666">3a</span> <span style="color:#666">76</span> <span style="color:#666">31</span> <span style="color:#666">3a</span> <span style="color:#666">6b</span> <span style="color:#666">65</span> <span style="color:#666">79</span> <span style="color:#666">31</span>  <span style="color:#666">3a</span> <span style="color:#666">c7</span> <span style="color:#666">6c</span> <span style="color:#666">e7</span> <span style="color:#666">d3</span> <span style="color:#666">09</span> <span style="color:#666">bc</span> <span style="color:#666">06</span>  |<span style="color:#b44">:v1:key1:.l.....</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000040</span>  <span style="color:#666">25</span> <span style="color:#666">51</span> <span style="color:#666">91</span> <span style="color:#666">e4</span> <span style="color:#666">e0</span> <span style="color:#666">6c</span> <span style="color:#666">e5</span> <span style="color:#666">b1</span>  <span style="color:#666">4d</span> <span style="color:#666">7a</span> <span style="color:#666">8b</span> <span style="color:#666">3d</span> <span style="color:#666">b9</span> <span style="color:#666">c2</span> <span style="color:#666">7c</span> <span style="color:#666">6e</span>  |<span style="color:#b44">%Q...l..Mz.=..|n</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000050</span>  <span style="color:#666">b4</span> <span style="color:#666">79</span> <span style="color:#666">df</span> <span style="color:#666">05</span> <span style="color:#666">28</span> <span style="color:#666">ae</span> <span style="color:#666">0d</span> <span style="color:#666">8e</span>  <span style="color:#666">5f</span> <span style="color:#666">35</span> <span style="color:#666">13</span> <span style="color:#666">2c</span> <span style="color:#666">c0</span> <span style="color:#666">18</span> <span style="color:#666">99</span> <span style="color:#666">3e</span>  |<span style="color:#b44">.y..(..._5.,...&gt;</span>|
</span></span><span style="display:flex"><span><span>[...]</span>
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000110</span>  <span style="color:#666">23</span> <span style="color:#666">3a</span> <span style="color:#666">0d</span> <span style="color:#666">fc</span> <span style="color:#666">28</span> <span style="color:#666">ca</span> <span style="color:#666">48</span> <span style="color:#666">2d</span>  <span style="color:#666">6b</span> <span style="color:#666">2d</span> <span style="color:#666">46</span> <span style="color:#666">cc</span> <span style="color:#666">72</span> <span style="color:#666">0b</span> <span style="color:#666">70</span> <span style="color:#666">4c</span>  |<span style="color:#b44">#:..(.H-k-F.r.pL</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000120</span>  <span style="color:#666">a5</span> <span style="color:#666">fc</span> <span style="color:#666">35</span> <span style="color:#666">43</span> <span style="color:#666">12</span> <span style="color:#666">4e</span> <span style="color:#666">60</span> <span style="color:#666">ef</span>  <span style="color:#666">bf</span> <span style="color:#666">6f</span> <span style="color:#666">fe</span> <span style="color:#666">cf</span> <span style="color:#666">df</span> <span style="color:#666">0b</span> <span style="color:#666">ad</span> <span style="color:#666">1f</span>  |<span style="color:#b44">..5C.N`..o......</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">00000130</span>  <span style="color:#666">82</span> <span style="color:#666">c4</span> <span style="color:#666">88</span> <span style="color:#666">53</span> <span style="color:#666">02</span> <span style="color:#666">da</span> <span style="color:#666">3e</span> <span style="color:#666">66</span>  <span style="color:#666">ff</span> <span style="color:#666">0a</span>                    |<span style="color:#b44">...S..&gt;f..</span>|
</span></span><span style="display:flex"><span><span style="color:#a0a000">0000013a</span>
</span></span></code></pre></div></li><li><p>Verify the stored Secret is prefixed with <code>k8s:enc:aescbc:v1:</code> which indicates
the <code>aescbc</code> provider has encrypted the resulting data. Confirm that the key name shown in <code>etcd</code>
matches the key name specified in the <code>EncryptionConfiguration</code> mentioned above. In this example,
you can see that the encryption key named <code>key1</code> is used in <code>etcd</code> and in <code>EncryptionConfiguration</code>.</p></li><li><p>Verify the Secret is correctly decrypted when retrieved via the API:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get secret secret1 -n default -o yaml
</span></span></code></pre></div><p>The output should contain <code>mykey: bXlkYXRh</code>, with contents of <code>mydata</code> encoded using base64;
read
<a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret">decoding a Secret</a>
to learn how to completely decode the Secret.</p></li></ol><h3 id="ensure-all-secrets-are-encrypted">Ensure all relevant data are encrypted</h3><p>It's often not enough to make sure that new objects get encrypted: you also want that
encryption to apply to the objects that are already stored.</p><p>For this example, you have configured your cluster so that Secrets are encrypted on write.
Performing a replace operation for each Secret will encrypt that content at rest,
where the objects are unchanged.</p><p>You can make this change across all Secrets in your cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this as an administrator that can read and write all Secrets</span>
</span></span><span style="display:flex"><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><p>The command above reads all Secrets and then updates them with the same data, in order to
apply server side encryption.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If an error occurs due to a conflicting write, retry the command.
It is safe to run that command more than once.</p><p>For larger clusters, you may wish to subdivide the Secrets by namespace,
or script an update.</p></div><h2 id="cleanup-all-secrets-encrypted">Prevent plain text retrieval</h2><p>If you want to make sure that the only access to a particular API kind is done using
encryption, you can remove the API server's ability to read that API's backing data
as plaintext.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><p>Making this change prevents the API server from retrieving resources that are marked
as encrypted at rest, but are actually stored in the clear.</p><p>When you have configured encryption at rest for an API (for example: the API kind
<code>Secret</code>, representing <code>secrets</code> resources in the core API group), you <strong>must</strong> ensure
that all those resources in this cluster really are encrypted at rest. Check this before
you carry on with the next steps.</p></div><p>Once all Secrets in your cluster are encrypted, you can remove the <code>identity</code>
part of the encryption configuration. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex;background-color:#dfdfdf"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># REMOVE THIS LINE</span></span></span></code></pre></div><p>…and then restart each API server in turn. This change prevents the API server
from accessing a plain-text Secret, even by accident.</p><h2 id="rotating-a-decryption-key">Rotate a decryption key</h2><p>Changing an encryption key for Kubernetes without incurring downtime requires a multi-step operation,
especially in the presence of a highly-available deployment where multiple <code>kube-apiserver</code> processes
are running.</p><ol><li>Generate a new key and add it as the second key entry for the current provider on all
control plane nodes.</li><li>Restart <strong>all</strong> <code>kube-apiserver</code> processes, to ensure each server can decrypt
any data that are encrypted with the new key.</li><li>Make a secure backup of the new encryption key. If you lose all copies of this key you would
need to delete all the resources were encrypted under the lost key, and workloads may not
operate as expected during the time that at-rest encryption is broken.</li><li>Make the new key the first entry in the <code>keys</code> array so that it is used for encryption-at-rest
for new writes</li><li>Restart all <code>kube-apiserver</code> processes to ensure each control plane host now encrypts using the new key</li><li>As a privileged user, run <code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code>
to encrypt all existing Secrets with the new key</li><li>After you have updated all existing Secrets to use the new key and have made a secure backup of the
new key, remove the old decryption key from the configuration.</li></ol><h2 id="decrypting-all-data">Decrypt all data</h2><p>This example shows how to stop encrypting the Secret API at rest. If you are encrypting
other API kinds, adjust the steps to match.</p><p>To disable encryption at rest, place the <code>identity</code> provider as the first
entry in your encryption configuration file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># list any other resources here that you previously were</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># encrypting at rest</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># keep this in place</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                                               </span><span style="color:#080;font-style:italic"># make sure it comes after "identity"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Then run the following command to force decryption of all Secrets:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><p>Once you have replaced all existing encrypted resources with backing data that
don't use encryption, you can remove the encryption settings from the
<code>kube-apiserver</code>.</p><h2 id="configure-automatic-reloading">Configure automatic reloading</h2><p>You can configure automatic reloading of encryption provider configuration.
That setting determines whether the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="API server">API server</a> should
load the file you specify for <code>--encryption-provider-config</code> only once at
startup, or automatically whenever you change that file. Enabling this option
allows you to change the keys for encryption at rest without restarting the
API server.</p><p>To allow automatic reloading, configure the API server to run with:
<code>--encryption-provider-config-automatic-reload=true</code>.
When enabled, file changes are polled every minute to observe the modifications.
The <code>apiserver_encryption_config_controller_automatic_reload_last_timestamp_seconds</code>
metric identifies when the new config becomes effective. This allows
encryption keys to be rotated without restarting the API server.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/tasks/administer-cluster/decrypt-data/">decrypting data that are already stored at rest</a></li><li>Learn more about the <a href="/docs/reference/config-api/apiserver-config.v1/">EncryptionConfiguration configuration API (v1)</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Migrating telemetry and security agents from dockershim</h1><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong> This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Kubernetes' support for direct integration with Docker Engine is deprecated and
has been removed. Most apps do not have a direct dependency on runtime hosting
containers. However, there are still a lot of telemetry and monitoring agents
that have a dependency on Docker to collect containers metadata, logs, and
metrics. This document aggregates information on how to detect these
dependencies as well as links on how to migrate these agents to use generic tools or
alternative runtimes.</p><h2 id="telemetry-and-security-agents">Telemetry and security agents</h2><p>Within a Kubernetes cluster there are a few different ways to run telemetry or
security agents. Some agents have a direct dependency on Docker Engine when
they run as DaemonSets or directly on nodes.</p><h3 id="why-do-some-telemetry-agents-communicate-with-docker-engine">Why do some telemetry agents communicate with Docker Engine?</h3><p>Historically, Kubernetes was written to work specifically with Docker Engine.
Kubernetes took care of networking and scheduling, relying on Docker Engine for
launching and running containers (within Pods) on a node. Some information that
is relevant to telemetry, such as a pod name, is only available from Kubernetes
components. Other data, such as container metrics, is not the responsibility of
the container runtime. Early telemetry agents needed to query the container
runtime <em>and</em> Kubernetes to report an accurate picture. Over time, Kubernetes
gained the ability to support multiple runtimes, and now supports any runtime
that is compatible with the <a href="/docs/concepts/architecture/cri/">container runtime interface</a>.</p><p>Some telemetry agents rely specifically on Docker Engine tooling. For example, an agent
might run a command such as
<a href="https://docs.docker.com/engine/reference/commandline/ps/"><code>docker ps</code></a>
or <a href="https://docs.docker.com/engine/reference/commandline/top/"><code>docker top</code></a> to list
containers and processes or <a href="https://docs.docker.com/engine/reference/commandline/logs/"><code>docker logs</code></a>
to receive streamed logs. If nodes in your existing cluster use
Docker Engine, and you switch to a different container runtime,
these commands will not work any longer.</p><h3 id="identify-docker-dependency">Identify DaemonSets that depend on Docker Engine</h3><p>If a pod wants to make calls to the <code>dockerd</code> running on the node, the pod must either:</p><ul><li>mount the filesystem containing the Docker daemon's privileged socket, as a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volume">volume</a>; or</li><li>mount the specific path of the Docker daemon's privileged socket directly, also as a volume.</li></ul><p>For example: on COS images, Docker exposes its Unix domain socket at
<code>/var/run/docker.sock</code> This means that the pod spec will include a
<code>hostPath</code> volume mount of <code>/var/run/docker.sock</code>.</p><p>Here's a sample shell script to find Pods that have a mount directly mapping the
Docker socket. This script outputs the namespace and name of the pod. You can
remove the <code>grep '/var/run/docker.sock'</code> to review other mounts.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pods --all-namespaces <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>-o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{range .items[*]}{"\n"}{.metadata.namespace}{":\t"}{.metadata.name}{":\t"}{range .spec.volumes[*]}{.hostPath.path}{", "}{end}{end}'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>| sort <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>| grep <span style="color:#b44">'/var/run/docker.sock'</span>
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There are alternative ways for a pod to access Docker on the host. For instance, the parent
directory <code>/var/run</code> may be mounted instead of the full path (like in <a href="https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd">this
example</a>).
The script above only detects the most common uses.</div><h3 id="detecting-docker-dependency-from-node-agents">Detecting Docker dependency from node agents</h3><p>If your cluster nodes are customized and install additional security and
telemetry agents on the node, check with the agent vendor
to verify whether it has any dependency on Docker.</p><h3 id="telemetry-and-security-agent-vendors">Telemetry and security agent vendors</h3><p>This section is intended to aggregate information about various telemetry and
security agents that may have a dependency on container runtimes.</p><p>We keep the work in progress version of migration instructions for various telemetry and security agent vendors
in <a href="https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit">Google doc</a>.
Please contact the vendor to get up to date instructions for migrating from dockershim.</p><h2 id="migration-from-dockershim">Migration from dockershim</h2><h3 id="aqua-https-www-aquasec-com"><a href="https://www.aquasec.com">Aqua</a></h3><p>No changes are needed: everything should work seamlessly on the runtime switch.</p><h3 id="datadog-https-www-datadoghq-com-product"><a href="https://www.datadoghq.com/product/">Datadog</a></h3><p>How to migrate:
<a href="https://docs.datadoghq.com/agent/guide/docker-deprecation/">Docker deprecation in Kubernetes</a>
The pod that accesses Docker Engine may have a name containing any of:</p><ul><li><code>datadog-agent</code></li><li><code>datadog</code></li><li><code>dd-agent</code></li></ul><h3 id="dynatrace-https-www-dynatrace-com"><a href="https://www.dynatrace.com/">Dynatrace</a></h3><p>How to migrate:
<a href="https://community.dynatrace.com/t5/Best-practices/Migrating-from-Docker-only-to-generic-container-metrics-in/m-p/167030#M49">Migrating from Docker-only to generic container metrics in Dynatrace</a></p><p>Containerd support announcement: <a href="https://www.dynatrace.com/news/blog/get-automated-full-stack-visibility-into-containerd-based-kubernetes-environments/">Get automated full-stack visibility into
containerd-based Kubernetes
environments</a></p><p>CRI-O support announcement: <a href="https://www.dynatrace.com/news/blog/get-automated-full-stack-visibility-into-your-cri-o-kubernetes-containers-beta/">Get automated full-stack visibility into your CRI-O Kubernetes containers (Beta)</a></p><p>The pod accessing Docker may have name containing:</p><ul><li><code>dynatrace-oneagent</code></li></ul><h3 id="falco-https-falco-org"><a href="https://falco.org">Falco</a></h3><p>How to migrate:</p><p><a href="https://falco.org/docs/getting-started/deployment/#docker-deprecation-in-kubernetes">Migrate Falco from dockershim</a>
Falco supports any CRI-compatible runtime (containerd is used in the default configuration); the documentation explains all details.
The pod accessing Docker may have name containing:</p><ul><li><code>falco</code></li></ul><h3 id="prisma-cloud-compute-https-docs-paloaltonetworks-com-prisma-prisma-cloud-html"><a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud.html">Prisma Cloud Compute</a></h3><p>Check <a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/install/install_kubernetes.html">documentation for Prisma Cloud</a>,
under the "Install Prisma Cloud on a CRI (non-Docker) cluster" section.
The pod accessing Docker may be named like:</p><ul><li><code>twistlock-defender-ds</code></li></ul><h3 id="signalfx-splunk-https-www-splunk-com-en-us-investor-relations-acquisitions-signalfx-html"><a href="https://www.splunk.com/en_us/investor-relations/acquisitions/signalfx.html">SignalFx (Splunk)</a></h3><p>The SignalFx Smart Agent (deprecated) uses several different monitors for Kubernetes including
<code>kubernetes-cluster</code>, <code>kubelet-stats/kubelet-metrics</code>, and <code>docker-container-stats</code>.
The <code>kubelet-stats</code> monitor was previously deprecated by the vendor, in favor of <code>kubelet-metrics</code>.
The <code>docker-container-stats</code> monitor is the one affected by dockershim removal.
Do not use the <code>docker-container-stats</code> with container runtimes other than Docker Engine.</p><p>How to migrate from dockershim-dependent agent:</p><ol><li>Remove <code>docker-container-stats</code> from the list of <a href="https://github.com/signalfx/signalfx-agent/blob/main/docs/monitor-config.md">configured monitors</a>.
Note, keeping this monitor enabled with non-dockershim runtime will result in incorrect metrics
being reported when docker is installed on node and no metrics when docker is not installed.</li><li><a href="https://github.com/signalfx/signalfx-agent/blob/main/docs/monitors/kubelet-metrics.md">Enable and configure <code>kubelet-metrics</code></a> monitor.</li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The set of collected metrics will change. Review your alerting rules and dashboards.</div><p>The Pod accessing Docker may be named something like:</p><ul><li><code>signalfx-agent</code></li></ul><h3 id="yahoo-kubectl-flame">Yahoo Kubectl Flame</h3><p>Flame does not support container runtimes other than Docker. See
<a href="https://github.com/yahoo/kubectl-flame/issues/51">https://github.com/yahoo/kubectl-flame/issues/51</a></p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Using CoreDNS for Service Discovery</h1><p>This page describes the CoreDNS upgrade process and how to install CoreDNS instead of kube-dns.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul>Your Kubernetes server must be at or later than version v1.9.<p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="about-coredns">About CoreDNS</h2><p><a href="https://coredns.io">CoreDNS</a> is a flexible, extensible DNS server
that can serve as the Kubernetes cluster DNS.
Like Kubernetes, the CoreDNS project is hosted by the
<a class="glossary-tooltip" title="Cloud Native Computing Foundation" data-toggle="tooltip" data-placement="top" href="https://cncf.io/" target="_blank" aria-label="CNCF">CNCF</a>.</p><p>You can use CoreDNS instead of kube-dns in your cluster by replacing
kube-dns in an existing deployment, or by using tools like kubeadm
that will deploy and upgrade the cluster for you.</p><h2 id="installing-coredns">Installing CoreDNS</h2><p>For manual deployment or replacement of kube-dns, see the documentation at the
<a href="https://coredns.io/manual/installation/">CoreDNS website</a>.</p><h2 id="migrating-to-coredns">Migrating to CoreDNS</h2><h3 id="upgrading-an-existing-cluster-with-kubeadm">Upgrading an existing cluster with kubeadm</h3><p>In Kubernetes version 1.21, kubeadm removed its support for <code>kube-dns</code> as a DNS application.
For <code>kubeadm</code> v1.34, the only supported cluster DNS application
is CoreDNS.</p><p>You can move to CoreDNS when you use <code>kubeadm</code> to upgrade a cluster that is
using <code>kube-dns</code>. In this case, <code>kubeadm</code> generates the CoreDNS configuration
("Corefile") based upon the <code>kube-dns</code> ConfigMap, preserving configurations for
stub domains, and upstream name server.</p><h2 id="upgrading-coredns">Upgrading CoreDNS</h2><p>You can check the version of CoreDNS that kubeadm installs for each version of
Kubernetes in the page
<a href="https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md">CoreDNS version in Kubernetes</a>.</p><p>CoreDNS can be upgraded manually in case you want to only upgrade CoreDNS
or use your own custom image.
There is a helpful <a href="https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md">guideline and walkthrough</a>
available to ensure a smooth upgrade.
Make sure the existing CoreDNS configuration ("Corefile") is retained when
upgrading your cluster.</p><p>If you are upgrading your cluster using the <code>kubeadm</code> tool, <code>kubeadm</code>
can take care of retaining the existing CoreDNS configuration automatically.</p><h2 id="tuning-coredns">Tuning CoreDNS</h2><p>When resource utilisation is a concern, it may be useful to tune the
configuration of CoreDNS. For more details, check out the
<a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">documentation on scaling CoreDNS</a>.</p><h2 id="what-s-next">What's next</h2><p>You can configure <a href="https://coredns.io">CoreDNS</a> to support many more use cases than
kube-dns does by modifying the CoreDNS configuration ("Corefile").
For more information, see the <a href="https://coredns.io/plugins/kubernetes/">documentation</a>
for the <code>kubernetes</code> CoreDNS plugin, or read the
<a href="https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/">Custom DNS Entries for Kubernetes</a>.
in the CoreDNS blog.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Access Services Running on Clusters</h1><p>This page shows how to connect to services running on the Kubernetes cluster.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="accessing-services-running-on-the-cluster">Accessing services running on the cluster</h2><p>In Kubernetes, <a href="/docs/concepts/architecture/nodes/">nodes</a>,
<a href="/docs/concepts/workloads/pods/">pods</a> and <a href="/docs/concepts/services-networking/service/">services</a> all have
their own IPs. In many cases, the node IPs, pod IPs, and some service IPs on a cluster will not be
routable, so they will not be reachable from a machine outside the cluster,
such as your desktop machine.</p><h3 id="ways-to-connect">Ways to connect</h3><p>You have several options for connecting to nodes, pods and services from outside the cluster:</p><ul><li>Access services through public IPs.<ul><li>Use a service with type <code>NodePort</code> or <code>LoadBalancer</code> to make the service reachable outside
the cluster. See the <a href="/docs/concepts/services-networking/service/">services</a> and
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose">kubectl expose</a> documentation.</li><li>Depending on your cluster environment, this may only expose the service to your corporate network,
or it may expose it to the internet. Think about whether the service being exposed is secure.
Does it do its own authentication?</li><li>Place pods behind services. To access one specific pod from a set of replicas, such as for debugging,
place a unique label on the pod and create a new service which selects this label.</li><li>In most cases, it should not be necessary for application developer to directly access
nodes via their nodeIPs.</li></ul></li><li>Access services, nodes, or pods using the Proxy Verb.<ul><li>Does apiserver authentication and authorization prior to accessing the remote service.
Use this if the services are not secure enough to expose to the internet, or to gain
access to ports on the node IP, or for debugging.</li><li>Proxies may cause problems for some web applications.</li><li>Only works for HTTP/HTTPS.</li><li>Described <a href="#manually-constructing-apiserver-proxy-urls">here</a>.</li></ul></li><li>Access from a node or pod in the cluster.<ul><li>Run a pod, and then connect to a shell in it using <a href="/docs/reference/generated/kubectl/kubectl-commands/#exec">kubectl exec</a>.
Connect to other nodes, pods, and services from that shell.</li><li>Some clusters may allow you to ssh to a node in the cluster. From there you may be able to
access cluster services. This is a non-standard method, and will work on some clusters but
not others. Browsers and other tools may or may not be installed. Cluster DNS may not work.</li></ul></li></ul><h3 id="discovering-builtin-services">Discovering builtin services</h3><p>Typically, there are several services which are started on a cluster by kube-system. Get a list of these
with the <code>kubectl cluster-info</code> command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl cluster-info
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Kubernetes master is running at https://192.0.2.1
elasticsearch-logging is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
kibana-logging is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/kibana-logging/proxy
kube-dns is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/kube-dns/proxy
grafana is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
heapster is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
</code></pre><p>This shows the proxy-verb URL for accessing each service.
For example, this cluster has cluster-level logging enabled (using Elasticsearch), which can be reached
at <code>https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>
if suitable credentials are passed, or through a kubectl proxy at, for example:
<code>http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>See <a href="/docs/tasks/administer-cluster/access-cluster-api/#accessing-the-kubernetes-api">Access Clusters Using the Kubernetes API</a>
for how to pass credentials or use kubectl proxy.</div><h4 id="manually-constructing-apiserver-proxy-urls">Manually constructing apiserver proxy URLs</h4><p>As mentioned above, you use the <code>kubectl cluster-info</code> command to retrieve the service's proxy URL. To create
proxy URLs that include service endpoints, suffixes, and parameters, you append to the service's proxy URL:
<code>http://</code><em><code>kubernetes_master_address</code></em><code>/api/v1/namespaces/</code><em><code>namespace_name</code></em><code>/services/</code><em><code>[https:]service_name[:port_name]</code></em><code>/proxy</code></p><p>If you haven't specified a name for your port, you don't have to specify <em>port_name</em> in the URL. You can also
use the port number in place of the <em>port_name</em> for both named and unnamed ports.</p><p>By default, the API server proxies to your service using HTTP. To use HTTPS, prefix the service name with <code>https:</code>:
<code>http://&lt;kubernetes_master_address&gt;/api/v1/namespaces/&lt;namespace_name&gt;/services/&lt;service_name&gt;/proxy</code></p><p>The supported formats for the <code>&lt;service_name&gt;</code> segment of the URL are:</p><ul><li><code>&lt;service_name&gt;</code> - proxies to the default or unnamed port using http</li><li><code>&lt;service_name&gt;:&lt;port_name&gt;</code> - proxies to the specified port name or port number using http</li><li><code>https:&lt;service_name&gt;:</code> - proxies to the default or unnamed port using https (note the trailing colon)</li><li><code>https:&lt;service_name&gt;:&lt;port_name&gt;</code> - proxies to the specified port name or port number using https</li></ul><h5 id="examples">Examples</h5><ul><li><p>To access the Elasticsearch service endpoint <code>_search?q=user:kimchy</code>, you would use:</p><pre tabindex="0"><code>http://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy
</code></pre></li><li><p>To access the Elasticsearch cluster health information <code>_cluster/health?pretty=true</code>, you would use:</p><pre tabindex="0"><code>https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true
</code></pre><p>The health information is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"cluster_name"</span> : <span style="color:#b44">"kubernetes_logging"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"status"</span> : <span style="color:#b44">"yellow"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"timed_out"</span> : <span style="color:#a2f;font-weight:700">false</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"number_of_nodes"</span> : <span style="color:#666">1</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"number_of_data_nodes"</span> : <span style="color:#666">1</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"active_primary_shards"</span> : <span style="color:#666">5</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"active_shards"</span> : <span style="color:#666">5</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"relocating_shards"</span> : <span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"initializing_shards"</span> : <span style="color:#666">0</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"unassigned_shards"</span> : <span style="color:#666">5</span>
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></li><li><p>To access the <em>https</em> Elasticsearch service health information <code>_cluster/health?pretty=true</code>, you would use:</p><pre tabindex="0"><code>https://192.0.2.1/api/v1/namespaces/kube-system/services/https:elasticsearch-logging:/proxy/_cluster/health?pretty=true
</code></pre></li></ul><h4 id="using-web-browsers-to-access-services-running-on-the-cluster">Using web browsers to access services running on the cluster</h4><p>You may be able to put an apiserver proxy URL into the address bar of a browser. However:</p><ul><li>Web browsers cannot usually pass tokens, so you may need to use basic (password) auth.
Apiserver can be configured to accept basic auth,
but your cluster may not be configured to accept basic auth.</li><li>Some web apps may not work, particularly those with client side javascript that construct URLs in a
way that is unaware of the proxy path prefix.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">List All Container Images Running in a Cluster</h1><p>This page shows how to use kubectl to list all of the Container images
for Pods running in a cluster.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><p>In this exercise you will use kubectl to fetch all of the Pods
running in a cluster, and format the output to pull out the list
of Containers for each.</p><h2 id="list-all-container-images-in-all-namespaces">List all Container images in all namespaces</h2><ul><li>Fetch all Pods in all namespaces using <code>kubectl get pods --all-namespaces</code></li><li>Format the output to include only the list of Container image names
using <code>-o jsonpath={.items[*].spec['initContainers', 'containers'][*].image}</code>. This will recursively parse out the
<code>image</code> field from the returned json.<ul><li>See the <a href="/docs/reference/kubectl/jsonpath/">jsonpath reference</a>
for further information on how to use jsonpath.</li></ul></li><li>Format the output using standard tools: <code>tr</code>, <code>sort</code>, <code>uniq</code><ul><li>Use <code>tr</code> to replace spaces with newlines</li><li>Use <code>sort</code> to sort the results</li><li>Use <code>uniq</code> to aggregate image counts</li></ul></li></ul><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --all-namespaces -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">"{.items[*].spec['initContainers', 'containers'][*].image}"</span> |<span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>tr -s <span style="color:#b44">'[[:space:]]'</span> <span style="color:#b44">'\n'</span> |<span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sort |<span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>uniq -c
</span></span></code></pre></div><p>The jsonpath is interpreted as follows:</p><ul><li><code>.items[*]</code>: for each returned value</li><li><code>.spec</code>: get the spec</li><li><code>['initContainers', 'containers'][*]</code>: for each container</li><li><code>.image</code>: get the image</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When fetching a single Pod by name, for example <code>kubectl get pod nginx</code>,
the <code>.items[*]</code> portion of the path should be omitted because a single
Pod is returned instead of a list of items.</div><h2 id="list-container-images-by-pod">List Container images by Pod</h2><p>The formatting can be controlled further by using the <code>range</code> operation to
iterate over elements individually.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --all-namespaces -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}'</span> |<span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>sort
</span></span></code></pre></div><h2 id="list-container-images-filtering-by-pod-label">List Container images filtering by Pod label</h2><p>To target only Pods matching a specific label, use the -l flag. The
following matches only Pods with labels matching <code>app=nginx</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --all-namespaces -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">"{.items[*].spec.containers[*].image}"</span> -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</span></span></code></pre></div><h2 id="list-container-images-filtering-by-pod-namespace">List Container images filtering by Pod namespace</h2><p>To target only pods in a specific namespace, use the namespace flag. The
following matches only Pods in the <code>kube-system</code> namespace.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --namespace kube-system -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">"{.items[*].spec.containers[*].image}"</span>
</span></span></code></pre></div><h2 id="list-container-images-using-a-go-template-instead-of-jsonpath">List Container images using a go-template instead of jsonpath</h2><p>As an alternative to jsonpath, Kubectl supports using <a href="https://pkg.go.dev/text/template">go-templates</a>
for formatting the output:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --all-namespaces -o go-template --template<span style="color:#666">=</span><span style="color:#b44">"{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="reference">Reference</h3><ul><li><a href="/docs/reference/kubectl/jsonpath/">Jsonpath</a> reference guide</li><li><a href="https://pkg.go.dev/text/template">Go template</a> reference guide</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configuring a cgroup driver</h1><p>This page explains how to configure the kubelet's cgroup driver to match the container
runtime cgroup driver for kubeadm clusters.</p><h2 id="before-you-begin">Before you begin</h2><p>You should be familiar with the Kubernetes
<a href="/docs/setup/production-environment/container-runtimes/">container runtime requirements</a>.</p><h2 id="configuring-the-container-runtime-cgroup-driver">Configuring the container runtime cgroup driver</h2><p>The <a href="/docs/setup/production-environment/container-runtimes/">Container runtimes</a> page
explains that the <code>systemd</code> driver is recommended for kubeadm based setups instead
of the kubelet's <a href="/docs/reference/config-api/kubelet-config.v1beta1/">default</a> <code>cgroupfs</code> driver,
because kubeadm manages the kubelet as a
<a href="/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">systemd service</a>.</p><p>The page also provides details on how to set up a number of different container runtimes with the
<code>systemd</code> driver by default.</p><h2 id="configuring-the-kubelet-cgroup-driver">Configuring the kubelet cgroup driver</h2><p>kubeadm allows you to pass a <code>KubeletConfiguration</code> structure during <code>kubeadm init</code>.
This <code>KubeletConfiguration</code> can include the <code>cgroupDriver</code> field which controls the cgroup
driver of the kubelet.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>In v1.22 and later, if the user does not set the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code>,
kubeadm defaults it to <code>systemd</code>.</p><p>In Kubernetes v1.28, you can enable automatic detection of the
cgroup driver as an alpha feature.
See <a href="/docs/setup/production-environment/container-runtimes/#systemd-cgroup-driver">systemd cgroup driver</a>
for more details.</p></div><p>A minimal example of configuring the field explicitly:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># kubeadm-config.yaml</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kubernetesVersion</span>:<span style="color:#bbb"> </span>v1.21.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">cgroupDriver</span>:<span style="color:#bbb"> </span>systemd<span style="color:#bbb">
</span></span></span></code></pre></div><p>Such a configuration file can then be passed to the kubeadm command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubeadm init --config kubeadm-config.yaml
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Kubeadm uses the same <code>KubeletConfiguration</code> for all nodes in the cluster.
The <code>KubeletConfiguration</code> is stored in a <a href="/docs/concepts/configuration/configmap/">ConfigMap</a>
object under the <code>kube-system</code> namespace.</p><p>Executing the sub commands <code>init</code>, <code>join</code> and <code>upgrade</code> would result in kubeadm
writing the <code>KubeletConfiguration</code> as a file under <code>/var/lib/kubelet/config.yaml</code>
and passing it to the local node kubelet.</p><p>On each node, kubeadm detects the CRI socket and stores its details into the <code>/var/lib/kubelet/instance-config.yaml</code> file.
When executing the <code>init</code>, <code>join</code>, or <code>upgrade</code> subcommands,
kubeadm patches the <code>containerRuntimeEndpoint</code> value from this instance configuration into <code>/var/lib/kubelet/config.yaml</code>.</p></div><h2 id="using-the-cgroupfs-driver">Using the <code>cgroupfs</code> driver</h2><p>To use <code>cgroupfs</code> and to prevent <code>kubeadm upgrade</code> from modifying the
<code>KubeletConfiguration</code> cgroup driver on existing setups, you must be explicit
about its value. This applies to a case where you do not wish future versions
of kubeadm to apply the <code>systemd</code> driver by default.</p><p>See the below section on "<a href="#modify-the-kubelet-configmap">Modify the kubelet ConfigMap</a>" for details on
how to be explicit about the value.</p><p>If you wish to configure a container runtime to use the <code>cgroupfs</code> driver,
you must refer to the documentation of the container runtime of your choice.</p><h2 id="migrating-to-the-systemd-driver">Migrating to the <code>systemd</code> driver</h2><p>To change the cgroup driver of an existing kubeadm cluster from <code>cgroupfs</code> to <code>systemd</code> in-place,
a similar procedure to a kubelet upgrade is required. This must include both
steps outlined below.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Alternatively, it is possible to replace the old nodes in the cluster with new ones
that use the <code>systemd</code> driver. This requires executing only the first step below
before joining the new nodes and ensuring the workloads can safely move to the new
nodes before deleting the old nodes.</div><h3 id="modify-the-kubelet-configmap">Modify the kubelet ConfigMap</h3><ul><li><p>Call <code>kubectl edit cm kubelet-config -n kube-system</code>.</p></li><li><p>Either modify the existing <code>cgroupDriver</code> value or add a new field that looks like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">cgroupDriver</span>:<span style="color:#bbb"> </span>systemd<span style="color:#bbb">
</span></span></span></code></pre></div><p>This field must be present under the <code>kubelet:</code> section of the ConfigMap.</p></li></ul><h3 id="update-the-cgroup-driver-on-all-nodes">Update the cgroup driver on all nodes</h3><p>For each node in the cluster:</p><ul><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Drain the node</a> using <code>kubectl drain &lt;node-name&gt; --ignore-daemonsets</code></li><li>Stop the kubelet using <code>systemctl stop kubelet</code></li><li>Stop the container runtime</li><li>Modify the container runtime cgroup driver to <code>systemd</code></li><li>Set <code>cgroupDriver: systemd</code> in <code>/var/lib/kubelet/config.yaml</code></li><li>Start the container runtime</li><li>Start the kubelet using <code>systemctl start kubelet</code></li><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Uncordon the node</a> using <code>kubectl uncordon &lt;node-name&gt;</code></li></ul><p>Execute these steps on nodes one at a time to ensure workloads
have sufficient time to schedule on different nodes.</p><p>Once the process is complete ensure that all nodes and workloads are healthy.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Decrypt Confidential Data that is Already Encrypted at Rest</h1><p>All of the APIs in Kubernetes that let you write persistent API resource data support
at-rest encryption. For example, you can enable at-rest encryption for
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secrets">Secrets</a>.
This at-rest encryption is additional to any system-level encryption for the
etcd cluster or for the filesystem(s) on hosts where you are running the
kube-apiserver.</p><p>This page shows how to switch from encryption of API data at rest, so that API data
are stored unencrypted. You might want to do this to improve performance; usually,
though, if it was a good idea to encrypt some data, it's also a good idea to leave them
encrypted.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>This task covers encryption for resource data stored using the
<a class="glossary-tooltip" title="The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/kubernetes-api/" target="_blank" aria-label="Kubernetes API">Kubernetes API</a>. For example, you can
encrypt Secret objects, including the key-value data they contain.</p><p>If you wanted to manage encryption for data in filesystems that are mounted into containers, you instead
need to either:</p><ul><li>use a storage integration that provides encrypted
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volumes">volumes</a></li><li>encrypt the data within your own application</li></ul></div><h2 id="before-you-begin">Before you begin</h2><ul><li><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul></li><li><p>This task assumes that you are running the Kubernetes API server as a
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static pod">static pod</a> on each control
plane node.</p></li><li><p>Your cluster's control plane <strong>must</strong> use etcd v3.x (major version 3, any minor version).</p></li><li><p>To encrypt a custom resource, your cluster must be running Kubernetes v1.26 or newer.</p></li><li><p>You should have some API data that are already encrypted.</p></li></ul><p>To check the version, enter <code>kubectl version</code>.</p><h2 id="determine-whether-encryption-at-rest-is-already-enabled">Determine whether encryption at rest is already enabled</h2><p>By default, the API server uses an <code>identity</code> provider that stores plain-text representations
of resources.
<strong>The default <code>identity</code> provider does not provide any confidentiality protection.</strong></p><p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code>
that specifies a path to a configuration file. The contents of that file, if you specify one,
control how Kubernetes API data is encrypted in etcd.
If it is not specified, you do not have encryption at rest enabled.</p><p>The format of that configuration file is YAML, representing a configuration API kind named
<a href="/docs/reference/config-api/apiserver-config.v1/"><code>EncryptionConfiguration</code></a>.
You can see an example configuration
in <a href="/docs/tasks/administer-cluster/encrypt-data/#understanding-the-encryption-at-rest-configuration">Encryption at rest configuration</a>.</p><p>If <code>--encryption-provider-config</code> is set, check which resources (such as <code>secrets</code>) are
configured for encryption, and what provider is used.
Make sure that the preferred provider for that resource type is <strong>not</strong> <code>identity</code>; you
only set <code>identity</code> (<em>no encryption</em>) as default when you want to disable encryption at
rest.
Verify that the first-listed provider for a resource is something <strong>other</strong> than <code>identity</code>,
which means that any new information written to resources of that type will be encrypted as
configured. If you do see <code>identity</code> as the first-listed provider for any resource, this
means that those resources are being written out to etcd without encryption.</p><h2 id="decrypting-all-data">Decrypt all data</h2><p>This example shows how to stop encrypting the Secret API at rest. If you are encrypting
other API kinds, adjust the steps to match.</p><h3 id="locate-the-encryption-configuration-file">Locate the encryption configuration file</h3><p>First, find the API server configuration files. On each control plane node, static Pod manifest
for the kube-apiserver specifies a command line argument, <code>--encryption-provider-config</code>.
You are likely to find that this file is mounted into the static Pod using a
<a href="/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code></a> volume mount. Once you locate the volume
you can find the file on the node filesystem and inspect it.</p><h3 id="configure-the-api-server-to-decrypt-objects">Configure the API server to decrypt objects</h3><p>To disable encryption at rest, place the <code>identity</code> provider as the first
entry in your encryption configuration file.</p><p>For example, if your existing EncryptionConfiguration file reads:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># Do not use this (invalid) example key for encryption</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>2KfZgdiq2K0g2YrYpyDYs9mF2LPZhQ==<span style="color:#bbb">
</span></span></span></code></pre></div><p>then change it to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- secrets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">providers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># add this line</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">aescbc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb"> </span>2KfZgdiq2K0g2YrYpyDYs9mF2LPZhQ==<span style="color:#bbb">
</span></span></span></code></pre></div><p>and restart the kube-apiserver Pod on this node.</p><h3 id="api-server-config-update-more-1">Reconfigure other control plane hosts</h3><p>If you have multiple API servers in your cluster, you should deploy the changes in turn to each API server.</p><p>Make sure that you use the same encryption configuration on each control plane host.</p><h3 id="force-decryption">Force decryption</h3><p>Then run the following command to force decryption of all Secrets:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># If you are decrypting a different kind of object, change "secrets" to match.</span>
</span></span><span style="display:flex"><span>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</span></span></code></pre></div><p>Once you have replaced <strong>all</strong> existing encrypted resources with backing data that
don't use encryption, you can remove the encryption settings from the
<code>kube-apiserver</code>.</p><p>The command line options to remove are:</p><ul><li><code>--encryption-provider-config</code></li><li><code>--encryption-provider-config-automatic-reload</code></li></ul><p>Restart the kube-apiserver Pod again to apply the new configuration.</p><h3 id="api-server-config-update-more-2">Reconfigure other control plane hosts</h3><p>If you have multiple API servers in your cluster, you should again deploy the changes in turn to each API server.</p><p>Make sure that you use the same encryption configuration on each control plane host.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the <a href="/docs/reference/config-api/apiserver-config.v1/">EncryptionConfiguration configuration API (v1)</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Access Clusters Using the Kubernetes API</h1><p>This page shows how to access clusters using the Kubernetes API.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="accessing-the-kubernetes-api">Accessing the Kubernetes API</h2><h3 id="accessing-for-the-first-time-with-kubectl">Accessing for the first time with kubectl</h3><p>When accessing the Kubernetes API for the first time, use the
Kubernetes command-line tool, <code>kubectl</code>.</p><p>To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a <a href="/docs/setup/">Getting started guide</a>,
or someone else set up the cluster and provided you with credentials and a location.</p><p>Check the location and credentials that kubectl knows about with this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config view
</span></span></code></pre></div><p>Many of the <a href="https://github.com/kubernetes/examples/tree/master/">examples</a> provide an introduction to using
kubectl. Complete documentation is found in the <a href="/docs/reference/kubectl/">kubectl manual</a>.</p><h3 id="directly-accessing-the-rest-api">Directly accessing the REST API</h3><p>kubectl handles locating and authenticating to the API server. If you want to directly access the REST API with an http client like
<code>curl</code> or <code>wget</code>, or a browser, there are multiple ways you can locate and authenticate against the API server:</p><ol><li>Run kubectl in proxy mode (recommended). This method is recommended, since it uses
the stored API server location and verifies the identity of the API server using a
self-signed certificate. No man-in-the-middle (MITM) attack is possible using this method.</li><li>Alternatively, you can provide the location and credentials directly to the http client.
This works with client code that is confused by proxies. To protect against man in the
middle attacks, you'll need to import a root cert into your browser.</li></ol><p>Using the Go or Python client libraries provides accessing kubectl in proxy mode.</p><h4 id="using-kubectl-proxy">Using kubectl proxy</h4><p>The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the API server and authenticating.</p><p>Run it like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span> &amp;
</span></span></code></pre></div><p>See <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl proxy</a> for more details.</p><p>Then you can explore the API with curl, wget, or a browser, like so:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl http://localhost:8080/api/
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"versions"</span>: [
</span></span><span style="display:flex"><span>    <span style="color:#b44">"v1"</span>
</span></span><span style="display:flex"><span>  ],
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"serverAddressByClientCIDRs"</span>: [
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"clientCIDR"</span>: <span style="color:#b44">"0.0.0.0/0"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"serverAddress"</span>: <span style="color:#b44">"10.0.1.149:443"</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  ]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><h4 id="without-kubectl-proxy">Without kubectl proxy</h4><p>It is possible to avoid using kubectl proxy by passing an authentication token
directly to the API server, like this:</p><p>Using <code>grep/cut</code> approach:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Check all possible clusters, as your .KUBECONFIG may have multiple contexts:</span>
</span></span><span style="display:flex"><span>kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{"Cluster name\tServer\n"}{range .clusters[*]}{.name}{"\t"}{.cluster.server}{"\n"}{end}'</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Select name of cluster you want to interact with from above output:</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">export</span> <span style="color:#b8860b">CLUSTER_NAME</span><span style="color:#666">=</span><span style="color:#b44">"some_server_name"</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Point to the API server referring the cluster name</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">"{.clusters[?(@.name==\"</span><span style="color:#b8860b">$CLUSTER_NAME</span><span style="color:#b44">\")].cluster.server}"</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Create a secret to hold a token for the default service account</span>
</span></span><span style="display:flex"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF
</span></span></span><span style="display:flex"><span><span style="color:#b44">apiVersion: v1
</span></span></span><span style="display:flex"><span><span style="color:#b44">kind: Secret
</span></span></span><span style="display:flex"><span><span style="color:#b44">metadata:
</span></span></span><span style="display:flex"><span><span style="color:#b44">  name: default-token
</span></span></span><span style="display:flex"><span><span style="color:#b44">  annotations:
</span></span></span><span style="display:flex"><span><span style="color:#b44">    kubernetes.io/service-account.name: default
</span></span></span><span style="display:flex"><span><span style="color:#b44">type: kubernetes.io/service-account-token
</span></span></span><span style="display:flex"><span><span style="color:#b44">EOF</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Wait for the token controller to populate the secret with a token:</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">while</span> ! kubectl describe secret default-token | grep -E <span style="color:#b44">'^token'</span> &gt;/dev/null; <span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>  <span style="color:#a2f">echo</span> <span style="color:#b44">"waiting for token..."</span> &gt;&amp;<span style="color:#666">2</span>
</span></span><span style="display:flex"><span>  sleep <span style="color:#666">1</span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">done</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Get the token value</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl get secret default-token -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.data.token}'</span> | base64 --decode<span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Explore the API with TOKEN</span>
</span></span><span style="display:flex"><span>curl -X GET <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">"Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">"</span> --insecure
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"APIVersions"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"versions"</span>: [
</span></span><span style="display:flex"><span>    <span style="color:#b44">"v1"</span>
</span></span><span style="display:flex"><span>  ],
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"serverAddressByClientCIDRs"</span>: [
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"clientCIDR"</span>: <span style="color:#b44">"0.0.0.0/0"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"serverAddress"</span>: <span style="color:#b44">"10.0.1.149:443"</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  ]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>The above example uses the <code>--insecure</code> flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
<code>~/.kube</code> directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.</p><p>On some clusters, the API server does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. <a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a>
describes how you can configure this as a cluster administrator.</p><h3 id="programmatic-access-to-the-api">Programmatic access to the API</h3><p>Kubernetes officially supports client libraries for <a href="#go-client">Go</a>, <a href="#python-client">Python</a>,
<a href="#java-client">Java</a>, <a href="#dotnet-client">dotnet</a>, <a href="#javascript-client">JavaScript</a>, and
<a href="#haskell-client">Haskell</a>. There are other client libraries that are provided and maintained by
their authors, not the Kubernetes team. See <a href="/docs/reference/using-api/client-libraries/">client libraries</a>
for accessing the API from other languages and how they authenticate.</p><h4 id="go-client">Go client</h4><ul><li>To get the library, run the following command: <code>go get k8s.io/client-go@kubernetes-&lt;kubernetes-version-number&gt;</code>
See <a href="https://github.com/kubernetes/client-go/releases">https://github.com/kubernetes/client-go/releases</a>
to see which versions are supported.</li><li>Write an application atop of the client-go clients.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>client-go</code> defines its own API objects, so if needed, import API definitions from client-go rather than
from the main repository. For example, <code>import "k8s.io/client-go/kubernetes"</code> is correct.</div><p>The Go client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this <a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">example</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-golang" data-lang="golang"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">package</span> main
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">import</span> (
</span></span><span style="display:flex"><span>  <span style="color:#b44">"context"</span>
</span></span><span style="display:flex"><span>  <span style="color:#b44">"fmt"</span>
</span></span><span style="display:flex"><span>  <span style="color:#b44">"k8s.io/apimachinery/pkg/apis/meta/v1"</span>
</span></span><span style="display:flex"><span>  <span style="color:#b44">"k8s.io/client-go/kubernetes"</span>
</span></span><span style="display:flex"><span>  <span style="color:#b44">"k8s.io/client-go/tools/clientcmd"</span>
</span></span><span style="display:flex"><span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">func</span> <span style="color:#00a000">main</span>() {
</span></span><span style="display:flex"><span>  <span style="color:#080;font-style:italic">// uses the current context in kubeconfig
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"/>  <span style="color:#080;font-style:italic">// path-to-kubeconfig -- for example, /root/.kube/config
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"/>  config, _ <span style="color:#666">:=</span> clientcmd.<span style="color:#00a000">BuildConfigFromFlags</span>(<span style="color:#b44">""</span>, <span style="color:#b44">"&lt;path-to-kubeconfig&gt;"</span>)
</span></span><span style="display:flex"><span>  <span style="color:#080;font-style:italic">// creates the clientset
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"/>  clientset, _ <span style="color:#666">:=</span> kubernetes.<span style="color:#00a000">NewForConfig</span>(config)
</span></span><span style="display:flex"><span>  <span style="color:#080;font-style:italic">// access the API to list pods
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"/>  pods, _ <span style="color:#666">:=</span> clientset.<span style="color:#00a000">CoreV1</span>().<span style="color:#00a000">Pods</span>(<span style="color:#b44">""</span>).<span style="color:#00a000">List</span>(context.<span style="color:#00a000">TODO</span>(), v1.ListOptions{})
</span></span><span style="display:flex"><span>  fmt.<span style="color:#00a000">Printf</span>(<span style="color:#b44">"There are %d pods in the cluster\n"</span>, <span style="color:#a2f">len</span>(pods.Items))
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>If the application is deployed as a Pod in the cluster, see
<a href="/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod">Accessing the API from within a Pod</a>.</p><h4 id="python-client">Python client</h4><p>To use <a href="https://github.com/kubernetes-client/python">Python client</a>, run the following command:
<code>pip install kubernetes</code>. See <a href="https://github.com/kubernetes-client/python">Python Client Library page</a>
for more installation options.</p><p>The Python client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py">example</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">from</span> <span style="color:#00f;font-weight:700">kubernetes</span> <span style="color:#a2f;font-weight:700">import</span> client, config
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>config<span style="color:#666">.</span>load_kube_config()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>v1<span style="color:#666">=</span>client<span style="color:#666">.</span>CoreV1Api()
</span></span><span style="display:flex"><span><span style="color:#a2f">print</span>(<span style="color:#b44">"Listing pods with their IPs:"</span>)
</span></span><span style="display:flex"><span>ret <span style="color:#666">=</span> v1<span style="color:#666">.</span>list_pod_for_all_namespaces(watch<span style="color:#666">=</span><span style="color:#a2f;font-weight:700">False</span>)
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">for</span> i <span style="color:#a2f;font-weight:700">in</span> ret<span style="color:#666">.</span>items:
</span></span><span style="display:flex"><span>    <span style="color:#a2f">print</span>(<span style="color:#b44">"</span><span style="color:#b68;font-weight:700">%s</span><span style="color:#b62;font-weight:700">\t</span><span style="color:#b68;font-weight:700">%s</span><span style="color:#b62;font-weight:700">\t</span><span style="color:#b68;font-weight:700">%s</span><span style="color:#b44">"</span> <span style="color:#666">%</span> (i<span style="color:#666">.</span>status<span style="color:#666">.</span>pod_ip, i<span style="color:#666">.</span>metadata<span style="color:#666">.</span>namespace, i<span style="color:#666">.</span>metadata<span style="color:#666">.</span>name))
</span></span></code></pre></div><h4 id="java-client">Java client</h4><p>To install the <a href="https://github.com/kubernetes-client/java">Java Client</a>, run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Clone java library</span>
</span></span><span style="display:flex"><span>git clone --recursive https://github.com/kubernetes-client/java
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Installing project artifacts, POM etc:</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">cd</span> java
</span></span><span style="display:flex"><span>mvn install
</span></span></code></pre></div><p>See <a href="https://github.com/kubernetes-client/java/releases">https://github.com/kubernetes-client/java/releases</a>
to see which versions are supported.</p><p>The Java client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java">example</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">package</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.examples</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.ApiClient</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.ApiException</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.Configuration</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.apis.CoreV1Api</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.models.V1Pod</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.models.V1PodList</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.util.ClientBuilder</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">io.kubernetes.client.util.KubeConfig</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">java.io.FileReader</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">import</span><span style="color:#bbb"> </span><span style="color:#00f;font-weight:700">java.io.IOException</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">/**
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"> * A simple example of how to use the Java API from an application outside a kubernetes cluster
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"> *
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"> * &lt;p&gt;Easiest way to run this: mvn exec:java
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"> * -Dexec.mainClass="io.kubernetes.client.examples.KubeConfigFileClientExample"
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"> *
</span></span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"> */</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">public</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">class</span> <span style="color:#00f">KubeConfigFileClientExample</span><span style="color:#bbb"> </span>{<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#a2f;font-weight:700">public</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">static</span><span style="color:#bbb"> </span><span style="color:#0b0;font-weight:700">void</span><span style="color:#bbb"> </span><span style="color:#00a000">main</span>(String<span style="color:#666">[]</span><span style="color:#bbb"> </span>args)<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">throws</span><span style="color:#bbb"> </span>IOException,<span style="color:#bbb"> </span>ApiException<span style="color:#bbb"> </span>{<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">// file path to your KubeConfig</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>String<span style="color:#bbb"> </span>kubeConfigPath<span style="color:#bbb"> </span><span style="color:#666">=</span><span style="color:#bbb"> </span><span style="color:#b44">"~/.kube/config"</span>;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">// loading the out-of-cluster config, a kubeconfig from file-system</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>ApiClient<span style="color:#bbb"> </span>client<span style="color:#bbb"> </span><span style="color:#666">=</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>ClientBuilder.<span style="color:#b44">kubeconfig</span>(KubeConfig.<span style="color:#b44">loadKubeConfig</span>(<span style="color:#a2f;font-weight:700">new</span><span style="color:#bbb"> </span>FileReader(kubeConfigPath))).<span style="color:#b44">build</span>();<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">// set the global default api-client to the in-cluster one from above</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>Configuration.<span style="color:#b44">setDefaultApiClient</span>(client);<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">// the CoreV1Api loads default api-client from global configuration.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>CoreV1Api<span style="color:#bbb"> </span>api<span style="color:#bbb"> </span><span style="color:#666">=</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">new</span><span style="color:#bbb"> </span>CoreV1Api();<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">// invokes the CoreV1Api client</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>V1PodList<span style="color:#bbb"> </span>list<span style="color:#bbb"> </span><span style="color:#666">=</span><span style="color:#bbb"> </span>api.<span style="color:#b44">listPodForAllNamespaces</span>(<span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>,<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span>);<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>System.<span style="color:#b44">out</span>.<span style="color:#b44">println</span>(<span style="color:#b44">"Listing all pods: "</span>);<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#a2f;font-weight:700">for</span><span style="color:#bbb"> </span>(V1Pod<span style="color:#bbb"> </span>item<span style="color:#bbb"> </span>:<span style="color:#bbb"> </span>list.<span style="color:#b44">getItems</span>())<span style="color:#bbb"> </span>{<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>System.<span style="color:#b44">out</span>.<span style="color:#b44">println</span>(item.<span style="color:#b44">getMetadata</span>().<span style="color:#b44">getName</span>());<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>}<span style="color:#bbb">
</span></span></span></code></pre></div><h4 id="dotnet-client">dotnet client</h4><p>To use <a href="https://github.com/kubernetes-client/csharp">dotnet client</a>,
run the following command: <code>dotnet add package KubernetesClient --version 1.6.1</code>.
See <a href="https://github.com/kubernetes-client/csharp">dotnet Client Library page</a>
for more installation options. See
<a href="https://github.com/kubernetes-client/csharp/releases">https://github.com/kubernetes-client/csharp/releases</a>
to see which versions are supported.</p><p>The dotnet client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs">example</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">using</span> <span style="color:#00f;font-weight:700">System</span>;
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">using</span> <span style="color:#00f;font-weight:700">k8s</span>;
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">namespace</span> <span style="color:#00f;font-weight:700">simple</span>
</span></span><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">internal</span> <span style="color:#a2f;font-weight:700">class</span> <span style="color:#00f">PodList</span>
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>        <span style="color:#a2f;font-weight:700">private</span> <span style="color:#a2f;font-weight:700">static</span> <span style="color:#a2f;font-weight:700">void</span> Main(<span style="color:#0b0;font-weight:700">string</span>[] args)
</span></span><span style="display:flex"><span>        {
</span></span><span style="display:flex"><span>            <span style="color:#0b0;font-weight:700">var</span> config = KubernetesClientConfiguration.BuildDefaultConfig();
</span></span><span style="display:flex"><span>            IKubernetes client = <span style="color:#a2f;font-weight:700">new</span> Kubernetes(config);
</span></span><span style="display:flex"><span>            Console.WriteLine(<span style="color:#b44">"Starting Request!"</span>);
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#0b0;font-weight:700">var</span> list = client.ListNamespacedPod(<span style="color:#b44">"default"</span>);
</span></span><span style="display:flex"><span>            <span style="color:#a2f;font-weight:700">foreach</span> (<span style="color:#0b0;font-weight:700">var</span> item <span style="color:#a2f;font-weight:700">in</span> list.Items)
</span></span><span style="display:flex"><span>            {
</span></span><span style="display:flex"><span>                Console.WriteLine(item.Metadata.Name);
</span></span><span style="display:flex"><span>            }
</span></span><span style="display:flex"><span>            <span style="color:#a2f;font-weight:700">if</span> (list.Items.Count == <span style="color:#666">0</span>)
</span></span><span style="display:flex"><span>            {
</span></span><span style="display:flex"><span>                Console.WriteLine(<span style="color:#b44">"Empty!"</span>);
</span></span><span style="display:flex"><span>            }
</span></span><span style="display:flex"><span>        }
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><h4 id="javascript-client">JavaScript client</h4><p>To install <a href="https://github.com/kubernetes-client/javascript">JavaScript client</a>,
run the following command: <code>npm install @kubernetes/client-node</code>. See
<a href="https://github.com/kubernetes-client/javascript/releases">https://github.com/kubernetes-client/javascript/releases</a>
to see which versions are supported.</p><p>The JavaScript client can use the same <a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/javascript/blob/master/examples/example.js">example</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">const</span> k8s <span style="color:#666">=</span> require(<span style="color:#b44">'@kubernetes/client-node'</span>);
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">const</span> kc <span style="color:#666">=</span> <span style="color:#a2f;font-weight:700">new</span> k8s.KubeConfig();
</span></span><span style="display:flex"><span>kc.loadFromDefault();
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">const</span> k8sApi <span style="color:#666">=</span> kc.makeApiClient(k8s.CoreV1Api);
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>k8sApi.listNamespacedPod({ namespace<span style="color:#666">:</span> <span style="color:#b44">'default'</span> }).then((res) =&gt; {
</span></span><span style="display:flex"><span>    console.log(res);
</span></span><span style="display:flex"><span>});
</span></span></code></pre></div><h4 id="haskell-client">Haskell client</h4><p>See <a href="https://github.com/kubernetes-client/haskell/releases">https://github.com/kubernetes-client/haskell/releases</a>
to see which versions are supported.</p><p>The <a href="https://github.com/kubernetes-client/haskell">Haskell client</a> can use the same
<a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig file</a>
as the kubectl CLI does to locate and authenticate to the API server. See this
<a href="https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs">example</a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="display:flex"><span><span style="color:#00a000">exampleWithKubeConfig</span> <span style="color:#a2f;font-weight:700">::</span> <span style="color:#0b0;font-weight:700">IO</span> <span style="color:#a2f">()</span>
</span></span><span style="display:flex"><span><span style="color:#00a000">exampleWithKubeConfig</span> <span style="color:#a2f;font-weight:700">=</span> <span style="color:#a2f;font-weight:700">do</span>
</span></span><span style="display:flex"><span>    oidcCache <span style="color:#a2f;font-weight:700">&lt;-</span> atomically <span style="color:#666">$</span> newTVar <span style="color:#666">$</span> <span style="color:#0b0;font-weight:700">Map</span><span style="color:#666">.</span>fromList <span style="color:#0b0;font-weight:700">[]</span>
</span></span><span style="display:flex"><span>    (mgr, kcfg) <span style="color:#a2f;font-weight:700">&lt;-</span> mkKubeClientConfig oidcCache <span style="color:#666">$</span> <span style="color:#0b0;font-weight:700">KubeConfigFile</span> <span style="color:#b44">"/path/to/kubeconfig"</span>
</span></span><span style="display:flex"><span>    dispatchMime
</span></span><span style="display:flex"><span>            mgr
</span></span><span style="display:flex"><span>            kcfg
</span></span><span style="display:flex"><span>            (<span style="color:#0b0;font-weight:700">CoreV1</span><span style="color:#666">.</span>listPodForAllNamespaces (<span style="color:#0b0;font-weight:700">Accept</span> <span style="color:#0b0;font-weight:700">MimeJSON</span>))
</span></span><span style="display:flex"><span>        <span style="color:#666">&gt;&gt;=</span> print
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/run-application/access-api-from-pod/">Accessing the Kubernetes API from a Pod</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Use Cilium for NetworkPolicy</h1><p>This page shows how to use Cilium for NetworkPolicy.</p><p>For background on Cilium, read the <a href="https://docs.cilium.io/en/stable/overview/intro">Introduction to Cilium</a>.</p><h2 id="before-you-begin">Before you begin</h2><p><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>To check the version, enter <code>kubectl version</code>.</p></p><h2 id="deploying-cilium-on-minikube-for-basic-testing">Deploying Cilium on Minikube for Basic Testing</h2><p>To get familiar with Cilium easily you can follow the
<a href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/">Cilium Kubernetes Getting Started Guide</a>
to perform a basic DaemonSet installation of Cilium in minikube.</p><p>To start minikube, minimal version required is &gt;= v1.5.2, run the with the
following arguments:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>minikube version
</span></span></code></pre></div><pre tabindex="0"><code>minikube version: v1.5.2
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>minikube start --network-plugin<span style="color:#666">=</span>cni
</span></span></code></pre></div><p>For minikube you can install Cilium using its CLI tool. To do so, first download the latest
version of the CLI with the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -LO https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz
</span></span></code></pre></div><p>Then extract the downloaded file to your <code>/usr/local/bin</code> directory with the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
</span></span><span style="display:flex"><span>rm cilium-linux-amd64.tar.gz
</span></span></code></pre></div><p>After running the above commands, you can now install Cilium with the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>cilium install
</span></span></code></pre></div><p>Cilium will then automatically detect the cluster configuration and create and
install the appropriate components for a successful installation.
The components are:</p><ul><li>Certificate Authority (CA) in Secret <code>cilium-ca</code> and certificates for Hubble (Cilium's observability layer).</li><li>Service accounts.</li><li>Cluster roles.</li><li>ConfigMap.</li><li>Agent DaemonSet and an Operator Deployment.</li></ul><p>After the installation, you can view the overall status of the Cilium deployment with the <code>cilium status</code> command.
See the expected output of the <code>status</code> command
<a href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#validate-the-installation">here</a>.</p><p>The remainder of the Getting Started Guide explains how to enforce both L3/L4
(i.e., IP address + port) security policies, as well as L7 (e.g., HTTP) security
policies using an example application.</p><h2 id="deploying-cilium-for-production-use">Deploying Cilium for Production Use</h2><p>For detailed instructions around deploying Cilium for production, see:
<a href="https://docs.cilium.io/en/stable/network/kubernetes/concepts/">Cilium Kubernetes Installation Guide</a>
This documentation includes detailed requirements, instructions and example
production DaemonSet files.</p><h2 id="understanding-cilium-components">Understanding Cilium components</h2><p>Deploying a cluster with Cilium adds Pods to the <code>kube-system</code> namespace. To see
this list of Pods run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>cilium
</span></span></code></pre></div><p>You'll see a list of Pods similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">NAME           READY   STATUS    RESTARTS   AGE
</span></span></span><span style="display:flex"><span><span style="color:#888">cilium-kkdhz   1/1     Running   0          3m23s
</span></span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span></code></pre></div><p>A <code>cilium</code> Pod runs on each node in your cluster and enforces network policy
on the traffic to/from Pods on that node using Linux BPF.</p><h2 id="what-s-next">What's next</h2><p>Once your cluster is running, you can follow the
<a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
to try out Kubernetes NetworkPolicy with Cilium.
Have fun, and if you have questions, contact us using the
<a href="https://cilium.herokuapp.com/">Cilium Slack Channel</a>.</p></div>
<hr>
<div class="td-content"><h1>Install a Network Policy Provider</h1><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/">Use Antrea for NetworkPolicy</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/">Use Calico for NetworkPolicy</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">Use Cilium for NetworkPolicy</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">Use Kube-router for NetworkPolicy</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">Romana for NetworkPolicy</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">Weave Net for NetworkPolicy</a></h5><p/></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Cloud Controller Manager Administration</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.11 [beta]</code></div><p>Since cloud providers develop and release at a different pace compared to the
Kubernetes project, abstracting the provider-specific code to the
<code><a class="glossary-tooltip" title="Control plane component that integrates Kubernetes with third-party cloud providers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/cloud-controller/" target="_blank" aria-label="cloud-controller-manager">cloud-controller-manager</a></code>
binary allows cloud vendors to evolve independently from the core Kubernetes code.</p><p>The <code>cloud-controller-manager</code> can be linked to any cloud provider that satisfies
<a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go">cloudprovider.Interface</a>.
For backwards compatibility, the
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">cloud-controller-manager</a>
provided in the core Kubernetes project uses the same cloud libraries as <code>kube-controller-manager</code>.
Cloud providers already supported in Kubernetes core are expected to use the in-tree
cloud-controller-manager to transition out of Kubernetes core.</p><h2 id="administration">Administration</h2><h3 id="requirements">Requirements</h3><p>Every cloud has their own set of requirements for running their own cloud provider
integration, it should not be too different from the requirements when running
<code>kube-controller-manager</code>. As a general rule of thumb you'll need:</p><ul><li>cloud authentication/authorization: your cloud may require a token or IAM rules
to allow access to their APIs</li><li>kubernetes authentication/authorization: cloud-controller-manager may need RBAC
rules set to speak to the kubernetes apiserver</li><li>high availability: like kube-controller-manager, you may want a high available
setup for cloud controller manager using leader election (on by default).</li></ul><h3 id="running-cloud-controller-manager">Running cloud-controller-manager</h3><p>Successfully running cloud-controller-manager requires some changes to your cluster configuration.</p><ul><li><code>kubelet</code>, <code>kube-apiserver</code>, and <code>kube-controller-manager</code> must be set according to the
user's usage of external CCM. If the user has an external CCM (not the internal cloud
controller loops in the Kubernetes Controller Manager), then <code>--cloud-provider=external</code>
must be specified. Otherwise, it should not be specified.</li></ul><p>Keep in mind that setting up your cluster to use cloud controller manager will
change your cluster behaviour in a few ways:</p><ul><li>Components that specify <code>--cloud-provider=external</code> will add a taint
<code>node.cloudprovider.kubernetes.io/uninitialized</code> with an effect <code>NoSchedule</code>
during initialization. This marks the node as needing a second initialization
from an external controller before it can be scheduled work. Note that in the
event that cloud controller manager is not available, new nodes in the cluster
will be left unschedulable. The taint is important since the scheduler may
require cloud specific information about nodes such as their region or type
(high cpu, gpu, high memory, spot instance, etc).</li><li>cloud information about nodes in the cluster will no longer be retrieved using
local metadata, but instead all API calls to retrieve node information will go
through cloud controller manager. This may mean you can restrict access to your
cloud API on the kubelets for better security. For larger clusters you may want
to consider if cloud controller manager will hit rate limits since it is now
responsible for almost all API calls to your cloud from within the cluster.</li></ul><p>The cloud controller manager can implement:</p><ul><li>Node controller - responsible for updating kubernetes nodes using cloud APIs
and deleting kubernetes nodes that were deleted on your cloud.</li><li>Service controller - responsible for loadbalancers on your cloud against
services of type LoadBalancer.</li><li>Route controller - responsible for setting up network routes on your cloud</li><li>any other features you would like to implement if you are running an out-of-tree provider.</li></ul><h2 id="examples">Examples</h2><p>If you are using a cloud that is currently supported in Kubernetes core and would
like to adopt cloud controller manager, see the
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">cloud controller manager in kubernetes core</a>.</p><p>For cloud controller managers not in Kubernetes core, you can find the respective
projects in repositories maintained by cloud vendors or by SIGs.</p><p>For providers already in Kubernetes core, you can run the in-tree cloud controller
manager as a DaemonSet in your cluster, use the following as a guideline:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/cloud/ccm-example.yaml" download="admin/cloud/ccm-example.yaml"><code>admin/cloud/ccm-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-cloud-ccm-example-yaml&quot;)" title="Copy admin/cloud/ccm-example.yaml to clipboard"/></div><div class="includecode" id="admin-cloud-ccm-example-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># This is an example of how to set up cloud-controller-manager as a Daemonset in your cluster.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># It assumes that your masters can run pods and has the role node-role.kubernetes.io/master</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Note that this Daemonset will not work straight out of the box for your cloud, this is</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># meant to be a guideline.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>system:cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">roleRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cluster-admin<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">subjects</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">serviceAccountName</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># for in-tree providers we use registry.k8s.io/cloud-controller-manager</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># this can be replaced with any other image for out-of-tree providers</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/cloud-controller-manager:v1.8.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- /usr/local/bin/cloud-controller-manager<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- --cloud-provider=[YOUR_CLOUD_PROVIDER] <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Add your own cloud provider here!</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- --leader-elect=true<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- --use-service-account-credentials<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># these flags will vary for every cloud provider</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- --allocate-node-cidrs=true<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- --configure-cloud-routes=true<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- --cluster-cidr=172.17.0.0/16<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is required so CCM can bootstrap itself</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>node.cloudprovider.kubernetes.io/uninitialized<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># these tolerations are to have the daemonset runnable on control plane nodes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># remove them if your control plane nodes should not run pods</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/control-plane<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is to restrict CCM to only run on master nodes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># the node selector may vary depending on your cluster setup</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">node-role.kubernetes.io/master</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="limitations">Limitations</h2><p>Running cloud controller manager comes with a few possible limitations. Although
these limitations are being addressed in upcoming releases, it's important that
you are aware of these limitations for production workloads.</p><h3 id="support-for-volumes">Support for Volumes</h3><p>Cloud controller manager does not implement any of the volume controllers found
in <code>kube-controller-manager</code> as the volume integrations also require coordination
with kubelets. As we evolve CSI (container storage interface) and add stronger
support for flex volume plugins, necessary support will be added to cloud
controller manager so that clouds can fully integrate with volumes. Learn more
about out-of-tree CSI volume plugins <a href="https://github.com/kubernetes/features/issues/178">here</a>.</p><h3 id="scalability">Scalability</h3><p>The cloud-controller-manager queries your cloud provider's APIs to retrieve
information for all nodes. For very large clusters, consider possible
bottlenecks such as resource requirements and API rate limiting.</p><h3 id="chicken-and-egg">Chicken and Egg</h3><p>The goal of the cloud controller manager project is to decouple development
of cloud features from the core Kubernetes project. Unfortunately, many aspects
of the Kubernetes project has assumptions that cloud provider features are tightly
integrated into the project. As a result, adopting this new architecture can create
several situations where a request is being made for information from a cloud provider,
but the cloud controller manager may not be able to return that information without
the original request being complete.</p><p>A good example of this is the TLS bootstrapping feature in the Kubelet.
TLS bootstrapping assumes that the Kubelet has the ability to ask the cloud provider
(or a local metadata service) for all its address types (private, public, etc)
but cloud controller manager cannot set a node's address types without being
initialized in the first place which requires that the kubelet has TLS certificates
to communicate with the apiserver.</p><p>As this initiative evolves, changes will be made to address these issues in upcoming releases.</p><h2 id="what-s-next">What's next</h2><p>To build and develop your own cloud controller manager, read
<a href="/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure Default CPU Requests and Limits for a Namespace</h1><div class="lead">Define a default CPU resource limits for a namespace, so that every new Pod in that namespace has a CPU resource limit configured.</div><p>This page shows how to configure default CPU requests and limits for a
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>.</p><p>A Kubernetes cluster can be divided into namespaces. If you create a Pod within a
namespace that has a default CPU
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">limit</a>, and any container in that Pod does not specify
its own CPU limit, then the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> assigns the default
CPU limit to that container.</p><p>Kubernetes assigns a default CPU
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">request</a>,
but only under certain conditions that are explained later in this page.</p><h2 id="before-you-begin">Before you begin</h2><p>You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
<a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">minikube</a>
or you can use one of these Kubernetes playgrounds:</p><ul><li><a href="https://labs.iximiuz.com/playgrounds?category=kubernetes&amp;filter=all">iximiuz Labs</a></li><li><a href="https://killercoda.com/playgrounds/scenario/kubernetes">Killercoda</a></li><li><a href="https://kodekloud.com/public-playgrounds">KodeKloud</a></li><li><a href="https://labs.play-with-k8s.com/">Play with Kubernetes</a></li></ul><p>You must have access to create namespaces in your cluster.</p><p>If you're not already familiar with what Kubernetes means by 1.0 CPU,
read <a href="/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">meaning of CPU</a>.</p><h2 id="create-a-namespace">Create a namespace</h2><p>Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create namespace default-cpu-example
</span></span></code></pre></div><h2 id="create-a-limitrange-and-a-pod">Create a LimitRange and a Pod</h2><p>Here's a manifest for an example <a class="glossary-tooltip" title="Provides constraints to limit resource consumption per Containers or Pods in a namespace." data-toggle="tooltip" data-placement="top" href="/docs/concepts/policy/limit-range/" target="_blank" aria-label="LimitRange">LimitRange</a>.
The manifest specifies a default CPU request and a default CPU limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults.yaml" download="admin/resource/cpu-defaults.yaml"><code>admin/resource/cpu-defaults.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-defaults-yaml&quot;)" title="Copy admin/resource/cpu-defaults.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-defaults-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu-limit-range<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">default</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">defaultRequest</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">0.5</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the LimitRange in the default-cpu-example namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</span></span></code></pre></div><p>Now if you create a Pod in the default-cpu-example namespace, and any container
in that Pod does not specify its own values for CPU request and CPU limit,
then the control plane applies default values: a CPU request of 0.5 and a default
CPU limit of 1.</p><p>Here's a manifest for a Pod that has one container. The container
does not specify a CPU request and limit.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults-pod.yaml" download="admin/resource/cpu-defaults-pod.yaml"><code>admin/resource/cpu-defaults-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-defaults-pod-yaml&quot;)" title="Copy admin/resource/cpu-defaults-pod.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-defaults-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-cpu-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</span></span></code></pre></div><p>View the Pod's specification:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pod default-cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-cpu-example
</span></span></code></pre></div><p>The output shows that the Pod's only container has a CPU request of 500m <code>cpu</code>
(which you can read as “500 millicpu”), and a CPU limit of 1 <code>cpu</code>.
These are the default values specified by the LimitRange.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>containers:
</span></span><span style="display:flex"><span>- image: nginx
</span></span><span style="display:flex"><span>  imagePullPolicy: Always
</span></span><span style="display:flex"><span>  name: default-cpu-demo-ctr
</span></span><span style="display:flex"><span>  resources:
</span></span><span style="display:flex"><span>    limits:
</span></span><span style="display:flex"><span>      cpu: <span style="color:#b44">"1"</span>
</span></span><span style="display:flex"><span>    requests:
</span></span><span style="display:flex"><span>      cpu: 500m
</span></span></code></pre></div><h2 id="what-if-you-specify-a-container-s-limit-but-not-its-request">What if you specify a container's limit, but not its request?</h2><p>Here's a manifest for a Pod that has one container. The container
specifies a CPU limit, but not a request:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults-pod-2.yaml" download="admin/resource/cpu-defaults-pod-2.yaml"><code>admin/resource/cpu-defaults-pod-2.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-defaults-pod-2-yaml&quot;)" title="Copy admin/resource/cpu-defaults-pod-2.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-defaults-pod-2-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-2-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</span></span></code></pre></div><p>View the <a href="/docs/concepts/overview/working-with-objects/#object-spec-and-status">specification</a>
of the Pod that you created:</p><pre tabindex="0"><code>kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example
</code></pre><p>The output shows that the container's CPU request is set to match its CPU limit.
Notice that the container was not assigned the default CPU request value of 0.5 <code>cpu</code>:</p><pre tabindex="0"><code>resources:
  limits:
    cpu: "1"
  requests:
    cpu: "1"
</code></pre><h2 id="what-if-you-specify-a-container-s-request-but-not-its-limit">What if you specify a container's request, but not its limit?</h2><p>Here's an example manifest for a Pod that has one container. The container
specifies a CPU request, but not a limit:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/resource/cpu-defaults-pod-3.yaml" download="admin/resource/cpu-defaults-pod-3.yaml"><code>admin/resource/cpu-defaults-pod-3.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-resource-cpu-defaults-pod-3-yaml&quot;)" title="Copy admin/resource/cpu-defaults-pod-3.yaml to clipboard"/></div><div class="includecode" id="admin-resource-cpu-defaults-pod-3-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-3-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0.75"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</span></span></code></pre></div><p>View the specification of the Pod that you created:</p><pre tabindex="0"><code>kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example
</code></pre><p>The output shows that the container's CPU request is set to the value you specified at
the time you created the Pod (in other words: it matches the manifest).
However, the same container's CPU limit is set to 1 <code>cpu</code>, which is the default CPU limit
for that namespace.</p><pre tabindex="0"><code>resources:
  limits:
    cpu: "1"
  requests:
    cpu: 750m
</code></pre><h2 id="motivation-for-default-cpu-limits-and-requests">Motivation for default CPU limits and requests</h2><p>If your namespace has a CPU <a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." data-toggle="tooltip" data-placement="top" href="/docs/concepts/policy/resource-quotas/" target="_blank" aria-label="resource quota">resource quota</a>
configured,
it is helpful to have a default value in place for CPU limit.
Here are two of the restrictions that a CPU resource quota imposes on a namespace:</p><ul><li>For every Pod that runs in the namespace, each of its containers must have a CPU limit.</li><li>CPU limits apply a resource reservation on the node where the Pod in question is scheduled.
The total amount of CPU that is reserved for use by all Pods in the namespace must not
exceed a specified limit.</li></ul><p>When you add a LimitRange:</p><p>If any Pod in that namespace that includes a container does not specify its own CPU limit,
the control plane applies the default CPU limit to that container, and the Pod can be
allowed to run in a namespace that is restricted by a CPU ResourceQuota.</p><h2 id="clean-up">Clean up</h2><p>Delete your namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete namespace default-cpu-example
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><h3 id="for-cluster-administrators">For cluster administrators</h3><ul><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a></p></li><li><p><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></p></li></ul><h3 id="for-app-developers">For app developers</h3><ul><li><p><a href="/docs/tasks/configure-pod-container/assign-memory-resource/">Assign Memory Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">Assign CPU Resources to Containers and Pods</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/assign-pod-level-resources/">Assign Pod-level CPU and memory resources</a></p></li><li><p><a href="/docs/tasks/configure-pod-container/quality-service-pod/">Configure Quality of Service for Pods</a></p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Changing The Kubernetes Package Repository</h1><p>This page explains how to enable a package repository for the desired
Kubernetes minor release upon upgrading a cluster. This is only needed
for users of the community-owned package repositories hosted at <code>pkgs.k8s.io</code>.
Unlike the legacy package repositories, the community-owned package
repositories are structured in a way that there's a dedicated package
repository for each Kubernetes minor version.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This guide only covers a part of the Kubernetes upgrade process. Please see the
<a href="/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">upgrade guide</a> for
more information about upgrading Kubernetes clusters.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This step is only needed upon upgrading a cluster to another <strong>minor</strong> release.
If you're upgrading to another patch release within the same minor release (e.g.
v1.34.5 to v1.34.7), you don't
need to follow this guide. However, if you're still using the legacy package
repositories, you'll need to migrate to the new community-owned package
repositories before upgrading (see the next section for more details on how to
do this).</div><h2 id="before-you-begin">Before you begin</h2><p>This document assumes that you're already using the community-owned
package repositories (<code>pkgs.k8s.io</code>). If that's not the case, it's strongly
recommended to migrate to the community-owned package repositories as described
in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p><div class="alert alert-secondary callout note" role="alert"><strong>Note:</strong> The legacy package repositories (<code>apt.kubernetes.io</code> and <code>yum.kubernetes.io</code>) have been
<a href="/blog/2023/08/31/legacy-package-repository-deprecation/">deprecated and frozen starting from September 13, 2023</a>.
<strong>Using the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">new package repositories hosted at <code>pkgs.k8s.io</code></a>
is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.</strong>
The deprecated legacy repositories, and their contents, might be removed at any time in the future and without
a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.</div><h3 id="verifying-if-the-kubernetes-package-repositories-are-used">Verifying if the Kubernetes package repositories are used</h3><p>If you're unsure whether you're using the community-owned package repositories or the
legacy package repositories, take the following steps to verify:</p><ul class="nav nav-tabs" id="k8s-install-versions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-versions-0" role="tab" aria-controls="k8s-install-versions-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-versions-1" role="tab" aria-controls="k8s-install-versions-1">CentOS, RHEL or Fedora</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-versions-2" role="tab" aria-controls="k8s-install-versions-2">openSUSE or SLES</a></li></ul><div class="tab-content" id="k8s-install-versions"><div id="k8s-install-versions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-versions-0"><p><p>Print the contents of the file that defines the Kubernetes <code>apt</code> repository:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># On your system, this configuration file could have a different name</span>
</span></span><span style="display:flex"><span>pager /etc/apt/sources.list.d/kubernetes.list
</span></span></code></pre></div><p>If you see a line similar to:</p><pre tabindex="0"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
</code></pre><p><strong>You're using the Kubernetes package repositories and this guide applies to you.</strong>
Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories
as described in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p></p></div><div id="k8s-install-versions-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-versions-1"><p><p>Print the contents of the file that defines the Kubernetes <code>yum</code> repository:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># On your system, this configuration file could have a different name</span>
</span></span><span style="display:flex"><span>cat /etc/yum.repos.d/kubernetes.repo
</span></span></code></pre></div><p>If you see a <code>baseurl</code> similar to the <code>baseurl</code> in the output below:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl
</code></pre><p><strong>You're using the Kubernetes package repositories and this guide applies to you.</strong>
Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories
as described in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p></p></div><div id="k8s-install-versions-2" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-versions-2"><p><p>Print the contents of the file that defines the Kubernetes <code>zypper</code> repository:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># On your system, this configuration file could have a different name</span>
</span></span><span style="display:flex"><span>cat /etc/zypp/repos.d/kubernetes.repo
</span></span></code></pre></div><p>If you see a <code>baseurl</code> similar to the <code>baseurl</code> in the output below:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl
</code></pre><p><strong>You're using the Kubernetes package repositories and this guide applies to you.</strong>
Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories
as described in the <a href="/blog/2023/08/15/pkgs-k8s-io-introduction/">official announcement</a>.</p></p></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The URL used for the Kubernetes package repositories is not limited to <code>pkgs.k8s.io</code>,
it can also be one of:</p><ul><li><code>pkgs.k8s.io</code></li><li><code>pkgs.kubernetes.io</code></li><li><code>packages.kubernetes.io</code></li></ul></div><h2 id="switching-to-another-kubernetes-package-repository">Switching to another Kubernetes package repository</h2><p>This step should be done upon upgrading from one to another Kubernetes minor
release in order to get access to the packages of the desired Kubernetes minor
version.</p><ul class="nav nav-tabs" id="k8s-upgrade-versions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-upgrade-versions-0" role="tab" aria-controls="k8s-upgrade-versions-0" aria-selected="true">Ubuntu, Debian or HypriotOS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-upgrade-versions-1" role="tab" aria-controls="k8s-upgrade-versions-1">CentOS, RHEL or Fedora</a></li></ul><div class="tab-content" id="k8s-upgrade-versions"><div id="k8s-upgrade-versions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-upgrade-versions-0"><p><ol><li><p>Open the file that defines the Kubernetes <code>apt</code> repository using a text editor of your choice:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>nano /etc/apt/sources.list.d/kubernetes.list
</span></span></code></pre></div><p>You should see a single line with the URL that contains your current Kubernetes
minor version. For example, if you're using v1.33,
you should see this:</p><pre tabindex="0"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
</code></pre></li><li><p>Change the version in the URL to <strong>the next available minor release</strong>, for example:</p><pre tabindex="0"><code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /
</code></pre></li><li><p>Save the file and exit your text editor. Continue following the relevant upgrade instructions.</p></li></ol></p></div><div id="k8s-upgrade-versions-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-upgrade-versions-1"><p><ol><li><p>Open the file that defines the Kubernetes <code>yum</code> repository using a text editor of your choice:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>nano /etc/yum.repos.d/kubernetes.repo
</span></span></code></pre></div><p>You should see a file with two URLs that contain your current Kubernetes
minor version. For example, if you're using v1.33,
you should see this:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
</code></pre></li><li><p>Change the version in these URLs to <strong>the next available minor release</strong>, for example:</p><pre tabindex="0"><code>[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
</code></pre></li><li><p>Save the file and exit your text editor. Continue following the relevant upgrade instructions.</p></li></ol></p></div></div><h2 id="what-s-next">What's next</h2><ul><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/">Upgrade Linux nodes</a>.</li><li>See how to <a href="/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes/">Upgrade Windows nodes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1>Administer a Cluster</h1><div class="lead">Learn common tasks for administering a cluster.</div><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubeadm/">Administration with kubeadm</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/node-overprovisioning/">Overprovision Node Capacity For A Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/certificates/">Generate Certificates Manually</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/manage-resources/">Manage Memory, CPU, and API Resources</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/network-policy-provider/">Install a Network Policy Provider</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/access-cluster-api/">Access Clusters Using the Kubernetes API</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/extended-resource-node/">Advertise Extended Resources for a Node</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/change-pv-access-mode-readwriteoncepod/">Change the Access Mode of a PersistentVolume to ReadWriteOncePod</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/change-default-storage-class/">Change the default StorageClass</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/switch-to-evented-pleg/">Switching from Polling to CRI Event-based Updates to Container Status</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/change-pv-reclaim-policy/">Change the Reclaim Policy of a PersistentVolume</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/running-cloud-controller/">Cloud Controller Manager Administration</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure a kubelet image credential provider</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/quota-api-object/">Configure Quotas for API Objects</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/cpu-management-policies/">Control CPU Management Policies on the Node</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/topology-manager/">Control Topology Management Policies on a node</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/dns-debugging-resolution/">Debugging DNS Resolution</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/enable-disable-api/">Enable Or Disable A Kubernetes API</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/encrypt-data/">Encrypting Confidential Data at Rest</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/decrypt-data/">Decrypt Confidential Data that is Already Encrypted at Rest</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">Guaranteed Scheduling For Critical Add-On Pods</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/ip-masq-agent/">IP Masquerade Agent User Guide</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/limit-storage-consumption/">Limit Storage Consumption</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/controller-manager-leader-migration/">Migrate Replicated Control Plane To Use Cloud Controller Manager</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/configure-upgrade-etcd/">Operating etcd clusters for Kubernetes</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubelet-in-userns/">Running Kubernetes Node Components as a Non-root User</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/safely-drain-node/">Safely Drain a Node</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kubelet-config-file/">Set Kubelet Parameters Via A Configuration File</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/namespaces/">Share a Cluster with Namespaces</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/cluster-upgrade/">Upgrade A Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/use-cascading-deletion/">Use Cascading Deletion in a Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/coredns/">Using CoreDNS for Service Discovery</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/nodelocaldns/">Using NodeLocal DNSCache in Kubernetes Clusters</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/sysctl-cluster/">Using sysctls in a Kubernetes Cluster</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/memory-manager/">Utilizing the NUMA-aware Memory Manager</a></h5><p/></div><div class="entry"><h5><a href="/docs/tasks/administer-cluster/verify-signed-artifacts/">Verify Signed Kubernetes Artifacts</a></h5><p/></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configure DNS for a Cluster</h1><p>Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm.</p><p>For more information on how to configure CoreDNS for a Kubernetes cluster, see the <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Customizing DNS Service</a>.</p></div>