<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Security For Windows Nodes</h1><p>This page describes security considerations and best practices specific to the Windows operating system.</p><h2 id="protection-for-secret-data-on-nodes">Protection for Secret data on nodes</h2><p>On Windows, data from Secrets are written out in clear text onto the node's local
storage (as compared to using tmpfs / in-memory filesystems on Linux). As a cluster
operator, you should take both of the following additional measures:</p><ol><li>Use file ACLs to secure the Secrets' file location.</li><li>Apply volume-level encryption using
<a href="https://docs.microsoft.com/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server">BitLocker</a>.</li></ol><h2 id="container-users">Container users</h2><p><a href="/docs/tasks/configure-pod-container/configure-runasusername/">RunAsUsername</a>
can be specified for Windows Pods or containers to execute the container
processes as specific user. This is roughly equivalent to
<a href="/docs/concepts/security/pod-security-policy/#users-and-groups">RunAsUser</a>.</p><p>Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator.
The differences between these two user accounts are covered in
<a href="https://docs.microsoft.com/virtualization/windowscontainers/manage-containers/container-security#when-to-use-containeradmin-and-containeruser-user-accounts">When to use ContainerAdmin and ContainerUser user accounts</a>
within Microsoft's <em>Secure Windows containers</em> documentation.</p><p>Local users can be added to container images during the container build process.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li><a href="https://hub.docker.com/_/microsoft-windows-nanoserver">Nano Server</a> based images run as
<code>ContainerUser</code> by default</li><li><a href="https://hub.docker.com/_/microsoft-windows-servercore">Server Core</a> based images run as
<code>ContainerAdministrator</code> by default</li></ul></div><p>Windows containers can also run as Active Directory identities by utilizing
<a href="/docs/tasks/configure-pod-container/configure-gmsa/">Group Managed Service Accounts</a></p><h2 id="pod-level-security-isolation">Pod-level security isolation</h2><p>Linux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom
POSIX capabilities) are not supported on Windows nodes.</p><p>Privileged containers are <a href="/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext">not supported</a>
on Windows.
Instead <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess containers</a>
can be used on Windows to perform many of the tasks performed by privileged containers on Linux.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Dynamic Volume Provisioning</h1><p>Dynamic volume provisioning allows storage volumes to be created on-demand.
Without dynamic provisioning, cluster administrators have to manually make
calls to their cloud or storage provider to create new storage volumes, and
then create <a href="/docs/concepts/storage/persistent-volumes/"><code>PersistentVolume</code> objects</a>
to represent them in Kubernetes. The dynamic provisioning feature eliminates
the need for cluster administrators to pre-provision storage. Instead, it
automatically provisions storage when users create
<a href="/docs/concepts/storage/persistent-volumes/"><code>PersistentVolumeClaim</code> objects</a>.</p><h2 id="background">Background</h2><p>The implementation of dynamic volume provisioning is based on the API object <code>StorageClass</code>
from the API group <code>storage.k8s.io</code>. A cluster administrator can define as many
<code>StorageClass</code> objects as needed, each specifying a <em>volume plugin</em> (aka
<em>provisioner</em>) that provisions a volume and the set of parameters to pass to
that provisioner when provisioning.
A cluster administrator can define and expose multiple flavors of storage (from
the same or different storage systems) within a cluster, each with a custom set
of parameters. This design also ensures that end users don't have to worry
about the complexity and nuances of how storage is provisioned, but still
have the ability to select from multiple storage options.</p><p>For more details, see the <a href="/docs/concepts/storage/storage-classes/">Storage Classes</a> concept.</p><h2 id="enabling-dynamic-provisioning">Enabling Dynamic Provisioning</h2><p>To enable dynamic provisioning, a cluster administrator needs to pre-create
one or more StorageClass objects for users.
StorageClass objects define which provisioner should be used and what parameters
should be passed to that provisioner when dynamic provisioning is invoked.
The name of a StorageClass object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>The following manifest creates a storage class "slow" which provisions standard
disk-like persistent disks.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/gce-pd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>pd-standard<span style="color:#bbb">
</span></span></span></code></pre></div><p>The following manifest creates a storage class "fast" which provisions
SSD-like persistent disks.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/gce-pd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>pd-ssd<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="using-dynamic-provisioning">Using Dynamic Provisioning</h2><p>Users request dynamically provisioned storage by including a storage class in
their <code>PersistentVolumeClaim</code>. Before Kubernetes v1.6, this was done via the
<code>volume.beta.kubernetes.io/storage-class</code> annotation. However, this annotation
is deprecated since v1.9. Users now can and should instead use the
<code>storageClassName</code> field of the <code>PersistentVolumeClaim</code> object. The value of
this field must match the name of a <code>StorageClass</code> configured by the
administrator (see <a href="#enabling-dynamic-provisioning">Enabling Dynamic Provisioning</a>).</p><p>To select the "fast" storage class, for example, a user would create the
following PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>claim1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>30Gi<span style="color:#bbb">
</span></span></span></code></pre></div><p>This claim results in an SSD-like Persistent Disk being automatically
provisioned. When the claim is deleted, the volume is destroyed.</p><h2 id="defaulting-behavior">Defaulting Behavior</h2><p>Dynamic provisioning can be enabled on a cluster such that all claims are
dynamically provisioned if no storage class is specified. A cluster administrator
can enable this behavior by:</p><ul><li>Marking one <code>StorageClass</code> object as <em>default</em>.</li><li>Making sure that the <a href="/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass"><code>DefaultStorageClass</code> admission controller</a>
is enabled on the API server.</li></ul><p>An administrator can mark a specific <code>StorageClass</code> as default by adding the
<a href="/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class"><code>storageclass.kubernetes.io/is-default-class</code> annotation</a> to it.
When a default <code>StorageClass</code> exists in a cluster and a user creates a
<code>PersistentVolumeClaim</code> with <code>storageClassName</code> unspecified, the
<code>DefaultStorageClass</code> admission controller automatically adds the
<code>storageClassName</code> field pointing to the default storage class.</p><p>Note that if you set the <code>storageclass.kubernetes.io/is-default-class</code>
annotation to true on more than one StorageClass in your cluster, and you then
create a <code>PersistentVolumeClaim</code> with no <code>storageClassName</code> set, Kubernetes
uses the most recently created default StorageClass.</p><h2 id="topology-awareness">Topology Awareness</h2><p>In <a href="/docs/setup/best-practices/multiple-zones/">Multi-Zone</a> clusters, Pods can be spread across
Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where
Pods are scheduled. This can be accomplished by setting the
<a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">Volume Binding Mode</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Hardening Guide - Authentication Mechanisms</h1><div class="lead">Information on authentication options in Kubernetes and their security properties.</div><p>Selecting the appropriate authentication mechanism(s) is a crucial aspect of securing your cluster.
Kubernetes provides several built-in mechanisms, each with its own strengths and weaknesses that
should be carefully considered when choosing the best authentication mechanism for your cluster.</p><p>In general, it is recommended to enable as few authentication mechanisms as possible to simplify
user management and prevent cases where users retain access to a cluster that is no longer required.</p><p>It is important to note that Kubernetes does not have an in-built user database within the cluster.
Instead, it takes user information from the configured authentication system and uses that to make
authorization decisions. Therefore, to audit user access, you need to review credentials from every
configured authentication source.</p><p>For production clusters with multiple users directly accessing the Kubernetes API, it is
recommended to use external authentication sources such as OIDC. The internal authentication
mechanisms, such as client certificates and service account tokens, described below, are not
suitable for this use case.</p><h2 id="x509-client-certificate-authentication">X.509 client certificate authentication</h2><p>Kubernetes leverages <a href="/docs/reference/access-authn-authz/authentication/#x509-client-certificates">X.509 client certificate</a>
authentication for system components, such as when the kubelet authenticates to the API Server.
While this mechanism can also be used for user authentication, it might not be suitable for
production use due to several restrictions:</p><ul><li>Client certificates cannot be individually revoked. Once compromised, a certificate can be used
by an attacker until it expires. To mitigate this risk, it is recommended to configure short
lifetimes for user authentication credentials created using client certificates.</li><li>If a certificate needs to be invalidated, the certificate authority must be re-keyed, which
can introduce availability risks to the cluster.</li><li>There is no permanent record of client certificates created in the cluster. Therefore, all
issued certificates must be recorded if you need to keep track of them.</li><li>Private keys used for client certificate authentication cannot be password-protected. Anyone
who can read the file containing the key will be able to make use of it.</li><li>Using client certificate authentication requires a direct connection from the client to the
API server without any intervening TLS termination points, which can complicate network architectures.</li><li>Group data is embedded in the <code>O</code> value of the client certificate, which means the user's group
memberships cannot be changed for the lifetime of the certificate.</li></ul><h2 id="static-token-file">Static token file</h2><p>Although Kubernetes allows you to load credentials from a
<a href="/docs/reference/access-authn-authz/authentication/#static-token-file">static token file</a> located
on the control plane node disks, this approach is not recommended for production servers due to
several reasons:</p><ul><li>Credentials are stored in clear text on control plane node disks, which can be a security risk.</li><li>Changing any credential requires a restart of the API server process to take effect, which can
impact availability.</li><li>There is no mechanism available to allow users to rotate their credentials. To rotate a
credential, a cluster administrator must modify the token on disk and distribute it to the users.</li><li>There is no lockout mechanism available to prevent brute-force attacks.</li></ul><h2 id="bootstrap-tokens">Bootstrap tokens</h2><p><a href="/docs/reference/access-authn-authz/bootstrap-tokens/">Bootstrap tokens</a> are used for joining
nodes to clusters and are not recommended for user authentication due to several reasons:</p><ul><li>They have hard-coded group memberships that are not suitable for general use, making them
unsuitable for authentication purposes.</li><li>Manually generating bootstrap tokens can lead to weak tokens that can be guessed by an attacker,
which can be a security risk.</li><li>There is no lockout mechanism available to prevent brute-force attacks, making it easier for
attackers to guess or crack the token.</li></ul><h2 id="serviceaccount-secret-tokens">ServiceAccount secret tokens</h2><p><a href="/docs/reference/access-authn-authz/service-accounts-admin/#manual-secret-management-for-serviceaccounts">Service account secrets</a>
are available as an option to allow workloads running in the cluster to authenticate to the
API server. In Kubernetes &lt; 1.23, these were the default option, however, they are being replaced
with TokenRequest API tokens. While these secrets could be used for user authentication, they are
generally unsuitable for a number of reasons:</p><ul><li>They cannot be set with an expiry and will remain valid until the associated service account is deleted.</li><li>The authentication tokens are visible to any cluster user who can read secrets in the namespace
that they are defined in.</li><li>Service accounts cannot be added to arbitrary groups complicating RBAC management where they are used.</li></ul><h2 id="tokenrequest-api-tokens">TokenRequest API tokens</h2><p>The TokenRequest API is a useful tool for generating short-lived credentials for service
authentication to the API server or third-party systems. However, it is not generally recommended
for user authentication as there is no revocation method available, and distributing credentials
to users in a secure manner can be challenging.</p><p>When using TokenRequest tokens for service authentication, it is recommended to implement a short
lifespan to reduce the impact of compromised tokens.</p><h2 id="openid-connect-token-authentication">OpenID Connect token authentication</h2><p>Kubernetes supports integrating external authentication services with the Kubernetes API using
<a href="/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">OpenID Connect (OIDC)</a>.
There is a wide variety of software that can be used to integrate Kubernetes with an identity
provider. However, when using OIDC authentication in Kubernetes, it is important to consider the
following hardening measures:</p><ul><li>The software installed in the cluster to support OIDC authentication should be isolated from
general workloads as it will run with high privileges.</li><li>Some Kubernetes managed services are limited in the OIDC providers that can be used.</li><li>As with TokenRequest tokens, OIDC tokens should have a short lifespan to reduce the impact of
compromised tokens.</li></ul><h2 id="webhook-token-authentication">Webhook token authentication</h2><p><a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">Webhook token authentication</a>
is another option for integrating external authentication providers into Kubernetes. This mechanism
allows for an authentication service, either running inside the cluster or externally, to be
contacted for an authentication decision over a webhook. It is important to note that the suitability
of this mechanism will likely depend on the software used for the authentication service, and there
are some Kubernetes-specific considerations to take into account.</p><p>To configure Webhook authentication, access to control plane server filesystems is required. This
means that it will not be possible with Managed Kubernetes unless the provider specifically makes it
available. Additionally, any software installed in the cluster to support this access should be
isolated from general workloads, as it will run with high privileges.</p><h2 id="authenticating-proxy">Authenticating proxy</h2><p>Another option for integrating external authentication systems into Kubernetes is to use an
<a href="/docs/reference/access-authn-authz/authentication/#authenticating-proxy">authenticating proxy</a>.
With this mechanism, Kubernetes expects to receive requests from the proxy with specific header
values set, indicating the username and group memberships to assign for authorization purposes.
It is important to note that there are specific considerations to take into account when using
this mechanism.</p><p>Firstly, securely configured TLS must be used between the proxy and Kubernetes API server to
mitigate the risk of traffic interception or sniffing attacks. This ensures that the communication
between the proxy and Kubernetes API server is secure.</p><p>Secondly, it is important to be aware that an attacker who is able to modify the headers of the
request may be able to gain unauthorized access to Kubernetes resources. As such, it is important
to ensure that the headers are properly secured and cannot be tampered with.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/access-authn-authz/authentication/">User Authentication</a></li><li><a href="/docs/reference/access-authn-authz/bootstrap-tokens/">Authenticating with Bootstrap Tokens</a></li><li><a href="/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication">kubelet Authentication</a></li><li><a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-tokens">Authenticating with Service Account Tokens</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Service Accounts</h1><div class="lead">Learn about ServiceAccount objects in Kubernetes.</div><p>This page introduces the ServiceAccount object in Kubernetes, providing
information about how service accounts work, use cases, limitations,
alternatives, and links to resources for additional guidance.</p><h2 id="what-are-service-accounts">What are service accounts?</h2><p>A service account is a type of non-human account that, in Kubernetes, provides
a distinct identity in a Kubernetes cluster. Application Pods, system
components, and entities inside and outside the cluster can use a specific
ServiceAccount's credentials to identify as that ServiceAccount. This identity
is useful in various situations, including authenticating to the API server or
implementing identity-based security policies.</p><p>Service accounts exist as ServiceAccount objects in the API server. Service
accounts have the following properties:</p><ul><li><p><strong>Namespaced:</strong> Each service account is bound to a Kubernetes
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>. Every namespace
gets a <a href="#default-service-accounts"><code>default</code> ServiceAccount</a> upon creation.</p></li><li><p><strong>Lightweight:</strong> Service accounts exist in the cluster and are
defined in the Kubernetes API. You can quickly create service accounts to
enable specific tasks.</p></li><li><p><strong>Portable:</strong> A configuration bundle for a complex containerized workload
might include service account definitions for the system's components. The
lightweight nature of service accounts and the namespaced identities make
the configurations portable.</p></li></ul><p>Service accounts are different from user accounts, which are authenticated
human users in the cluster. By default, user accounts don't exist in the Kubernetes
API server; instead, the API server treats user identities as opaque
data. You can authenticate as a user account using multiple methods. Some
Kubernetes distributions might add custom extension APIs to represent user
accounts in the API server.</p><table><caption style="display:none">Comparison between service accounts and users</caption><thead><tr><th>Description</th><th>ServiceAccount</th><th>User or group</th></tr></thead><tbody><tr><td>Location</td><td>Kubernetes API (ServiceAccount object)</td><td>External</td></tr><tr><td>Access control</td><td>Kubernetes RBAC or other <a href="/docs/reference/access-authn-authz/authorization/#authorization-modules">authorization mechanisms</a></td><td>Kubernetes RBAC or other identity and access management mechanisms</td></tr><tr><td>Intended use</td><td>Workloads, automation</td><td>People</td></tr></tbody></table><h3 id="default-service-accounts">Default service accounts</h3><p>When you create a cluster, Kubernetes automatically creates a ServiceAccount
object named <code>default</code> for every namespace in your cluster. The <code>default</code>
service accounts in each namespace get no permissions by default other than the
<a href="/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings">default API discovery permissions</a>
that Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled.
If you delete the <code>default</code> ServiceAccount object in a namespace, the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>
replaces it with a new one.</p><p>If you deploy a Pod in a namespace, and you don't
<a href="#assign-to-pod">manually assign a ServiceAccount to the Pod</a>, Kubernetes
assigns the <code>default</code> ServiceAccount for that namespace to the Pod.</p><h2 id="use-cases">Use cases for Kubernetes service accounts</h2><p>As a general guideline, you can use service accounts to provide identities in
the following scenarios:</p><ul><li>Your Pods need to communicate with the Kubernetes API server, for example in
situations such as the following:<ul><li>Providing read-only access to sensitive information stored in Secrets.</li><li>Granting <a href="#cross-namespace">cross-namespace access</a>, such as allowing a
Pod in namespace <code>example</code> to read, list, and watch for Lease objects in
the <code>kube-node-lease</code> namespace.</li></ul></li><li>Your Pods need to communicate with an external service. For example, a
workload Pod requires an identity for a commercially available cloud API,
and the commercial provider allows configuring a suitable trust relationship.</li><li><a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Authenticating to a private image registry using an <code>imagePullSecret</code></a>.</li><li>An external service needs to communicate with the Kubernetes API server. For
example, authenticating to the cluster as part of a CI/CD pipeline.</li><li>You use third-party security software in your cluster that relies on the
ServiceAccount identity of different Pods to group those Pods into different
contexts.</li></ul><h2 id="how-to-use">How to use service accounts</h2><p>To use a Kubernetes service account, you do the following:</p><ol><li><p>Create a ServiceAccount object using a Kubernetes
client like <code>kubectl</code> or a manifest that defines the object.</p></li><li><p>Grant permissions to the ServiceAccount object using an authorization
mechanism such as
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>.</p></li><li><p>Assign the ServiceAccount object to Pods during Pod creation.</p><p>If you're using the identity from an external service,
<a href="#get-a-token">retrieve the ServiceAccount token</a> and use it from that
service instead.</p></li></ol><p>For instructions, refer to
<a href="/docs/tasks/configure-pod-container/configure-service-account/">Configure Service Accounts for Pods</a>.</p><h3 id="grant-permissions">Grant permissions to a ServiceAccount</h3><p>You can use the built-in Kubernetes
<a href="/docs/reference/access-authn-authz/rbac/">role-based access control (RBAC)</a>
mechanism to grant the minimum permissions required by each service account.
You create a <em>role</em>, which grants access, and then <em>bind</em> the role to your
ServiceAccount. RBAC lets you define a minimum set of permissions so that the
service account permissions follow the principle of least privilege. Pods that
use that service account don't get more permissions than are required to
function correctly.</p><p>For instructions, refer to
<a href="/docs/reference/access-authn-authz/rbac/#service-account-permissions">ServiceAccount permissions</a>.</p><h4 id="cross-namespace">Cross-namespace access using a ServiceAccount</h4><p>You can use RBAC to allow service accounts in one namespace to perform actions
on resources in a different namespace in the cluster. For example, consider a
scenario where you have a service account and Pod in the <code>dev</code> namespace and
you want your Pod to see Jobs running in the <code>maintenance</code> namespace. You could
create a Role object that grants permissions to list Job objects. Then,
you'd create a RoleBinding object in the <code>maintenance</code> namespace to bind the
Role to the ServiceAccount object. Now, Pods in the <code>dev</code> namespace can list
Job objects in the <code>maintenance</code> namespace using that service account.</p><h3 id="assign-to-pod">Assign a ServiceAccount to a Pod</h3><p>To assign a ServiceAccount to a Pod, you set the <code>spec.serviceAccountName</code>
field in the Pod specification. Kubernetes then automatically provides the
credentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes
gets a short-lived, <strong>automatically rotating</strong> token using the <code>TokenRequest</code>
API and mounts the token as a
<a href="/docs/concepts/storage/projected-volumes/#serviceaccounttoken">projected volume</a>.</p><p>By default, Kubernetes provides the Pod
with the credentials for an assigned ServiceAccount, whether that is the
<code>default</code> ServiceAccount or a custom ServiceAccount that you specify.</p><p>To prevent Kubernetes from automatically injecting
credentials for a specified ServiceAccount or the <code>default</code> ServiceAccount, set the
<code>automountServiceAccountToken</code> field in your Pod specification to <code>false</code>.</p><p>In versions earlier than 1.22, Kubernetes provides a long-lived, static token
to the Pod as a Secret.</p><h4 id="get-a-token">Manually retrieve ServiceAccount credentials</h4><p>If you need the credentials for a ServiceAccount to mount in a non-standard
location, or for an audience that isn't the API server, use one of the
following methods:</p><ul><li><a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest API</a>
(recommended): Request a short-lived service account token from within
your own <em>application code</em>. The token expires automatically and can rotate
upon expiration.
If you have a legacy application that is not aware of Kubernetes, you
could use a sidecar container within the same pod to fetch these tokens
and make them available to the application workload.</li><li><a href="/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">Token Volume Projection</a>
(also recommended): In Kubernetes v1.20 and later, use the Pod specification to
tell the kubelet to add the service account token to the Pod as a
<em>projected volume</em>. Projected tokens expire automatically, and the kubelet
rotates the token before it expires.</li><li><a href="/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount">Service Account Token Secrets</a>
(not recommended): You can mount service account tokens as Kubernetes
Secrets in Pods. These tokens don't expire and don't rotate. In versions prior to v1.24, a permanent token was automatically created for each service account.
This method is not recommended anymore, especially at scale, because of the risks associated
with static, long-lived credentials. The <a href="/docs/reference/command-line-tools-reference/feature-gates-removed/">LegacyServiceAccountTokenNoAutoGeneration feature gate</a>
(which was enabled by default from Kubernetes v1.24 to v1.26), prevented Kubernetes from automatically creating these tokens for
ServiceAccounts. The feature gate is removed in v1.27, because it was elevated to GA status; you can still create indefinite service account tokens manually, but should take into account the security implications.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>For applications running outside your Kubernetes cluster, you might be considering
creating a long-lived ServiceAccount token that is stored in a Secret. This allows authentication, but the Kubernetes project recommends you avoid this approach.
Long-lived bearer tokens represent a security risk as, once disclosed, the token
can be misused. Instead, consider using an alternative. For example, your external
application can authenticate using a well-protected private key <code>and</code> a certificate,
or using a custom mechanism such as an <a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">authentication webhook</a> that you implement yourself.</p><p>You can also use TokenRequest to obtain short-lived tokens for your external application.</p></div><h3 id="enforce-mountable-secrets">Restricting access to Secrets (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [deprecated]</code></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>kubernetes.io/enforce-mountable-secrets</code> is deprecated since Kubernetes v1.32. Use separate namespaces to isolate access to mounted secrets.</div><p>Kubernetes provides an annotation called <code>kubernetes.io/enforce-mountable-secrets</code>
that you can add to your ServiceAccounts. When this annotation is applied,
the ServiceAccount's secrets can only be mounted on specified types of resources,
enhancing the security posture of your cluster.</p><p>You can add the annotation to a ServiceAccount using a manifest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/enforce-mountable-secrets</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-serviceaccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>my-namespace<span style="color:#bbb">
</span></span></span></code></pre></div><p>When this annotation is set to "true", the Kubernetes control plane ensures that
the Secrets from this ServiceAccount are subject to certain mounting restrictions.</p><ol><li>The name of each Secret that is mounted as a volume in a Pod must appear in the <code>secrets</code> field of the
Pod's ServiceAccount.</li><li>The name of each Secret referenced using <code>envFrom</code> in a Pod must also appear in the <code>secrets</code>
field of the Pod's ServiceAccount.</li><li>The name of each Secret referenced using <code>imagePullSecrets</code> in a Pod must also appear in the <code>secrets</code>
field of the Pod's ServiceAccount.</li></ol><p>By understanding and enforcing these restrictions, cluster administrators can maintain a tighter security profile and ensure that secrets are accessed only by the appropriate resources.</p><h2 id="authenticating-credentials">Authenticating service account credentials</h2><p>ServiceAccounts use signed
<a class="glossary-tooltip" title="A means of representing claims to be transferred between two parties." data-toggle="tooltip" data-placement="top" href="https://www.rfc-editor.org/rfc/rfc7519" target="_blank" aria-label="JSON Web Tokens">JSON Web Tokens</a> (JWTs)
to authenticate to the Kubernetes API server, and to any other system where a
trust relationship exists. Depending on how the token was issued
(either time-limited using a <code>TokenRequest</code> or using a legacy mechanism with
a Secret), a ServiceAccount token might also have an expiry time, an audience,
and a time after which the token <em>starts</em> being valid. When a client that is
acting as a ServiceAccount tries to communicate with the Kubernetes API server,
the client includes an <code>Authorization: Bearer &lt;token&gt;</code> header with the HTTP
request. The API server checks the validity of that bearer token as follows:</p><ol><li>Checks the token signature.</li><li>Checks whether the token has expired.</li><li>Checks whether object references in the token claims are currently valid.</li><li>Checks whether the token is currently valid.</li><li>Checks the audience claims.</li></ol><p>The TokenRequest API produces <em>bound tokens</em> for a ServiceAccount. This
binding is linked to the lifetime of the client, such as a Pod, that is acting
as that ServiceAccount. See <a href="/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">Token Volume Projection</a>
for an example of a bound pod service account token's JWT schema and payload.</p><p>For tokens issued using the <code>TokenRequest</code> API, the API server also checks that
the specific object reference that is using the ServiceAccount still exists,
matching by the <a class="glossary-tooltip" title="A Kubernetes systems-generated string to uniquely identify objects." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/names" target="_blank" aria-label="unique ID">unique ID</a> of that
object. For legacy tokens that are mounted as Secrets in Pods, the API server
checks the token against the Secret.</p><p>For more information about the authentication process, refer to
<a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">Authentication</a>.</p><h3 id="authenticating-in-code">Authenticating service account credentials in your own code</h3><p>If you have services of your own that need to validate Kubernetes service
account credentials, you can use the following methods:</p><ul><li><a href="/docs/reference/kubernetes-api/authentication-resources/token-review-v1/">TokenReview API</a>
(recommended)</li><li>OIDC discovery</li></ul><p>The Kubernetes project recommends that you use the TokenReview API, because
this method invalidates tokens that are bound to API objects such as Secrets,
ServiceAccounts, Pods or Nodes when those objects are deleted. For example, if you
delete the Pod that contains a projected ServiceAccount token, the cluster
invalidates that token immediately and a TokenReview immediately fails.
If you use OIDC validation instead, your clients continue to treat the token
as valid until the token reaches its expiration timestamp.</p><p>Your application should always define the audience that it accepts, and should
check that the token's audiences match the audiences that the application
expects. This helps to minimize the scope of the token so that it can only be
used in your application and nowhere else.</p><h2 id="alternatives">Alternatives</h2><ul><li>Issue your own tokens using another mechanism, and then use
<a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">Webhook Token Authentication</a>
to validate bearer tokens using your own validation service.</li><li>Provide your own identities to Pods.<ul><li><p><a href="https://cert-manager.io/docs/projects/csi-driver-spiffe/">Use the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods</a>.</p><div class="alert alert-secondary callout third-party-content" role="alert">ðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div></li><li><p><a href="https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/">Use a service mesh such as Istio to provide certificates to Pods</a>.</p></li></ul></li><li>Authenticate from outside the cluster to the API server without using service account tokens:<ul><li><a href="/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">Configure the API server to accept OpenID Connect (OIDC) tokens from your identity provider</a>.</li><li>Use service accounts or user accounts created using an external Identity
and Access Management (IAM) service, such as from a cloud provider, to
authenticate to your cluster.</li><li><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Use the CertificateSigningRequest API with client certificates</a>.</li></ul></li><li><a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure the kubelet to retrieve credentials from an image registry</a>.</li><li>Use a Device Plugin to access a virtual Trusted Platform Module (TPM), which
then allows authentication using a private key.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/reference/access-authn-authz/service-accounts-admin/">manage your ServiceAccounts as a cluster administrator</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/configure-service-account/">assign a ServiceAccount to a Pod</a>.</li><li>Read the <a href="/docs/reference/kubernetes-api/authentication-resources/service-account-v1/">ServiceAccount API reference</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Volumes</h1><p>Kubernetes <em>volumes</em> provide a way for containers in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="pod">pod</a>
to access and share data via the filesystem. There are different kinds of volume that you can use for different purposes,
such as:</p><ul><li>populating a configuration file based on a <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/configmap/" target="_blank" aria-label="ConfigMap">ConfigMap</a>
or a <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a></li><li>providing some temporary scratch space for a pod</li><li>sharing a filesystem between two different containers in the same pod</li><li>sharing a filesystem between two different pods (even if those Pods run on different nodes)</li><li>durably storing data so that it stays available even if the Pod restarts or is replaced</li><li>passing configuration information to an app running in a container, based on details of the Pod
the container is in
(for example: telling a <a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" aria-label="sidecar container">sidecar container</a>
what namespace the Pod is running in)</li><li>providing read-only access to data in a different container image</li></ul><p>Data sharing can be between different local processes within a container, or between different containers,
or between Pods.</p><h2 id="why-volumes-are-important">Why volumes are important</h2><ul><li><p><strong>Data persistence:</strong> On-disk files in a container are ephemeral, which presents some problems for
non-trivial applications when running in containers. One problem occurs when
a container crashes or is stopped, the container state is not saved so all of the
files that were created or modified during the lifetime of the container are lost.
After a crash, kubelet restarts the container with a clean state.</p></li><li><p><strong>Shared storage:</strong> Another problem occurs when multiple containers are running in a <code>Pod</code> and
need to share files. It can be challenging to set up
and access a shared filesystem across all of the containers.</p></li></ul><p>The Kubernetes <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volume">volume</a> abstraction
can help you to solve both of these problems.</p><p>Before you learn about volumes, PersistentVolumes and PersistentVolumeClaims, you should read up
about <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> and make sure that you understand how
Kubernetes uses Pods to run containers.</p><h2 id="how-volumes-work">How volumes work</h2><p>Kubernetes supports many types of volumes. A <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>
can use any number of volume types simultaneously.
<a href="/docs/concepts/storage/ephemeral-volumes/">Ephemeral volume</a> types have a lifetime linked to a specific Pod,
but <a href="/docs/concepts/storage/persistent-volumes/">persistent volumes</a> exist beyond
the lifetime of any individual pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;
however, Kubernetes does not destroy persistent volumes.
For any kind of volume in a given pod, data is preserved across container restarts.</p><p>At its core, a volume is a directory, possibly with some data in it, which
is accessible to the containers in a pod. How that directory comes to be, the
medium that backs it, and the contents of it are determined by the particular
volume type used.</p><p>To use a volume, specify the volumes to provide for the Pod in <code>.spec.volumes</code>
and declare where to mount those volumes into containers in <code>.spec.containers[*].volumeMounts</code>.</p><p>When a pod is launched, a process in the container sees a filesystem view composed from the initial contents of
the <a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-image" target="_blank" aria-label="container image">container image</a>, plus volumes
(if defined) mounted inside the container.
The process sees a root filesystem that initially matches the contents of the container image.
Any writes to within that filesystem hierarchy, if allowed, affect what that process views
when it performs a subsequent filesystem access.
Volumes are mounted at <a href="#using-subpath">specified paths</a> within the container filesystem.
For each container defined within a Pod, you must independently specify where
to mount each volume that the container uses.</p><p>Volumes cannot mount within other volumes (but see <a href="#using-subpath">Using subPath</a>
for a related mechanism). Also, a volume cannot contain a hard link to anything in
a different volume.</p><h2 id="volume-types">Types of volumes</h2><p>Kubernetes supports several types of volumes.</p><h3 id="awselasticblockstore">awsElasticBlockStore (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>awsElasticBlockStore</code> type
are redirected to the <code>ebs.csi.aws.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> driver.</p><p>The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS</a>
third party storage driver instead.</p><h3 id="azuredisk">azureDisk (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>azureDisk</code> type
are redirected to the <code>disk.csi.azure.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> driver.</p><p>The AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver">Azure Disk</a>
third party storage driver instead.</p><h3 id="azurefile">azureFile (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>azureFile</code> type
are redirected to the <code>file.csi.azure.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> driver.</p><p>The AzureFile in-tree storage driver was deprecated in the Kubernetes v1.21 release
and then removed entirely in the v1.30 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/azurefile-csi-driver">Azure File</a>
third party storage driver instead.</p><h3 id="cephfs">cephfs (removed)</h3><p>Kubernetes 1.34 does not include a <code>cephfs</code> volume type.</p><p>The <code>cephfs</code> in-tree storage driver was deprecated in the Kubernetes v1.28
release and then removed entirely in the v1.31 release.</p><h3 id="cinder">cinder (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>cinder</code> type
are redirected to the <code>cinder.csi.openstack.org</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> driver.</p><p>The OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release
and then removed entirely in the v1.26 release.</p><p>The Kubernetes project suggests that you use the
<a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md">OpenStack Cinder</a>
third party storage driver instead.</p><h3 id="configmap">configMap</h3><p>A <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>
provides a way to inject configuration data into pods.
The data stored in a ConfigMap can be referenced in a volume of type
<code>configMap</code> and then consumed by containerized applications running in a pod.</p><p>When referencing a ConfigMap, you provide the name of the ConfigMap in the
volume. You can customize the path to use for a specific
entry in the ConfigMap. The following configuration shows how to mount
the <code>log-config</code> ConfigMap onto a Pod called <code>configmap-pod</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>configmap-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo "The app is running!" &amp;&amp; tail -f /dev/null'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>config-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/etc/config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>config-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">configMap</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>log-config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>log_level<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>log_level.conf<span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>log-config</code> ConfigMap is mounted as a volume, and all contents stored in
its <code>log_level</code> entry are mounted into the Pod at path <code>/etc/config/log_level.conf</code>.
Note that this path is derived from the volume's <code>mountPath</code> and the <code>path</code>
keyed with <code>log_level</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li><p>You must <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/#create-a-configmap">create a ConfigMap</a>
before you can use it.</p></li><li><p>A ConfigMap is always mounted as <code>readOnly</code>.</p></li><li><p>A container using a ConfigMap as a <a href="#using-subpath"><code>subPath</code></a> volume mount will not
receive updates when the ConfigMap changes.</p></li><li><p>Text data is exposed as files using the UTF-8 character encoding.
For other character encodings, use <code>binaryData</code>.</p></li></ul></div><h3 id="downwardapi">downwardAPI</h3><p>A <code>downwardAPI</code> volume makes <a class="glossary-tooltip" title="A mechanism to expose Pod and container field values to code running in a container." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/downward-api/" target="_blank" aria-label="downward API">downward API</a>
data available to applications. Within the volume, you can find the exposed
data as read-only files in plain text format.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A container using the downward API as a <a href="#using-subpath"><code>subPath</code></a> volume mount does not
receive updates when field values change.</div><p>See <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">Expose Pod Information to Containers Through Files</a>
to learn more.</p><h3 id="emptydir">emptyDir</h3><p>For a Pod that defines an <code>emptyDir</code> volume, the volume is created when the Pod is assigned to a node.
As the name says, the <code>emptyDir</code> volume is initially empty. All containers in the Pod can read and write the same
files in the <code>emptyDir</code> volume, though that volume can be mounted at the same
or different paths in each container. When a Pod is removed from a node for
any reason, the data in the <code>emptyDir</code> is deleted permanently.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A container crashing does <em>not</em> remove a Pod from a node. The data in an <code>emptyDir</code> volume
is safe across container crashes.</div><p>Some uses for an <code>emptyDir</code> are:</p><ul><li>scratch space, such as for a disk-based merge sort</li><li>checkpointing a long computation for recovery from crashes</li><li>holding files that a content-manager container fetches while a webserver
container serves the data</li></ul><p>The <code>emptyDir.medium</code> field controls where <code>emptyDir</code> volumes are stored. By
default <code>emptyDir</code> volumes are stored on whatever medium that backs the node
such as disk, SSD, or network storage, depending on your environment. If you set
the <code>emptyDir.medium</code> field to <code>"Memory"</code>, Kubernetes mounts a tmpfs (RAM-backed
filesystem) for you instead. While tmpfs is very fast, be aware that, unlike
disks, files you write count against the memory limit of the container that wrote them.</p><p>A size limit can be specified for the default medium, which limits the capacity
of the <code>emptyDir</code> volume. The storage is allocated from
<a href="/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage">node ephemeral storage</a>.
If that is filled up from another source (for example, log files or image overlays),
the <code>emptyDir</code> may run out of capacity before this limit.
If no size is specified, memory-backed volumes are sized to node allocatable memory.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Please check <a href="/docs/concepts/configuration/manage-resources-containers/#memory-backed-emptydir">here</a>
for points to note in terms of resource management when using memory-backed <code>emptyDir</code>.</div><h4 id="emptydir-configuration-example">emptyDir configuration example</h4><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/cache<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cache-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cache-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sizeLimit</span>:<span style="color:#bbb"> </span>500Mi<span style="color:#bbb">
</span></span></span></code></pre></div><h4 id="emptydir-memory-configuration-example">emptyDir memory configuration example</h4><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/cache<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cache-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cache-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sizeLimit</span>:<span style="color:#bbb"> </span>500Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">medium</span>:<span style="color:#bbb"> </span>Memory<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="fc">fc (fibre channel)</h3><p>An <code>fc</code> volume type allows an existing fibre channel block storage volume
to be mounted in a Pod. You can specify single or multiple target world wide names (WWNs)
using the parameter <code>targetWWNs</code> in your Volume configuration. If multiple WWNs are specified,
targetWWNs expect that those WWNs are from multi-path connections.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs
beforehand so that Kubernetes hosts can access them.</div><h3 id="gcepersistentdisk">gcePersistentDisk (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>gcePersistentDisk</code> type
are redirected to the <code>pd.csi.storage.gke.io</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> driver.</p><p>The <code>gcePersistentDisk</code> in-tree storage driver was deprecated in the Kubernetes v1.17 release
and then removed entirely in the v1.28 release.</p><p>The Kubernetes project suggests that you use the
<a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">Google Compute Engine Persistent Disk CSI</a>
third party storage driver instead.</p><h3 id="gitrepo">gitRepo (deprecated)</h3><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><p>The <code>gitRepo</code> volume plugin is deprecated and is disabled by default.</p><p>To provision a Pod that has a Git repository mounted, you can mount an
<a href="#emptydir"><code>emptyDir</code></a> volume into an <a href="/docs/concepts/workloads/pods/init-containers/">init container</a>
that clones the repo using Git, then mount the <a href="#emptydir">EmptyDir</a> into the Pod's container.</p><hr/><p>You can restrict the use of <code>gitRepo</code> volumes in your cluster using
<a href="/docs/concepts/policy/">policies</a>, such as
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicy</a>.
You can use the following Common Expression Language (CEL) expression as
part of a policy to reject use of <code>gitRepo</code> volumes:</p><pre tabindex="0"><code class="language-cel" data-lang="cel">!has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))
</code></pre></div><p>You can use this deprecated storage plugin in your cluster if you explicitly
enable the <code>GitRepoVolumeDriver</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>A <code>gitRepo</code> volume is an example of a volume plugin. This plugin
mounts an empty directory and clones a git repository into this directory
for your Pod to use.</p><p>Here is an example of a <code>gitRepo</code> volume:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/mypath<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>git-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>git-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">gitRepo</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">repository</span>:<span style="color:#bbb"> </span><span style="color:#b44">"git@somewhere:me/my-git-repository.git"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">revision</span>:<span style="color:#bbb"> </span><span style="color:#b44">"22f1d8406d464b0c0874075539c1f2e96c253775"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="glusterfs">glusterfs (removed)</h3><p>Kubernetes 1.34 does not include a <code>glusterfs</code> volume type.</p><p>The GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release
and then removed entirely in the v1.26 release.</p><h3 id="hostpath">hostPath</h3><p>A <code>hostPath</code> volume mounts a file or directory from the host node's filesystem
into your Pod. This is not something that most Pods will need, but it offers a
powerful escape hatch for some applications.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><p>Using the <code>hostPath</code> volume type presents many security risks.
If you can avoid using a <code>hostPath</code> volume, you should. For example,
define a <a href="#local"><code>local</code> PersistentVolume</a>, and use that instead.</p><p>If you are restricting access to specific directories on the node using
admission-time validation, that restriction is only effective when you
additionally require that any mounts of that <code>hostPath</code> volume are
<strong>read only</strong>. If you allow a read-write mount of any host path by an
untrusted Pod, the containers in that Pod may be able to subvert the
read-write host mount.</p><hr/><p>Take care when using <code>hostPath</code> volumes, whether these are mounted as read-only
or as read-write, because:</p><ul><li>Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs
(such as the container runtime socket) that can be used for container escape or to attack other
parts of the cluster.</li><li>Pods with identical configuration (such as created from a PodTemplate) may
behave differently on different nodes due to different files on the nodes.</li><li><code>hostPath</code> volume usage is not treated as ephemeral storage usage.
You need to monitor the disk usage by yourself because excessive <code>hostPath</code> disk
usage will lead to disk pressure on the node.</li></ul></div><p>Some uses for a <code>hostPath</code> are:</p><ul><li>running a container that needs access to node-level system components
(such as a container that transfers system logs to a central location,
accessing those logs using a read-only mount of <code>/var/log</code>)</li><li>making a configuration file stored on the host system available read-only
to a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static pod">static pod</a>;
unlike normal Pods, static Pods cannot access ConfigMaps</li></ul><h4 id="hostpath-volume-types"><code>hostPath</code> volume types</h4><p>In addition to the required <code>path</code> property, you can optionally specify a
<code>type</code> for a <code>hostPath</code> volume.</p><p>The available values for <code>type</code> are:</p><table><thead><tr><th style="text-align:left">Value</th><th style="text-align:left">Behavior</th></tr></thead><tbody><tr><td style="text-align:left"><code>â€Œ""</code></td><td style="text-align:left">Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the <code>hostPath</code> volume.</td></tr><tr><td style="text-align:left"><code>DirectoryOrCreate</code></td><td style="text-align:left">If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.</td></tr><tr><td style="text-align:left"><code>Directory</code></td><td style="text-align:left">A directory must exist at the given path</td></tr><tr><td style="text-align:left"><code>FileOrCreate</code></td><td style="text-align:left">If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.</td></tr><tr><td style="text-align:left"><code>File</code></td><td style="text-align:left">A file must exist at the given path</td></tr><tr><td style="text-align:left"><code>Socket</code></td><td style="text-align:left">A UNIX socket must exist at the given path</td></tr><tr><td style="text-align:left"><code>CharDevice</code></td><td style="text-align:left"><em>(Linux nodes only)</em> A character device must exist at the given path</td></tr><tr><td style="text-align:left"><code>BlockDevice</code></td><td style="text-align:left"><em>(Linux nodes only)</em> A block device must exist at the given path</td></tr></tbody></table><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>The <code>FileOrCreate</code> mode does <strong>not</strong> create the parent directory of the file. If the parent directory
of the mounted file does not exist, the pod fails to start. To ensure that this mode works,
you can try to mount directories and files separately, as shown in the
<a href="#hostpath-fileorcreate-example"><code>FileOrCreate</code> example</a> for <code>hostPath</code>.</div><p>Some files or directories created on the underlying hosts might only be
accessible by root. You then either need to run your process as root in a
<a href="/docs/tasks/configure-pod-container/security-context/">privileged container</a>
or modify the file permissions on the host to read from or write to a <code>hostPath</code> volume.</p><h4 id="hostpath-configuration-example">hostPath configuration example</h4><ul class="nav nav-tabs" id="hostpath-examples" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#hostpath-examples-0" role="tab" aria-controls="hostpath-examples-0" aria-selected="true">Linux node</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#hostpath-examples-1" role="tab" aria-controls="hostpath-examples-1">Windows node</a></li></ul><div class="tab-content" id="hostpath-examples"><div id="hostpath-examples-0" class="tab-pane show active" role="tabpanel" aria-labelledby="hostpath-examples-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># This manifest mounts /data/foo on the host as /foo inside the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># single container that runs within the hostpath-example-linux Pod.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># The mount into the container is read-only.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hostpath-example-linux<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">os</span>:<span style="color:#bbb"> </span>{<span style="color:#bbb"> </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>linux }<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span>linux<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># mount /data/foo, but only if that directory already exists</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/data/foo<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># directory location on host</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Directory<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># this field is optional</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="hostpath-examples-1" class="tab-pane" role="tabpanel" aria-labelledby="hostpath-examples-1"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># This manifest mounts C:\Data\foo on the host as C:\foo, inside the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># single container that runs within the hostpath-example-windows Pod.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># The mount into the container is read-only.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hostpath-example-windows<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">os</span>:<span style="color:#bbb"> </span>{<span style="color:#bbb"> </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>windows }<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>microsoft/windowsservercore:1709<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"C:\\foo"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># mount C:\Data\foo from the host, but only if that directory already exists</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"C:\\Data\\foo"</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># directory location on host</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Directory      <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># this field is optional</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div></div><h4 id="hostpath-fileorcreate-example">hostPath FileOrCreate configuration example</h4><p>The following manifest defines a Pod that mounts <code>/var/local/aaa</code>
inside the single container in the Pod. If the node does not
already have a path <code>/var/local/aaa</code>, the kubelet creates
it as a directory and then mounts it into the Pod.</p><p>If <code>/var/local/aaa</code> already exists but is not a directory,
the Pod fails. Additionally, the kubelet attempts to make
a file named <code>/var/local/aaa/1.txt</code> inside that directory
(as seen from the host); if something already exists at
that path and isn't a regular file, the Pod fails.</p><p>Here's the example manifest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">os</span>:<span style="color:#bbb"> </span>{<span style="color:#bbb"> </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>linux }<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span>linux<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/test-webserver:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/local/aaa<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mydir<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/local/aaa/1.txt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myfile<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mydir<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Ensure the file directory is created.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/var/local/aaa<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>DirectoryOrCreate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myfile<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/var/local/aaa/1.txt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>FileOrCreate<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="image">image</h3><div class="feature-state-notice feature-beta" title="Feature Gate: ImageVolume"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><p>An <code>image</code> volume source represents an OCI object (a container image or
artifact) which is available on the kubelet's host machine.</p><p>An example of using the <code>image</code> volume source is:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/image-volumes.yaml" download="pods/image-volumes.yaml"><code>pods/image-volumes.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-image-volumes-yaml&quot;)" title="Copy pods/image-volumes.yaml to clipboard"/></div><div class="includecode" id="pods-image-volumes-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>image-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>shell<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"infinity"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">reference</span>:<span style="color:#bbb"> </span>quay.io/crio/artifact:v2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">pullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The volume is resolved at pod startup depending on which <code>pullPolicy</code> value is
provided:</p><dl><dt><code>Always</code></dt><dd>the kubelet always attempts to pull the reference. If the pull fails,
the kubelet sets the Pod to <code>Failed</code>.</dd><dt><code>Never</code></dt><dd>the kubelet never pulls the reference and only uses a local image or artifact.
The Pod becomes <code>Failed</code> if any layers of the image aren't already present locally,
or if the manifest for that image isn't already cached.</dd><dt><code>IfNotPresent</code></dt><dd>the kubelet pulls if the reference isn't already present on disk. The Pod becomes
<code>Failed</code> if the reference isn't present and the pull fails.</dd></dl><p>The volume gets re-resolved if the pod gets deleted and recreated, which means
that new remote content will become available on pod recreation. A failure to
resolve or pull the image during pod startup will block containers from starting
and may add significant latency. Failures will be retried using normal volume
backoff and will be reported on the pod reason and message.</p><p>The types of objects that may be mounted by this volume are defined by the
container runtime implementation on a host machine. At a minimum, they must include
all valid types supported by the container image field. The OCI object gets
mounted in a single directory (<code>spec.containers[*].volumeMounts.mountPath</code>)
and will be mounted read-only. On Linux, the container runtime typically also mounts the
volume with file execution blocked (<code>noexec</code>).</p><p>Besides that:</p><ul><li><a href="/docs/concepts/storage/volumes/#using-subpath"><code>subPath</code></a> or
<a href="/docs/concepts/storage/volumes/#using-subpath-expanded-environment"><code>subPathExpr</code></a>
mounts for containers (<code>spec.containers[*].volumeMounts.[subPath,subPathExpr]</code>)
are only supported from Kubernetes v1.33.</li><li>The field <code>spec.securityContext.fsGroupChangePolicy</code> has no effect on this
volume type.</li><li>The <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages"><code>AlwaysPullImages</code> Admission Controller</a>
does also work for this volume source like for container images.</li></ul><p>The following fields are available for the <code>image</code> type:</p><dl><dt><code>reference</code></dt><dd>Artifact reference to be used. For example, you could specify
<code>registry.k8s.io/conformance:v1.34.0</code> to load the
files from the Kubernetes conformance test image. Behaves in the same way as
<code>pod.spec.containers[*].image</code>. Pull secrets will be assembled in the same way
as for the container image by looking up node credentials, service account image
pull secrets, and pod spec image pull secrets. This field is optional to allow
higher level config management to default or override container images in
workload controllers like Deployments and StatefulSets.
<a href="/docs/concepts/containers/images/">More info about container images</a></dd><dt><code>pullPolicy</code></dt><dd>Policy for pulling OCI objects. Possible values are: <code>Always</code>, <code>Never</code> or
<code>IfNotPresent</code>. Defaults to <code>Always</code> if <code>:latest</code> tag is specified, or
<code>IfNotPresent</code> otherwise.</dd></dl><p>See the <a href="/docs/tasks/configure-pod-container/image-volumes/"><em>Use an Image Volume With a Pod</em></a>
example for more details on how to use the volume source.</p><h3 id="iscsi">iscsi</h3><p>An <code>iscsi</code> volume allows an existing iSCSI (SCSI over IP) volume to be mounted
into your Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is removed, the
contents of an <code>iscsi</code> volume are preserved and the volume is merely
unmounted. This means that an iscsi volume can be pre-populated with data, and
that data can be shared between pods.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must have your own iSCSI server running with the volume created before you can use it.</div><p>A feature of iSCSI is that it can be mounted as read-only by multiple consumers
simultaneously. This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need. Unfortunately,
iSCSI volumes can only be mounted by a single consumer in read-write mode.
Simultaneous writers are not allowed.</p><h3 id="local">local</h3><p>A <code>local</code> volume represents a mounted local storage device such as a disk,
partition or directory.</p><p>Local volumes can only be used as a statically created PersistentVolume. Dynamic
provisioning is not supported.</p><p>Compared to <code>hostPath</code> volumes, <code>local</code> volumes are used in a durable and
portable manner without manually scheduling pods to nodes. The system is aware
of the volume's node constraints by looking at the node affinity on the PersistentVolume.</p><p>However, <code>local</code> volumes are subject to the availability of the underlying
node and are not suitable for all applications. If a node becomes unhealthy,
then the <code>local</code> volume becomes inaccessible to the pod. The pod using this volume
is unable to run. Applications using <code>local</code> volumes must be able to tolerate this
reduced availability, as well as potential data loss, depending on the
durability characteristics of the underlying disk.</p><p>The following example shows a PersistentVolume using a <code>local</code> volume and
<code>nodeAffinity</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-pv<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>100Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>local-storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">local</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/mnt/disks/ssd1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">required</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>kubernetes.io/hostname<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- example-node<span style="color:#bbb">
</span></span></span></code></pre></div><p>You must set a PersistentVolume <code>nodeAffinity</code> when using <code>local</code> volumes.
The Kubernetes scheduler uses the PersistentVolume <code>nodeAffinity</code> to schedule
these Pods to the correct node.</p><p>PersistentVolume <code>volumeMode</code> can be set to "Block" (instead of the default
value "Filesystem") to expose the local volume as a raw block device.</p><p>When using local volumes, it is recommended to create a StorageClass with
<code>volumeBindingMode</code> set to <code>WaitForFirstConsumer</code>. For more details, see the
local <a href="/docs/concepts/storage/storage-classes/#local">StorageClass</a> example.
Delaying volume binding ensures that the PersistentVolumeClaim binding decision
will also be evaluated with any other node constraints the Pod may have,
such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.</p><p>An external static provisioner can be run separately for improved management of
the local volume lifecycle. Note that this provisioner does not support dynamic
provisioning yet. For an example on how to run an external local provisioner, see the
<a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">local volume provisioner user guide</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The local PersistentVolume requires manual cleanup and deletion by the
user if the external static provisioner is not used to manage the volume
lifecycle.</div><h3 id="nfs">nfs</h3><p>An <code>nfs</code> volume allows an existing NFS (Network File System) share to be
mounted into a Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is
removed, the contents of an <code>nfs</code> volume are preserved and the volume is merely
unmounted. This means that an NFS volume can be pre-populated with data, and
that data can be shared between pods. NFS can be mounted by multiple
writers simultaneously.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/my-nfs-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nfs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span>my-nfs-server.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/my-nfs-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>You must have your own NFS server running with the share exported before you can use it.</p><p>Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or
use <a href="https://man7.org/linux/man-pages/man5/nfsmount.conf.5.html">/etc/nfsmount.conf</a>.
You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.</p></div><h3 id="persistentvolumeclaim">persistentVolumeClaim</h3><p>A <code>persistentVolumeClaim</code> volume is used to mount a
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> into a Pod. PersistentVolumeClaims
are a way for users to "claim" durable storage (such as an iSCSI volume)
without knowing the details of the particular cloud environment.</p><p>See the information about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> for more
details.</p><h3 id="portworxvolume">portworxVolume (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [deprecated]</code></div><p>A <code>portworxVolume</code> is an elastic block storage layer that runs hyperconverged with
Kubernetes. <a href="https://portworx.com/use-case/kubernetes-storage/">Portworx</a> fingerprints storage
in a server, tiers based on capabilities, and aggregates capacity across multiple servers.
Portworx runs in-guest in virtual machines or on bare metal Linux nodes.</p><p>A <code>portworxVolume</code> can be dynamically created through Kubernetes or it can also
be pre-provisioned and referenced inside a Pod.
Here is an example Pod referencing a pre-provisioned Portworx volume:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-portworx-volume-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/test-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pxvol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pxvol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># This Portworx volume must already exist.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">portworxVolume</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeID</span>:<span style="color:#bbb"> </span><span style="color:#b44">"pxvol"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">fsType</span>:<span style="color:#bbb"> </span><span style="color:#b44">"&lt;fs-type&gt;"</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Make sure you have an existing PortworxVolume with name <code>pxvol</code>
before using it in the Pod.</div><h4 id="portworx-csi-migration">Portworx CSI migration</h4><div class="feature-state-notice feature-stable" title="Feature Gate: CSIMigrationPortworx"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>In Kubernetes 1.34, all operations for the in-tree
Portworx volumes are redirected to the <code>pxd.portworx.com</code>
Container Storage Interface (CSI) Driver by default.<br/><a href="https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi">Portworx CSI Driver</a>
must be installed on the cluster.</p><h3 id="projected">projected</h3><p>A projected volume maps several existing volume sources into the same
directory. For more details, see <a href="/docs/concepts/storage/projected-volumes/">projected volumes</a>.</p><h3 id="rbd">rbd (removed)</h3><p>Kubernetes 1.34 does not include a <code>rbd</code> volume type.</p><p>The <a href="https://docs.ceph.com/en/latest/rbd/">Rados Block Device</a> (RBD) in-tree storage driver
and its csi migration support were deprecated in the Kubernetes v1.28 release
and then removed entirely in the v1.31 release.</p><h3 id="secret">secret</h3><p>A <code>secret</code> volume is used to pass sensitive information, such as passwords, to
Pods. You can store secrets in the Kubernetes API and mount them as files for
use by pods without coupling to Kubernetes directly. <code>secret</code> volumes are
backed by tmpfs (a RAM-backed filesystem) so they are never written to
non-volatile storage.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li><p>You must create a Secret in the Kubernetes API before you can use it.</p></li><li><p>A Secret is always mounted as <code>readOnly</code>.</p></li><li><p>A container using a Secret as a <a href="#using-subpath"><code>subPath</code></a> volume mount will not
receive Secret updates.</p></li></ul></div><p>For more details, see <a href="/docs/concepts/configuration/secret/">Configuring Secrets</a>.</p><h3 id="vspherevolume">vsphereVolume (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>vsphereVolume</code> type
are redirected to the <code>csi.vsphere.vmware.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> driver.</p><p>The <code>vsphereVolume</code> in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.30 release.</p><p>The Kubernetes project suggests that you use the
<a href="https://github.com/kubernetes-sigs/vsphere-csi-driver">vSphere CSI</a>
third party storage driver instead.</p><h2 id="using-subpath">Using subPath</h2><p>Sometimes, it is useful to share one volume for multiple uses in a single pod.
The <code>volumeMounts[*].subPath</code> property specifies a sub-path inside the referenced volume
instead of its root.</p><p>The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)
using a single, shared volume. This sample <code>subPath</code> configuration is not recommended
for production use.</p><p>The PHP application's code and assets map to the volume's <code>html</code> folder and
the MySQL database is stored in the volume's <code>mysql</code> folder. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-lamp-site<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>MYSQL_ROOT_PASSWORD<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"rootpasswd"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>site-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">subPath</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>php<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>php:7.0-apache<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/www/html<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>site-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">subPath</span>:<span style="color:#bbb"> </span>html<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>site-data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">persistentVolumeClaim</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">claimName</span>:<span style="color:#bbb"> </span>my-lamp-site-data<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="using-subpath-expanded-environment">Using subPath with expanded environment variables</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>Use the <code>subPathExpr</code> field to construct <code>subPath</code> directory names from
downward API environment variables.
The <code>subPath</code> and <code>subPathExpr</code> properties are mutually exclusive.</p><p>In this example, a <code>Pod</code> uses <code>subPathExpr</code> to create a directory <code>pod1</code> within
the <code>hostPath</code> volume <code>/var/log/pods</code>.
The <code>hostPath</code> volume takes the <code>Pod</code> name from the <code>downwardAPI</code>.
The host directory <code>/var/log/pods/pod1</code> is mounted at <code>/logs</code> in the container.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>container1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>POD_NAME<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">valueFrom</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">fieldRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">fieldPath</span>:<span style="color:#bbb"> </span>metadata.name<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">"sh"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-c"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt"</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>workdir1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/logs<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The variable expansion uses round brackets (not curly brackets).</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">subPathExpr</span>:<span style="color:#bbb"> </span>$(POD_NAME)<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>workdir1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/var/log/pods<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="resources">Resources</h2><p>The storage medium (such as Disk or SSD) of an <code>emptyDir</code> volume is determined by the
medium of the filesystem holding the kubelet root dir (typically
<code>/var/lib/kubelet</code>). There is no limit on how much space an <code>emptyDir</code> or
<code>hostPath</code> volume can consume, and no isolation between containers or
pods.</p><p>To learn about requesting space using a resource specification, see
<a href="/docs/concepts/configuration/manage-resources-containers/">how to manage resources</a>.</p><h2 id="out-of-tree-volume-plugins">Out-of-tree volume plugins</h2><p>The out-of-tree volume plugins include
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="Container Storage Interface">Container Storage Interface</a> (CSI), and also
FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins
without adding their plugin source code to the Kubernetes repository.</p><p>Previously, all volume plugins were "in-tree". The "in-tree" plugins were built, linked, compiled,
and shipped with the core Kubernetes binaries. This meant that adding a new storage system to
Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.</p><p>Both CSI and FlexVolume allow volume plugins to be developed independently of
the Kubernetes code base, and deployed (installed) on Kubernetes clusters as
extensions.</p><p>For storage vendors looking to create an out-of-tree volume plugin, please refer
to the <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">volume plugin FAQ</a>.</p><h3 id="csi">csi</h3><p><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface</a>
(CSI) defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.</p><p>Please read the <a href="https://git.k8s.io/design-proposals-archive/storage/container-storage-interface.md">CSI design proposal</a>
for more information.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Support for CSI spec versions 0.2 and 0.3 is deprecated in Kubernetes
v1.13 and will be removed in a future release.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>CSI drivers may not be compatible across all Kubernetes releases.
Please check the specific CSI driver's documentation for supported
deployments steps for each Kubernetes release and a compatibility matrix.</div><p>Once a CSI-compatible volume driver is deployed on a Kubernetes cluster, users
may use the <code>csi</code> volume type to attach or mount the volumes exposed by the
CSI driver.</p><p>A <code>csi</code> volume can be used in a Pod in three different ways:</p><ul><li>through a reference to a <a href="#persistentvolumeclaim">PersistentVolumeClaim</a></li><li>with a <a href="/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">generic ephemeral volume</a></li><li>with a <a href="/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">CSI ephemeral volume</a>
if the driver supports that</li></ul><p>The following fields are available to storage administrators to configure a CSI
persistent volume:</p><ul><li><code>driver</code>: A string value that specifies the name of the volume driver to use.
This value must correspond to the value returned in the <code>GetPluginInfoResponse</code>
by the CSI driver as defined in the
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo">CSI spec</a>.
It is used by Kubernetes to identify which CSI driver to call out to, and by
CSI driver components to identify which PV objects belong to the CSI driver.</li><li><code>volumeHandle</code>: A string value that uniquely identifies the volume. This value
must correspond to the value returned in the <code>volume.id</code> field of the
<code>CreateVolumeResponse</code> by the CSI driver as defined in the
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI spec</a>.
The value is passed as <code>volume_id</code> in all calls to the CSI volume driver when
referencing the volume.</li><li><code>readOnly</code>: An optional boolean value indicating whether the volume is to be
"ControllerPublished" (attached) as read only. Default is false. This value is passed
to the CSI driver via the <code>readonly</code> field in the <code>ControllerPublishVolumeRequest</code>.</li><li><code>fsType</code>: If the PV's <code>VolumeMode</code> is <code>Filesystem</code>, then this field may be used
to specify the filesystem that should be used to mount the volume. If the
volume has not been formatted and formatting is supported, this value will be
used to format the volume.
This value is passed to the CSI driver via the <code>VolumeCapability</code> field of
<code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, and
<code>NodePublishVolumeRequest</code>.</li><li><code>volumeAttributes</code>: A map of string to string that specifies static properties
of a volume. This map must correspond to the map returned in the
<code>volume.attributes</code> field of the <code>CreateVolumeResponse</code> by the CSI driver as
defined in the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI spec</a>.
The map is passed to the CSI driver via the <code>volume_context</code> field in the
<code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, and
<code>NodePublishVolumeRequest</code>.</li><li><code>controllerPublishSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>ControllerPublishVolume</code> and <code>ControllerUnpublishVolume</code> calls. This field is
optional, and may be empty if no secret is required. If the Secret
contains more than one secret, all secrets are passed.</li><li><code>nodeExpandSecretRef</code>: A reference to the secret containing sensitive
information to pass to the CSI driver to complete the CSI
<code>NodeExpandVolume</code> call. This field is optional and may be empty if no
secret is required. If the object contains more than one secret, all
secrets are passed. When you have configured secret data for node-initiated
volume expansion, the kubelet passes that data via the <code>NodeExpandVolume()</code>
call to the CSI driver. All supported versions of Kubernetes offer the
<code>nodeExpandSecretRef</code> field, and have it available by default. Kubernetes releases
prior to v1.25 did not include this support.</li><li>Enable the <a href="/docs/reference/command-line-tools-reference/feature-gates-removed/">feature gate</a>
named <code>CSINodeExpandSecret</code> for each kube-apiserver and for the kubelet on every
node. Since Kubernetes version 1.27, this feature has been enabled by default
and no explicit enablement of the feature gate is required.
You must also be using a CSI driver that supports or requires secret data during
node-initiated storage resize operations.</li><li><code>nodePublishSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>NodePublishVolume</code> call. This field is optional and may be empty if no
secret is required. If the secret object contains more than one secret, all
secrets are passed.</li><li><code>nodeStageSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>NodeStageVolume</code> call. This field is optional and may be empty if no secret
is required. If the Secret contains more than one secret, all secrets
are passed.</li></ul><h4 id="csi-raw-block-volume-support">CSI raw block volume support</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>Vendors with external CSI drivers can implement raw block volume support
in Kubernetes workloads.</p><p>You can set up your
<a href="/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">PersistentVolume/PersistentVolumeClaim with raw block volume support</a>
as usual, without any CSI-specific changes.</p><h4 id="csi-ephemeral-volumes">CSI ephemeral volumes</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across pod restarts. See
<a href="/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">Ephemeral Volumes</a>
for more information.</p><p>For more information on how to develop a CSI driver, refer to the
<a href="https://kubernetes-csi.github.io/docs/">kubernetes-csi documentation</a></p><h4 id="windows-csi-proxy">Windows CSI proxy</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [stable]</code></div><p>CSI node plugins need to perform various privileged
operations like scanning of disk devices and mounting of file systems. These operations
differ for each host operating system. For Linux worker nodes, containerized CSI node
plugins are typically deployed as privileged containers. For Windows worker nodes,
privileged operations for containerized CSI node plugins is supported using
<a href="https://github.com/kubernetes-csi/csi-proxy">csi-proxy</a>, a community-managed,
stand-alone binary that needs to be pre-installed on each Windows node.</p><p>For more details, refer to the deployment guide of the CSI plugin you wish to deploy.</p><h4 id="migrating-to-csi-drivers-from-in-tree-plugins">Migrating to CSI drivers from in-tree plugins</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>The <code>CSIMigration</code> feature directs operations against existing in-tree
plugins to corresponding CSI plugins (which are expected to be installed and configured).
As a result, operators do not have to make any
configuration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims
(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Existing PVs created by an in-tree volume plugin can still be used in the future without any configuration
changes, even after the migration to CSI is completed for that volume type, and even after you upgrade to a
version of Kubernetes that doesn't have compiled-in support for that kind of storage.</p><p>As part of that migration, you - or another cluster administrator - <strong>must</strong> have installed and configured
the appropriate CSI driver for that storage. The core of Kubernetes does not install that software for you.</p><hr/><p>After that migration, you can also define new PVCs and PVs that refer to the legacy, built-in
storage integrations.
Provided you have the appropriate CSI driver installed and configured, the PV creation continues
to work, even for brand new volumes. The actual storage management now happens through
the CSI driver.</p></div><p>The operations and features that are supported include:
provisioning/delete, attach/detach, mount/unmount and resizing of volumes.</p><p>In-tree plugins that support <code>CSIMigration</code> and have a corresponding CSI driver implemented
are listed in <a href="#volume-types">Types of Volumes</a>.</p><h3 id="flexvolume">flexVolume (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [deprecated]</code></div><p>FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface
with storage drivers. The FlexVolume driver binaries must be installed in a pre-defined
volume plugin path on each node and in some cases the control plane nodes as well.</p><p>Pods interact with FlexVolume drivers through the <code>flexVolume</code> in-tree volume plugin.</p><p>The following FlexVolume <a href="https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows">plugins</a>,
deployed as PowerShell scripts on the host, support Windows nodes:</p><ul><li><a href="https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd">SMB</a></li><li><a href="https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd">iSCSI</a></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.</p><p>Maintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.
Users of FlexVolume should move their workloads to use the equivalent CSI Driver.</p></div><h2 id="mount-propagation">Mount propagation</h2><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Mount propagation is a low-level feature that does not work consistently on all
volume types. The Kubernetes project recommends only using mount propagation with <code>hostPath</code>
or memory-backed <code>emptyDir</code> volumes. See
<a href="https://github.com/kubernetes/kubernetes/issues/95049">Kubernetes issue #95049</a>
for more context.</div><p>Mount propagation allows for sharing volumes mounted by a container to
other containers in the same pod, or even to other pods on the same node.</p><p>Mount propagation of a volume is controlled by the <code>mountPropagation</code> field
in <code>containers[*].volumeMounts</code>. Its values are:</p><ul><li><p><code>None</code> - This volume mount will not receive any subsequent mounts
that are mounted to this volume or any of its subdirectories by the host.
In similar fashion, no mounts created by the container will be visible on
the host. This is the default mode.</p><p>This mode is equal to <code>rprivate</code> mount propagation as described in
<a href="https://man7.org/linux/man-pages/man8/mount.8.html"><code>mount(8)</code></a></p><p>However, the CRI runtime may choose <code>rslave</code> mount propagation (i.e.,
<code>HostToContainer</code>) instead, when <code>rprivate</code> propagation is not applicable.
cri-dockerd (Docker) is known to choose <code>rslave</code> mount propagation when the
mount source contains the Docker daemon's root directory (<code>/var/lib/docker</code>).</p></li><li><p><code>HostToContainer</code> - This volume mount will receive all subsequent mounts
that are mounted to this volume or any of its subdirectories.</p><p>In other words, if the host mounts anything inside the volume mount, the
container will see it mounted there.</p><p>Similarly, if any Pod with <code>Bidirectional</code> mount propagation to the same
volume mounts anything there, the container with <code>HostToContainer</code> mount
propagation will see it.</p><p>This mode is equal to <code>rslave</code> mount propagation as described in the
<a href="https://man7.org/linux/man-pages/man8/mount.8.html"><code>mount(8)</code></a></p></li><li><p><code>Bidirectional</code> - This volume mount behaves the same the <code>HostToContainer</code> mount.
In addition, all volume mounts created by the container will be propagated
back to the host and to all containers of all pods that use the same volume.</p><p>A typical use case for this mode is a Pod with a FlexVolume or CSI driver or
a Pod that needs to mount something on the host using a <code>hostPath</code> volume.</p><p>This mode is equal to <code>rshared</code> mount propagation as described in the
<a href="https://man7.org/linux/man-pages/man8/mount.8.html"><code>mount(8)</code></a></p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><code>Bidirectional</code> mount propagation can be dangerous. It can damage
the host operating system and therefore it is allowed only in privileged
containers. Familiarity with Linux kernel behavior is strongly recommended.
In addition, any volume mounts created by containers in pods must be destroyed
(unmounted) by the containers on termination.</div></li></ul><h2 id="read-only-mounts">Read-only mounts</h2><p>A mount can be made read-only by setting the <code>.spec.containers[].volumeMounts[].readOnly</code>
field to <code>true</code>.
This does not make the volume itself read-only, but that specific container will
not be able to write to it.
Other containers in the Pod may mount the same volume as read-write.</p><p>On Linux, read-only mounts are not recursively read-only by default.
For example, consider a Pod which mounts the hosts <code>/mnt</code> as a <code>hostPath</code> volume. If
there is another filesystem mounted read-write on <code>/mnt/&lt;SUBMOUNT&gt;</code> (such as tmpfs,
NFS, or USB storage), the volume mounted into the container(s) will also have a writeable
<code>/mnt/&lt;SUBMOUNT&gt;</code>, even if the mount itself was specified as read-only.</p><h3 id="recursive-read-only-mounts">Recursive read-only mounts</h3><div class="feature-state-notice feature-stable" title="Feature Gate: RecursiveReadOnlyMounts"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Recursive read-only mounts can be enabled by setting the
<code>RecursiveReadOnlyMounts</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
for kubelet and kube-apiserver, and setting the <code>.spec.containers[].volumeMounts[].recursiveReadOnly</code>
field for a pod.</p><p>The allowed values are:</p><ul><li><p><code>Disabled</code> (default): no effect.</p></li><li><p><code>Enabled</code>: makes the mount recursively read-only.
Needs all the following requirements to be satisfied:</p><ul><li><code>readOnly</code> is set to <code>true</code></li><li><code>mountPropagation</code> is unset, or, set to <code>None</code></li><li>The host is running with Linux kernel v5.12 or later</li><li>The <a href="/docs/concepts/architecture/cri">CRI-level</a> container runtime supports recursive read-only mounts</li><li>The OCI-level container runtime supports recursive read-only mounts.</li></ul><p>It will fail if any of these is not true.</p></li><li><p><code>IfPossible</code>: attempts to apply <code>Enabled</code>, and falls back to <code>Disabled</code>
if the feature is not supported by the kernel or the runtime class.</p></li></ul><p>Example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/rro.yaml" download="storage/rro.yaml"><code>storage/rro.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-rro-yaml&quot;)" title="Copy storage/rro.yaml to clipboard"/></div><div class="includecode" id="storage-rro-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>rro<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># tmpfs is mounted on /mnt/tmpfs</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"infinity"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># /mnt-rro/tmpfs is not writable</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/mnt-rro<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPropagation</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">recursiveReadOnly</span>:<span style="color:#bbb"> </span>Enabled<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># /mnt-ro/tmpfs is writable</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/mnt-ro<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># /mnt-rw/tmpfs is writable</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mnt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/mnt-rw<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>When this property is recognized by kubelet and kube-apiserver,
the <code>.status.containerStatuses[].volumeMounts[].recursiveReadOnly</code> field is set to either
<code>Enabled</code> or <code>Disabled</code>.</p><h4 id="implementations-rro">Implementations</h4><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The following container runtimes are known to support recursive read-only mounts.</p><p>CRI-level:</p><ul><li><a href="https://containerd.io/">containerd</a>, since v2.0</li><li><a href="https://cri-o.io/">CRI-O</a>, since v1.30</li></ul><p>OCI-level:</p><ul><li><a href="https://runc.io/">runc</a>, since v1.1</li><li><a href="https://github.com/containers/crun">crun</a>, since v1.8.6</li></ul><h2 id="what-s-next">What's next</h2><p>Follow an example of <a href="/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/">deploying WordPress and MySQL with Persistent Volumes</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Gateway API</h1><div class="lead">Gateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.</div><p>Make network services available by using an extensible, role-oriented, protocol-aware configuration
mechanism. <a href="https://gateway-api.sigs.k8s.io/">Gateway API</a> is an <a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/cluster-administration/addons/" target="_blank" aria-label="add-on">add-on</a>
containing API <a href="https://gateway-api.sigs.k8s.io/references/spec/">kinds</a> that provide dynamic infrastructure
provisioning and advanced traffic routing.</p><h2 id="design-principles">Design principles</h2><p>The following principles shaped the design and architecture of Gateway API:</p><ul><li><strong>Role-oriented:</strong> Gateway API kinds are modeled after organizational roles that are
responsible for managing Kubernetes service networking:<ul><li><strong>Infrastructure Provider:</strong> Manages infrastructure that allows multiple isolated clusters
to serve multiple tenants, e.g. a cloud provider.</li><li><strong>Cluster Operator:</strong> Manages clusters and is typically concerned with policies, network
access, application permissions, etc.</li><li><strong>Application Developer:</strong> Manages an application running in a cluster and is typically
concerned with application-level configuration and <a href="/docs/concepts/services-networking/service/">Service</a>
composition.</li></ul></li><li><strong>Portable:</strong> Gateway API specifications are defined as <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</a>
and are supported by many <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations</a>.</li><li><strong>Expressive:</strong> Gateway API kinds support functionality for common traffic routing use cases
such as header-based matching, traffic weighting, and others that were only possible in
<a href="/docs/concepts/services-networking/ingress/">Ingress</a> by using custom annotations.</li><li><strong>Extensible:</strong> Gateway allows for custom resources to be linked at various layers of the API.
This makes granular customization possible at the appropriate places within the API structure.</li></ul><h2 id="resource-model">Resource model</h2><p>Gateway API has four stable API kinds:</p><ul><li><p><strong>GatewayClass:</strong> Defines a set of gateways with common configuration and managed by a controller
that implements the class.</p></li><li><p><strong>Gateway:</strong> Defines an instance of traffic handling infrastructure, such as cloud load balancer.</p></li><li><p><strong>HTTPRoute:</strong> Defines HTTP-specific rules for mapping traffic from a Gateway listener to a
representation of backend network endpoints. These endpoints are often represented as a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>.</p></li><li><p><strong>GRPCRoute:</strong> Defines gRPC-specific rules for mapping traffic from a Gateway listener to a
representation of backend network endpoints. These endpoints are often represented as a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>.</p></li></ul><p>Gateway API is organized into different API kinds that have interdependent relationships to support
the role-oriented nature of organizations. A Gateway object is associated with exactly one GatewayClass;
the GatewayClass describes the gateway controller responsible for managing Gateways of this class.
One or more route kinds such as HTTPRoute, are then associated to Gateways. A Gateway can filter the routes
that may be attached to its <code>listeners</code>, forming a bidirectional trust model with routes.</p><p>The following figure illustrates the relationships of the three stable Gateway API kinds:</p><figure class="diagram-medium"><img src="/docs/images/gateway-kind-relationships.svg" alt="A figure illustrating the relationships of the three stable Gateway API kinds"/></figure><h3 id="api-kind-gateway-class">GatewayClass</h3><p>Gateways can be implemented by different controllers, often with different configurations. A Gateway
must reference a GatewayClass that contains the name of the controller that implements the
class.</p><p>A minimal GatewayClass example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>gateway.networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>GatewayClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-class<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">controllerName</span>:<span style="color:#bbb"> </span>example.com/gateway-controller<span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, a controller that has implemented Gateway API is configured to manage GatewayClasses
with the controller name <code>example.com/gateway-controller</code>. Gateways of this class will be managed by
the implementation's controller.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.GatewayClass">GatewayClass</a>
reference for a full definition of this API kind.</p><h3 id="api-kind-gateway">Gateway</h3><p>A Gateway describes an instance of traffic handling infrastructure. It defines a network endpoint
that can be used for processing traffic, i.e. filtering, balancing, splitting, etc. for backends
such as a Service. For example, a Gateway may represent a cloud load balancer or an in-cluster proxy
server that is configured to accept HTTP traffic.</p><p>A minimal Gateway resource example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>gateway.networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Gateway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-gateway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">gatewayClassName</span>:<span style="color:#bbb"> </span>example-class<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">listeners</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>HTTP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, an instance of traffic handling infrastructure is programmed to listen for HTTP
traffic on port 80. Since the <code>addresses</code> field is unspecified, an address or hostname is assigned
to the Gateway by the implementation's controller. This address is used as a network endpoint for
processing traffic of backend network endpoints defined in routes.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.Gateway">Gateway</a>
reference for a full definition of this API kind.</p><h3 id="api-kind-httproute">HTTPRoute</h3><p>The HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to backend network
endpoints. For a Service backend, an implementation may represent the backend network endpoint as a Service
IP or the backing EndpointSlices of the Service. An HTTPRoute represents configuration that is applied to the
underlying Gateway implementation. For example, defining a new HTTPRoute may result in configuring additional
traffic routes in a cloud load balancer or in-cluster proxy server.</p><p>A minimal HTTPRoute example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>gateway.networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>HTTPRoute<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-httproute<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parentRefs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-gateway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostnames</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:#b44">"www.example.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">matches</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>PathPrefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>/login<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">backendRefs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-svc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, HTTP traffic from Gateway <code>example-gateway</code> with the Host: header set to <code>www.example.com</code>
and the request path specified as <code>/login</code> will be routed to Service <code>example-svc</code> on port <code>8080</code>.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.HTTPRoute">HTTPRoute</a>
reference for a full definition of this API kind.</p><h3 id="api-kind-grpcroute">GRPCRoute</h3><p>The GRPCRoute kind specifies routing behavior of gRPC requests from a Gateway listener to backend network
endpoints. For a Service backend, an implementation may represent the backend network endpoint as a Service
IP or the backing EndpointSlices of the Service. A GRPCRoute represents configuration that is applied to the
underlying Gateway implementation. For example, defining a new GRPCRoute may result in configuring additional
traffic routes in a cloud load balancer or in-cluster proxy server.</p><p>Gateways supporting GRPCRoute are required to support HTTP/2 without an initial upgrade from HTTP/1,
so gRPC traffic is guaranteed to flow properly.</p><p>A minimal GRPCRoute example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>gateway.networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>GRPCRoute<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-grpcroute<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parentRefs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-gateway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostnames</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:#b44">"svc.example.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">backendRefs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-svc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">50051</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, gRPC traffic from Gateway <code>example-gateway</code> with the host set to <code>svc.example.com</code>
will be directed to the service <code>example-svc</code> on port <code>50051</code> from the same namespace.</p><p>GRPCRoute allows matching specific gRPC services, as per the following example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>gateway.networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>GRPCRoute<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-grpcroute<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parentRefs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-gateway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostnames</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:#b44">"svc.example.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">matches</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">method</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb"> </span>com.example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">method</span>:<span style="color:#bbb"> </span>Login<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">backendRefs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo-svc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">50051</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this case, the GRPCRoute will match any traffic for svc.example.com and apply its routing rules
to forward the traffic to the correct backend. Since there is only one match specified,only requests
for the com.example.User.Login method to svc.example.com will be forwarded.
RPCs of any other method` will not be matched by this Route.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/reference/spec/#grpcroute">GRPCRoute</a>
reference for a full definition of this API kind.</p><h2 id="request-flow">Request flow</h2><p>Here is a simple example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute:</p><figure class="diagram-medium"><img src="/docs/images/gateway-request-flow.svg" alt="A diagram that provides an example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute"/></figure><p>In this example, the request flow for a Gateway implemented as a reverse proxy is:</p><ol><li>The client starts to prepare an HTTP request for the URL <code>http://www.example.com</code></li><li>The client's DNS resolver queries for the destination name and learns a mapping to
one or more IP addresses associated with the Gateway.</li><li>The client sends a request to the Gateway IP address; the reverse proxy receives the HTTP
request and uses the Host: header to match a configuration that was derived from the Gateway
and attached HTTPRoute.</li><li>Optionally, the reverse proxy can perform request header and/or path matching based
on match rules of the HTTPRoute.</li><li>Optionally, the reverse proxy can modify the request; for example, to add or remove headers,
based on filter rules of the HTTPRoute.</li><li>Lastly, the reverse proxy forwards the request to one or more backends.</li></ol><h2 id="conformance">Conformance</h2><p>Gateway API covers a broad set of features and is widely implemented. This combination requires
clear conformance definitions and tests to ensure that the API provides a consistent experience
wherever it is used.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/concepts/conformance/">conformance</a> documentation to
understand details such as release channels, support levels, and running conformance tests.</p><h2 id="migrating-from-ingress">Migrating from Ingress</h2><p>Gateway API is the successor to the <a href="/docs/concepts/services-networking/ingress/">Ingress</a> API.
However, it does not include the Ingress kind. As a result, a one-time conversion from your existing
Ingress resources to Gateway API resources is necessary.</p><p>Refer to the <a href="https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress/#migrating-from-ingress">ingress migration</a>
guide for details on migrating Ingress resources to Gateway API resources.</p><h2 id="what-s-next">What's next</h2><p>Instead of Gateway API resources being natively implemented by Kubernetes, the specifications
are defined as <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a>
supported by a wide range of <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations</a>.
<a href="https://gateway-api.sigs.k8s.io/guides/#installing-gateway-api">Install</a> the Gateway API CRDs or
follow the installation instructions of your selected implementation. After installing an
implementation, use the <a href="https://gateway-api.sigs.k8s.io/guides/">Getting Started</a> guide to help
you quickly start working with Gateway API.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Make sure to review the documentation of your selected implementation to understand any caveats.</div><p>Refer to the <a href="https://gateway-api.sigs.k8s.io/reference/spec/">API specification</a> for additional
details of all Gateway API kinds.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">EndpointSlices</h1><div class="lead">The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div>EndpointSlices track the IP addresses of backend endpoints.
EndpointSlices are normally associated with a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> and the backend endpoints typically represent
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>.<h2 id="endpointslice-resource">EndpointSlice API</h2><p>In Kubernetes, an EndpointSlice contains references to a set of network
endpoints. The control plane automatically creates EndpointSlices
for any Kubernetes Service that has a <a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels/" target="_blank" aria-label="selector">selector</a> specified. These EndpointSlices include
references to all the Pods that match the Service selector. EndpointSlices group
network endpoints together by unique combinations of IP family, protocol,
port number, and Service name.
The name of a EndpointSlice object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>As an example, here's a sample EndpointSlice object, that's owned by the <code>example</code>
Kubernetes Service.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>discovery.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EndpointSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-abc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/service-name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">addressType</span>:<span style="color:#bbb"> </span>IPv4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">endpoints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">addresses</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"10.1.2.3"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">conditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostname</span>:<span style="color:#bbb"> </span>pod-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodeName</span>:<span style="color:#bbb"> </span>node-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">zone</span>:<span style="color:#bbb"> </span>us-west2-a<span style="color:#bbb">
</span></span></span></code></pre></div><p>By default, the control plane creates and manages EndpointSlices to have no
more than 100 endpoints each. You can configure this with the
<code>--max-endpoints-per-slice</code>
<a class="glossary-tooltip" title="Control Plane component that runs controller processes." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" aria-label="kube-controller-manager">kube-controller-manager</a>
flag, up to a maximum of 1000.</p><p>EndpointSlices act as the source of truth for
<a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a> when it comes to
how to route internal traffic.</p><h3 id="address-types">Address types</h3><p>EndpointSlices support two address types:</p><ul><li>IPv4</li><li>IPv6</li></ul><p>Each <code>EndpointSlice</code> object represents a specific IP address type. If you have
a Service that is available via IPv4 and IPv6, there will be at least two
<code>EndpointSlice</code> objects (one for IPv4, and one for IPv6).</p><h3 id="conditions">Conditions</h3><p>The EndpointSlice API stores conditions about endpoints that may be useful for consumers.
The three conditions are <code>serving</code>, <code>terminating</code>, and <code>ready</code>.</p><h4 id="serving">Serving</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>The <code>serving</code> condition indicates that the endpoint is currently serving responses, and
so it should be used as a target for Service traffic. For endpoints backed by a Pod, this
maps to the Pod's <code>Ready</code> condition.</p><h4 id="terminating">Terminating</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>The <code>terminating</code> condition indicates that the endpoint is
terminating. For endpoints backed by a Pod, this condition is set when
the Pod is first deleted (that is, when it receives a deletion
timestamp, but most likely before the Pod's containers exit).</p><p>Service proxies will normally ignore endpoints that are <code>terminating</code>,
but they may route traffic to endpoints that are both <code>serving</code> and
<code>terminating</code> if all available endpoints are <code>terminating</code>. (This
helps to ensure that no Service traffic is lost during rolling updates
of the underlying Pods.)</p><h4 id="ready">Ready</h4><p>The <code>ready</code> condition is essentially a shortcut for checking
"<code>serving</code> and not <code>terminating</code>" (though it will also always be
<code>true</code> for Services with <code>spec.publishNotReadyAddresses</code> set to
<code>true</code>).</p><h3 id="topology">Topology information</h3><p>Each endpoint within an EndpointSlice can contain relevant topology information.
The topology information includes the location of the endpoint and information
about the corresponding Node and zone. These are available in the following
per endpoint fields on EndpointSlices:</p><ul><li><code>nodeName</code> - The name of the Node this endpoint is on.</li><li><code>zone</code> - The zone this endpoint is in.</li></ul><h3 id="management">Management</h3><p>Most often, the control plane (specifically, the endpoint slice
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>) creates and
manages EndpointSlice objects. There are a variety of other use cases for
EndpointSlices, such as service mesh implementations, that could result in other
entities or controllers managing additional sets of EndpointSlices.</p><p>To ensure that multiple entities can manage EndpointSlices without interfering
with each other, Kubernetes defines the
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="label">label</a>
<code>endpointslice.kubernetes.io/managed-by</code>, which indicates the entity managing
an EndpointSlice.
The endpoint slice controller sets <code>endpointslice-controller.k8s.io</code> as the value
for this label on all EndpointSlices it manages. Other entities managing
EndpointSlices should also set a unique value for this label.</p><h3 id="ownership">Ownership</h3><p>In most use cases, EndpointSlices are owned by the Service that the endpoint
slice object tracks endpoints for. This ownership is indicated by an owner
reference on each EndpointSlice as well as a <code>kubernetes.io/service-name</code>
label that enables simple lookups of all EndpointSlices belonging to a Service.</p><h3 id="distribution-of-endpointslices">Distribution of EndpointSlices</h3><p>Each EndpointSlice has a set of ports that applies to all endpoints within the
resource. When named ports are used for a Service, Pods may end up with
different target port numbers for the same named port, requiring different
EndpointSlices.</p><p>The control plane tries to fill EndpointSlices as full as possible, but does not
actively rebalance them. The logic is fairly straightforward:</p><ol><li>Iterate through existing EndpointSlices, remove endpoints that are no longer
desired and update matching endpoints that have changed.</li><li>Iterate through EndpointSlices that have been modified in the first step and
fill them up with any new endpoints needed.</li><li>If there's still new endpoints left to add, try to fit them into a previously
unchanged slice and/or create new ones.</li></ol><p>Importantly, the third step prioritizes limiting EndpointSlice updates over a
perfectly full distribution of EndpointSlices. As an example, if there are 10
new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,
this approach will create a new EndpointSlice instead of filling up the 2
existing EndpointSlices. In other words, a single EndpointSlice creation is
preferable to multiple EndpointSlice updates.</p><p>With kube-proxy running on each Node and watching EndpointSlices, every change
to an EndpointSlice becomes relatively expensive since it will be transmitted to
every Node in the cluster. This approach is intended to limit the number of
changes that need to be sent to every Node, even if it may result with multiple
EndpointSlices that are not full.</p><p>In practice, this less than ideal distribution should be rare. Most changes
processed by the EndpointSlice controller will be small enough to fit in an
existing EndpointSlice, and if not, a new EndpointSlice is likely going to be
necessary soon anyway. Rolling updates of Deployments also provide a natural
repacking of EndpointSlices with all Pods and their corresponding endpoints
getting replaced.</p><h3 id="duplicate-endpoints">Duplicate endpoints</h3><p>Due to the nature of EndpointSlice changes, endpoints may be represented in more
than one EndpointSlice at the same time. This naturally occurs as changes to
different EndpointSlice objects can arrive at the Kubernetes client watch / cache
at different times.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Clients of the EndpointSlice API must iterate through all the existing EndpointSlices
associated to a Service and build a complete list of unique network endpoints. It is
important to mention that endpoints may be duplicated in different EndpointSlices.</p><p>You can find a reference implementation for how to perform this endpoint aggregation
and deduplication as part of the <code>EndpointSliceCache</code> code within <code>kube-proxy</code>.</p></div><h3 id="endpointslice-mirroring">EndpointSlice mirroring</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [deprecated]</code></div><p>The EndpointSlice API is a replacement for the older Endpoints API. To
preserve compatibility with older controllers and user workloads that
expect <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a>
to route traffic based on Endpoints resources, the cluster's control
plane mirrors most user-created Endpoints resources to corresponding
EndpointSlices.</p><p>(However, this feature, like the rest of the Endpoints API, is
deprecated. Users who manually specify endpoints for selectorless
Services should do so by creating EndpointSlice resources directly,
rather than by creating Endpoints resources and allowing them to be
mirrored.)</p><p>The control plane mirrors Endpoints resources unless:</p><ul><li>the Endpoints resource has a <code>endpointslice.kubernetes.io/skip-mirror</code> label
set to <code>true</code>.</li><li>the Endpoints resource has a <code>control-plane.alpha.kubernetes.io/leader</code>
annotation.</li><li>the corresponding Service resource does not exist.</li><li>the corresponding Service resource has a non-nil selector.</li></ul><p>Individual Endpoints resources may translate into multiple EndpointSlices. This
will occur if an Endpoints resource has multiple subsets or includes endpoints
with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per
subset will be mirrored to EndpointSlices.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li><li>Read the <a href="/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/">API reference</a> for the EndpointSlice API</li><li>Read the <a href="/docs/reference/kubernetes-api/service-resources/endpoints-v1/">API reference</a> for the Endpoints API</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Multi-tenancy</h1><p>This page provides an overview of available configuration options and best practices for cluster
multi-tenancy.</p><p>Sharing clusters saves costs and simplifies administration. However, sharing clusters also
presents challenges such as security, fairness, and managing <em>noisy neighbors</em>.</p><p>Clusters can be shared in many ways. In some cases, different applications may run in the same
cluster. In other cases, multiple instances of the same application may run in the same cluster,
one for each end user. All these types of sharing are frequently described using the umbrella term
<em>multi-tenancy</em>.</p><p>While Kubernetes does not have first-class concepts of end users or tenants, it provides several
features to help manage different tenancy requirements. These are discussed below.</p><h2 id="use-cases">Use cases</h2><p>The first step to determining how to share your cluster is understanding your use case, so you can
evaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters falls
into two broad categories, though many variations and hybrids are also possible.</p><h3 id="multiple-teams">Multiple teams</h3><p>A common form of multi-tenancy is to share a cluster between multiple teams within an
organization, each of whom may operate one or more workloads. These workloads frequently need to
communicate with each other, and with other workloads located on the same or different clusters.</p><p>In this scenario, members of the teams often have direct access to Kubernetes resources via tools
such as <code>kubectl</code>, or indirect access through GitOps controllers or other types of release
automation tools. There is often some level of trust between members of different teams, but
Kubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly
share clusters.</p><h3 id="multiple-customers">Multiple customers</h3><p>The other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor
running multiple instances of a workload for customers. This business model is so strongly
associated with this deployment style that many people call it "SaaS tenancy." However, a better
term might be "multi-customer tenancy," since SaaS vendors may also use other deployment models,
and this deployment model can also be used outside of SaaS.</p><p>In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from
their perspective and is only used by the vendor to manage the workloads. Cost optimization is
frequently a critical concern, and Kubernetes policies are used to ensure that the workloads are
strongly isolated from each other.</p><h2 id="terminology">Terminology</h2><h3 id="tenants">Tenants</h3><p>When discussing multi-tenancy in Kubernetes, there is no single definition for a "tenant".
Rather, the definition of a tenant will vary depending on whether multi-team or multi-customer
tenancy is being discussed.</p><p>In multi-team usage, a tenant is typically a team, where each team typically deploys a small
number of workloads that scales with the complexity of the service. However, the definition of
"team" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided
into smaller teams.</p><p>By contrast, if each team deploys dedicated workloads for each new client, they are using a
multi-customer model of tenancy. In this case, a "tenant" is simply a group of users who share a
single workload. This may be as large as an entire company, or as small as a single team at that
company.</p><p>In many cases, the same organization may use both definitions of "tenants" in different contexts.
For example, a platform team may offer shared services such as security tools and databases to
multiple internal â€œcustomersâ€ and a SaaS vendor may also have multiple teams sharing a development
cluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a
combination of per-customer workloads for sensitive data, combined with multi-tenant shared
services.</p><figure class="diagram-large"><img src="/images/docs/multi-tenancy.png"/><figcaption><h4>A cluster showing coexisting tenancy models</h4></figcaption></figure><h3 id="isolation">Isolation</h3><p>There are several ways to design and build multi-tenant solutions with Kubernetes. Each of these
methods comes with its own set of tradeoffs that impact the isolation level, implementation
effort, operational complexity, and cost of service.</p><p>A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane
consisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can be
applied in both the control plane and the data plane based on organizational requirements.</p><p>The level of isolation offered is sometimes described using terms like â€œhardâ€ multi-tenancy, which
implies strong isolation, and â€œsoftâ€ multi-tenancy, which implies weaker isolation. In particular,
"hard" multi-tenancy is often used to describe cases where the tenants do not trust each other,
often from security and resource sharing perspectives (e.g. guarding against attacks such as data
exfiltration or DoS). Since data planes typically have much larger attack surfaces, "hard"
multi-tenancy often requires extra attention to isolating the data-plane, though control plane
isolation also remains critical.</p><p>However, the terms "hard" and "soft" can often be confusing, as there is no single definition that
will apply to all users. Rather, "hardness" or "softness" is better understood as a broad
spectrum, with many different techniques that can be used to maintain different types of isolation
in your clusters, based on your requirements.</p><p>In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and
assign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs are
not considered an adequate security boundary. This may be easier with managed Kubernetes clusters,
where the overhead of creating and operating clusters is at least somewhat taken on by a cloud
provider. The benefit of stronger tenant isolation must be evaluated against the cost and
complexity of managing multiple clusters. The <a href="https://git.k8s.io/community/sig-multicluster/README.md">Multi-cluster SIG</a>
is responsible for addressing these types of use cases.</p><p>The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.
However, even if you are considering dedicated clusters, it may be valuable to review these
recommendations, as it will give you the flexibility to shift to shared clusters in the future if
your needs or capabilities change.</p><h2 id="control-plane-isolation">Control plane isolation</h2><p>Control plane isolation ensures that different tenants cannot access or affect each others'
Kubernetes API resources.</p><h3 id="namespaces">Namespaces</h3><p>In Kubernetes, a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="Namespace">Namespace</a> provides a
mechanism for isolating groups of API resources within a single cluster. This isolation has two
key dimensions:</p><ol><li><p>Object names within a namespace can overlap with names in other namespaces, similar to files in
folders. This allows tenants to name their resources without having to consider what other
tenants are doing.</p></li><li><p>Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network
Policies are namespace-scoped resources. Using RBAC, Users and Service Accounts can be
restricted to a namespace.</p></li></ol><p>In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and
distinct management unit. In fact, a common practice is to isolate every workload in its own
namespace, even if multiple workloads are operated by the same tenant. This ensures that each
workload has its own identity and can be configured with an appropriate security policy.</p><p>The namespace isolation model requires configuration of several other Kubernetes resources,
networking plugins, and adherence to security best practices to properly isolate tenant workloads.
These considerations are discussed below.</p><h3 id="access-controls">Access controls</h3><p>The most important type of isolation for the control plane is authorization. If teams or their
workloads can access or modify each others' API resources, they can change or disable all other
types of policies thereby negating any protection those policies may offer. As a result, it is
critical to ensure that each tenant has the appropriate access to only the namespaces they need,
and no more. This is known as the "Principle of Least Privilege."</p><p>Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes
control plane, for both users and workloads (service accounts).
<a href="/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">Roles</a> and
<a href="/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding">RoleBindings</a> are
Kubernetes objects that are used at a namespace level to enforce access control in your
application; similar objects exist for authorizing access to cluster-level objects, though these
are less useful for multi-tenant clusters.</p><p>In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate
namespaces, and ensure that cluster-wide resources can only be accessed or modified by privileged
users such as cluster administrators.</p><p>If a policy ends up granting a user more permissions than they need, this is likely a signal that
the namespace containing the affected resources should be refactored into finer-grained
namespaces. Namespace management tools may simplify the management of these finer-grained
namespaces by applying common RBAC policies to different namespaces, while still allowing
fine-grained policies where necessary.</p><h3 id="quotas">Quotas</h3><p>Kubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment,
you can use <a href="/docs/concepts/policy/resource-quotas/">Resource Quotas</a> to manage resource usage of
tenant workloads. For the multiple teams use case, where tenants have access to the Kubernetes
API, you can use resource quotas to limit the number of API resources (for example: the number of
Pods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensure
fairness and aim to avoid <em>noisy neighbor</em> issues from affecting other tenants that share a
control plane.</p><p>Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use
quotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its control
plane. Namespace management tools simplify the administration of quotas. In addition, while
Kubernetes quotas only apply within a single namespace, some namespace management tools allow
groups of namespaces to share quotas, giving administrators far more flexibility with less effort
than built-in quotas.</p><p>Quotas prevent a single tenant from consuming greater than their allocated share of resources
hence minimizing the â€œnoisy neighborâ€ issue, where one tenant negatively impacts the performance
of other tenants' workloads.</p><p>When you apply a quota to namespace, Kubernetes requires you to also specify resource requests and
limits for each container. Limits are the upper bound for the amount of resources that a container
can consume. Containers that attempt to consume resources that exceed the configured limits will
either be throttled or killed, based on the resource type. When resource requests are set lower
than limits, each container is guaranteed the requested amount but there may still be some
potential for impact across workloads.</p><p>Quotas cannot protect against all kinds of resource sharing, such as network traffic.
Node isolation (described below) may be a better solution for this problem.</p><h2 id="data-plane-isolation">Data Plane Isolation</h2><p>Data plane isolation ensures that pods and workloads for different tenants are sufficiently
isolated.</p><h3 id="network-isolation">Network isolation</h3><p>By default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all
network traffic is unencrypted. This can lead to security vulnerabilities where traffic is
accidentally or maliciously sent to an unintended destination, or is intercepted by a workload on
a compromised node.</p><p>Pod-to-pod communication can be controlled using <a href="/docs/concepts/services-networking/network-policies/">Network Policies</a>,
which restrict communication between pods using namespace labels or IP address ranges.
In a multi-tenant environment where strict network isolation between tenants is required, starting
with a default policy that denies communication between pods is recommended with another rule that
allows all pods to query the DNS server for name resolution. With such a default policy in place,
you can begin adding more permissive rules that allow for communication within a namespace.
It is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition,
in case traffic need to be allowed between namespaces.
This scheme can be further refined as required. Note that this only applies to pods within a single
control plane; pods that belong to different virtual control planes cannot talk to each other via
Kubernetes networking.</p><p>Namespace management tools may simplify the creation of default or common network policies.
In addition, some of these tools allow you to enforce a consistent set of namespace labels across
your cluster, ensuring that they are a trusted basis for your policies.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Network policies require a <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni">CNI plugin</a>
that supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored.</div><p>More advanced network isolation may be provided by service meshes, which provide OSI Layer 7
policies based on workload identity, in addition to namespaces. These higher-level policies can
make it easier to manage namespace-based multi-tenancy, especially when multiple namespaces are
dedicated to a single tenant. They frequently also offer encryption using mutual TLS, protecting
your data even in the presence of a compromised node, and work across dedicated or virtual clusters.
However, they can be significantly more complex to manage and may not be appropriate for all users.</p><h3 id="storage-isolation">Storage isolation</h3><p>Kubernetes offers several types of volumes that can be used as persistent storage for workloads.
For security and data-isolation, <a href="/docs/concepts/storage/dynamic-provisioning/">dynamic volume provisioning</a>
is recommended and volume types that use node resources should be avoided.</p><p><a href="/docs/concepts/storage/storage-classes/">StorageClasses</a> allow you to describe custom "classes"
of storage offered by your cluster, based on quality-of-service levels, backup policies, or custom
policies determined by the cluster administrators.</p><p>Pods can request storage using a <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a>.
A PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storage
system and dedicating it to tenants within the shared Kubernetes cluster.
However, it is important to note that a PersistentVolume is a cluster-wide resource and has a
lifecycle independent of workloads and namespaces.</p><p>For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation.
If a StorageClass is shared, you should set a <a href="/docs/concepts/storage/storage-classes/#reclaim-policy">reclaim policy of <code>Delete</code></a>
to ensure that a PersistentVolume cannot be reused across different namespaces.</p><h3 id="sandboxing-containers">Sandboxing containers</h3><p>Kubernetes pods are composed of one or more containers that execute on worker nodes.
Containers utilize OS-level virtualization and hence offer a weaker isolation boundary than
virtual machines that utilize hardware-based virtualization.</p><p>In a shared environment, unpatched vulnerabilities in the application and system layers can be
exploited by attackers for container breakouts and remote code execution that allow access to host
resources. In some applications, like a Content Management System (CMS), customers may be allowed
the ability to upload and execute untrusted scripts or code. In either case, mechanisms to further
isolate and protect workloads using strong isolation are desirable.</p><p>Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involves
running each pod in a separate execution environment such as a virtual machine or a userspace
kernel. Sandboxing is often recommended when you are running untrusted code, where workloads are
assumed to be malicious. Part of the reason this type of isolation is necessary is because
containers are processes running on a shared kernel; they mount file systems like <code>/sys</code> and <code>/proc</code>
from the underlying host, making them less secure than an application that runs on a virtual
machine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can be
used to strengthen the security of containers, it is hard to apply a universal set of rules to all
workloads running in a shared cluster. Running workloads in a sandbox environment helps to
insulate the host from container escapes, where an attacker exploits a vulnerability to gain
access to the host system and all the processes/files running on that host.</p><p>Virtual machines and userspace kernels are two popular approaches to sandboxing.</p><h3 id="node-isolation">Node Isolation</h3><p>Node isolation is another technique that you can use to isolate tenant workloads from each other.
With node isolation, a set of nodes is dedicated to running pods from a particular tenant and
co-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, as
all pods running on a node will belong to a single tenant. The risk of information disclosure is
slightly lower with node isolation because an attacker that manages to escape from a container
will only have access to the containers and volumes mounted to that node.</p><p>Although workloads from different tenants are running on different nodes, it is important to be
aware that the kubelet and (unless using virtual control planes) the API service are still shared
services. A skilled attacker could use the permissions assigned to the kubelet or other pods
running on the node to move laterally within the cluster and gain access to tenant workloads
running on other nodes. If this is a major concern, consider implementing compensating controls
such as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separate
clusters for each tenant.</p><p>Node isolation is a little easier to reason about from a billing standpoint than sandboxing
containers since you can charge back per node rather than per pod. It also has fewer compatibility
and performance issues and may be easier to implement than sandboxing containers.
For example, nodes for each tenant can be configured with taints so that only pods with the
corresponding toleration can run on them. A mutating webhook could then be used to automatically
add tolerations and node affinities to pods deployed into tenant namespaces so that they run on a
specific set of nodes designated for that tenant.</p><p>Node isolation can be implemented using <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">pod node selectors</a>.</p><h2 id="additional-considerations">Additional Considerations</h2><p>This section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy.</p><h3 id="api-priority-and-fairness">API Priority and Fairness</h3><p><a href="/docs/concepts/cluster-administration/flow-control/">API priority and fairness</a> is a Kubernetes
feature that allows you to assign a priority to certain pods running within the cluster.
When an application calls the Kubernetes API, the API server evaluates the priority assigned to pod.
Calls from pods with higher priority are fulfilled before those with a lower priority.
When contention is high, lower priority calls can be queued until the server is less busy or you
can reject the requests.</p><p>Using API priority and fairness will not be very common in SaaS environments unless you are
allowing customers to run applications that interface with the Kubernetes API, for example,
a controller.</p><h3 id="qos">Quality-of-Service (QoS)</h3><p>When youâ€™re running a SaaS application, you may want the ability to offer different
Quality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium
service that comes with fewer performance guarantees and features and a for-fee service tier with
specific performance guarantees. Fortunately, there are several Kubernetes constructs that can
help you accomplish this within a shared cluster, including network QoS, storage classes, and pod
priority and preemption. The idea with each of these is to provide tenants with the quality of
service that they paid for. Letâ€™s start by looking at networking QoS.</p><p>Typically, all pods on a node share a network interface. Without network QoS, some pods may
consume an unfair share of the available bandwidth at the expense of other pods.
The Kubernetes <a href="https://www.cni.dev/plugins/current/meta/bandwidth/">bandwidth plugin</a> creates an
<a href="/docs/concepts/configuration/manage-resources-containers/#extended-resources">extended resource</a>
for networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to
apply rate limits to pods by using Linux tc queues.
Be aware that the plugin is considered experimental as per the
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping">Network Plugins</a>
documentation and should be thoroughly tested before use in production environments.</p><p>For storage QoS, you will likely want to create different storage classes or profiles with
different performance characteristics. Each storage profile can be associated with a different
tier of service that is optimized for different workloads such IO, redundancy, or throughput.
Additional logic might be necessary to allow the tenant to associate the appropriate storage
profile with their workload.</p><p>Finally, thereâ€™s <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">pod priority and preemption</a>
where you can assign priority values to pods. When scheduling pods, the scheduler will try
evicting pods with lower priority when there are insufficient resources to schedule pods that are
assigned a higher priority. If you have a use case where tenants have different service tiers in a
shared cluster e.g. free and paid, you may want to give higher priority to certain tiers using
this feature.</p><h3 id="dns">DNS</h3><p>Kubernetes clusters include a Domain Name System (DNS) service to provide translations from names
to IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookups
across all namespaces in the cluster.</p><p>In multi-tenant environments where tenants can access pods and other Kubernetes resources, or where
stronger isolation is required, it may be necessary to prevent pods from looking up services in other
Namespaces.
You can restrict cross-namespace DNS lookups by configuring security rules for the DNS service.
For example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadata
to restrict queries to Pods and Services within a namespace. For more information, read an
<a href="https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy">example</a> of
configuring this within the CoreDNS documentation.</p><p>When a <a href="#virtual-control-plane-per-tenant">Virtual Control Plane per tenant</a> model is used, a DNS
service must be configured per tenant or a multi-tenant DNS service must be used.
Here is an example of a <a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested/blob/main/virtualcluster/doc/tenant-dns.md">customized version of CoreDNS</a>
that supports multiple tenants.</p><h3 id="operators">Operators</h3><p><a href="/docs/concepts/extend-kubernetes/operator/">Operators</a> are Kubernetes controllers that manage
applications. Operators can simplify the management of multiple instances of an application, like
a database service, which makes them a common building block in the multi-consumer (SaaS)
multi-tenancy use case.</p><p>Operators used in a multi-tenant environment should follow a stricter set of guidelines.
Specifically, the Operator should:</p><ul><li>Support creating resources within different tenant namespaces, rather than just in the namespace
in which the Operator is deployed.</li><li>Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.</li><li>Support configuration of Pods for data-plane isolation techniques such as node isolation and
sandboxed containers.</li></ul><h2 id="implementations">Implementations</h2><p>There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces
(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control
plane per tenant).</p><p>In both cases, data plane isolation, and management of additional considerations such as API
Priority and Fairness, is also recommended.</p><p>Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides
mechanisms to allow tenants to interact appropriately, such as by allowing service-to-service
communication. However, it can be difficult to configure, and doesn't apply to Kubernetes
resources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.</p><p>Control plane virtualization allows for isolation of non-namespaced resources at the cost of
somewhat higher resource usage and more difficult cross-tenant sharing. It is a good option when
namespace isolation is insufficient but dedicated clusters are undesirable, due to the high cost
of maintaining them (especially on-prem) or due to their higher overhead and lack of resource
sharing. However, even within a virtualized control plane, you will likely see benefits by using
namespaces as well.</p><p>The two options are discussed in more detail in the following sections.</p><h3 id="namespace-per-tenant">Namespace per tenant</h3><p>As previously mentioned, you should consider isolating each workload in its own namespace, even if
you are using dedicated clusters or virtualized control planes. This ensures that each workload
only has access to its own resources, such as ConfigMaps and Secrets, and allows you to tailor
dedicated security policies for each workload. In addition, it is a best practice to give each
namespace names that are unique across your entire fleet (that is, even if they are in separate
clusters), as this gives you the flexibility to switch between dedicated and shared clusters in
the future, or to use multi-cluster tooling such as service meshes.</p><p>Conversely, there are also advantages to assigning namespaces at the tenant level, not just the
workload level, since there are often policies that apply to all workloads owned by a single
tenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to
customize policies to individual workloads, and secondly, it may be challenging to come up with a
single level of "tenancy" that should be given a namespace. For example, an organization may have
divisions, teams, and subteams - which should be assigned a namespace?</p><p>One possible approach is to organize your namespaces into hierarchies, and share certain policies and
resources between them. This could include managing namespace labels, namespace lifecycles,
delegated access, and shared resource quotas across related namespaces. These capabilities can
be useful in both multi-team and multi-customer scenarios.</p><h3 id="virtual-control-plane-per-tenant">Virtual control plane per tenant</h3><p>Another form of control-plane isolation is to use Kubernetes extensions to provide each tenant a
virtual control-plane that enables segmentation of cluster-wide API resources.
<a href="#data-plane-isolation">Data plane isolation</a> techniques can be used with this model to securely
manage worker nodes across tenants.</p><p>The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by
providing each tenant with dedicated control plane components, and hence complete control over
cluster-wide resources and add-on services. Worker nodes are shared across all tenants, and are
managed by a Kubernetes cluster that is normally inaccessible to tenants.
This cluster is often referred to as a <em>super-cluster</em> (or sometimes as a <em>host-cluster</em>).
Since a tenantâ€™s control-plane is not directly associated with underlying compute resources it is
referred to as a <em>virtual control plane</em>.</p><p>A virtual control plane typically consists of the Kubernetes API server, the controller manager,
and the etcd data store. It interacts with the super cluster via a metadata synchronization
controller which coordinates changes across tenant control planes and the control plane of the
super-cluster.</p><p>By using per-tenant dedicated control planes, most of the isolation problems due to sharing one
API server among all tenants are solved. Examples include noisy neighbors in the control plane,
uncontrollable blast radius of policy misconfigurations, and conflicts between cluster scope
objects such as webhooks and CRDs. Hence, the virtual control plane model is particularly
suitable for cases where each tenant requires access to a Kubernetes API server and expects the
full cluster manageability.</p><p>The improved isolation comes at the cost of running and maintaining an individual virtual control
plane per tenant. In addition, per-tenant control planes do not solve isolation problems in the
data plane, such as node-level noisy neighbors or security threats. These must still be addressed
separately.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Projected Volumes</h1><p>This document describes <em>projected volumes</em> in Kubernetes. Familiarity with <a href="/docs/concepts/storage/volumes/">volumes</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>A <code>projected</code> volume maps several existing volume sources into the same directory.</p><p>Currently, the following types of volume sources can be projected:</p><ul><li><a href="/docs/concepts/storage/volumes/#secret"><code>secret</code></a></li><li><a href="/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code></a></li><li><a href="/docs/concepts/storage/volumes/#configmap"><code>configMap</code></a></li><li><a href="#serviceaccounttoken"><code>serviceAccountToken</code></a></li><li><a href="#clustertrustbundle"><code>clusterTrustBundle</code></a></li><li><a href="#podcertificate"><code>podCertificate</code></a></li></ul><p>All sources are required to be in the same namespace as the Pod. For more details,
see the <a href="https://git.k8s.io/design-proposals-archive/node/all-in-one-volume.md">all-in-one volume</a> design document.</p><h3 id="example-configuration-secret-downwardapi-configmap">Example configuration with a secret, a downwardAPI, and a configMap</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-secret-downwardapi-configmap.yaml" download="pods/storage/projected-secret-downwardapi-configmap.yaml"><code>pods/storage/projected-secret-downwardapi-configmap.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-storage-projected-secret-downwardapi-configmap-yaml&quot;)" title="Copy pods/storage/projected-secret-downwardapi-configmap.yaml to clipboard"/></div><div class="includecode" id="pods-storage-projected-secret-downwardapi-configmap-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>volume-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"3600"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/projected-volume"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">projected</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>my-group/my-username<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">downwardAPI</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"labels"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">fieldRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">fieldPath</span>:<span style="color:#bbb"> </span>metadata.labels<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"cpu_limit"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">resourceFieldRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">containerName</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">resource</span>:<span style="color:#bbb"> </span>limits.cpu<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">configMap</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>my-group/my-config<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h3 id="example-configuration-secrets-nondefault-permission-mode">Example configuration: secrets with a non-default permission mode set</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-secrets-nondefault-permission-mode.yaml" download="pods/storage/projected-secrets-nondefault-permission-mode.yaml"><code>pods/storage/projected-secrets-nondefault-permission-mode.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-storage-projected-secrets-nondefault-permission-mode-yaml&quot;)" title="Copy pods/storage/projected-secrets-nondefault-permission-mode.yaml to clipboard"/></div><div class="includecode" id="pods-storage-projected-secrets-nondefault-permission-mode-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>volume-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"3600"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/projected-volume"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">projected</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>my-group/my-username<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mysecret2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>my-group/my-password<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">mode</span>:<span style="color:#bbb"> </span><span style="color:#666">511</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Each projected volume source is listed in the spec under <code>sources</code>. The
parameters are nearly the same with two exceptions:</p><ul><li>For secrets, the <code>secretName</code> field has been changed to <code>name</code> to be consistent
with ConfigMap naming.</li><li>The <code>defaultMode</code> can only be specified at the projected level and not for each
volume source. However, as illustrated above, you can explicitly set the <code>mode</code>
for each individual projection.</li></ul><h2 id="serviceaccounttoken">serviceAccountToken projected volumes</h2><p>You can inject the token for the current <a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">service account</a>
into a Pod at a specified path. For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-service-account-token.yaml" download="pods/storage/projected-service-account-token.yaml"><code>pods/storage/projected-service-account-token.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-storage-projected-service-account-token-yaml&quot;)" title="Copy pods/storage/projected-service-account-token.yaml to clipboard"/></div><div class="includecode" id="pods-storage-projected-service-account-token-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>sa-token-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"3600"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>token-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/service-account"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">serviceAccountName</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>token-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">projected</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">serviceAccountToken</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">audience</span>:<span style="color:#bbb"> </span>api<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">expirationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3600</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>token<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The example Pod has a projected volume containing the injected service account
token. Containers in this Pod can use that token to access the Kubernetes API
server, authenticating with the identity of <a href="/docs/tasks/configure-pod-container/configure-service-account/">the pod's ServiceAccount</a>.
The <code>audience</code> field contains the intended audience of the
token. A recipient of the token must identify itself with an identifier specified
in the audience of the token, and otherwise should reject the token. This field
is optional and it defaults to the identifier of the API server.</p><p>The <code>expirationSeconds</code> is the expected duration of validity of the service account
token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator
can also limit its maximum value by specifying the <code>--service-account-max-token-expiration</code>
option for the API server. The <code>path</code> field specifies a relative path to the mount point
of the projected volume.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A container using a projected volume source as a <a href="/docs/concepts/storage/volumes/#using-subpath"><code>subPath</code></a>
volume mount will not receive updates for those volume sources.</div><h2 id="clustertrustbundle">clusterTrustBundle projected volumes</h2><div class="feature-state-notice feature-beta" title="Feature Gate: ClusterTrustBundleProjection"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To use this feature in Kubernetes 1.34, you must enable support for ClusterTrustBundle objects with the <code>ClusterTrustBundle</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> and <code>--runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true</code> kube-apiserver flag, then enable the <code>ClusterTrustBundleProjection</code> feature gate.</div><p>The <code>clusterTrustBundle</code> projected volume source injects the contents of one or more <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#cluster-trust-bundles">ClusterTrustBundle</a> objects as an automatically-updating file in the container filesystem.</p><p>ClusterTrustBundles can be selected either by <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#ctb-signer-unlinked">name</a> or by <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#ctb-signer-linked">signer name</a>.</p><p>To select by name, use the <code>name</code> field to designate a single ClusterTrustBundle object.</p><p>To select by signer name, use the <code>signerName</code> field (and optionally the
<code>labelSelector</code> field) to designate a set of ClusterTrustBundle objects that use
the given signer name. If <code>labelSelector</code> is not present, then all
ClusterTrustBundles for that signer are selected.</p><p>The kubelet deduplicates the certificates in the selected ClusterTrustBundle objects, normalizes the PEM representations (discarding comments and headers), reorders the certificates, and writes them into the file named by <code>path</code>. As the set of selected ClusterTrustBundles or their content changes, kubelet keeps the file up-to-date.</p><p>By default, the kubelet will prevent the pod from starting if the named ClusterTrustBundle is not found, or if <code>signerName</code> / <code>labelSelector</code> do not match any ClusterTrustBundles. If this behavior is not what you want, then set the <code>optional</code> field to <code>true</code>, and the pod will start up with an empty file at <code>path</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-clustertrustbundle.yaml" download="pods/storage/projected-clustertrustbundle.yaml"><code>pods/storage/projected-clustertrustbundle.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-storage-projected-clustertrustbundle-yaml&quot;)" title="Copy pods/storage/projected-clustertrustbundle.yaml to clipboard"/></div><div class="includecode" id="pods-storage-projected-clustertrustbundle-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>sa-ctb-name-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"3600"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>token-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/root-certificates"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">serviceAccountName</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>token-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">projected</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">clusterTrustBundle</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>example-roots.pem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">clusterTrustBundle</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">signerName</span>:<span style="color:#bbb"> </span><span style="color:#b44">"example.com/mysigner"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">version</span>:<span style="color:#bbb"> </span>live<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>mysigner-roots.pem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">optional</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="podcertificate">podCertificate projected volumes</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: PodCertificateRequest"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In Kubernetes 1.34, you must enable support for Pod
Certificates using the <code>PodCertificateRequest</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> and the
<code>--runtime-config=certificates.k8s.io/v1alpha1/podcertificaterequests=true</code>
kube-apiserver flag.</div><p>The <code>podCertificate</code> projected volumes source securely provisions a private key
and X.509 certificate chain for pod to use as client or server credentials.
Kubelet will then handle refreshing the private key and certificate chain when
they get close to expiration. The application just has to make sure that it
reloads the file promptly when it changes, with a mechanism like <code>inotify</code> or
polling.</p><p>Each <code>podCertificate</code> projection supports the following configuration fields:</p><ul><li><code>signerName</code>: The
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/#signers">signer</a>
you want to issue the certificate. Note that signers may have their own
access requirements, and may refuse to issue certificates to your pod.</li><li><code>keyType</code>: The type of private key that should be generated. Valid values are
<code>ED25519</code>, <code>ECDSAP256</code>, <code>ECDSAP384</code>, <code>ECDSAP521</code>, <code>RSA3072</code>, and <code>RSA4096</code>.</li><li><code>maxExpirationSeconds</code>: The maximum lifetime you will accept for the
certificate issued to the pod. If not set, will be defaulted to <code>86400</code> (24
hours). Must be at least <code>3600</code> (1 hour), and at most <code>7862400</code> (91 days).
Kubernetes built-in signers are restricted to a max lifetime of <code>86400</code> (1
day). The signer is allowed to issue a certificate with a lifetime shorter
than what you've specified.</li><li><code>credentialBundlePath</code>: Relative path within the projection where the
credential bundle should be written. The credential bundle is a PEM-formatted
file, where the first block is a "PRIVATE KEY" block that contains a
PKCS#8-serialized private key, and the remaining blocks are "CERTIFICATE"
blocks that comprise the certificate chain (leaf certificate and any
intermediates).</li><li><code>keyPath</code> and <code>certificateChainPath</code>: Separate paths where Kubelet should
write <em>just</em> the private key or certificate chain.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Most applications should prefer using <code>credentialBundlePath</code> unless they need
the key and certificates in separate files for compatibility reasons. Kubelet
uses an atomic writing strategy based on symlinks to make sure that when you
open the files it projects, you read either the old content or the new content.
However, if you read the key and certificate chain from separate files, Kubelet
may rotate the credentials after your first read and before your second read,
resulting in your application loading a mismatched key and certificate.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-podcertificate.yaml" download="pods/storage/projected-podcertificate.yaml"><code>pods/storage/projected-podcertificate.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-storage-projected-podcertificate-yaml&quot;)" title="Copy pods/storage/projected-podcertificate.yaml to clipboard"/></div><div class="includecode" id="pods-storage-projected-podcertificate-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Sample Pod spec that uses a podCertificate projection to request an ED25519</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># private key, a certificate from the `coolcert.example.com/foo` signer, and</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># write the results to `/var/run/my-x509-credentials/credentialbundle.pem`.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>podcertificate-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">serviceAccountName</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>main<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"infinity"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-x509-credentials<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/run/my-x509-credentials<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-x509-credentials<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">projected</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">defaultMode</span>:<span style="color:#bbb"> </span><span style="color:#666">420</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">sources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">podCertificate</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">keyType</span>:<span style="color:#bbb"> </span>ED25519<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">signerName</span>:<span style="color:#bbb"> </span>coolcert.example.com/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">credentialBundlePath</span>:<span style="color:#bbb"> </span>credentialbundle.pem<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="securitycontext-interactions">SecurityContext interactions</h2><p>The <a href="https://git.k8s.io/enhancements/keps/sig-storage/2451-service-account-token-volumes#proposal">proposal</a> for file permission handling in projected service account volume enhancement introduced the projected files having the correct owner permissions set.</p><h3 id="linux">Linux</h3><p>In Linux pods that have a projected volume and <code>RunAsUser</code> set in the Pod
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>SecurityContext</code></a>,
the projected files have the correct ownership set including container user
ownership.</p><p>When all containers in a pod have the same <code>runAsUser</code> set in their
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>PodSecurityContext</code></a>
or container
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1"><code>SecurityContext</code></a>,
then the kubelet ensures that the contents of the <code>serviceAccountToken</code> volume are owned by that user,
and the token file has its permission mode set to <code>0600</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p><a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank" aria-label="Ephemeral containers">Ephemeral containers</a>
added to a Pod after it is created do <em>not</em> change volume permissions that were
set when the pod was created.</p><p>If a Pod's <code>serviceAccountToken</code> volume permissions were set to <code>0600</code> because
all other containers in the Pod have the same <code>runAsUser</code>, ephemeral
containers must use the same <code>runAsUser</code> to be able to read the token.</p></div><h3 id="windows">Windows</h3><p>In Windows pods that have a projected volume and <code>RunAsUsername</code> set in the
Pod <code>SecurityContext</code>, the ownership is not enforced due to the way user
accounts are managed in Windows. Windows stores and manages local user and group
accounts in a database file called Security Account Manager (SAM). Each
container maintains its own instance of the SAM database, to which the host has
no visibility into while the container is running. Windows containers are
designed to run the user mode portion of the OS in isolation from the host,
hence the maintenance of a virtual SAM database. As a result, the kubelet running
on the host does not have the ability to dynamically configure host file
ownership for virtualized container accounts. It is recommended that if files on
the host machine are to be shared with the container then they should be placed
into their own volume mount outside of <code>C:\</code>.</p><p>By default, the projected files will have the following ownership as shown for
an example projected volume file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="display:flex"><span><span style="color:#a2f">PS </span>C:\&gt; <span style="color:#a2f">Get-Acl</span> C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.<span style="color:#666">318230061</span>\ca.crt | <span style="color:#a2f">Format-List</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>Path   <span>:</span> Microsoft.PowerShell.Core\FileSystem::C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.<span style="color:#666">318230061</span>\ca.crt
</span></span><span style="display:flex"><span>Owner  <span>:</span> BUILTIN\Administrators
</span></span><span style="display:flex"><span><span style="color:#a2f">Group </span> <span>:</span> NT AUTHORITY\SYSTEM
</span></span><span style="display:flex"><span>Access <span>:</span> NT AUTHORITY\SYSTEM Allow  FullControl
</span></span><span style="display:flex"><span>         BUILTIN\Administrators Allow  FullControl
</span></span><span style="display:flex"><span>         BUILTIN\Users Allow  ReadAndExecute, Synchronize
</span></span><span style="display:flex"><span>Audit  <span>:</span>
</span></span><span style="display:flex"><span>Sddl   <span>:</span> O:BAG<span>:</span>SYD<span>:</span>AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)
</span></span></code></pre></div><p>This implies all administrator users like <code>ContainerAdministrator</code> will have
read, write and execute access while, non-administrator users will have read and
execute access.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>In general, granting the container access to the host is discouraged as it can
open the door for potential security exploits.</p><p>Creating a Windows Pod with <code>RunAsUser</code> in it's <code>SecurityContext</code> will result in
the Pod being stuck at <code>ContainerCreating</code> forever. So it is advised to not use
the Linux only <code>RunAsUser</code> option with Windows Pods.</p></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Volume Attributes Classes</h1><div class="feature-state-notice feature-stable" title="Feature Gate: VolumeAttributesClass"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This page assumes that you are familiar with <a href="/docs/concepts/storage/storage-classes/">StorageClasses</a>,
<a href="/docs/concepts/storage/volumes/">volumes</a> and <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>
in Kubernetes.</p><p>A VolumeAttributesClass provides a way for administrators to describe the mutable
"classes" of storage they offer. Different classes might map to different quality-of-service levels.
Kubernetes itself is un-opinionated about what these classes represent.</p><p>This feature is generally available (GA) as of version 1.34, and users have the option to disable it.</p><p>You can also only use VolumeAttributesClasses with storage backed by
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="Container Storage Interface">Container Storage Interface</a>, and only where the
relevant CSI driver implements the <code>ModifyVolume</code> API.</p><h2 id="the-volumeattributesclass-api">The VolumeAttributesClass API</h2><p>Each VolumeAttributesClass contains the <code>driverName</code> and <code>parameters</code>, which are
used when a PersistentVolume (PV) belonging to the class needs to be dynamically provisioned
or modified.</p><p>The name of a VolumeAttributesClass object is significant and is how users can request a particular class.
Administrators set the name and other parameters of a class when first creating VolumeAttributesClass objects.
While the name of a VolumeAttributesClass object in a <code>PersistentVolumeClaim</code> is mutable, the parameters in an existing class are immutable.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeAttributesClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>silver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">driverName</span>:<span style="color:#bbb"> </span>pd.csi.storage.gke.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">provisioned-iops</span>:<span style="color:#bbb"> </span><span style="color:#b44">"3000"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">provisioned-throughput</span>:<span style="color:#bbb"> </span><span style="color:#b44">"50"</span><span style="color:#bbb"> 
</span></span></span></code></pre></div><h3 id="provisioner">Provisioner</h3><p>Each VolumeAttributesClass has a provisioner that determines what volume plugin is used for
provisioning PVs. The field <code>driverName</code> must be specified.</p><p>The feature support for VolumeAttributesClass is implemented in
<a href="https://github.com/kubernetes-csi/external-provisioner">kubernetes-csi/external-provisioner</a>.</p><p>You are not restricted to specifying the <a href="https://github.com/kubernetes-csi/external-provisioner">kubernetes-csi/external-provisioner</a>.
You can also run and specify external provisioners,
which are independent programs that follow a specification defined by Kubernetes.
Authors of external provisioners have full discretion over where their code lives, how
the provisioner is shipped, how it needs to be run, what volume plugin it uses, etc.</p><p>To understand how the provisioner works with VolumeAttributesClass, refer to
the <a href="https://kubernetes-csi.github.io/docs/external-provisioner.html">CSI external-provisioner documentation</a>.</p><h3 id="resizer">Resizer</h3><p>Each VolumeAttributesClass has a resizer that determines what volume plugin is used
for modifying PVs. The field <code>driverName</code> must be specified.</p><p>The modifying volume feature support for VolumeAttributesClass is implemented in
<a href="https://github.com/kubernetes-csi/external-resizer">kubernetes-csi/external-resizer</a>.</p><p>For example, an existing PersistentVolumeClaim is using a VolumeAttributesClass named silver:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pv-claim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>â€¦<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeAttributesClassName</span>:<span style="color:#bbb"> </span>silver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>â€¦<span style="color:#bbb">
</span></span></span></code></pre></div><p>A new VolumeAttributesClass gold is available in the cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeAttributesClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>gold<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">driverName</span>:<span style="color:#bbb"> </span>pd.csi.storage.gke.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">iops</span>:<span style="color:#bbb"> </span><span style="color:#b44">"4000"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">throughput</span>:<span style="color:#bbb"> </span><span style="color:#b44">"60"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The end user can update the PVC with the new VolumeAttributesClass gold and apply:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pv-claim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>â€¦<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeAttributesClassName</span>:<span style="color:#bbb"> </span>gold<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>â€¦<span style="color:#bbb">
</span></span></span></code></pre></div><p>To understand how the resizer works with VolumeAttributesClass, refer to
the <a href="https://kubernetes-csi.github.io/docs/external-resizer.html">CSI external-resizer documentation</a>.</p><h2 id="parameters">Parameters</h2><p>VolumeAttributeClasses have parameters that describe volumes belonging to them. Different parameters may be accepted
depending on the provisioner or the resizer. For example, the value <code>4000</code>, for the parameter <code>iops</code>,
and the parameter <code>throughput</code> are specific to GCE PD.
When a parameter is omitted, the default is used at volume provisioning.
If a user applies the PVC with a different VolumeAttributesClass with omitted parameters, the default value of
the parameters may be used depending on the CSI driver implementation.
Please refer to the related CSI driver documentation for more details.</p><p>There can be at most 512 parameters defined for a VolumeAttributesClass.
The total length of the parameters object including its keys and values cannot exceed 256 KiB.</p></div>
<hr>
<div class="td-content"><h1>Storage</h1><div class="lead">Ways to provide both long-term and temporary storage to Pods in your cluster.</div><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/concepts/storage/volumes/">Volumes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/projected-volumes/">Projected Volumes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/ephemeral-volumes/">Ephemeral Volumes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/storage-classes/">Storage Classes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-attributes-classes/">Volume Attributes Classes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/dynamic-provisioning/">Dynamic Volume Provisioning</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-snapshots/">Volume Snapshots</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-snapshot-classes/">Volume Snapshot Classes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-pvc-datasource/">CSI Volume Cloning</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/storage-capacity/">Storage Capacity</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/storage-limits/">Node-specific Volume Limits</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-health-monitoring/">Volume Health Monitoring</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/storage/windows-storage/">Windows Storage</a></h5><p/></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Assigning Pods to Nodes</h1><p>You can constrain a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a> so that it is
<em>restricted</em> to run on particular <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node(s)">node(s)</a>,
or to <em>prefer</em> to run on particular nodes.
There are several ways to do this and the recommended approaches all use
<a href="/docs/concepts/overview/working-with-objects/labels/">label selectors</a> to facilitate the selection.
Often, you do not need to set any such constraints; the
<a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="scheduler">scheduler</a> will automatically do a reasonable placement
(for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources).
However, there are some circumstances where you may want to control which node
the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it,
or to co-locate Pods from two different services that communicate a lot into the same availability zone.</p><p>You can use any of the following methods to choose where Kubernetes schedules
specific Pods:</p><ul><li><a href="#nodeselector">nodeSelector</a> field matching against <a href="#built-in-node-labels">node labels</a></li><li><a href="#affinity-and-anti-affinity">Affinity and anti-affinity</a></li><li><a href="#nodename">nodeName</a> field</li><li><a href="#pod-topology-spread-constraints">Pod topology spread constraints</a></li></ul><h2 id="built-in-node-labels">Node labels</h2><p>Like many other Kubernetes objects, nodes have
<a href="/docs/concepts/overview/working-with-objects/labels/">labels</a>. You can
<a href="/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node">attach labels manually</a>.
Kubernetes also populates a <a href="/docs/reference/node/node-labels/">standard set of labels</a>
on all nodes in a cluster.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The value of these labels is cloud provider specific and is not guaranteed to be reliable.
For example, the value of <code>kubernetes.io/hostname</code> may be the same as the node name in some environments
and a different value in other environments.</div><h3 id="node-isolation-restriction">Node isolation/restriction</h3><p>Adding labels to nodes allows you to target Pods for scheduling on specific
nodes or groups of nodes. You can use this functionality to ensure that specific
Pods only run on nodes with certain isolation, security, or regulatory
properties.</p><p>If you use labels for node isolation, choose label keys that the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a>
cannot modify. This prevents a compromised node from setting those labels on
itself so that the scheduler schedules workloads onto the compromised node.</p><p>The <a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction"><code>NodeRestriction</code> admission plugin</a>
prevents the kubelet from setting or modifying labels with a
<code>node-restriction.kubernetes.io/</code> prefix.</p><p>To make use of that label prefix for node isolation:</p><ol><li>Ensure you are using the <a href="/docs/reference/access-authn-authz/node/">Node authorizer</a> and have <em>enabled</em> the <code>NodeRestriction</code> admission plugin.</li><li>Add labels with the <code>node-restriction.kubernetes.io/</code> prefix to your nodes, and use those labels in your <a href="#nodeselector">node selectors</a>.
For example, <code>example.com.node-restriction.kubernetes.io/fips=true</code> or <code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>.</li></ol><h2 id="nodeselector">nodeSelector</h2><p><code>nodeSelector</code> is the simplest recommended form of node selection constraint.
You can add the <code>nodeSelector</code> field to your Pod specification and specify the
<a href="#built-in-node-labels">node labels</a> you want the target node to have.
Kubernetes only schedules the Pod onto nodes that have each of the labels you
specify.</p><p>See <a href="/docs/tasks/configure-pod-container/assign-pods-nodes/">Assign Pods to Nodes</a> for more
information.</p><h2 id="affinity-and-anti-affinity">Affinity and anti-affinity</h2><p><code>nodeSelector</code> is the simplest way to constrain Pods to nodes with specific
labels. Affinity and anti-affinity expand the types of constraints you can
define. Some of the benefits of affinity and anti-affinity include:</p><ul><li>The affinity/anti-affinity language is more expressive. <code>nodeSelector</code> only
selects nodes with all the specified labels. Affinity/anti-affinity gives you
more control over the selection logic.</li><li>You can indicate that a rule is <em>soft</em> or <em>preferred</em>, so that the scheduler
still schedules the Pod even if it can't find a matching node.</li><li>You can constrain a Pod using labels on other Pods running on the node (or other topological domain),
instead of just node labels, which allows you to define rules for which Pods
can be co-located on a node.</li></ul><p>The affinity feature consists of two types of affinity:</p><ul><li><em>Node affinity</em> functions like the <code>nodeSelector</code> field but is more expressive and
allows you to specify soft rules.</li><li><em>Inter-pod affinity/anti-affinity</em> allows you to constrain Pods against labels
on other Pods.</li></ul><h3 id="node-affinity">Node affinity</h3><p>Node affinity is conceptually similar to <code>nodeSelector</code>, allowing you to constrain which nodes your
Pod can be scheduled on based on node labels. There are two types of node
affinity:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code>: The scheduler can't
schedule the Pod unless the rule is met. This functions like <code>nodeSelector</code>,
but with a more expressive syntax.</li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code>: The scheduler tries to
find a node that meets the rule. If a matching node is not available, the
scheduler still schedules the Pod.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In the preceding types, <code>IgnoredDuringExecution</code> means that if the node labels
change after Kubernetes schedules the Pod, the Pod continues to run.</div><p>You can specify node affinities using the <code>.spec.affinity.nodeAffinity</code> field in
your Pod spec.</p><p>For example, consider the following Pod spec:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-node-affinity.yaml" download="pods/pod-with-node-affinity.yaml"><code>pods/pod-with-node-affinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-with-node-affinity-yaml&quot;)" title="Copy pods/pod-with-node-affinity.yaml to clipboard"/></div><div class="includecode" id="pods-pod-with-node-affinity-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>with-node-affinity<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodeAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- antarctica-east1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- antarctica-west1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">preference</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>another-node-label-key<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- another-node-label-value<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>with-node-affinity<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.8</span></span></code></pre></div></div></div><p>In this example, the following rules apply:</p><ul><li>The node <em>must</em> have a label with the key <code>topology.kubernetes.io/zone</code> and
the value of that label <em>must</em> be either <code>antarctica-east1</code> or <code>antarctica-west1</code>.</li><li>The node <em>preferably</em> has a label with the key <code>another-node-label-key</code> and
the value <code>another-node-label-value</code>.</li></ul><p>You can use the <code>operator</code> field to specify a logical operator for Kubernetes to use when
interpreting the rules. You can use <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>,
<code>Gt</code> and <code>Lt</code>.</p><p>Read <a href="#operators">Operators</a>
to learn more about how these work.</p><p><code>NotIn</code> and <code>DoesNotExist</code> allow you to define node anti-affinity behavior.
Alternatively, you can use <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">node taints</a>
to repel Pods from specific nodes.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you specify both <code>nodeSelector</code> and <code>nodeAffinity</code>, <em>both</em> must be satisfied
for the Pod to be scheduled onto a node.</p><p>If you specify multiple terms in <code>nodeSelectorTerms</code> associated with <code>nodeAffinity</code>
types, then the Pod can be scheduled onto a node if one of the specified terms
can be satisfied (terms are ORed).</p><p>If you specify multiple expressions in a single <code>matchExpressions</code> field associated with a
term in <code>nodeSelectorTerms</code>, then the Pod can be scheduled onto a node only
if all the expressions are satisfied (expressions are ANDed).</p></div><p>See <a href="/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">Assign Pods to Nodes using Node Affinity</a>
for more information.</p><h4 id="node-affinity-weight">Node affinity weight</h4><p>You can specify a <code>weight</code> between 1 and 100 for each instance of the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> affinity type. When the
scheduler finds nodes that meet all the other scheduling requirements of the Pod, the
scheduler iterates through every preferred rule that the node satisfies and adds the
value of the <code>weight</code> for that expression to a sum.</p><p>The final sum is added to the score of other priority functions for the node.
Nodes with the highest total score are prioritized when the scheduler makes a
scheduling decision for the Pod.</p><p>For example, consider the following Pod spec:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-affinity-preferred-weight.yaml" download="pods/pod-with-affinity-preferred-weight.yaml"><code>pods/pod-with-affinity-preferred-weight.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-with-affinity-preferred-weight-yaml&quot;)" title="Copy pods/pod-with-affinity-preferred-weight.yaml to clipboard"/></div><div class="includecode" id="pods-pod-with-affinity-preferred-weight-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>with-affinity-preferred-weight<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodeAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>kubernetes.io/os<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- linux<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">preference</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>label-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- key-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">preference</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>label-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- key-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>with-node-affinity<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.8<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>If there are two possible nodes that match the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> rule, one with the
<code>label-1:key-1</code> label and another with the <code>label-2:key-2</code> label, the scheduler
considers the <code>weight</code> of each node and adds the weight to the other scores for
that node, and schedules the Pod onto the node with the highest final score.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you want Kubernetes to successfully schedule the Pods in this example, you
must have existing nodes with the <code>kubernetes.io/os=linux</code> label.</div><h4 id="node-affinity-per-scheduling-profile">Node affinity per scheduling profile</h4><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [beta]</code></div><p>When configuring multiple <a href="/docs/reference/scheduling/config/#multiple-profiles">scheduling profiles</a>, you can associate
a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes.
To do so, add an <code>addedAffinity</code> to the <code>args</code> field of the <a href="/docs/reference/scheduling/config/#scheduling-plugins"><code>NodeAffinity</code> plugin</a>
in the <a href="/docs/reference/scheduling/config/">scheduler configuration</a>. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">profiles</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">schedulerName</span>:<span style="color:#bbb"> </span>foo-scheduler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pluginConfig</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>NodeAffinity<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">addedAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span>- <span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>scheduler-profile<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                  </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                  </span>- foo<span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>addedAffinity</code> is applied to all Pods that set <code>.spec.schedulerName</code> to <code>foo-scheduler</code>, in addition to the
NodeAffinity specified in the PodSpec.
That is, in order to match the Pod, nodes need to satisfy <code>addedAffinity</code> and
the Pod's <code>.spec.NodeAffinity</code>.</p><p>Since the <code>addedAffinity</code> is not visible to end users, its behavior might be
unexpected to them. Use node labels that have a clear correlation to the
scheduler profile name.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The DaemonSet controller, which <a href="/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled">creates Pods for DaemonSets</a>,
does not support scheduling profiles. When the DaemonSet controller creates
Pods, the default Kubernetes scheduler places those Pods and honors any
<code>nodeAffinity</code> rules in the DaemonSet controller.</div><h3 id="inter-pod-affinity-and-anti-affinity">Inter-pod affinity and anti-affinity</h3><p>Inter-pod affinity and anti-affinity allow you to constrain which nodes your
Pods can be scheduled on based on the labels of Pods already running on that
node, instead of the node labels.</p><h4 id="types-of-inter-pod-affinity-and-anti-affinity">Types of Inter-pod Affinity and Anti-affinity</h4><p>Inter-pod affinity and anti-affinity take the form "this
Pod should (or, in the case of anti-affinity, should not) run in an X if that X
is already running one or more Pods that meet rule Y", where X is a topology
domain like node, rack, cloud provider zone or region, or similar and Y is the
rule Kubernetes tries to satisfy.</p><p>You express these rules (Y) as <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selectors</a>
with an optional associated list of namespaces. Pods are namespaced objects in
Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors
for Pod labels should specify the namespaces in which Kubernetes should look for those
labels.</p><p>You express the topology domain (X) using a <code>topologyKey</code>, which is the key for
the node label that the system uses to denote the domain. For examples, see
<a href="/docs/reference/labels-annotations-taints/">Well-Known Labels, Annotations and Taints</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Inter-pod affinity and anti-affinity require substantial amounts of
processing which can slow down scheduling in large clusters significantly. We do
not recommend using them in clusters larger than several hundred nodes.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Pod anti-affinity requires nodes to be consistently labeled, in other words,
every node in the cluster must have an appropriate label matching <code>topologyKey</code>.
If some or all nodes are missing the specified <code>topologyKey</code> label, it can lead
to unintended behavior.</div><p>Similar to <a href="#node-affinity">node affinity</a> are two types of Pod affinity and
anti-affinity as follows:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li></ul><p>For example, you could use
<code>requiredDuringSchedulingIgnoredDuringExecution</code> affinity to tell the scheduler to
co-locate Pods of two services in the same cloud provider zone because they
communicate with each other a lot. Similarly, you could use
<code>preferredDuringSchedulingIgnoredDuringExecution</code> anti-affinity to spread Pods
from a service across multiple cloud provider zones.</p><p>To use inter-pod affinity, use the <code>affinity.podAffinity</code> field in the Pod spec.
For inter-pod anti-affinity, use the <code>affinity.podAntiAffinity</code> field in the Pod
spec.</p><h4 id="scheduling-behavior">Scheduling Behavior</h4><p>When scheduling a new Pod, the Kubernetes scheduler evaluates the Pod's affinity/anti-affinity rules in the context of the current cluster state:</p><ol><li><p>Hard Constraints (Node Filtering):</p><ul><li><code>podAffinity.requiredDuringSchedulingIgnoredDuringExecution</code> and <code>podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>The scheduler ensures the new Pod is assigned to nodes that satisfy these required affinity and anti-affinity rules based on existing Pods.</li></ul></li></ul></li><li><p>Soft Constraints (Scoring):</p><ul><li><code>podAffinity.preferredDuringSchedulingIgnoredDuringExecution</code> and <code>podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>The scheduler scores nodes based on how well they meet these preferred affinity and anti-affinity rules to optimize Pod placement.</li></ul></li></ul></li><li><p>Ignored Fields:</p><ul><li>Existing Pods' <code>podAffinity.preferredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>These preferred affinity rules are not considered during the scheduling decision for new Pods.</li></ul></li><li>Existing Pods' <code>podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>Similarly, preferred anti-affinity rules of existing Pods are ignored during scheduling.</li></ul></li></ul></li></ol><h4 id="scheduling-a-group-of-pods-with-inter-pod-affinity-to-themselves">Scheduling a Group of Pods with Inter-pod Affinity to Themselves</h4><p>If the current Pod being scheduled is the first in a series that have affinity to themselves,
it is allowed to be scheduled if it passes all other affinity checks. This is determined by
verifying that no other Pod in the cluster matches the namespace and selector of this Pod,
that the Pod matches its own terms, and the chosen node matches all requested topologies.
This ensures that there will not be a deadlock even if all the Pods have inter-pod affinity
specified.</p><h4 id="an-example-of-a-pod-that-uses-pod-affinity">Pod Affinity Example</h4><p>Consider the following Pod spec:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-pod-affinity.yaml" download="pods/pod-with-pod-affinity.yaml"><code>pods/pod-with-pod-affinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-with-pod-affinity-yaml&quot;)" title="Copy pods/pod-with-pod-affinity.yaml to clipboard"/></div><div class="includecode" id="pods-pod-with-pod-affinity-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>with-pod-affinity<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">podAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>security<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- S1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">podAntiAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAffinityTerm</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>security<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span>- S2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>with-pod-affinity<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.8<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>This example defines one Pod affinity rule and one Pod anti-affinity rule. The
Pod affinity rule uses the "hard"
<code>requiredDuringSchedulingIgnoredDuringExecution</code>, while the anti-affinity rule
uses the "soft" <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</p><p>The affinity rule specifies that the scheduler is allowed to place the example Pod
on a node only if that node belongs to a specific <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">zone</a>
where other Pods have been labeled with <code>security=S1</code>.
For instance, if we have a cluster with a designated zone, let's call it "Zone V,"
consisting of nodes labeled with <code>topology.kubernetes.io/zone=V</code>, the scheduler can
assign the Pod to any node within Zone V, as long as there is at least one Pod within
Zone V already labeled with <code>security=S1</code>. Conversely, if there are no Pods with <code>security=S1</code>
labels in Zone V, the scheduler will not assign the example Pod to any node in that zone.</p><p>The anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod
on a node if that node belongs to a specific <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">zone</a>
where other Pods have been labeled with <code>security=S2</code>.
For instance, if we have a cluster with a designated zone, let's call it "Zone R,"
consisting of nodes labeled with <code>topology.kubernetes.io/zone=R</code>, the scheduler should avoid
assigning the Pod to any node within Zone R, as long as there is at least one Pod within
Zone R already labeled with <code>security=S2</code>. Conversely, the anti-affinity rule does not impact
scheduling into Zone R if there are no Pods with <code>security=S2</code> labels.</p><p>To get yourself more familiar with the examples of Pod affinity and anti-affinity,
refer to the <a href="https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md">design proposal</a>.</p><p>You can use the <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code> values in the
<code>operator</code> field for Pod affinity and anti-affinity.</p><p>Read <a href="#operators">Operators</a>
to learn more about how these work.</p><p>In principle, the <code>topologyKey</code> can be any allowed label key with the following
exceptions for performance and security reasons:</p><ul><li>For Pod affinity and anti-affinity, an empty <code>topologyKey</code> field is not allowed in both
<code>requiredDuringSchedulingIgnoredDuringExecution</code>
and <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</li><li>For <code>requiredDuringSchedulingIgnoredDuringExecution</code> Pod anti-affinity rules,
the admission controller <code>LimitPodHardAntiAffinityTopology</code> limits
<code>topologyKey</code> to <code>kubernetes.io/hostname</code>. You can modify or disable the
admission controller if you want to allow custom topologies.</li></ul><p>In addition to <code>labelSelector</code> and <code>topologyKey</code>, you can optionally specify a list
of namespaces which the <code>labelSelector</code> should match against using the
<code>namespaces</code> field at the same level as <code>labelSelector</code> and <code>topologyKey</code>.
If omitted or empty, <code>namespaces</code> defaults to the namespace of the Pod where the
affinity/anti-affinity definition appears.</p><h4 id="namespace-selector">Namespace Selector</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can also select matching namespaces using <code>namespaceSelector</code>, which is a label query over the set of namespaces.
The affinity term is applied to namespaces selected by both <code>namespaceSelector</code> and the <code>namespaces</code> field.
Note that an empty <code>namespaceSelector</code> ({}) matches all namespaces, while a null or empty <code>namespaces</code> list and
null <code>namespaceSelector</code> matches the namespace of the Pod where the rule is defined.</p><h4 id="matchlabelkeys">matchLabelKeys</h4><div class="feature-state-notice feature-stable" title="Feature Gate: MatchLabelKeysInPodAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The <code>matchLabelKeys</code> field is a beta-level field and is enabled by default in
Kubernetes 1.34.
When you want to disable it, you have to disable it explicitly via the
<code>MatchLabelKeysInPodAffinity</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p></div><p>Kubernetes includes an optional <code>matchLabelKeys</code> field for Pod affinity
or anti-affinity. The field specifies keys for the labels that should match with the incoming Pod's labels,
when satisfying the Pod (anti)affinity.</p><p>The keys are used to look up values from the Pod labels; those key-value labels are combined
(using <code>AND</code>) with the match restrictions defined using the <code>labelSelector</code> field. The combined
filtering selects the set of existing Pods that will be taken into Pod (anti)affinity calculation.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>It's not recommended to use <code>matchLabelKeys</code> with labels that might be updated directly on pods.
Even if you edit the pod's label that is specified at <code>matchLabelKeys</code> <strong>directly</strong>, (that is, not via a deployment),
kube-apiserver doesn't reflect the label update onto the merged <code>labelSelector</code>.</div><p>A common use case is to use <code>matchLabelKeys</code> with <code>pod-template-hash</code> (set on Pods
managed as part of a Deployment, where the value is unique for each revision).
Using <code>pod-template-hash</code> in <code>matchLabelKeys</code> allows you to target the Pods that belong
to the same revision as the incoming Pod, so that a rolling upgrade won't break affinity.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>application-server<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span>- database<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># Only Pods from a given rollout are taken into consideration when calculating pod affinity.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># If you update the Deployment, the replacement Pods follow their own affinity rules</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># (if there are any defined in the new Pod template)</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">matchLabelKeys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- pod-template-hash<span style="color:#bbb">
</span></span></span></code></pre></div><h4 id="mismatchlabelkeys">mismatchLabelKeys</h4><div class="feature-state-notice feature-stable" title="Feature Gate: MatchLabelKeysInPodAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The <code>mismatchLabelKeys</code> field is a beta-level field and is enabled by default in
Kubernetes 1.34.
When you want to disable it, you have to disable it explicitly via the
<code>MatchLabelKeysInPodAffinity</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p></div><p>Kubernetes includes an optional <code>mismatchLabelKeys</code> field for Pod affinity
or anti-affinity. The field specifies keys for the labels that should not match with the incoming Pod's labels,
when satisfying the Pod (anti)affinity.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>It's not recommended to use <code>mismatchLabelKeys</code> with labels that might be updated directly on pods.
Even if you edit the pod's label that is specified at <code>mismatchLabelKeys</code> <strong>directly</strong>, (that is, not via a deployment),
kube-apiserver doesn't reflect the label update onto the merged <code>labelSelector</code>.</div><p>One example use case is to ensure Pods go to the topology domain (node, zone, etc) where only Pods from the same tenant or team are scheduled in.
In other words, you want to avoid running Pods from two different tenants on the same topology domain at the same time.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Assume that all relevant Pods have a "tenant" label set</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tenant</span>:<span style="color:#bbb"> </span>tenant-a<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">podAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ensure that Pods associated with this tenant land on the correct node pool</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">matchLabelKeys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- tenant<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>node-pool<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">podAntiAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ensure that Pods associated with this tenant can't schedule to nodes used for another tenant</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">mismatchLabelKeys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- tenant<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># whatever the value of the "tenant" label for this Pod, prevent</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                 </span><span style="color:#080;font-style:italic"># scheduling to nodes in any pool where any Pod from a different</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                 </span><span style="color:#080;font-style:italic"># tenant is running.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># We have to have the labelSelector which selects only Pods with the tenant label,</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># otherwise this Pod would have anti-affinity against Pods from daemonsets as well, for example,</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># which aren't supposed to have the tenant label.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>tenant<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>node-pool<span style="color:#bbb">
</span></span></span></code></pre></div><h4 id="more-practical-use-cases">More practical use-cases</h4><p>Inter-pod affinity and anti-affinity can be even more useful when they are used with higher
level collections such as ReplicaSets, StatefulSets, Deployments, etc. These
rules allow you to configure that a set of workloads should
be co-located in the same defined topology; for example, preferring to place two related
Pods onto the same node.</p><p>For example: imagine a three-node cluster. You use the cluster to run a web application
and also an in-memory cache (such as Redis). For this example, also assume that latency between
the web application and the memory cache should be as low as is practical. You could use inter-pod
affinity and anti-affinity to co-locate the web servers with the cache as much as possible.</p><p>In the following example Deployment for the Redis cache, the replicas get the label <code>app=store</code>. The
<code>podAntiAffinity</code> rule tells the scheduler to avoid placing multiple replicas
with the <code>app=store</code> label on a single node. This creates each cache in a
separate node.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>redis-cache<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAntiAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span>- store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">"kubernetes.io/hostname"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>redis-server<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>redis:3.2-alpine<span style="color:#bbb">
</span></span></span></code></pre></div><p>The following example Deployment for the web servers creates replicas with the label <code>app=web-store</code>.
The Pod affinity rule tells the scheduler to place each replica on a node that has a Pod
with the label <code>app=store</code>. The Pod anti-affinity rule tells the scheduler never to place
multiple <code>app=web-store</code> servers on a single node.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>web-server<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>web-store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>web-store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAntiAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span>- web-store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">"kubernetes.io/hostname"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">podAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span>- store<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">"kubernetes.io/hostname"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>web-app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.16-alpine<span style="color:#bbb">
</span></span></span></code></pre></div><p>Creating the two preceding Deployments results in the following cluster layout,
where each web server is co-located with a cache, on three separate nodes.</p><table><thead><tr><th style="text-align:center">node-1</th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th></tr></thead><tbody><tr><td style="text-align:center"><em>webserver-1</em></td><td style="text-align:center"><em>webserver-2</em></td><td style="text-align:center"><em>webserver-3</em></td></tr><tr><td style="text-align:center"><em>cache-1</em></td><td style="text-align:center"><em>cache-2</em></td><td style="text-align:center"><em>cache-3</em></td></tr></tbody></table><p>The overall effect is that each cache instance is likely to be accessed by a single client that
is running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.</p><p>You might have other reasons to use Pod anti-affinity.
See the <a href="/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure">ZooKeeper tutorial</a>
for an example of a StatefulSet configured with anti-affinity for high
availability, using the same technique as this example.</p><h2 id="nodename">nodeName</h2><p><code>nodeName</code> is a more direct form of node selection than affinity or
<code>nodeSelector</code>. <code>nodeName</code> is a field in the Pod spec. If the <code>nodeName</code> field
is not empty, the scheduler ignores the Pod and the kubelet on the named node
tries to place the Pod on that node. Using <code>nodeName</code> overrules using
<code>nodeSelector</code> or affinity and anti-affinity rules.</p><p>Some of the limitations of using <code>nodeName</code> to select nodes are:</p><ul><li>If the named node does not exist, the Pod will not run, and in
some cases may be automatically deleted.</li><li>If the named node does not have the resources to accommodate the
Pod, the Pod will fail and its reason will indicate why,
for example OutOfmemory or OutOfcpu.</li><li>Node names in cloud environments are not always predictable or stable.</li></ul><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><code>nodeName</code> is intended for use by custom schedulers or advanced use cases where
you need to bypass any configured schedulers. Bypassing the schedulers might lead to
failed Pods if the assigned Nodes get oversubscribed. You can use <a href="#node-affinity">node affinity</a>
or the <a href="#nodeselector"><code>nodeSelector</code> field</a> to assign a Pod to a specific Node without bypassing the schedulers.</div><p>Here is an example of a Pod spec using the <code>nodeName</code> field:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeName</span>:<span style="color:#bbb"> </span>kube-01<span style="color:#bbb">
</span></span></span></code></pre></div><p>The above Pod will only run on the node <code>kube-01</code>.</p><h2 id="nominatednodename">nominatedNodeName</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: NominatedNodeNameForExpectation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p><code>nominatedNodeName</code> can be used for external components to nominate node for a pending pod.
This nomination is best effort: it might be ignored if the scheduler determines the pod cannot go to a nominated node.</p><p>Also, this field can be (over)written by the scheduler:</p><ul><li>If the scheduler finds a node to nominate via the preemption.</li><li>If the scheduler decides where the pod is going, and move it to the binding cycle.<ul><li>Note that, in this case, <code>nominatedNodeName</code> is put only when the pod has to go through <code>WaitOnPermit</code> or <code>PreBind</code> extension points.</li></ul></li></ul><p>Here is an example of a Pod status using the <code>nominatedNodeName</code> field:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nominatedNodeName</span>:<span style="color:#bbb"> </span>kube-01<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="pod-topology-spread-constraints">Pod topology spread constraints</h2><p>You can use <em>topology spread constraints</em> to control how <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>
are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other
topology domains that you define. You might do this to improve performance, expected availability, or
overall utilization.</p><p>Read <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a>
to learn more about how these work.</p><h2 id="operators">Operators</h2><p>The following are all the logical operators that you can use in the <code>operator</code> field for <code>nodeAffinity</code> and <code>podAffinity</code> mentioned above.</p><table><thead><tr><th style="text-align:center">Operator</th><th style="text-align:center">Behavior</th></tr></thead><tbody><tr><td style="text-align:center"><code>In</code></td><td style="text-align:center">The label value is present in the supplied set of strings</td></tr><tr><td style="text-align:center"><code>NotIn</code></td><td style="text-align:center">The label value is not contained in the supplied set of strings</td></tr><tr><td style="text-align:center"><code>Exists</code></td><td style="text-align:center">A label with this key exists on the object</td></tr><tr><td style="text-align:center"><code>DoesNotExist</code></td><td style="text-align:center">No label with this key exists on the object</td></tr></tbody></table><p>The following operators can only be used with <code>nodeAffinity</code>.</p><table><thead><tr><th style="text-align:center">Operator</th><th style="text-align:center">Behavior</th></tr></thead><tbody><tr><td style="text-align:center"><code>Gt</code></td><td style="text-align:center">The field value will be parsed as an integer, and that integer is less than the integer that results from parsing the value of a label named by this selector</td></tr><tr><td style="text-align:center"><code>Lt</code></td><td style="text-align:center">The field value will be parsed as an integer, and that integer is greater than the integer that results from parsing the value of a label named by this selector</td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>Gt</code> and <code>Lt</code> operators will not work with non-integer values. If the given value
doesn't parse as an integer, the Pod will fail to get scheduled. Also, <code>Gt</code> and <code>Lt</code>
are not available for <code>podAffinity</code>.</div><h2 id="what-s-next">What's next</h2><ul><li>Read more about <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a>.</li><li>Read the design docs for <a href="https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md">node affinity</a>
and for <a href="https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md">inter-pod affinity/anti-affinity</a>.</li><li>Learn about how the <a href="/docs/tasks/administer-cluster/topology-manager/">topology manager</a> takes part in node-level
resource allocation decisions.</li><li>Learn how to use <a href="/docs/tasks/configure-pod-container/assign-pods-nodes/">nodeSelector</a>.</li><li>Learn how to use <a href="/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">affinity and anti-affinity</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Hardening Guide - Scheduler Configuration</h1><div class="lead">Information about how to make the Kubernetes scheduler more secure.</div><p>The Kubernetes <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="scheduler">scheduler</a> is
one of the critical components of the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>.</p><p>This document covers how to improve the security posture of the Scheduler.</p><p>A misconfigured scheduler can have security implications.
Such a scheduler can target specific nodes and evict the workloads or applications that are sharing the node and its resources.
This can aid an attacker with a <a href="https://arxiv.org/abs/2105.00542">Yo-Yo attack</a>: an attack on a vulnerable autoscaler.</p><h2 id="kube-scheduler-configuration">kube-scheduler configuration</h2><h3 id="scheduler-authentication-authorization-command-line-options">Scheduler authentication &amp; authorization command line options</h3><p>When setting up authentication configuration, it should be made sure that kube-scheduler's authentication remains consistent with kube-api-server's authentication.
If any request has missing authentication headers,
the <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/#original-request-username-and-group">authentication should happen through the kube-api-server allowing all authentication to be consistent in the cluster</a>.</p><ul><li><code>authentication-kubeconfig</code>: Make sure to provide a proper kubeconfig so that the scheduler can retrieve authentication configuration options from the API Server. This kubeconfig file should be protected with strict file permissions.</li><li><code>authentication-tolerate-lookup-failure</code>: Set this to <code>false</code> to make sure the scheduler <em>always</em> looks up its authentication configuration from the API server.</li><li><code>authentication-skip-lookup</code>: Set this to <code>false</code> to make sure the scheduler <em>always</em> looks up its authentication configuration from the API server.</li><li><code>authorization-always-allow-paths</code>: These paths should respond with data that is appropriate for anonymous authorization. Defaults to <code>/healthz,/readyz,/livez</code>.</li><li><code>profiling</code>: Set to <code>false</code> to disable the profiling endpoints which are provide debugging information but which should not be enabled on production clusters as they present a risk of denial of service or information leakage. The <code>--profiling</code> argument is deprecated and can now be provided through the <a href="https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1/#DebuggingConfiguration">KubeScheduler DebuggingConfiguration</a>. Profiling can be disabled through the kube-scheduler config by setting <code>enableProfiling</code> to <code>false</code>.</li><li><code>requestheader-client-ca-file</code>: Avoid passing this argument.</li></ul><h3 id="scheduler-networking-command-line-options">Scheduler networking command line options</h3><ul><li><code>bind-address</code>: In most cases, the kube-scheduler does not need to be externally accessible. Setting the bind address to <code>localhost</code> is a secure practice.</li><li><code>permit-address-sharing</code>: Set this to <code>false</code> to disable connection sharing through <code>SO_REUSEADDR</code>. <code>SO_REUSEADDR</code> can lead to reuse of terminated connections that are in <code>TIME_WAIT</code> state.</li><li><code>permit-port-sharing</code>: Default <code>false</code>. Use the default unless you are confident you understand the security implications.</li></ul><h3 id="scheduler-tls-command-line-options">Scheduler TLS command line options</h3><ul><li><code>tls-cipher-suites</code>: Always provide a list of preferred cipher suites. This ensures encryption never happens with insecure cipher suites.</li></ul><h2 id="scheduling-configurations-for-custom-schedulers">Scheduling configurations for custom schedulers</h2><p>When using custom schedulers based on the Kubernetes scheduling code, cluster administrators need to be careful with
plugins that use the <code>queueSort</code>, <code>prefilter</code>, <code>filter</code>, or <code>permit</code> <a href="/docs/reference/scheduling/config/#extension-points">extension points</a>.
These extension points control various stages of a scheduling process, and the wrong configuration can impact the kube-scheduler's behavior in your cluster.</p><h3 id="key-considerations">Key considerations</h3><ul><li>Exactly one plugin that uses the <code>queueSort</code> extension point can be enabled at a time. Any plugins that use <code>queueSort</code> should be scrutinized.</li><li>Plugins that implement the <code>prefilter</code> or <code>filter</code> extension point can potentially mark all nodes as unschedulable. This can bring scheduling of new pods to a halt.</li><li>Plugins that implement the <code>permit</code> extension point can prevent or delay the binding of a Pod. Such plugins should be thoroughly reviewed by the cluster administrator.</li></ul><p>When using a plugin that is not one of the <a href="/docs/reference/scheduling/config/#scheduling-plugins">default plugins</a>, consider disabling the <code>queueSort</code>, <code>filter</code> and <code>permit</code> extension points as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">profiles</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">schedulerName</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">plugins</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Disable specific plugins for different extension points</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># You can disable all plugins for an extension point using "*"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">queueSort</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">disabled</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"*"</span><span style="color:#bbb">             </span><span style="color:#080;font-style:italic"># Disable all queueSort plugins</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># - name: "PrioritySort"  # Disable specific queueSort plugin</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">filter</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">disabled</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"*"</span><span style="color:#bbb">                 </span><span style="color:#080;font-style:italic"># Disable all filter plugins</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># - name: "NodeResourcesFit"  # Disable specific filter plugin</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">permit</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">disabled</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"*"</span><span style="color:#bbb">               </span><span style="color:#080;font-style:italic"># Disables all permit plugins</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># - name: "TaintToleration" # Disable specific permit plugin</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This creates a scheduler profile <code>my-custom-scheduler</code>.
Whenever the <code>.spec</code> of a Pod does not have a value for <code>.spec.schedulerName</code>, the kube-scheduler runs for that Pod,
using its main configuration, and default plugins.
If you define a Pod with <code>.spec.schedulerName</code> set to <code>my-custom-scheduler</code>, the kube-scheduler runs but with a custom configuration; in that custom configuration,
the <code>queueSort</code>, <code>filter</code> and <code>permit</code> extension points are disabled.
If you use this KubeSchedulerConfiguration, and don't run any custom scheduler,
and you then define a Pod with <code>.spec.schedulerName</code> set to <code>nonexistent-scheduler</code>
(or any other scheduler name that doesn't exist in your cluster), no events would be generated for a pod.</p><h2 id="disallow-labeling-nodes">Disallow labeling nodes</h2><p>A cluster administrator should ensure that cluster users cannot label the nodes.
A malicious actor can use <code>nodeSelector</code> to schedule workloads on nodes where those workloads should not be present.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Security Checklist</h1><div class="lead">Baseline checklist for ensuring security in Kubernetes clusters.</div><p>This checklist aims at providing a basic list of guidance with links to more
comprehensive documentation on each topic. It does not claim to be exhaustive
and is meant to evolve.</p><p>On how to read and use this document:</p><ul><li>The order of topics does not reflect an order of priority.</li><li>Some checklist items are detailed in the paragraph below the list of each section.</li></ul><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Checklists are <strong>not</strong> sufficient for attaining a good security posture on their
own. A good security posture requires constant attention and improvement, but a
checklist can be the first step on the never-ending journey towards security
preparedness. Some of the recommendations in this checklist may be too
restrictive or too lax for your specific security needs. Since Kubernetes
security is not "one size fits all", each category of checklist items should be
evaluated on its merits.</div><h2 id="authentication-authorization">Authentication &amp; Authorization</h2><ul><li><input disabled="" type="checkbox"/> <code>system:masters</code> group is not used for user or component authentication after bootstrapping.</li><li><input disabled="" type="checkbox"/> The kube-controller-manager is running with <code>--use-service-account-credentials</code>
enabled.</li><li><input disabled="" type="checkbox"/> The root certificate is protected (either an offline CA, or a managed
online CA with effective access controls).</li><li><input disabled="" type="checkbox"/> Intermediate and leaf certificates have an expiry date no more than 3
years in the future.</li><li><input disabled="" type="checkbox"/> A process exists for periodic access review, and reviews occur no more
than 24 months apart.</li><li><input disabled="" type="checkbox"/> The <a href="/docs/concepts/security/rbac-good-practices/">Role Based Access Control Good Practices</a>
are followed for guidance related to authentication and authorization.</li></ul><p>After bootstrapping, neither users nor components should authenticate to the
Kubernetes API as <code>system:masters</code>. Similarly, running all of
kube-controller-manager as <code>system:masters</code> should be avoided. In fact,
<code>system:masters</code> should only be used as a break-glass mechanism, as opposed to
an admin user.</p><h2 id="network-security">Network security</h2><ul><li><input disabled="" type="checkbox"/> CNI plugins in use support network policies.</li><li><input disabled="" type="checkbox"/> Ingress and egress network policies are applied to all workloads in the
cluster.</li><li><input disabled="" type="checkbox"/> Default network policies within each namespace, selecting all pods, denying
everything, are in place.</li><li><input disabled="" type="checkbox"/> If appropriate, a service mesh is used to encrypt all communications inside of the cluster.</li><li><input disabled="" type="checkbox"/> The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.</li><li><input disabled="" type="checkbox"/> Access from the workloads to the cloud metadata API is filtered.</li><li><input disabled="" type="checkbox"/> Use of LoadBalancer and ExternalIPs is restricted.</li></ul><p>A number of <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Container Network Interface (CNI) plugins</a>
plugins provide the functionality to
restrict network resources that pods may communicate with. This is most commonly done
through <a href="/docs/concepts/services-networking/network-policies/">Network Policies</a>
which provide a namespaced resource to define rules. Default network policies
that block all egress and ingress, in each namespace, selecting all pods, can be
useful to adopt an allow list approach to ensure that no workloads are missed.</p><p>Not all CNI plugins provide encryption in transit. If the chosen plugin lacks this
feature, an alternative solution could be to use a service mesh to provide that
functionality.</p><p>The etcd datastore of the control plane should have controls to limit access and
not be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) should
be used to communicate securely with it. The certificate authority for this
should be unique to etcd.</p><p>External Internet access to the Kubernetes API server should be restricted to
not expose the API publicly. Be careful, as many managed Kubernetes distributions
are publicly exposing the API server by default. You can then use a bastion host
to access the server.</p><p>The <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> API access
should be restricted and not exposed publicly, the default authentication and
authorization settings, when no configuration file specified with the <code>--config</code>
flag, are overly permissive.</p><p>If a cloud provider is used for hosting Kubernetes, the access from pods to the cloud
metadata API <code>169.254.169.254</code> should also be restricted or blocked if not needed
because it may leak information.</p><p>For restricted LoadBalancer and ExternalIPs use, see
<a href="https://github.com/kubernetes/kubernetes/issues/97076">CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>
and the <a href="/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips">DenyServiceExternalIPs admission controller</a>
for further information.</p><h2 id="pod-security">Pod security</h2><ul><li><input disabled="" type="checkbox"/> RBAC rights to <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> workloads is only granted if necessary.</li><li><input disabled="" type="checkbox"/> Appropriate Pod Security Standards policy is applied for all namespaces and enforced.</li><li><input disabled="" type="checkbox"/> Memory limit is set for the workloads with a limit equal or inferior to the request.</li><li><input disabled="" type="checkbox"/> CPU limit might be set on sensitive workloads.</li><li><input disabled="" type="checkbox"/> For nodes that support it, Seccomp is enabled with appropriate syscalls
profile for programs.</li><li><input disabled="" type="checkbox"/> For nodes that support it, AppArmor or SELinux is enabled with appropriate
profile for programs.</li></ul><p>RBAC authorization is crucial but
<a href="/docs/concepts/security/rbac-good-practices/#workload-creation">cannot be granular enough to have authorization on the Pods' resources</a>
(or on any resource that manages Pods). The only granularity is the API verbs
on the resource itself, for example, <code>create</code> on Pods. Without
additional admission, the authorization to create these resources allows direct
unrestricted access to the schedulable nodes of a cluster.</p><p>The <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>
define three different policies, privileged, baseline and restricted that limit
how fields can be set in the <code>PodSpec</code> regarding security.
These standards can be enforced at the namespace level with the new
<a href="/docs/concepts/security/pod-security-admission/">Pod Security</a> admission,
enabled by default, or by third-party admission webhook. Please note that,
contrary to the removed PodSecurityPolicy admission it replaces,
<a href="/docs/concepts/security/pod-security-admission/">Pod Security</a>
admission can be easily combined with admission webhooks and external services.</p><p>Pod Security admission <code>restricted</code> policy, the most restrictive policy of the
<a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> set,
<a href="/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces">can operate in several modes</a>,
<code>warn</code>, <code>audit</code> or <code>enforce</code> to gradually apply the most appropriate
<a href="/docs/tasks/configure-pod-container/security-context/">security context</a>
according to security best practices. Nevertheless, pods'
<a href="/docs/tasks/configure-pod-container/security-context/">security context</a>
should be separately investigated to limit the privileges and access pods may
have on top of the predefined security standards, for specific use cases.</p><p>For a hands-on tutorial on <a href="/docs/concepts/security/pod-security-admission/">Pod Security</a>,
see the blog post
<a href="/blog/2021/12/09/pod-security-admission-beta/">Kubernetes 1.23: Pod Security Graduates to Beta</a>.</p><p><a href="/docs/concepts/configuration/manage-resources-containers/">Memory and CPU limits</a>
should be set in order to restrict the memory and CPU resources a pod can
consume on a node, and therefore prevent potential DoS attacks from malicious or
breached workloads. Such policy can be enforced by an admission controller.
Please note that CPU limits will throttle usage and thus can have unintended
effects on auto-scaling features or efficiency i.e. running the process in best
effort with the CPU resource available.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Memory limit superior to request can expose the whole node to OOM issues.</div><h3 id="enabling-seccomp">Enabling Seccomp</h3><p>Seccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12.
It can be used to sandbox the privileges of a process, restricting the calls it is able to make
from userspace into the kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto
a node to your Pods and containers.</p><p>Seccomp can improve the security of your workloads by reducing the Linux kernel syscall attack
surface available inside containers. The seccomp filter mode leverages BPF to create an allow or
deny list of specific syscalls, named profiles.</p><p>Since Kubernetes 1.27, you can enable the use of <code>RuntimeDefault</code> as the default seccomp profile
for all workloads. A <a href="/docs/tutorials/security/seccomp/">security tutorial</a> is available on this
topic. In addition, the
<a href="https://github.com/kubernetes-sigs/security-profiles-operator">Kubernetes Security Profiles Operator</a>
is a project that facilitates the management and use of seccomp in clusters.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Seccomp is only available on Linux nodes.</div><h3 id="enabling-apparmor-or-selinux">Enabling AppArmor or SELinux</h3><h4 id="apparmor">AppArmor</h4><p><a href="/docs/tutorials/security/apparmor/">AppArmor</a> is a Linux kernel security module that can
provide an easy way to implement Mandatory Access Control (MAC) and better
auditing through system logs. A default AppArmor profile is enforced on nodes that support it, or a custom profile can be configured.
Like seccomp, AppArmor is also configured
through profiles, where each profile is either running in enforcing mode, which
blocks access to disallowed resources or complain mode, which only reports
violations. AppArmor profiles are enforced on a per-container basis, with an
annotation, allowing for processes to gain just the right privileges.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>AppArmor is only available on Linux nodes, and enabled in
<a href="https://gitlab.com/apparmor/apparmor/-/wikis/home#distributions-and-ports">some Linux distributions</a>.</div><h4 id="selinux">SELinux</h4><p><a href="https://github.com/SELinuxProject/selinux-notebook/blob/main/src/selinux_overview.md">SELinux</a> is also a
Linux kernel security module that can provide a mechanism for supporting access
control security policies, including Mandatory Access Controls (MAC). SELinux
labels can be assigned to containers or pods
<a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">via their <code>securityContext</code> section</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>SELinux is only available on Linux nodes, and enabled in
<a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux#Implementations">some Linux distributions</a>.</div><h2 id="logs-and-auditing">Logs and auditing</h2><ul><li><input disabled="" type="checkbox"/> Audit logs, if enabled, are protected from general access.</li></ul><h2 id="pod-placement">Pod placement</h2><ul><li><input disabled="" type="checkbox"/> Pod placement is done in accordance with the tiers of sensitivity of the
application.</li><li><input disabled="" type="checkbox"/> Sensitive applications are running isolated on nodes or with specific
sandboxed runtimes.</li></ul><p>Pods that are on different tiers of sensitivity, for example, an application pod
and the Kubernetes API server, should be deployed onto separate nodes. The
purpose of node isolation is to prevent an application container breakout to
directly providing access to applications with higher level of sensitivity to easily
pivot within the cluster. This separation should be enforced to prevent pods
accidentally being deployed onto the same node. This could be enforced with the
following features:</p><dl><dt><a href="/docs/concepts/scheduling-eviction/assign-pod-node/">Node Selectors</a></dt><dd>Key-value pairs, as part of the pod specification, that specify which nodes to
deploy onto. These can be enforced at the namespace and cluster level with the
<a href="/docs/reference/access-authn-authz/admission-controllers/#podnodeselector">PodNodeSelector</a>
admission controller.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction">PodTolerationRestriction</a></dt><dd>An admission controller that allows administrators to restrict permitted
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</a> within a
namespace. Pods within a namespace may only utilize the tolerations specified on
the namespace object annotation keys that provide a set of default and allowed
tolerations.</dd><dt><a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a></dt><dd>RuntimeClass is a feature for selecting the container runtime configuration.
The container runtime configuration is used to run a Pod's containers and can
provide more or less isolation from the host at the cost of performance
overhead.</dd></dl><h2 id="secrets">Secrets</h2><ul><li><input disabled="" type="checkbox"/> ConfigMaps are not used to hold confidential data.</li><li><input disabled="" type="checkbox"/> Encryption at rest is configured for the Secret API.</li><li><input disabled="" type="checkbox"/> If appropriate, a mechanism to inject secrets stored in third-party storage
is deployed and available.</li><li><input disabled="" type="checkbox"/> Service account tokens are not mounted in pods that don't require them.</li><li><input disabled="" type="checkbox"/> <a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">Bound service account token volume</a>
is in-use instead of non-expiring tokens.</li></ul><p>Secrets required for pods should be stored within Kubernetes Secrets as opposed
to alternatives such as ConfigMap. Secret resources stored within etcd should
be <a href="/docs/tasks/administer-cluster/encrypt-data/">encrypted at rest</a>.</p><p>Pods needing secrets should have these automatically mounted through volumes,
preferably stored in memory like with the <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir.medium</code> option</a>.
Mechanism can be used to also inject secrets from third-party storages as
volume, like the <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Secrets Store CSI Driver</a>.
This should be done preferentially as compared to providing the pods service
account RBAC access to secrets. This would allow adding secrets into the pod as
environment variables or files. Please note that the environment variable method
might be more prone to leakage due to crash dumps in logs and the
non-confidential nature of environment variable in Linux, as opposed to the
permission mechanism on files.</p><p>Service account tokens should not be mounted into pods that do not require them. This can be configured by setting
<a href="/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server"><code>automountServiceAccountToken</code></a>
to <code>false</code> either within the service account to apply throughout the namespace
or specifically for a pod. For Kubernetes v1.22 and above, use
<a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">Bound Service Accounts</a>
for time-bound service account credentials.</p><h2 id="images">Images</h2><ul><li><input disabled="" type="checkbox"/> Minimize unnecessary content in container images.</li><li><input disabled="" type="checkbox"/> Container images are configured to be run as unprivileged user.</li><li><input disabled="" type="checkbox"/> References to container images are made by sha256 digests (rather than
tags) or the provenance of the image is validated by verifying the image's
digital signature at deploy time <a href="/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller">via admission control</a>.</li><li><input disabled="" type="checkbox"/> Container images are regularly scanned during creation and in deployment, and
known vulnerable software is patched.</li></ul><p>Container image should contain the bare minimum to run the program they
package. Preferably, only the program and its dependencies, building the image
from the minimal possible base. In particular, image used in production should not
contain shells or debugging utilities, as an
<a href="/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container">ephemeral debug container</a>
can be used for troubleshooting.</p><p>Build images to directly start with an unprivileged user by using the
<a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user"><code>USER</code> instruction in Dockerfile</a>.
The <a href="/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod">Security Context</a>
allows a container image to be started with a specific user and group with
<code>runAsUser</code> and <code>runAsGroup</code>, even if not specified in the image manifest.
However, the file permissions in the image layers might make it impossible to just
start the process with a new unprivileged user without image modification.</p><p>Avoid using image tags to reference an image, especially the <code>latest</code> tag, the
image behind a tag can be easily modified in a registry. Prefer using the
complete <code>sha256</code> digest which is unique to the image manifest. This policy can be
enforced via an <a href="/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">ImagePolicyWebhook</a>.
Image signatures can also be automatically <a href="/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller">verified with an admission controller</a>
at deploy time to validate their authenticity and integrity.</p><p>Scanning a container image can prevent critical vulnerabilities from being
deployed to the cluster alongside the container image. Image scanning should be
completed before deploying a container image to a cluster and is usually done
as part of the deployment process in a CI/CD pipeline. The purpose of an image
scan is to obtain information about possible vulnerabilities and their
prevention in the container image, such as a
<a href="https://www.first.org/cvss/">Common Vulnerability Scoring System (CVSS)</a>
score. If the result of the image scans is combined with the pipeline
compliance rules, only properly patched container images will end up in
Production.</p><h2 id="admission-controllers">Admission controllers</h2><ul><li><input disabled="" type="checkbox"/> An appropriate selection of admission controllers is enabled.</li><li><input disabled="" type="checkbox"/> A pod security policy is enforced by the Pod Security Admission or/and a
webhook admission controller.</li><li><input disabled="" type="checkbox"/> The admission chain plugins and webhooks are securely configured.</li></ul><p>Admission controllers can help improve the security of the cluster. However,
they can present risks themselves as they extend the API server and
<a href="/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/">should be properly secured</a>.</p><p>The following lists present a number of admission controllers that could be
considered to enhance the security posture of your cluster and application. It
includes controllers that may be referenced in other parts of this document.</p><p>This first group of admission controllers includes plugins
<a href="/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default">enabled by default</a>,
consider to leave them enabled unless you know what you are doing:</p><dl><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#certificateapproval"><code>CertificateApproval</code></a></dt><dd>Performs additional authorization checks to ensure the approving user has
permission to approve certificate request.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#certificatesigning"><code>CertificateSigning</code></a></dt><dd>Performs additional authorization checks to ensure the signing user has
permission to sign certificate requests.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction"><code>CertificateSubjectRestriction</code></a></dt><dd>Rejects any certificate request that specifies a 'group' (or 'organization
attribute') of <code>system:masters</code>.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#limitranger"><code>LimitRanger</code></a></dt><dd>Enforces the LimitRange API constraints.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook"><code>MutatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers may
mutate requests that they review.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#podsecurity"><code>PodSecurity</code></a></dt><dd>Replacement for Pod Security Policy, restricts security contexts of deployed
Pods.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#resourcequota"><code>ResourceQuota</code></a></dt><dd>Enforces resource quotas to prevent over-usage of resources.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook"><code>ValidatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers do
not mutate requests that it reviews.</dd></dl><p>The second group includes plugins that are not enabled by default but are in general
availability state and are recommended to improve your security posture:</p><dl><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips"><code>DenyServiceExternalIPs</code></a></dt><dd>Rejects all net-new usage of the <code>Service.spec.externalIPs</code> field. This is a mitigation for
<a href="https://github.com/kubernetes/kubernetes/issues/97076">CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction"><code>NodeRestriction</code></a></dt><dd>Restricts kubelet's permissions to only modify the pods API resources they own
or the node API resource that represent themselves. It also prevents kubelet
from using the <code>node-restriction.kubernetes.io/</code> annotation, which can be used
by an attacker with access to the kubelet's credentials to influence pod
placement to the controlled node.</dd></dl><p>The third group includes plugins that are not enabled by default but could be
considered for certain use cases:</p><dl><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages"><code>AlwaysPullImages</code></a></dt><dd>Enforces the usage of the latest version of a tagged image and ensures that the deployer
has permissions to use the image.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook"><code>ImagePolicyWebhook</code></a></dt><dd>Allows enforcing additional controls for images through webhooks.</dd></dl><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/access-authn-authz/authorization/#privilege-escalation-via-pod-creation">Privilege escalation via Pod creation</a>
warns you about a specific access control risk; check how you're managing that
threat.<ul><li>If you use Kubernetes RBAC, read
<a href="/docs/concepts/security/rbac-good-practices/">RBAC Good Practices</a> for
further information on authorization.</li></ul></li><li><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing a Cluster</a> for
information on protecting a cluster from accidental or malicious access.</li><li><a href="/docs/concepts/security/multi-tenancy/">Cluster Multi-tenancy guide</a> for
configuration options recommendations and best practices on multi-tenancy.</li><li><a href="/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#building-secure-container-images">Blog post "A Closer Look at NSA/CISA Kubernetes Hardening Guidance"</a>
for complementary resource on hardening Kubernetes clusters.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Security For Linux Nodes</h1><p>This page describes security considerations and best practices specific to the Linux operating system.</p><h2 id="protection-for-secret-data-on-nodes">Protection for Secret data on nodes</h2><p>On Linux nodes, memory-backed volumes (such as <a href="/docs/concepts/configuration/secret/"><code>secret</code></a>
volume mounts, or <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a> with <code>medium: Memory</code>)
are implemented with a <code>tmpfs</code> filesystem.</p><p>If you have swap configured and use an older Linux kernel (or a current kernel and an unsupported configuration of Kubernetes),
<strong>memory</strong> backed volumes can have data written to persistent storage.</p><p>The Linux kernel officially supports the <code>noswap</code> option from version 6.3,
therefore it is recommended the used kernel version is 6.3 or later,
or supports the <code>noswap</code> option via a backport, if swap is enabled on the node.</p><p>Read <a href="/docs/concepts/cluster-administration/swap-memory-management/#memory-backed-volumes">swap memory management</a>
for more info.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">CSI Volume Cloning</h1><p>This document describes the concept of cloning existing CSI Volumes in Kubernetes.
Familiarity with <a href="/docs/concepts/storage/volumes/">Volumes</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>The <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> Volume Cloning feature adds
support for specifying existing <a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" aria-label="PVC">PVC</a>s
in the <code>dataSource</code> field to indicate a user would like to clone a <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="Volume">Volume</a>.</p><p>A Clone is defined as a duplicate of an existing Kubernetes Volume that can be
consumed as any standard Volume would be. The only difference is that upon
provisioning, rather than creating a "new" empty Volume, the back end device
creates an exact duplicate of the specified Volume.</p><p>The implementation of cloning, from the perspective of the Kubernetes API, adds
the ability to specify an existing PVC as a dataSource during new PVC creation.
The source PVC must be bound and available (not in use).</p><p>Users need to be aware of the following when using this feature:</p><ul><li>Cloning support (<code>VolumePVCDataSource</code>) is only available for CSI drivers.</li><li>Cloning support is only available for dynamic provisioners.</li><li>CSI drivers may or may not have implemented the volume cloning functionality.</li><li>You can only clone a PVC when it exists in the same namespace as the destination PVC
(source and destination must be in the same namespace).</li><li>Cloning is supported with a different Storage Class.<ul><li>Destination volume can be the same or a different storage class as the source.</li><li>Default storage class can be used and storageClassName omitted in the spec.</li></ul></li><li>Cloning can only be performed between two volumes that use the same VolumeMode setting
(if you request a block mode volume, the source MUST also be block mode)</li></ul><h2 id="provisioning">Provisioning</h2><p>Clones are provisioned like any other PVC with the exception of adding a dataSource
that references an existing PVC in the same namespace.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>clone-of-pvc-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>myns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>cloning<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>5Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dataSource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pvc-1<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must specify a capacity value for <code>spec.resources.requests.storage</code>, and the
value you specify must be the same or larger than the capacity of the source volume.</div><p>The result is a new PVC with the name <code>clone-of-pvc-1</code> that has the exact same
content as the specified source <code>pvc-1</code>.</p><h2 id="usage">Usage</h2><p>Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC.
It's also expected at this point that the newly created PVC is an independent object.
It can be consumed, cloned, snapshotted, or deleted independently and without
consideration for it's original dataSource PVC. This also implies that the source
is not linked in any way to the newly created clone, it may also be modified or
deleted without affecting the newly created clone.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Volume Snapshots</h1><p>In Kubernetes, a <em>VolumeSnapshot</em> represents a snapshot of a volume on a storage
system. This document assumes that you are already familiar with Kubernetes
<a href="/docs/concepts/storage/persistent-volumes/">persistent volumes</a>.</p><h2 id="introduction">Introduction</h2><p>Similar to how API resources <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> are
used to provision volumes for users and administrators, <code>VolumeSnapshotContent</code>
and <code>VolumeSnapshot</code> API resources are provided to create volume snapshots for
users and administrators.</p><p>A <code>VolumeSnapshotContent</code> is a snapshot taken from a volume in the cluster that
has been provisioned by an administrator. It is a resource in the cluster just
like a PersistentVolume is a cluster resource.</p><p>A <code>VolumeSnapshot</code> is a request for snapshot of a volume by a user. It is similar
to a PersistentVolumeClaim.</p><p><code>VolumeSnapshotClass</code> allows you to specify different attributes belonging to a
<code>VolumeSnapshot</code>. These attributes may differ among snapshots taken from the same
volume on the storage system and therefore cannot be expressed by using the same
<code>StorageClass</code> of a <code>PersistentVolumeClaim</code>.</p><p>Volume snapshots provide Kubernetes users with a standardized way to copy a volume's
contents at a particular point in time without creating an entirely new volume. This
functionality enables, for example, database administrators to backup databases before
performing edit or delete modifications.</p><p>Users need to be aware of the following when using this feature:</p><ul><li>API Objects <code>VolumeSnapshot</code>, <code>VolumeSnapshotContent</code>, and <code>VolumeSnapshotClass</code>
are <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." data-toggle="tooltip" data-placement="top" href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank" aria-label="CRDs">CRDs</a>, not
part of the core API.</li><li><code>VolumeSnapshot</code> support is only available for CSI drivers.</li><li>As part of the deployment process of <code>VolumeSnapshot</code>, the Kubernetes team provides
a snapshot controller to be deployed into the control plane, and a sidecar helper
container called csi-snapshotter to be deployed together with the CSI driver.
The snapshot controller watches <code>VolumeSnapshot</code> and <code>VolumeSnapshotContent</code> objects
and is responsible for the creation and deletion of <code>VolumeSnapshotContent</code> object.
The sidecar csi-snapshotter watches <code>VolumeSnapshotContent</code> objects and triggers
<code>CreateSnapshot</code> and <code>DeleteSnapshot</code> operations against a CSI endpoint.</li><li>There is also a validating webhook server which provides tightened validation on
snapshot objects. This should be installed by the Kubernetes distros along with
the snapshot controller and CRDs, not CSI drivers. It should be installed in all
Kubernetes clusters that has the snapshot feature enabled.</li><li>CSI drivers may or may not have implemented the volume snapshot functionality.
The CSI drivers that have provided support for volume snapshot will likely use
the csi-snapshotter. See <a href="https://kubernetes-csi.github.io/docs/">CSI Driver documentation</a> for details.</li><li>The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.</li></ul><p>For advanced use cases, such as creating group snapshots of multiple volumes, see the external
<a href="https://kubernetes-csi.github.io/docs/group-snapshot-restore-feature.html">CSI Volume Group Snapshot documentation</a>.</p><h2 id="lifecycle-of-a-volume-snapshot-and-volume-snapshot-content">Lifecycle of a volume snapshot and volume snapshot content</h2><p><code>VolumeSnapshotContents</code> are resources in the cluster. <code>VolumeSnapshots</code> are requests
for those resources. The interaction between <code>VolumeSnapshotContents</code> and <code>VolumeSnapshots</code>
follow this lifecycle:</p><h3 id="provisioning-volume-snapshot">Provisioning Volume Snapshot</h3><p>There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.</p><h4 id="static">Pre-provisioned</h4><p>A cluster administrator creates a number of <code>VolumeSnapshotContents</code>. They carry the details
of the real volume snapshot on the storage system which is available for use by cluster users.
They exist in the Kubernetes API and are available for consumption.</p><h4 id="dynamic">Dynamic</h4><p>Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically
taken from a PersistentVolumeClaim. The <a href="/docs/concepts/storage/volume-snapshot-classes/">VolumeSnapshotClass</a>
specifies storage provider-specific parameters to use when taking a snapshot.</p><h3 id="binding">Binding</h3><p>The snapshot controller handles the binding of a <code>VolumeSnapshot</code> object with an appropriate
<code>VolumeSnapshotContent</code> object, in both pre-provisioned and dynamically provisioned scenarios.
The binding is a one-to-one mapping.</p><p>In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the
requested VolumeSnapshotContent object is created.</p><h3 id="persistent-volume-claim-as-snapshot-source-protection">Persistent Volume Claim as Snapshot Source Protection</h3><p>The purpose of this protection is to ensure that in-use
<a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" aria-label="PersistentVolumeClaim">PersistentVolumeClaim</a>
API objects are not removed from the system while a snapshot is being taken from it
(as this may result in data loss).</p><p>While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim
is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot
source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of
the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.</p><h3 id="delete">Delete</h3><p>Deletion is triggered by deleting the <code>VolumeSnapshot</code> object, and the <code>DeletionPolicy</code>
will be followed. If the <code>DeletionPolicy</code> is <code>Delete</code>, then the underlying storage snapshot
will be deleted along with the <code>VolumeSnapshotContent</code> object. If the <code>DeletionPolicy</code> is
<code>Retain</code>, then both the underlying snapshot and <code>VolumeSnapshotContent</code> remain.</p><h2 id="volumesnapshots">VolumeSnapshots</h2><p>Each VolumeSnapshot contains a spec and a status.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeSnapshotClassName</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">source</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">persistentVolumeClaimName</span>:<span style="color:#bbb"> </span>pvc-test<span style="color:#bbb">
</span></span></span></code></pre></div><p><code>persistentVolumeClaimName</code> is the name of the PersistentVolumeClaim data source
for the snapshot. This field is required for dynamically provisioning a snapshot.</p><p>A volume snapshot can request a particular class by specifying the name of a
<a href="/docs/concepts/storage/volume-snapshot-classes/">VolumeSnapshotClass</a>
using the attribute <code>volumeSnapshotClassName</code>. If nothing is set, then the
default class is used if available.</p><p>For pre-provisioned snapshots, you need to specify a <code>volumeSnapshotContentName</code>
as the source for the snapshot as shown in the following example. The
<code>volumeSnapshotContentName</code> source field is required for pre-provisioned snapshots.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-snapshot<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">source</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeSnapshotContentName</span>:<span style="color:#bbb"> </span>test-content<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="volume-snapshot-contents">Volume Snapshot Contents</h2><p>Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning,
the snapshot common controller creates <code>VolumeSnapshotContent</code> objects. Here is an example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>snapcontent-72d9a349-aacd-42d2-a240-d775650d2455<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">source</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeHandle</span>:<span style="color:#bbb"> </span>ee0cfb94-f8d4-11e9-b2d8-0242ac110002<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">sourceVolumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeSnapshotClassName</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeSnapshotRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">uid</span>:<span style="color:#bbb"> </span>72d9a349-aacd-42d2-a240-d775650d2455<span style="color:#bbb">
</span></span></span></code></pre></div><p><code>volumeHandle</code> is the unique identifier of the volume created on the storage
backend and returned by the CSI driver during the volume creation. This field
is required for dynamically provisioning a snapshot.
It specifies the volume source of the snapshot.</p><p>For pre-provisioned snapshots, you (as cluster administrator) are responsible
for creating the <code>VolumeSnapshotContent</code> object as follows.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-content-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">source</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">snapshotHandle</span>:<span style="color:#bbb"> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">sourceVolumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeSnapshotRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span></code></pre></div><p><code>snapshotHandle</code> is the unique identifier of the volume snapshot created on
the storage backend. This field is required for the pre-provisioned snapshots.
It specifies the CSI snapshot id on the storage system that this
<code>VolumeSnapshotContent</code> represents.</p><p><code>sourceVolumeMode</code> is the mode of the volume whose snapshot is taken. The value
of the <code>sourceVolumeMode</code> field can be either <code>Filesystem</code> or <code>Block</code>. If the
source volume mode is not specified, Kubernetes treats the snapshot as if the
source volume's mode is unknown.</p><p><code>volumeSnapshotRef</code> is the reference of the corresponding <code>VolumeSnapshot</code>. Note that
when the <code>VolumeSnapshotContent</code> is being created as a pre-provisioned snapshot, the
<code>VolumeSnapshot</code> referenced in <code>volumeSnapshotRef</code> might not exist yet.</p><h2 id="convert-volume-mode">Converting the volume mode of a Snapshot</h2><p>If the <code>VolumeSnapshots</code> API installed on your cluster supports the <code>sourceVolumeMode</code>
field, then the API has the capability to prevent unauthorized users from converting
the mode of a volume.</p><p>To check if your cluster has capability for this feature, run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span>$ kubectl get crd volumesnapshotcontent -o yaml<span style="color:#bbb">
</span></span></span></code></pre></div><p>If you want to allow users to create a <code>PersistentVolumeClaim</code> from an existing
<code>VolumeSnapshot</code>, but with a different volume mode than the source, the annotation
<code>snapshot.storage.kubernetes.io/allow-volume-mode-change: "true"</code>needs to be added to
the <code>VolumeSnapshotContent</code> that corresponds to the <code>VolumeSnapshot</code>.</p><p>For pre-provisioned snapshots, <code>spec.sourceVolumeMode</code> needs to be populated
by the cluster administrator.</p><p>An example <code>VolumeSnapshotContent</code> resource with this feature enabled would look like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-content-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">snapshot.storage.kubernetes.io/allow-volume-mode-change</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">source</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">snapshotHandle</span>:<span style="color:#bbb"> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">sourceVolumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeSnapshotRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="provisioning-volumes-from-snapshots">Provisioning Volumes from Snapshots</h2><p>You can provision a new volume, pre-populated with data from a snapshot, by using
the <em>dataSource</em> field in the <code>PersistentVolumeClaim</code> object.</p><p>For more details, see
<a href="/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support">Volume Snapshot and Restore Volume from Snapshot</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Scheduling, Preemption and Eviction</h1><p>In Kubernetes, scheduling refers to making sure that <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>
are matched to <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Nodes">Nodes</a> so that the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> can run them. Preemption
is the process of terminating Pods with lower <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank" aria-label="Priority">Priority</a>
so that Pods with higher Priority can schedule on Nodes. Eviction is the process
of terminating one or more Pods on Nodes.</p><h2 id="scheduling">Scheduling</h2><ul><li><a href="/docs/concepts/scheduling-eviction/kube-scheduler/">Kubernetes Scheduler</a></li><li><a href="/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning Pods to Nodes</a></li><li><a href="/docs/concepts/scheduling-eviction/pod-overhead/">Pod Overhead</a></li><li><a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod Topology Spread Constraints</a></li><li><a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Tolerations</a></li><li><a href="/docs/concepts/scheduling-eviction/scheduling-framework/">Scheduling Framework</a></li><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation</a></li><li><a href="/docs/concepts/scheduling-eviction/scheduler-perf-tuning/">Scheduler Performance Tuning</a></li><li><a href="/docs/concepts/scheduling-eviction/resource-bin-packing/">Resource Bin Packing for Extended Resources</a></li><li><a href="/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">Pod Scheduling Readiness</a></li><li><a href="https://github.com/kubernetes-sigs/descheduler#descheduler-for-kubernetes">Descheduler</a></li></ul><h2 id="pod-disruption">Pod Disruption</h2><p><a href="/docs/concepts/workloads/pods/disruptions/">Pod disruption</a> is the process by which
Pods on Nodes are terminated either voluntarily or involuntarily.</p><p>Voluntary disruptions are started intentionally by application owners or cluster
administrators. Involuntary disruptions are unintentional and can be triggered by
unavoidable issues like Nodes running out of <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank" aria-label="resources">resources</a>,
or by accidental deletions.</p><ul><li><a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority and Preemption</a></li><li><a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a></li><li><a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated Eviction</a></li></ul><div class="section-index"/></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Service ClusterIP allocation</h1><p>In Kubernetes, <a href="/docs/concepts/services-networking/service/">Services</a> are an abstract way to expose
an application running on a set of Pods. Services
can have a cluster-scoped virtual IP address (using a Service of <code>type: ClusterIP</code>).
Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that
Service across the different backing Pods.</p><h2 id="how-service-clusterips-are-allocated">How Service ClusterIPs are allocated?</h2><p>When Kubernetes needs to assign a virtual IP address for a Service,
that assignment happens one of two ways:</p><dl><dt><em>dynamically</em></dt><dd>the cluster's control plane automatically picks a free IP address from within the configured IP range for <code>type: ClusterIP</code> Services.</dd><dt><em>statically</em></dt><dd>you specify an IP address of your choice, from within the configured IP range for Services.</dd></dl><p>Across your whole cluster, every Service <code>ClusterIP</code> must be unique.
Trying to create a Service with a specific <code>ClusterIP</code> that has already
been allocated will return an error.</p><h2 id="why-do-you-need-to-reserve-service-cluster-ips">Why do you need to reserve Service Cluster IPs?</h2><p>Sometimes you may want to have Services running in well-known IP addresses, so other components and
users in the cluster can use them.</p><p>The best example is the DNS Service for the cluster. As a soft convention, some Kubernetes installers assign the 10th IP address from
the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range
10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like
this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/name</span>:<span style="color:#bbb"> </span>CoreDNS<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kube-dns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.96.0.10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>UDP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dns-tcp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>ClusterIP<span style="color:#bbb">
</span></span></span></code></pre></div><p>But, as it was explained before, the IP address 10.96.0.10 has not been reserved.
If other Services are created before or in parallel with dynamic allocation, there is a chance they can allocate this IP.
Hence, you will not be able to create the DNS Service because it will fail with a conflict error.</p><h2 id="avoid-ClusterIP-conflict">How can you avoid Service ClusterIP conflicts?</h2><p>The allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces the
risk of collision.</p><p>The <code>ClusterIP</code> range is divided, based on the formula <code>min(max(16, cidrSize / 16), 256)</code>,
described as <em>never less than 16 or more than 256 with a graduated step between them</em>.</p><p>Dynamic IP assignment uses the upper band by default, once this has been exhausted it will
use the lower range. This will allow users to use static allocations on the lower band with a low
risk of collision.</p><h2 id="allocation-examples">Examples</h2><h3 id="allocation-example-1">Example 1</h3><p>This example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses
of Services.</p><p>Range Size: 2<sup>8</sup> - 2 = 254<br/>Band Offset: <code>min(max(16, 256/16), 256)</code> = <code>min(16, 256)</code> = 16<br/>Static band start: 10.96.0.1<br/>Static band end: 10.96.0.16<br/>Range end: 10.96.0.254</p><figure><div class="mermaid">pie showData
title 10.96.0.0/24
"Static" : 16
"Dynamic" : 238</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h3 id="allocation-example-2">Example 2</h3><p>This example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses
of Services.</p><p>Range Size: 2<sup>12</sup> - 2 = 4094<br/>Band Offset: <code>min(max(16, 4096/16), 256)</code> = <code>min(256, 256)</code> = 256<br/>Static band start: 10.96.0.1<br/>Static band end: 10.96.1.0<br/>Range end: 10.96.15.254</p><figure><div class="mermaid">pie showData
title 10.96.0.0/20
"Static" : 256
"Dynamic" : 3838</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h3 id="allocation-example-3">Example 3</h3><p>This example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses
of Services.</p><p>Range Size: 2<sup>16</sup> - 2 = 65534<br/>Band Offset: <code>min(max(16, 65536/16), 256)</code> = <code>min(4096, 256)</code> = 256<br/>Static band start: 10.96.0.1<br/>Static band ends: 10.96.1.0<br/>Range end: 10.96.255.254</p><figure><div class="mermaid">pie showData
title 10.96.0.0/16
"Static" : 256
"Dynamic" : 65278</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">Service External Traffic Policy</a></li><li>Read about <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a></li><li>Read about <a href="/docs/concepts/services-networking/service/">Services</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Scheduling Framework</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [stable]</code></div><p>The <em>scheduling framework</em> is a pluggable architecture for the Kubernetes scheduler.
It consists of a set of "plugin" APIs that are compiled directly into the scheduler.
These APIs allow most scheduling features to be implemented as plugins,
while keeping the scheduling "core" lightweight and maintainable. Refer to the
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md">design proposal of the scheduling framework</a> for more technical information on
the design of the framework.</p><h2 id="framework-workflow">Framework workflow</h2><p>The Scheduling Framework defines a few extension points. Scheduler plugins
register to be invoked at one or more extension points. Some of these plugins
can change the scheduling decisions and some are informational only.</p><p>Each attempt to schedule one Pod is split into two phases, the
<strong>scheduling cycle</strong> and the <strong>binding cycle</strong>.</p><h3 id="scheduling-cycle-binding-cycle">Scheduling cycle &amp; binding cycle</h3><p>The scheduling cycle selects a node for the Pod, and the binding cycle applies
that decision to the cluster. Together, a scheduling cycle and binding cycle are
referred to as a "scheduling context".</p><p>Scheduling cycles are run serially, while binding cycles may run concurrently.</p><p>A scheduling or binding cycle can be aborted if the Pod is determined to
be unschedulable or if there is an internal error. The Pod will be returned to
the queue and retried.</p><h2 id="interfaces">Interfaces</h2><p>The following picture shows the scheduling context of a Pod and the interfaces
that the scheduling framework exposes.</p><p>One plugin may implement multiple interfaces to perform more complex or
stateful tasks.</p><p>Some interfaces match the scheduler extension points which can be configured through
<a href="/docs/reference/scheduling/config/#extension-points">Scheduler Configuration</a>.</p><figure class="diagram-large"><img src="/images/docs/scheduling-framework-extensions.png"/><figcaption><h4>Scheduling framework extension points</h4></figcaption></figure><h3 id="pre-enqueue">PreEnqueue</h3><p>These plugins are called prior to adding Pods to the internal active queue, where Pods are marked as
ready for scheduling.</p><p>Only when all PreEnqueue plugins return <code>Success</code>, the Pod is allowed to enter the active queue.
Otherwise, it's placed in the internal unschedulable Pods list, and doesn't get an <code>Unschedulable</code> condition.</p><p>For more details about how internal scheduler queues work, read
<a href="https://github.com/kubernetes/community/blob/f03b6d5692bd979f07dd472e7b6836b2dad0fd9b/contributors/devel/sig-scheduling/scheduler_queues.md">Scheduling queue in kube-scheduler</a>.</p><h3 id="enqueueextension">EnqueueExtension</h3><p>EnqueueExtension is the interface where the plugin can control
whether to retry scheduling of Pods rejected by this plugin, based on changes in the cluster.
Plugins that implement PreEnqueue, PreFilter, Filter, Reserve or Permit should implement this interface.</p><h3 id="queueinghint">QueueingHint</h3><div class="feature-state-notice feature-stable" title="Feature Gate: SchedulerQueueingHints"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>QueueingHint is a callback function for deciding whether a Pod can be requeued to the active queue or backoff queue.
It's executed every time a certain kind of event or change happens in the cluster.
When the QueueingHint finds that the event might make the Pod schedulable,
the Pod is put into the active queue or the backoff queue
so that the scheduler will retry the scheduling of the Pod.</p><h3 id="queue-sort">QueueSort</h3><p>These plugins are used to sort Pods in the scheduling queue. A queue sort plugin
essentially provides a <code>Less(Pod1, Pod2)</code> function. Only one queue sort
plugin may be enabled at a time.</p><h3 id="pre-filter">PreFilter</h3><p>These plugins are used to pre-process info about the Pod, or to check certain
conditions that the cluster or the Pod must meet. If a PreFilter plugin returns
an error, the scheduling cycle is aborted.</p><h3 id="filter">Filter</h3><p>These plugins are used to filter out nodes that cannot run the Pod. For each
node, the scheduler will call filter plugins in their configured order. If any
filter plugin marks the node as infeasible, the remaining plugins will not be
called for that node. Nodes may be evaluated concurrently.</p><h3 id="post-filter">PostFilter</h3><p>These plugins are called after the Filter phase, but only when no feasible nodes
were found for the pod. Plugins are called in their configured order. If
any postFilter plugin marks the node as <code>Schedulable</code>, the remaining plugins
will not be called. A typical PostFilter implementation is preemption, which
tries to make the pod schedulable by preempting other Pods.</p><h3 id="pre-score">PreScore</h3><p>These plugins are used to perform "pre-scoring" work, which generates a sharable
state for Score plugins to use. If a PreScore plugin returns an error, the
scheduling cycle is aborted.</p><h3 id="scoring">Score</h3><p>These plugins are used to rank nodes that have passed the filtering phase. The
scheduler will call each scoring plugin for each node. There will be a well
defined range of integers representing the minimum and maximum scores. After the
<a href="#normalize-scoring">NormalizeScore</a> phase, the scheduler will combine node
scores from all plugins according to the configured plugin weights.</p><h4 id="scoring-capacity">Capacity scoring</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: StorageCapacityScoring"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>The feature gate <code>VolumeCapacityPriority</code> was used in v1.32 to support storage that are
statically provisioned. Starting from v1.33, the new feature gate <code>StorageCapacityScoring</code>
replaces the old <code>VolumeCapacityPriority</code> gate with added support to dynamically provisioned storage.
When <code>StorageCapacityScoring</code> is enabled, the VolumeBinding plugin in the kube-scheduler is extended
to score Nodes based on the storage capacity on each of them.
This feature is applicable to CSI volumes that supported <a href="/docs/concepts/storage/storage-capacity/">Storage Capacity</a>,
including local storage backed by a CSI driver.</p><h3 id="normalize-scoring">NormalizeScore</h3><p>These plugins are used to modify scores before the scheduler computes a final
ranking of Nodes. A plugin that registers for this extension point will be
called with the <a href="#scoring">Score</a> results from the same plugin. This is called
once per plugin per scheduling cycle.</p><p>For example, suppose a plugin <code>BlinkingLightScorer</code> ranks Nodes based on how
many blinking lights they have.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">func</span> <span style="color:#00a000">ScoreNode</span>(_ <span style="color:#666">*</span>v1.pod, n <span style="color:#666">*</span>v1.Node) (<span style="color:#0b0;font-weight:700">int</span>, <span style="color:#0b0;font-weight:700">error</span>) {
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">return</span> <span style="color:#00a000">getBlinkingLightCount</span>(n)
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>However, the maximum count of blinking lights may be small compared to
<code>NodeScoreMax</code>. To fix this, <code>BlinkingLightScorer</code> should also register for this
extension point.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">func</span> <span style="color:#00a000">NormalizeScores</span>(scores <span style="color:#a2f;font-weight:700">map</span>[<span style="color:#0b0;font-weight:700">string</span>]<span style="color:#0b0;font-weight:700">int</span>) {
</span></span><span style="display:flex"><span>    highest <span style="color:#666">:=</span> <span style="color:#666">0</span>
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">for</span> _, score <span style="color:#666">:=</span> <span style="color:#a2f;font-weight:700">range</span> scores {
</span></span><span style="display:flex"><span>        highest = <span style="color:#a2f">max</span>(highest, score)
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>    <span style="color:#a2f;font-weight:700">for</span> node, score <span style="color:#666">:=</span> <span style="color:#a2f;font-weight:700">range</span> scores {
</span></span><span style="display:flex"><span>        scores[node] = score<span style="color:#666">*</span>NodeScoreMax<span style="color:#666">/</span>highest
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>If any NormalizeScore plugin returns an error, the scheduling cycle is
aborted.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Plugins wishing to perform "pre-reserve" work should use the
NormalizeScore extension point.</div><h3 id="reserve">Reserve</h3><p>A plugin that implements the Reserve interface has two methods, namely <code>Reserve</code>
and <code>Unreserve</code>, that back two informational scheduling phases called Reserve
and Unreserve, respectively. Plugins which maintain runtime state (aka "stateful
plugins") should use these phases to be notified by the scheduler when resources
on a node are being reserved and unreserved for a given Pod.</p><p>The Reserve phase happens before the scheduler actually binds a Pod to its
designated node. It exists to prevent race conditions while the scheduler waits
for the bind to succeed. The <code>Reserve</code> method of each Reserve plugin may succeed
or fail; if one <code>Reserve</code> method call fails, subsequent plugins are not executed
and the Reserve phase is considered to have failed. If the <code>Reserve</code> method of
all plugins succeed, the Reserve phase is considered to be successful and the
rest of the scheduling cycle and the binding cycle are executed.</p><p>The Unreserve phase is triggered if the Reserve phase or a later phase fails.
When this happens, the <code>Unreserve</code> method of <strong>all</strong> Reserve plugins will be
executed in the reverse order of <code>Reserve</code> method calls. This phase exists to
clean up the state associated with the reserved Pod.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>The implementation of the <code>Unreserve</code> method in Reserve plugins must be
idempotent and may not fail.</div><h3 id="permit">Permit</h3><p><em>Permit</em> plugins are invoked at the end of the scheduling cycle for each Pod, to
prevent or delay the binding to the candidate node. A permit plugin can do one of
the three things:</p><ol><li><p><strong>approve</strong><br/>Once all Permit plugins approve a Pod, it is sent for binding.</p></li><li><p><strong>deny</strong><br/>If any Permit plugin denies a Pod, it is returned to the scheduling queue.
This will trigger the Unreserve phase in <a href="#reserve">Reserve plugins</a>.</p></li><li><p><strong>wait</strong> (with a timeout)<br/>If a Permit plugin returns "wait", then the Pod is kept in an internal "waiting"
Pods list, and the binding cycle of this Pod starts but directly blocks until it
gets approved. If a timeout occurs, <strong>wait</strong> becomes <strong>deny</strong>
and the Pod is returned to the scheduling queue, triggering the
Unreserve phase in <a href="#reserve">Reserve plugins</a>.</p></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>While any plugin can access the list of "waiting" Pods and approve them
(see <a href="https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle"><code>FrameworkHandle</code></a>),
we expect only the permit plugins to approve binding of reserved Pods that are in "waiting" state.
Once a Pod is approved, it is sent to the <a href="#pre-bind">PreBind</a> phase.</div><h3 id="pre-bind">PreBind</h3><p>These plugins are used to perform any work required before a Pod is bound. For
example, a pre-bind plugin may provision a network volume and mount it on the
target node before allowing the Pod to run there.</p><p>If any PreBind plugin returns an error, the Pod is <a href="#reserve">rejected</a> and
returned to the scheduling queue.</p><h3 id="bind">Bind</h3><p>These plugins are used to bind a Pod to a Node. Bind plugins will not be called
until all PreBind plugins have completed. Each bind plugin is called in the
configured order. A bind plugin may choose whether or not to handle the given
Pod. If a bind plugin chooses to handle a Pod, <strong>the remaining bind plugins are
skipped</strong>.</p><h3 id="post-bind">PostBind</h3><p>This is an informational interface. Post-bind plugins are called after a
Pod is successfully bound. This is the end of a binding cycle, and can be used
to clean up associated resources.</p><h2 id="plugin-api">Plugin API</h2><p>There are two steps to the plugin API. First, plugins must register and get
configured, then they use the extension point interfaces. Extension point
interfaces have the following form.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">type</span> Plugin <span style="color:#a2f;font-weight:700">interface</span> {
</span></span><span style="display:flex"><span>    <span style="color:#00a000">Name</span>() <span style="color:#0b0;font-weight:700">string</span>
</span></span><span style="display:flex"><span>}
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">type</span> QueueSortPlugin <span style="color:#a2f;font-weight:700">interface</span> {
</span></span><span style="display:flex"><span>    Plugin
</span></span><span style="display:flex"><span>    <span style="color:#00a000">Less</span>(<span style="color:#666">*</span>v1.pod, <span style="color:#666">*</span>v1.pod) <span style="color:#0b0;font-weight:700">bool</span>
</span></span><span style="display:flex"><span>}
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#a2f;font-weight:700">type</span> PreFilterPlugin <span style="color:#a2f;font-weight:700">interface</span> {
</span></span><span style="display:flex"><span>    Plugin
</span></span><span style="display:flex"><span>    <span style="color:#00a000">PreFilter</span>(context.Context, <span style="color:#666">*</span>framework.CycleState, <span style="color:#666">*</span>v1.pod) <span style="color:#0b0;font-weight:700">error</span>
</span></span><span style="display:flex"><span>}
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic">// ...
</span></span></span></code></pre></div><h2 id="plugin-configuration">Plugin configuration</h2><p>You can enable or disable plugins in the scheduler configuration. If you are using
Kubernetes v1.18 or later, most scheduling
<a href="/docs/reference/scheduling/config/#scheduling-plugins">plugins</a> are in use and
enabled by default.</p><p>In addition to default plugins, you can also implement your own scheduling
plugins and get them configured along with default plugins. You can visit
<a href="https://github.com/kubernetes-sigs/scheduler-plugins">scheduler-plugins</a> for more details.</p><p>If you are using Kubernetes v1.18 or later, you can configure a set of plugins as
a scheduler profile and then define multiple profiles to fit various kinds of workload.
Learn more at <a href="/docs/reference/scheduling/config/#multiple-profiles">multiple profiles</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Linux kernel security constraints for Pods and containers</h1><div class="lead">Overview of Linux kernel security modules and constraints that you can use to harden your Pods and containers.</div><p>This page describes some of the security features that are built into the Linux
kernel that you can use in your Kubernetes workloads. To learn how to apply
these features to your Pods and containers, refer to
<a href="/docs/tasks/configure-pod-container/security-context/">Configure a SecurityContext for a Pod or Container</a>.
You should already be familiar with Linux and with the basics of Kubernetes
workloads.</p><h2 id="run-without-root">Run workloads without root privileges</h2><p>When you deploy a workload in Kubernetes, use the Pod specification to restrict
that workload from running as the root user on the node. You can use the Pod
<code>securityContext</code> to define the specific Linux user and group for the processes in
the Pod, and explicitly restrict containers from running as root users. Setting
these values in the Pod manifest takes precedence over similar values in the
container image, which is especially useful if you're running images that you
don't own.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Ensure that the user or group that you assign to the workload has the permissions
required for the application to function correctly. Changing the user or group
to one that doesn't have the correct permissions could lead to file access
issues or failed operations.</div><p>Configuring the kernel security features on this page provides fine-grained
control over the actions that processes in your cluster can take, but managing
these configurations can be challenging at scale. Running containers as
non-root, or in user namespaces if you need root privileges, helps to reduce the
chance that you'll need to enforce your configured kernel security capabilities.</p><h2 id="linux-security-features">Security features in the Linux kernel</h2><p>Kubernetes lets you configure and use Linux kernel features to improve isolation
and harden your containerized workloads. Common features include the following:</p><ul><li><strong>Secure computing mode (seccomp)</strong>: Filter which system calls a process can
make</li><li><strong>AppArmor</strong>: Restrict the access privileges of individual programs</li><li><strong>Security Enhanced Linux (SELinux)</strong>: Assign security labels to objects for
more manageable security policy enforcement</li></ul><p>To configure settings for one of these features, the operating system that you
choose for your nodes must enable the feature in the kernel. For example,
Ubuntu 7.10 and later enable AppArmor by default. To learn whether your OS
enables a specific feature, consult the OS documentation.</p><p>You use the <code>securityContext</code> field in your Pod specification to define the
constraints that apply to those processes. The <code>securityContext</code> field also
supports other security settings, such as specific Linux capabilities or file
access permissions using UIDs and GIDs. To learn more, refer to
<a href="/docs/tasks/configure-pod-container/security-context/">Configure a SecurityContext for a Pod or Container</a>.</p><h3 id="seccomp">seccomp</h3><p>Some of your workloads might need privileges to perform specific actions as the
root user on your node's host machine. Linux uses <em>capabilities</em> to divide the
available privileges into categories, so that processes can get the privileges
required to perform specific actions without being granted all privileges. Each
capability has a set of system calls (syscalls) that a process can make. seccomp
lets you restrict these individual syscalls. It can be used to sandbox the privileges of a process, restricting the calls it
is able to make from userspace into the kernel.</p><p>In Kubernetes, you use a <em>container runtime</em> on each node to run your
containers. Example runtimes include CRI-O, Docker, or containerd. Each runtime
allows only a subset of Linux capabilities by default. You can further limit the
allowed syscalls individually by using a seccomp profile. Container runtimes
usually include a default seccomp profile. Kubernetes lets you automatically
apply seccomp profiles loaded onto a node to your Pods and containers.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes also has the <code>allowPrivilegeEscalation</code> setting for Pods and
containers. When set to <code>false</code>, this prevents processes from gaining new
capabilities and restricts unprivileged users from changing the applied seccomp
profile to a more permissive profile.</div><p>To learn how to implement seccomp in Kubernetes, refer to
<a href="/docs/tutorials/security/seccomp/">Restrict a Container's Syscalls with seccomp</a>
or the <a href="/docs/reference/node/seccomp/">Seccomp node reference</a></p><p>To learn more about seccomp, see
<a href="https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html">Seccomp BPF</a>
in the Linux kernel documentation.</p><h4 id="seccomp-considerations">Considerations for seccomp</h4><p>seccomp is a low-level security configuration that you should only configure
yourself if you require fine-grained control over Linux syscalls. Using
seccomp, especially at scale, has the following risks:</p><ul><li>Configurations might break during application updates</li><li>Attackers can still use allowed syscalls to exploit vulnerabilities</li><li>Profile management for individual applications becomes challenging at scale</li></ul><p><strong>Recommendation</strong>: Use the default seccomp profile that's bundled with your
container runtime. If you need a more isolated environment, consider using a
sandbox, such as gVisor. Sandboxes solve the preceding risks with custom
seccomp profiles, but require more compute resources on your nodes and might
have compatibility issues with GPUs and other specialized hardware.</p><h3 id="policy-based-mac">AppArmor and SELinux: policy-based mandatory access control</h3><p>You can use Linux policy-based mandatory access control (MAC) mechanisms, such
as AppArmor and SELinux, to harden your Kubernetes workloads.</p><h4 id="apparmor">AppArmor</h4><p><a href="https://apparmor.net/">AppArmor</a> is a Linux kernel security module that
supplements the standard Linux user and group based permissions to confine
programs to a limited set of resources. AppArmor can be configured for any
application to reduce its potential attack surface and provide greater in-depth
defense. It is configured through profiles tuned to allow the access needed by a
specific program or container, such as Linux capabilities, network access, and
file permissions. Each profile can be run in either enforcing mode, which blocks
access to disallowed resources, or complain mode, which only reports violations.</p><p>AppArmor can help you to run a more secure deployment by restricting what
containers are allowed to do, and/or provide better auditing through system
logs. The container runtime that you use might ship with a default AppArmor
profile, or you can use a custom profile.</p><p>To learn how to use AppArmor in Kubernetes, refer to
<a href="/docs/tutorials/security/apparmor/">Restrict a Container's Access to Resources with AppArmor</a>.</p><h4 id="selinux">SELinux</h4><p>SELinux is a Linux kernel security module that lets you restrict the access
that a specific <em>subject</em>, such as a process, has to the files on your system.
You define security policies that apply to subjects that have specific SELinux
labels. When a process that has an SELinux label attempts to access a file, the
SELinux server checks whether that process' security policy allows the access
and makes an authorization decision.</p><p>In Kubernetes, you can set an SELinux label in the <code>securityContext</code> field of
your manifest. The specified labels are assigned to those processes. If you
have configured security policies that affect those labels, the host OS kernel
enforces these policies.</p><p>To learn how to use SELinux in Kubernetes, refer to
<a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">Assign SELinux labels to a container</a>.</p><h4 id="apparmor-selinux-diff">Differences between AppArmor and SELinux</h4><p>The operating system on your Linux nodes usually includes one of either
AppArmor or SELinux. Both mechanisms provide similar types of protection, but
have differences such as the following:</p><ul><li><strong>Configuration</strong>: AppArmor uses profiles to define access to resources.
SELinux uses policies that apply to specific labels.</li><li><strong>Policy application</strong>: In AppArmor, you define resources using file paths.
SELinux uses the index node (inode) of a resource to identify the resource.</li></ul><h3 id="summary">Summary of features</h3><p>The following table describes the use cases and scope of each security control.
You can use all of these controls together to build a more hardened system.</p><table><caption>Summary of Linux kernel security features</caption><thead><tr><th>Security feature</th><th>Description</th><th>How to use</th><th>Example</th></tr></thead><tbody><tr><td>seccomp</td><td>Restrict individual kernel calls in the userspace. Reduces the
likelihood that a vulnerability that uses a restricted syscall would
compromise the system.</td><td>Specify a loaded seccomp profile in the Pod or container specification
to apply its constraints to the processes in the Pod.</td><td>Reject the <code>unshare</code> syscall, which was used in
<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-0185">CVE-2022-0185</a>.</td></tr><tr><td>AppArmor</td><td>Restrict program access to specific resources. Reduces the attack
surface of the program. Improves audit logging.</td><td>Specify a loaded AppArmor profile in the container specification.</td><td>Restrict a read-only program from writing to any file path
in the system.</td></tr><tr><td>SELinux</td><td>Restrict access to resources such as files, applications, ports, and
processes using labels and security policies.</td><td>Specify access restrictions for specific labels. Tag processes with
those labels to enforce the access restrictions related to the label.</td><td>Restrict a container from accessing files outside its own filesystem.</td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Mechanisms like AppArmor and SELinux can provide protection that extends beyond
the container. For example, you can use SELinux to help mitigate
<a href="https://access.redhat.com/security/cve/cve-2019-5736">CVE-2019-5736</a>.</div><h3 id="considerations-custom-configurations">Considerations for managing custom configurations</h3><p>seccomp, AppArmor, and SELinux usually have a default configuration that offers
basic protections. You can also create custom profiles and policies that meet
the requirements of your workloads. Managing and distributing these custom
configurations at scale might be challenging, especially if you use all three
features together. To help you to manage these configurations at scale, use a
tool like the
<a href="https://github.com/kubernetes-sigs/security-profiles-operator">Kubernetes Security Profiles Operator</a>.</p><h2 id="kernel-security-features-privileged-containers">Kernel-level security features and privileged containers</h2><p>Kubernetes lets you specify that some trusted containers can run in
<em>privileged</em> mode. Any container in a Pod can run in privileged mode to use
operating system administrative capabilities that would otherwise be
inaccessible. This is available for both Windows and Linux.</p><p>Privileged containers explicitly override some of the Linux kernel constraints
that you might use in your workloads, as follows:</p><ul><li><strong>seccomp</strong>: Privileged containers run as the <code>Unconfined</code> seccomp profile,
overriding any seccomp profile that you specified in your manifest.</li><li><strong>AppArmor</strong>: Privileged containers ignore any applied AppArmor profiles.</li><li><strong>SELinux</strong>: Privileged containers run as the <code>unconfined_t</code> domain.</li></ul><h3 id="privileged-containers">Privileged containers</h3><p>Any container in a Pod can enable <em>Privileged mode</em> if you set the
<code>privileged: true</code> field in the
<a href="/docs/tasks/configure-pod-container/security-context/"><code>securityContext</code></a>
field for the container. Privileged containers override or undo many other hardening settings such as the applied seccomp profile, AppArmor profile, or
SELinux constraints. Privileged containers are given all Linux capabilities,
including capabilities that they don't require. For example, a root user in a
privileged container might be able to use the <code>CAP_SYS_ADMIN</code> and
<code>CAP_NET_ADMIN</code> capabilities on the node, bypassing the runtime seccomp
configuration and other restrictions.</p><p>In most cases, you should avoid using privileged containers, and instead grant
the specific capabilities required by your container using the <code>capabilities</code>
field in the <code>securityContext</code> field. Only use privileged mode if you have a
capability that you can't grant with the securityContext. This is useful for
containers that want to use operating system administrative capabilities such
as manipulating the network stack or accessing hardware devices.</p><p>In Kubernetes version 1.26 and later, you can also run Windows containers in a
similarly privileged mode by setting the <code>windowsOptions.hostProcess</code> flag on
the security context of the Pod spec. For details and instructions, see
<a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod</a>.</p><h2 id="recommendations-best-practices">Recommendations and best practices</h2><ul><li>Before configuring kernel-level security capabilities, you should consider
implementing network-level isolation. For more information, read the
<a href="/docs/concepts/security/security-checklist/#network-security">Security Checklist</a>.</li><li>Unless necessary, run Linux workloads as non-root by setting specific user and
group IDs in your Pod manifest and by specifying <code>runAsNonRoot: true</code>.</li></ul><p>Additionally, you can run workloads in user namespaces by setting
<code>hostUsers: false</code> in your Pod manifest. This lets you run containers as root
users in the user namespace, but as non-root users in the host namespace on the
node. This is still in early stages of development and might not have the level
of support that you need. For instructions, refer to
<a href="/docs/tasks/configure-pod-container/user-namespaces/">Use a User Namespace With a Pod</a>.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tutorials/security/apparmor/">Learn how to use AppArmor</a></li><li><a href="/docs/tutorials/security/seccomp/">Learn how to use seccomp</a></li><li><a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">Learn how to use SELinux</a></li><li><a href="/docs/reference/node/seccomp/">Seccomp Node Reference</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">IPv4/IPv6 dual-stack</h1><div class="lead">Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> and <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Services">Services</a>.</p><p>IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in
1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.</p><h2 id="supported-features">Supported Features</h2><p>IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:</p><ul><li>Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)</li><li>IPv4 and IPv6 enabled Services</li><li>Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces</li></ul><h2 id="prerequisites">Prerequisites</h2><p>The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:</p><ul><li><p>Kubernetes 1.20 or later</p><p>For information about using dual-stack services with earlier
Kubernetes versions, refer to the documentation for that version
of Kubernetes.</p></li><li><p>Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide
Kubernetes nodes with routable IPv4/IPv6 network interfaces)</p></li><li><p>A <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a> that
supports dual-stack networking.</p></li></ul><h2 id="configure-ipv4-ipv6-dual-stack">Configure IPv4/IPv6 dual-stack</h2><p>To configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:</p><ul><li>kube-apiserver:<ul><li><code>--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li></ul></li><li>kube-controller-manager:<ul><li><code>--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li><li><code>--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li><li><code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> defaults to /24 for IPv4 and /64 for IPv6</li></ul></li><li>kube-proxy:<ul><li><code>--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li></ul></li><li>kubelet:<ul><li><code>--node-ip=&lt;IPv4 IP&gt;,&lt;IPv6 IP&gt;</code><ul><li>This option is required for bare metal dual-stack nodes (nodes that do not define a
cloud provider with the <code>--cloud-provider</code> flag). If you are using a cloud provider
and choose to override the node IPs chosen by the cloud provider, set the
<code>--node-ip</code> option.</li><li>(The legacy built-in cloud providers do not support dual-stack <code>--node-ip</code>.)</li></ul></li></ul></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>An example of an IPv4 CIDR: <code>10.244.0.0/16</code> (though you would supply your own address range)</p><p>An example of an IPv6 CIDR: <code>fdXY:IJKL:MNOP:15::/64</code> (this shows the format but is not a valid
address - see <a href="https://tools.ietf.org/html/rfc4193">RFC 4193</a>)</p></div><h2 id="services">Services</h2><p>You can create <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Services">Services</a> which can use IPv4, IPv6, or both.</p><p>The address family of a Service defaults to the address family of the first service cluster IP
range (configured via the <code>--service-cluster-ip-range</code> flag to the kube-apiserver).</p><p>When you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you
set the <code>.spec.ipFamilyPolicy</code> field to one of the following values:</p><ul><li><code>SingleStack</code>: Single-stack service. The control plane allocates a cluster IP for the Service,
using the first configured service cluster IP range.</li><li><code>PreferDualStack</code>: Allocates both IPv4 and IPv6 cluster IPs for the Service when dual-stack is enabled. If dual-stack is not enabled or supported, it falls back to single-stack behavior.</li><li><code>RequireDualStack</code>: Allocates Service <code>.spec.clusterIPs</code> from both IPv4 and IPv6 address ranges when dual-stack is enabled. If dual-stack is not enabled or supported, the Service API object creation fails.<ul><li>Selects the <code>.spec.clusterIP</code> from the list of <code>.spec.clusterIPs</code> based on the address family
of the first element in the <code>.spec.ipFamilies</code> array.</li></ul></li></ul><p>If you would like to define which IP family to use for single stack or define the order of IP
families for dual-stack, you can choose the address families by setting an optional field,
<code>.spec.ipFamilies</code>, on the Service.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>.spec.ipFamilies</code> field is conditionally mutable: you can add or remove a secondary
IP address family, but you cannot change the primary IP address family of an existing Service.</div><p>You can set <code>.spec.ipFamilies</code> to any of the following array values:</p><ul><li><code>["IPv4"]</code></li><li><code>["IPv6"]</code></li><li><code>["IPv4","IPv6"]</code> (dual stack)</li><li><code>["IPv6","IPv4"]</code> (dual stack)</li></ul><p>The first family you list is used for the legacy <code>.spec.clusterIP</code> field.</p><h3 id="dual-stack-service-configuration-scenarios">Dual-stack Service configuration scenarios</h3><p>These examples demonstrate the behavior of various dual-stack Service configuration scenarios.</p><h4 id="dual-stack-options-on-new-services">Dual-stack options on new Services</h4><ol><li><p>This Service specification does not explicitly define <code>.spec.ipFamilyPolicy</code>. When you create
this Service, Kubernetes assigns a cluster IP for the Service from the first configured
<code>service-cluster-ip-range</code> and sets the <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>. (<a href="/docs/concepts/services-networking/service/#services-without-selectors">Services
without selectors</a> and
<a href="/docs/concepts/services-networking/service/#headless-services">headless Services</a> with selectors
will behave in this same way.)</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-dual-stack-default-svc-yaml&quot;)" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"/></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div></li><li><p>This Service specification explicitly defines <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>. When
you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6
addresses for the service. The control plane updates the <code>.spec</code> for the Service to record the IP
address assignments. The field <code>.spec.clusterIPs</code> is the primary field, and contains both assigned
IP addresses; <code>.spec.clusterIP</code> is a secondary field with its value calculated from
<code>.spec.clusterIPs</code>.</p><ul><li>For the <code>.spec.clusterIP</code> field, the control plane records the IP address that is from the
same address family as the first service cluster IP range.</li><li>On a single-stack cluster, the <code>.spec.clusterIPs</code> and <code>.spec.clusterIP</code> fields both only list
one address.</li><li>On a cluster with dual-stack enabled, specifying <code>RequireDualStack</code> in <code>.spec.ipFamilyPolicy</code>
behaves the same as <code>PreferDualStack</code>.</li></ul><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-svc.yaml" download="service/networking/dual-stack-preferred-svc.yaml"><code>service/networking/dual-stack-preferred-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-dual-stack-preferred-svc-yaml&quot;)" title="Copy service/networking/dual-stack-preferred-svc.yaml to clipboard"/></div><div class="includecode" id="service-networking-dual-stack-preferred-svc-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div></li><li><p>This Service specification explicitly defines <code>IPv6</code> and <code>IPv4</code> in <code>.spec.ipFamilies</code> as well
as defining <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>. When Kubernetes assigns an IPv6 and
IPv4 address in <code>.spec.clusterIPs</code>, <code>.spec.clusterIP</code> is set to the IPv6 address because that is
the first element in the <code>.spec.clusterIPs</code> array, overriding the default.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-ipfamilies-svc.yaml" download="service/networking/dual-stack-preferred-ipfamilies-svc.yaml"><code>service/networking/dual-stack-preferred-ipfamilies-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-dual-stack-preferred-ipfamilies-svc-yaml&quot;)" title="Copy service/networking/dual-stack-preferred-ipfamilies-svc.yaml to clipboard"/></div><div class="includecode" id="service-networking-dual-stack-preferred-ipfamilies-svc-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- IPv6<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div></li></ol><h4 id="dual-stack-defaults-on-existing-services">Dual-stack defaults on existing Services</h4><p>These examples demonstrate the default behavior when dual-stack is newly enabled on a cluster
where Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable
dual-stack.)</p><ol><li><p>When dual-stack is enabled on a cluster, existing Services (whether <code>IPv4</code> or <code>IPv6</code>) are
configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code> and set
<code>.spec.ipFamilies</code> to the address family of the existing Service. The existing Service cluster IP
will be stored in <code>.spec.clusterIPs</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-dual-stack-default-svc-yaml&quot;)" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"/></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>You can validate this behavior by using kubectl to inspect an existing service.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.197.123</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIPs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:#666">10.0.197.123</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>ClusterIP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">loadBalancer</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span></code></pre></div></li><li><p>When dual-stack is enabled on a cluster, existing
<a href="/docs/concepts/services-networking/service/#headless-services">headless Services</a> with selectors are
configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code> and set
<code>.spec.ipFamilies</code> to the address family of the first service cluster IP range (configured via the
<code>--service-cluster-ip-range</code> flag to the kube-apiserver) even though <code>.spec.clusterIP</code> is set to
<code>None</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-dual-stack-default-svc-yaml&quot;)" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"/></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>You can validate this behavior by using kubectl to inspect an existing headless service with selectors.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIPs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- None<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span></code></pre></div></li></ol><h4 id="switching-services-between-single-stack-and-dual-stack">Switching Services between single-stack and dual-stack</h4><p>Services can be changed from single-stack to dual-stack and from dual-stack to single-stack.</p><ol><li><p>To change a Service from single-stack to dual-stack, change <code>.spec.ipFamilyPolicy</code> from
<code>SingleStack</code> to <code>PreferDualStack</code> or <code>RequireDualStack</code> as desired. When you change this
Service from single-stack to dual-stack, Kubernetes assigns the missing address family so that the
Service now has IPv4 and IPv6 addresses.</p><p>Edit the Service specification updating the <code>.spec.ipFamilyPolicy</code> from <code>SingleStack</code> to <code>PreferDualStack</code>.</p><p>Before:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span></span></span></code></pre></div><p>After:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span></span></span></code></pre></div></li><li><p>To change a Service from dual-stack to single-stack, change <code>.spec.ipFamilyPolicy</code> from
<code>PreferDualStack</code> or <code>RequireDualStack</code> to <code>SingleStack</code>. When you change this Service from
dual-stack to single-stack, Kubernetes retains only the first element in the <code>.spec.clusterIPs</code>
array, and sets <code>.spec.clusterIP</code> to that IP address and sets <code>.spec.ipFamilies</code> to the address
family of <code>.spec.clusterIPs</code>.</p></li></ol><h3 id="headless-services-without-selector">Headless Services without selector</h3><p>For <a href="/docs/concepts/services-networking/service/#without-selectors">Headless Services without selectors</a>
and without <code>.spec.ipFamilyPolicy</code> explicitly set, the <code>.spec.ipFamilyPolicy</code> field defaults to
<code>RequireDualStack</code>.</p><h3 id="service-type-loadbalancer">Service type LoadBalancer</h3><p>To provision a dual-stack load balancer for your Service:</p><ul><li>Set the <code>.spec.type</code> field to <code>LoadBalancer</code></li><li>Set <code>.spec.ipFamilyPolicy</code> field to <code>PreferDualStack</code> or <code>RequireDualStack</code></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>To use a dual-stack <code>LoadBalancer</code> type Service, your cloud provider must support IPv4 and IPv6
load balancers.</div><h2 id="egress-traffic">Egress traffic</h2><p>If you want to enable egress traffic in order to reach off-cluster destinations (eg. the public
Internet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod to
use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP
masquerading. The <a href="https://github.com/kubernetes-sigs/ip-masq-agent">ip-masq-agent</a> project
supports IP masquerading on dual-stack clusters.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Ensure your <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" aria-label="CNI">CNI</a> provider supports IPv6.</div><h2 id="windows-support">Windows support</h2><p>Kubernetes on Windows does not support single-stack "IPv6-only" networking. However,
dual-stack IPv4/IPv6 networking for pods and nodes with single-family services
is supported.</p><p>You can use IPv4/IPv6 dual-stack networking with <code>l2bridge</code> networks.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Overlay (VXLAN) networks on Windows <strong>do not</strong> support dual-stack networking.</div><p>You can read more about the different network modes for Windows within the
<a href="/docs/concepts/services-networking/windows-networking/#network-modes">Networking on Windows</a> topic.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/network/validate-dual-stack/">Validate IPv4/IPv6 dual-stack</a> networking</li><li><a href="/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">Enable dual-stack networking using kubeadm</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Networking on Windows</h1><p>Kubernetes supports running nodes on either Linux or Windows. You can mix both kinds of node
within a single cluster.
This page provides an overview to networking specific to the Windows operating system.</p><h2 id="networking">Container networking on Windows</h2><p>Networking for Windows containers is exposed through
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">CNI plugins</a>.
Windows containers function similarly to virtual machines in regards to
networking. Each container has a virtual network adapter (vNIC) which is connected
to a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and the
Host Compute Service (HCS) work together to create containers and attach container
vNICs to networks. HCS is responsible for the management of containers whereas HNS
is responsible for the management of networking resources such as:</p><ul><li>Virtual networks (including creation of vSwitches)</li><li>Endpoints / vNICs</li><li>Namespaces</li><li>Policies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.</li></ul><p>The Windows HNS and vSwitch implement namespacing and can
create virtual NICs as needed for a pod or container. However, many configurations such
as DNS, routes, and metrics are stored in the Windows registry database rather than as
files inside <code>/etc</code>, which is how Linux stores those configurations. The Windows registry for the container
is separate from that of the host, so concepts like mapping <code>/etc/resolv.conf</code> from
the host into a container don't have the same effect they would on Linux. These must
be configured using Windows APIs run in the context of that container. Therefore
CNI implementations need to call the HNS instead of relying on file mappings to pass
network details into the pod or container.</p><h2 id="network-modes">Network modes</h2><p>Windows supports five different networking drivers/modes: L2bridge, L2tunnel,
Overlay (Beta), Transparent, and NAT. In a heterogeneous cluster with Windows and Linux
worker nodes, you need to select a networking solution that is compatible on both
Windows and Linux. The following table lists the out-of-tree plugins are supported on Windows,
with recommendations on when to use each CNI:</p><table><thead><tr><th>Network Driver</th><th>Description</th><th>Container Packet Modifications</th><th>Network Plugins</th><th>Network Plugin Characteristics</th></tr></thead><tbody><tr><td>L2bridge</td><td>Containers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress.</td><td>MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy.</td><td><a href="https://www.cni.dev/plugins/current/main/win-bridge/">win-bridge</a>, <a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure-CNI</a>, <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw">Flannel host-gateway</a> uses win-bridge</td><td>win-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity.</td></tr><tr><td>L2Tunnel</td><td>This is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied.</td><td>MAC rewritten, IP visible on the underlay network</td><td><a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure-CNI</a></td><td>Azure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that <a href="https://azure.microsoft.com/en-us/services/virtual-network/">Azure Virtual Network provides</a>. For example, securely connect to Azure services or use Azure NSGs. See <a href="https://docs.microsoft.com/azure/aks/concepts-network#azure-cni-advanced-networking">azure-cni for some examples</a></td></tr><tr><td>Overlay</td><td>Containers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation.</td><td>Encapsulated with an outer header.</td><td><a href="https://www.cni.dev/plugins/current/main/win-overlay/">win-overlay</a>, <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#vxlan">Flannel VXLAN</a> (uses win-overlay)</td><td>win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags) if you are restricted on IPs in your datacenter. This option requires <a href="https://support.microsoft.com/help/4489899">KB4489899</a> on Windows Server 2019.</td></tr><tr><td>Transparent (special use case for <a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a>)</td><td>Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers).</td><td>Packet is encapsulated either via <a href="https://datatracker.ietf.org/doc/draft-gross-geneve/">GENEVE</a> or <a href="https://datatracker.ietf.org/doc/draft-davie-stt/">STT</a> tunneling to reach pods which are not on the same host.<br/>Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller.<br/>NAT is done for north-south communication.</td><td><a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a></td><td><a href="https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib">Deploy via ansible</a>. Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh.</td></tr><tr><td>NAT (<em>not used in Kubernetes</em>)</td><td>Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called <a href="https://techcommunity.microsoft.com/t5/virtualization/windows-nat-winnat-capabilities-and-limitations/ba-p/382303">WinNAT</a></td><td>MAC and IP is rewritten to host MAC/IP.</td><td><a href="https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat">nat</a></td><td>Included here for completeness</td></tr></tbody></table><p>As outlined above, the <a href="https://github.com/coreos/flannel">Flannel</a>
<a href="https://github.com/flannel-io/cni-plugin">CNI plugin</a>
is also <a href="https://github.com/flannel-io/cni-plugin#windows-support-experimental">supported</a> on Windows via the
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">VXLAN network backend</a> (<strong>Beta support</strong> ; delegates to win-overlay)
and <a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw">host-gateway network backend</a> (stable support; delegates to win-bridge).</p><p>This plugin supports delegating to one of the reference CNI plugins (win-overlay,
win-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) for
automatic node subnet lease assignment and HNS network creation. This plugin reads
in its own configuration file (cni.conf), and aggregates it with the environment
variables from the FlannelD generated subnet.env file. It then delegates to one of
the reference CNI plugins for network plumbing, and sends the correct configuration
containing the node-assigned subnet to the IPAM plugin (for example: <code>host-local</code>).</p><p>For Node, Pod, and Service objects, the following network flows are supported for
TCP/UDP traffic:</p><ul><li>Pod â†’ Pod (IP)</li><li>Pod â†’ Pod (Name)</li><li>Pod â†’ Service (Cluster IP)</li><li>Pod â†’ Service (PQDN, but only if there are no ".")</li><li>Pod â†’ Service (FQDN)</li><li>Pod â†’ external (IP)</li><li>Pod â†’ external (DNS)</li><li>Node â†’ Pod</li><li>Pod â†’ Node</li></ul><h2 id="ipam">IP address management (IPAM)</h2><p>The following IPAM options are supported on Windows:</p><ul><li><a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local">host-local</a></li><li><a href="https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md">azure-vnet-ipam</a> (for azure-cni only)</li><li><a href="https://docs.microsoft.com/windows-server/networking/technologies/ipam/ipam-top">Windows Server IPAM</a> (fallback option if no IPAM is set)</li></ul><h2 id="dsr">Direct Server Return (DSR)</h2><div class="feature-state-notice feature-stable" title="Feature Gate: WinDSR"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly;
service traffic arrives with the source IP set as the originating pod IP.
This provides performance optimizations by allowing the return traffic routed through load balancers
to bypass the load balancer and respond directly to the client;
reducing load on the load balancer and also reducing overall latency.
For more information, read
<a href="https://techcommunity.microsoft.com/blog/networkingblog/direct-server-return-dsr-in-a-nutshell/693710">Direct Server Return (DSR) in a nutshell</a>.</p><h2 id="load-balancing-and-services">Load balancing and Services</h2><p>A Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> is an abstraction
that defines a logical set of Pods and a means to access them over a network.
In a cluster that includes Windows nodes, you can use the following types of Service:</p><ul><li><code>NodePort</code></li><li><code>ClusterIP</code></li><li><code>LoadBalancer</code></li><li><code>ExternalName</code></li></ul><p>Windows container networking differs in some important ways from Linux networking.
The <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture">Microsoft documentation for Windows Container Networking</a>
provides additional details and background.</p><p>On Windows, you can use the following settings to configure Services and load
balancing behavior:</p><table><caption style="display:none">Windows Service Settings</caption><thead><tr><th>Feature</th><th>Description</th><th>Minimum Supported Windows OS build</th><th>How to enable</th></tr></thead><tbody><tr><td>Session affinity</td><td>Ensures that connections from a particular client are passed to the same Pod each time.</td><td>Windows Server 2022</td><td>Set <code>service.spec.sessionAffinity</code> to "ClientIP"</td></tr><tr><td>Direct Server Return (DSR)</td><td>See <a href="#dsr">DSR</a> notes above.</td><td>Windows Server 2019</td><td>Set the following command line argument (assuming version 1.34): <code>--enable-dsr=true</code></td></tr><tr><td>Preserve-Destination</td><td>Skips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. Also disables node-node forwarding.</td><td>Windows Server, version 1903</td><td>Set <code>"preserve-destination": "true"</code> in service annotations and enable DSR in kube-proxy.</td></tr><tr><td>IPv4/IPv6 dual-stack networking</td><td>Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a cluster</td><td>Windows Server 2019</td><td>See <a href="/docs/concepts/services-networking/dual-stack/#windows-support">IPv4/IPv6 dual-stack</a></td></tr><tr><td>Client IP preservation</td><td>Ensures that source IP of incoming ingress traffic gets preserved. Also disables node-node forwarding.</td><td>Windows Server 2019</td><td>Set <code>service.spec.externalTrafficPolicy</code> to "Local" and enable DSR in kube-proxy</td></tr></tbody></table><h2 id="limitations">Limitations</h2><p>The following networking functionality is <em>not</em> supported on Windows nodes:</p><ul><li>Host networking mode</li><li>Local NodePort access from the node itself (works for other nodes or external clients)</li><li>More than 64 backend pods (or unique destination addresses) for a single Service</li><li>IPv6 communication between Windows pods connected to overlay networks</li><li>Local Traffic Policy in non-DSR mode</li><li>Outbound communication using the ICMP protocol via the <code>win-overlay</code>, <code>win-bridge</code>, or using the Azure-CNI plugin.
Specifically, the Windows data plane (<a href="https://www.microsoft.com/research/project/azure-virtual-filtering-platform/">VFP</a>)
doesn't support ICMP packet transpositions, and this means:<ul><li>ICMP packets directed to destinations within the same network (such as pod to pod communication via ping)
work as expected;</li><li>TCP/UDP packets work as expected;</li><li>ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping)
cannot be transposed and thus will not be routed back to their source;</li><li>Since TCP/UDP packets can still be transposed, you can substitute <code>ping &lt;destination&gt;</code> with
<code>curl &lt;destination&gt;</code> when debugging connectivity with the outside world.</li></ul></li></ul><p>Other limitations:</p><ul><li>Windows reference network plugins win-bridge and win-overlay do not implement
<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI spec</a> v0.4.0,
due to a missing <code>CHECK</code> implementation.</li><li>The Flannel VXLAN CNI plugin has the following limitations on Windows:<ul><li>Node-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).</li><li>Flannel is restricted to using VNI 4096 and UDP port 4789. See the official
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">Flannel VXLAN</a>
backend docs for more details on these parameters.</li></ul></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Controlling Access to the Kubernetes API</h1><p>This page provides an overview of controlling access to the Kubernetes API.</p><p>Users access the <a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a> using <code>kubectl</code>,
client libraries, or by making REST requests. Both human users and
<a href="/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes service accounts</a> can be
authorized for API access.
When a request reaches the API, it goes through several stages, illustrated in the
following diagram:</p><p><img alt="Diagram of request handling steps for Kubernetes API request" src="/images/docs/admin/access-control-overview.svg"/></p><h2 id="transport-security">Transport security</h2><p>By default, the Kubernetes API server listens on port 6443 on the first non-localhost
network interface, protected by TLS. In a typical production Kubernetes cluster, the
API serves on port 443. The port can be changed with the <code>--secure-port</code>, and the
listening IP address with the <code>--bind-address</code> flag.</p><p>The API server presents a certificate. This certificate may be signed using
a private certificate authority (CA), or based on a public key infrastructure linked
to a generally recognized CA. The certificate and corresponding private key can be set
by using the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> flags.</p><p>If your cluster uses a private certificate authority, you need a copy of that CA
certificate configured into your <code>~/.kube/config</code> on the client, so that you can
trust the connection and be confident it was not intercepted.</p><p>Your client can present a TLS client certificate at this stage.</p><h2 id="authentication">Authentication</h2><p>Once TLS is established, the HTTP request moves to the Authentication step.
This is shown as step <strong>1</strong> in the diagram.
The cluster creation script or cluster admin configures the API server to run
one or more Authenticator modules.
Authenticators are described in more detail in
<a href="/docs/reference/access-authn-authz/authentication/">Authentication</a>.</p><p>The input to the authentication step is the entire HTTP request; however, it typically
examines the headers and/or client certificate.</p><p>Authentication modules include client certificates, password, and plain tokens,
bootstrap tokens, and JSON Web Tokens (used for service accounts).</p><p>Multiple authentication modules can be specified, in which case each one is tried in sequence,
until one of them succeeds.</p><p>If the request cannot be authenticated, it is rejected with HTTP status code 401.
Otherwise, the user is authenticated as a specific <code>username</code>, and the user name
is available to subsequent steps to use in their decisions. Some authenticators
also provide the group memberships of the user, while other authenticators
do not.</p><p>While Kubernetes uses usernames for access control decisions and in request logging,
it does not have a <code>User</code> object nor does it store usernames or other information about
users in its API.</p><h2 id="authorization">Authorization</h2><p>After the request is authenticated as coming from a specific user, the request must
be authorized. This is shown as stepÂ <strong>2</strong>Â in the diagram.</p><p>A request must include the username of the requester, the requested action, and
the object affected by the action. The request is authorized if an existing policy
declares that the user has permissions to complete the requested action.</p><p>For example, if Bob has the policy below, then he can read pods only in the namespace <code>projectCaribou</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"apiVersion"</span>: <span style="color:#b44">"abac.authorization.kubernetes.io/v1beta1"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"Policy"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"spec"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"user"</span>: <span style="color:#b44">"bob"</span>,
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"namespace"</span>: <span style="color:#b44">"projectCaribou"</span>,
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"resource"</span>: <span style="color:#b44">"pods"</span>,
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"readonly"</span>: <span style="color:#a2f;font-weight:700">true</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>If Bob makes the following request, the request is authorized because he is
allowed to read objects in the <code>projectCaribou</code> namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"apiVersion"</span>: <span style="color:#b44">"authorization.k8s.io/v1beta1"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"SubjectAccessReview"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"spec"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"resourceAttributes"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"namespace"</span>: <span style="color:#b44">"projectCaribou"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"verb"</span>: <span style="color:#b44">"get"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"group"</span>: <span style="color:#b44">"unicorn.example.org"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"resource"</span>: <span style="color:#b44">"pods"</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>If Bob makes a request to write (<code>create</code> or <code>update</code>) to the objects in the
<code>projectCaribou</code> namespace, his authorization is denied. If Bob makes a request
to read (<code>get</code>) objects in a different namespace such as <code>projectFish</code>, then his authorization is denied.</p><p>Kubernetes authorization requires that you use common REST attributes to interact
with existing organization-wide or cloud-provider-wide access control systems.
It is important to use REST formatting because these control systems might
interact with other APIs besides the Kubernetes API.</p><p>Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode,
and Webhook mode. When an administrator creates a cluster, they configure the
authorization modules that should be used in the API server. If more than one
authorization modules are configured, Kubernetes checks each module, and if
any module authorizes the request, then the request can proceed. If all of
the modules deny the request, then the request is denied (HTTP status code 403).</p><p>To learn more about Kubernetes authorization, including details about creating
policies using the supported authorization modules, see <a href="/docs/reference/access-authn-authz/authorization/">Authorization</a>.</p><h2 id="admission-control">Admission control</h2><p>Admission Control modules are software modules that can modify or reject requests.
In addition to the attributes available to Authorization modules, Admission
Control modules can access the contents of the object that is being created or modified.</p><p>Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object.
Admission controllers do not act on requests that merely read objects.
When multiple admission controllers are configured, they are called in order.</p><p>This is shown as step <strong>3</strong> in the diagram.</p><p>Unlike Authentication and Authorization modules, if any admission controller module
rejects, then the request is immediately rejected.</p><p>In addition to rejecting objects, admission controllers can also set complex defaults for
fields.</p><p>The available Admission Control modules are described in <a href="/docs/reference/access-authn-authz/admission-controllers/">Admission Controllers</a>.</p><p>Once a request passes all admission controllers, it is validated using the validation routines
for the corresponding API object, and then written to the object store (shown as step <strong>4</strong>).</p><h2 id="auditing">Auditing</h2><p>Kubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.
The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.</p><p>For more information, see <a href="/docs/tasks/debug/debug-cluster/audit/">Auditing</a>.</p><h2 id="what-s-next">What's next</h2><p>Read more documentation on authentication, authorization and API access control:</p><ul><li><a href="/docs/reference/access-authn-authz/authentication/">Authenticating</a><ul><li><a href="/docs/reference/access-authn-authz/bootstrap-tokens/">Authenticating with Bootstrap Tokens</a></li></ul></li><li><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission Controllers</a><ul><li><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Control</a></li></ul></li><li><a href="/docs/reference/access-authn-authz/authorization/">Authorization</a><ul><li><a href="/docs/reference/access-authn-authz/rbac/">Role Based Access Control</a></li><li><a href="/docs/reference/access-authn-authz/abac/">Attribute Based Access Control</a></li><li><a href="/docs/reference/access-authn-authz/node/">Node Authorization</a></li><li><a href="/docs/reference/access-authn-authz/webhook/">Webhook Authorization</a></li></ul></li><li><a href="/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a><ul><li>including <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection">CSR approval</a>
and <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#signing">certificate signing</a></li></ul></li><li>Service accounts<ul><li><a href="/docs/tasks/configure-pod-container/configure-service-account/">Developer guide</a></li><li><a href="/docs/reference/access-authn-authz/service-accounts-admin/">Administration</a></li></ul></li></ul><p>You can learn about:</p><ul><li>how Pods can use
<a href="/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials">Secrets</a>
to obtain API credentials.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Node-pressure Eviction</h1><p>Node-pressure eviction is the process by which the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> proactively terminates
pods to reclaim <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank" aria-label="resource">resource</a>
on nodes.</p><p>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> monitors resources
like memory, disk space, and filesystem inodes on your cluster's nodes.
When one or more of these resources reach specific consumption levels, the
kubelet can proactively fail one or more pods on the node to reclaim resources
and prevent starvation.</p><p>During a node-pressure eviction, the kubelet sets the <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">phase</a> for the
selected pods to <code>Failed</code>, and terminates the Pod.</p><p>Node-pressure eviction is not the same as
<a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated eviction</a>.</p><p>The kubelet does not respect your configured <a class="glossary-tooltip" title="An object that limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-pod-disruption-budget" target="_blank" aria-label="PodDisruptionBudget">PodDisruptionBudget</a>
or the pod's
<code>terminationGracePeriodSeconds</code>. If you use <a href="#soft-eviction-thresholds">soft eviction thresholds</a>,
the kubelet respects your configured <code>eviction-max-pod-grace-period</code>. If you use
<a href="#hard-eviction-thresholds">hard eviction thresholds</a>, the kubelet uses a <code>0s</code> grace period (immediate shutdown) for termination.</p><h2 id="self-healing-behavior">Self healing behavior</h2><p>The kubelet attempts to <a href="#reclaim-node-resources">reclaim node-level resources</a>
before it terminates end-user pods. For example, it removes unused container
images when disk resources are starved.</p><p>If the pods are managed by a <a class="glossary-tooltip" title="A workload is an application running on Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/" target="_blank" aria-label="workload">workload</a>
management object (such as <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a>
or <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>) that
replaces failed pods, the control plane (<code>kube-controller-manager</code>) creates new
pods in place of the evicted pods.</p><h3 id="self-healing-for-static-pods">Self healing for static pods</h3><p>If you are running a <a href="/docs/concepts/workloads/pods/#static-pods">static pod</a>
on a node that is under resource pressure, the kubelet may evict that static
Pod. The kubelet then tries to create a replacement, because static Pods always
represent an intent to run a Pod on that node.</p><p>The kubelet takes the <em>priority</em> of the static pod into account when creating
a replacement. If the static pod manifest specifies a low priority, and there
are higher-priority Pods defined within the cluster's control plane, and the
node is under resource pressure, the kubelet may not be able to make room for
that static pod. The kubelet continues to attempt to run all static pods even
when there is resource pressure on a node.</p><h2 id="eviction-signals-and-thresholds">Eviction signals and thresholds</h2><p>The kubelet uses various parameters to make eviction decisions, like the following:</p><ul><li>Eviction signals</li><li>Eviction thresholds</li><li>Monitoring intervals</li></ul><h3 id="eviction-signals">Eviction signals</h3><p>Eviction signals are the current state of a particular resource at a specific
point in time. The kubelet uses eviction signals to make eviction decisions by
comparing the signals to eviction thresholds, which are the minimum amount of
the resource that should be available on the node.</p><p>The kubelet uses the following eviction signals:</p><table><thead><tr><th>Eviction Signal</th><th>Description</th><th>Linux Only</th></tr></thead><tbody><tr><td><code>memory.available</code></td><td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td><td/></tr><tr><td><code>nodefs.available</code></td><td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td><td/></tr><tr><td><code>nodefs.inodesFree</code></td><td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td><td>â€¢</td></tr><tr><td><code>imagefs.available</code></td><td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td><td/></tr><tr><td><code>imagefs.inodesFree</code></td><td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td><td>â€¢</td></tr><tr><td><code>containerfs.available</code></td><td><code>containerfs.available</code> := <code>node.stats.runtime.containerfs.available</code></td><td/></tr><tr><td><code>containerfs.inodesFree</code></td><td><code>containerfs.inodesFree</code> := <code>node.stats.runtime.containerfs.inodesFree</code></td><td>â€¢</td></tr><tr><td><code>pid.available</code></td><td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td><td>â€¢</td></tr></tbody></table><p>In this table, the <strong>Description</strong> column shows how kubelet gets the value of the
signal. Each signal supports either a percentage or a literal value. The kubelet
calculates the percentage value relative to the total capacity associated with
the signal.</p><h4 id="memory-signals">Memory signals</h4><p>On Linux nodes, the value for <code>memory.available</code> is derived from the cgroupfs instead of tools
like <code>free -m</code>. This is important because <code>free -m</code> does not work in a
container, and if users use the <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">node allocatable</a>
feature, out of resource decisions
are made local to the end user Pod part of the cgroup hierarchy as well as the
root node. This <a href="/examples/admin/resource/memory-available.sh">script</a> or
<a href="/examples/admin/resource/memory-available-cgroupv2.sh">cgroupv2 script</a>
reproduces the same set of steps that the kubelet performs to calculate
<code>memory.available</code>. The kubelet excludes inactive_file (the number of bytes of
file-backed memory on the inactive LRU list) from its calculation, as it assumes that
memory is reclaimable under pressure.</p><p>On Windows nodes, the value for <code>memory.available</code> is derived from the node's global
memory commit levels (queried through the <a href="https://learn.microsoft.com/windows/win32/api/psapi/nf-psapi-getperformanceinfo"><code>GetPerformanceInfo()</code></a>
system call) by subtracting the node's global <a href="https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information"><code>CommitTotal</code></a> from the node's <a href="https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information"><code>CommitLimit</code></a>. Please note that <code>CommitLimit</code> can change if the node's page-file size changes!</p><h4 id="filesystem-signals">Filesystem signals</h4><p>The kubelet recognizes three specific filesystem identifiers that can be used with
eviction signals (<code>&lt;identifier&gt;.inodesFree</code> or <code>&lt;identifier&gt;.available</code>):</p><ol><li><p><code>nodefs</code>: The node's main filesystem, used for local disk volumes,
emptyDir volumes not backed by memory, log storage, ephemeral storage,
and more. For example, <code>nodefs</code> contains <code>/var/lib/kubelet</code>.</p></li><li><p><code>imagefs</code>: An optional filesystem that container runtimes can use to store
container images (which are the read-only layers) and container writable
layers.</p></li><li><p><code>containerfs</code>: An optional filesystem that container runtime can use to
store the writeable layers. Similar to the main filesystem (see <code>nodefs</code>),
it's used to store local disk volumes, emptyDir volumes not backed by memory,
log storage, and ephemeral storage, except for the container images. When
<code>containerfs</code> is used, the <code>imagefs</code> filesystem can be split to only store
images (read-only layers) and nothing else.</p></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-beta" title="Feature Gate: KubeletSeparateDiskGC"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [beta]</code> (enabled by default: true)</div><p>The <em>split image filesystem</em> feature, which enables support for the <code>containerfs</code>
filesystem, adds several new eviction signals, thresholds and metrics. To use
<code>containerfs</code>, the Kubernetes release v1.34 requires the
<code>KubeletSeparateDiskGC</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
to be enabled. Currently, only CRI-O (v1.29 or higher) offers the <code>containerfs</code>
filesystem support.</p></div><p>As such, kubelet generally allows three options for container filesystems:</p><ul><li><p>Everything is on the single <code>nodefs</code>, also referred to as "rootfs" or
simply "root", and there is no dedicated image filesystem.</p></li><li><p>Container storage (see <code>nodefs</code>) is on a dedicated disk, and <code>imagefs</code>
(writable and read-only layers) is separate from the root filesystem.
This is often referred to as "split disk" (or "separate disk") filesystem.</p></li><li><p>Container filesystem <code>containerfs</code> (same as <code>nodefs</code> plus writable
layers) is on root and the container images (read-only layers) are
stored on separate <code>imagefs</code>. This is often referred to as "split image"
filesystem.</p></li></ul><p>The kubelet will attempt to auto-discover these filesystems with their current
configuration directly from the underlying container runtime and will ignore
other local node filesystems.</p><p>The kubelet does not support other container filesystems or storage configurations,
and it does not currently support multiple filesystems for images and containers.</p><h3 id="deprecated-kubelet-garbage-collection-features">Deprecated kubelet garbage collection features</h3><p>Some kubelet garbage collection features are deprecated in favor of eviction:</p><table><thead><tr><th>Existing Flag</th><th>Rationale</th></tr></thead><tbody><tr><td><code>--maximum-dead-containers</code></td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td>deprecated once old logs are stored outside of container's context</td></tr></tbody></table><h3 id="eviction-thresholds">Eviction thresholds</h3><p>You can specify custom eviction thresholds for the kubelet to use when it makes
eviction decisions. You can configure <a href="#soft-eviction-thresholds">soft</a> and
<a href="#hard-eviction-thresholds">hard</a> eviction thresholds.</p><p>Eviction thresholds have the form <code>[eviction-signal][operator][quantity]</code>, where:</p><ul><li><code>eviction-signal</code> is the <a href="#eviction-signals">eviction signal</a> to use.</li><li><code>operator</code> is the <a href="https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators">relational operator</a>
you want, such as <code>&lt;</code> (less than).</li><li><code>quantity</code> is the eviction threshold amount, such as <code>1Gi</code>. The value of <code>quantity</code>
must match the quantity representation used by Kubernetes. You can use either
literal values or percentages (<code>%</code>).</li></ul><p>For example, if a node has 10GiB of total memory and you want trigger eviction if
the available memory falls below 1GiB, you can define the eviction threshold as
either <code>memory.available&lt;10%</code> or <code>memory.available&lt;1Gi</code> (you cannot use both).</p><h4 id="soft-eviction-thresholds">Soft eviction thresholds</h4><p>A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. The kubelet does not evict pods until the
grace period is exceeded. The kubelet returns an error on startup if you do
not specify a grace period.</p><p>You can specify both a soft eviction threshold grace period and a maximum
allowed pod termination grace period for kubelet to use during evictions. If you
specify a maximum allowed grace period and the soft eviction threshold is met,
the kubelet uses the lesser of the two grace periods. If you do not specify a
maximum allowed grace period, the kubelet kills evicted pods immediately without
graceful termination.</p><p>You can use the following flags to configure soft eviction thresholds:</p><ul><li><code>eviction-soft</code>: A set of eviction thresholds like <code>memory.available&lt;1.5Gi</code>
that can trigger pod eviction if held over the specified grace period.</li><li><code>eviction-soft-grace-period</code>: A set of eviction grace periods like <code>memory.available=1m30s</code>
that define how long a soft eviction threshold must hold before triggering a Pod eviction.</li><li><code>eviction-max-pod-grace-period</code>: The maximum allowed grace period (in seconds)
to use when terminating pods in response to a soft eviction threshold being met.</li></ul><h4 id="hard-eviction-thresholds">Hard eviction thresholds</h4><p>A hard eviction threshold has no grace period. When a hard eviction threshold is
met, the kubelet kills pods immediately without graceful termination to reclaim
the starved resource.</p><p>You can use the <code>eviction-hard</code> flag to configure a set of hard eviction
thresholds like <code>memory.available&lt;1Gi</code>.</p><p>The kubelet has the following default hard eviction thresholds:</p><ul><li><code>memory.available&lt;100Mi</code> (Linux nodes)</li><li><code>memory.available&lt;500Mi</code> (Windows nodes)</li><li><code>nodefs.available&lt;10%</code></li><li><code>imagefs.available&lt;15%</code></li><li><code>nodefs.inodesFree&lt;5%</code> (Linux nodes)</li><li><code>imagefs.inodesFree&lt;5%</code> (Linux nodes)</li></ul><p>These default values of hard eviction thresholds will only be set if none
of the parameters is changed. If you change the value of any parameter,
then the values of other parameters will not be inherited as the default
values and will be set to zero. In order to provide custom values, you
should provide all the thresholds respectively. You can also set the kubelet config
MergeDefaultEvictionSettings to true in the kubelet configuration file.
If set to true and any parameter is changed, then the other parameters will
inherit their default values instead of 0.</p><p>The <code>containerfs.available</code> and <code>containerfs.inodesFree</code> (Linux nodes) default
eviction thresholds will be set as follows:</p><ul><li><p>If a single filesystem is used for everything, then <code>containerfs</code> thresholds
are set the same as <code>nodefs</code>.</p></li><li><p>If separate filesystems are configured for both images and containers,
then <code>containerfs</code> thresholds are set the same as <code>imagefs</code>.</p></li></ul><p>Setting custom overrides for thresholds related to <code>containersfs</code> is currently
not supported, and a warning will be issued if an attempt to do so is made; any
provided custom values will, as such, be ignored.</p><h2 id="eviction-monitoring-interval">Eviction monitoring interval</h2><p>The kubelet evaluates eviction thresholds based on its configured <code>housekeeping-interval</code>,
which defaults to <code>10s</code>.</p><h2 id="node-conditions">Node conditions</h2><p>The kubelet reports <a href="/docs/concepts/architecture/nodes/#condition">node conditions</a>
to reflect that the node is under pressure because hard or soft eviction
threshold is met, independent of configured grace periods.</p><p>The kubelet maps eviction signals to node conditions as follows:</p><table><thead><tr><th>Node Condition</th><th>Eviction Signal</th><th>Description</th></tr></thead><tbody><tr><td><code>MemoryPressure</code></td><td><code>memory.available</code></td><td>Available memory on the node has satisfied an eviction threshold</td></tr><tr><td><code>DiskPressure</code></td><td><code>nodefs.available</code>, <code>nodefs.inodesFree</code>, <code>imagefs.available</code>, <code>imagefs.inodesFree</code>, <code>containerfs.available</code>, or <code>containerfs.inodesFree</code></td><td>Available disk space and inodes on either the node's root filesystem, image filesystem, or container filesystem has satisfied an eviction threshold</td></tr><tr><td><code>PIDPressure</code></td><td><code>pid.available</code></td><td>Available processes identifiers on the (Linux) node has fallen below an eviction threshold</td></tr></tbody></table><p>The control plane also <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition">maps</a>
these node conditions to taints.</p><p>The kubelet updates the node conditions based on the configured
<code>--node-status-update-frequency</code>, which defaults to <code>10s</code>.</p><h3 id="node-condition-oscillation">Node condition oscillation</h3><p>In some cases, nodes oscillate above and below soft eviction thresholds without
holding for the defined grace periods. This causes the reported node condition
to constantly switch between <code>true</code> and <code>false</code>, leading to bad eviction decisions.</p><p>To protect against oscillation, you can use the <code>eviction-pressure-transition-period</code>
flag, which controls how long the kubelet must wait before transitioning a node
condition to a different state. The transition period has a default value of <code>5m</code>.</p><h3 id="reclaim-node-resources">Reclaiming node level resources</h3><p>The kubelet tries to reclaim node-level resources before it evicts end-user pods.</p><p>When a <code>DiskPressure</code> node condition is reported, the kubelet reclaims node-level
resources based on the filesystems on the node.</p><h4 id="without-imagefs-or-containerfs">Without <code>imagefs</code> or <code>containerfs</code></h4><p>If the node only has a <code>nodefs</code> filesystem that meets eviction thresholds,
the kubelet frees up disk space in the following order:</p><ol><li>Garbage collect dead pods and containers.</li><li>Delete unused images.</li></ol><h4 id="with-imagefs">With <code>imagefs</code></h4><p>If the node has a dedicated <code>imagefs</code> filesystem for container runtimes to use,
the kubelet does the following:</p><ul><li><p>If the <code>nodefs</code> filesystem meets the eviction thresholds, the kubelet garbage
collects dead pods and containers.</p></li><li><p>If the <code>imagefs</code> filesystem meets the eviction thresholds, the kubelet
deletes all unused images.</p></li></ul><h4 id="with-imagefs-and-containerfs">With <code>imagefs</code> and <code>containerfs</code></h4><p>If the node has a dedicated <code>containerfs</code> alongside the <code>imagefs</code> filesystem
configured for the container runtimes to use, then kubelet will attempt to
reclaim resources as follows:</p><ul><li><p>If the <code>containerfs</code> filesystem meets the eviction thresholds, the kubelet
garbage collects dead pods and containers.</p></li><li><p>If the <code>imagefs</code> filesystem meets the eviction thresholds, the kubelet
deletes all unused images.</p></li></ul><h3 id="pod-selection-for-kubelet-eviction">Pod selection for kubelet eviction</h3><p>If the kubelet's attempts to reclaim node-level resources don't bring the eviction
signal below the threshold, the kubelet begins to evict end-user pods.</p><p>The kubelet uses the following parameters to determine the pod eviction order:</p><ol><li>Whether the pod's resource usage exceeds requests</li><li><a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority</a></li><li>The pod's resource usage relative to requests</li></ol><p>As a result, kubelet ranks and evicts pods in the following order:</p><ol><li><p><code>BestEffort</code> or <code>Burstable</code> pods where the usage exceeds requests. These pods
are evicted based on their Priority and then by how much their usage level
exceeds the request.</p></li><li><p><code>Guaranteed</code> pods and <code>Burstable</code> pods where the usage is less than requests
are evicted last, based on their Priority.</p></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The kubelet does not use the pod's <a href="/docs/concepts/workloads/pods/pod-qos/">QoS class</a> to determine the eviction order.
You can use the QoS class to estimate the most likely pod eviction order when
reclaiming resources like memory. QoS classification does not apply to EphemeralStorage requests,
so the above scenario will not apply if the node is, for example, under <code>DiskPressure</code>.</div><p><code>Guaranteed</code> pods are guaranteed only when requests and limits are specified for
all the containers and they are equal. These pods will never be evicted because
of another pod's resource consumption. If a system daemon (such as <code>kubelet</code>
and <code>journald</code>) is consuming more resources than were reserved via
<code>system-reserved</code> or <code>kube-reserved</code> allocations, and the node only has
<code>Guaranteed</code> or <code>Burstable</code> pods using less resources than requests left on it,
then the kubelet must choose to evict one of these pods to preserve node stability
and to limit the impact of resource starvation on other pods. In this case, it
will choose to evict pods of lowest Priority first.</p><p>If you are running a <a href="/docs/concepts/workloads/pods/#static-pods">static pod</a>
and want to avoid having it evicted under resource pressure, set the
<code>priority</code> field for that Pod directly. Static pods do not support the
<code>priorityClassName</code> field.</p><p>When the kubelet evicts pods in response to inode or process ID starvation, it uses
the Pods' relative priority to determine the eviction order, because inodes and PIDs have no
requests.</p><p>The kubelet sorts pods differently based on whether the node has a dedicated
<code>imagefs</code> or <code>containerfs</code> filesystem:</p><h4 id="without-imagefs">Without <code>imagefs</code> or <code>containerfs</code> (<code>nodefs</code> and <code>imagefs</code> use the same filesystem)</h4><ul><li>If <code>nodefs</code> triggers evictions, the kubelet sorts pods based on their
total disk usage (<code>local volumes + logs and a writable layer of all containers</code>).</li></ul><h4 id="with-imagefs">With <code>imagefs</code> (<code>nodefs</code> and <code>imagefs</code> filesystems are separate)</h4><ul><li><p>If <code>nodefs</code> triggers evictions, the kubelet sorts pods based on <code>nodefs</code>
usage (<code>local volumes + logs of all containers</code>).</p></li><li><p>If <code>imagefs</code> triggers evictions, the kubelet sorts pods based on the
writable layer usage of all containers.</p></li></ul><h4 id="with-containersfs">With <code>imagesfs</code> and <code>containerfs</code> (<code>imagefs</code> and <code>containerfs</code> have been split)</h4><ul><li><p>If <code>containerfs</code> triggers evictions, the kubelet sorts pods based on
<code>containerfs</code> usage (<code>local volumes + logs and a writable layer of all containers</code>).</p></li><li><p>If <code>imagefs</code> triggers evictions, the kubelet sorts pods based on the
<code>storage of images</code> rank, which represents the disk usage of a given image.</p></li></ul><h3 id="minimum-eviction-reclaim">Minimum eviction reclaim</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>As of Kubernetes v1.34, you cannot set a custom value
for the <code>containerfs.available</code> metric. The configuration for this specific
metric will be set automatically to reflect values set for either the <code>nodefs</code>
or <code>imagefs</code>, depending on the configuration.</div><p>In some cases, pod eviction only reclaims a small amount of the starved resource.
This can lead to the kubelet repeatedly hitting the configured eviction thresholds
and triggering multiple evictions.</p><p>You can use the <code>--eviction-minimum-reclaim</code> flag or a <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet config file</a>
to configure a minimum reclaim amount for each resource. When the kubelet notices
that a resource is starved, it continues to reclaim that resource until it
reclaims the quantity you specify.</p><p>For example, the following configuration sets minimum reclaim amounts:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">evictionHard</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">memory.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"500Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">imagefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">evictionMinimumReclaim</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">memory.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"500Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">imagefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2Gi"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, if the <code>nodefs.available</code> signal meets the eviction threshold,
the kubelet reclaims the resource until the signal reaches the threshold of 1GiB,
and then continues to reclaim the minimum amount of 500MiB, until the available
nodefs storage value reaches 1.5GiB.</p><p>Similarly, the kubelet tries to reclaim the <code>imagefs</code> resource until the <code>imagefs.available</code>
value reaches <code>102Gi</code>, representing 102 GiB of available container image storage. If the amount
of storage that the kubelet could reclaim is less than 2GiB, the kubelet doesn't reclaim anything.</p><p>The default <code>eviction-minimum-reclaim</code> is <code>0</code> for all resources.</p><h2 id="node-out-of-memory-behavior">Node out of memory behavior</h2><p>If the node experiences an <em>out of memory</em> (OOM) event prior to the kubelet
being able to reclaim memory, the node depends on the <a href="https://lwn.net/Articles/391222/">oom_killer</a>
to respond.</p><p>The kubelet sets an <code>oom_score_adj</code> value for each container based on the QoS for the pod.</p><table><thead><tr><th>Quality of Service</th><th><code>oom_score_adj</code></th></tr></thead><tbody><tr><td><code>Guaranteed</code></td><td>-997</td></tr><tr><td><code>BestEffort</code></td><td>1000</td></tr><tr><td><code>Burstable</code></td><td><em>min(max(2, 1000 - (1000 Ã— memoryRequestBytes) / machineMemoryCapacityBytes), 999)</em></td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The kubelet also sets an <code>oom_score_adj</code> value of <code>-997</code> for any containers in Pods that have
<code>system-node-critical</code> <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank" aria-label="Priority">Priority</a>.</div><p>If the kubelet can't reclaim memory before a node experiences OOM, the
<code>oom_killer</code> calculates an <code>oom_score</code> based on the percentage of memory it's
using on the node, and then adds the <code>oom_score_adj</code> to get an effective <code>oom_score</code>
for each container. It then kills the container with the highest score.</p><p>This means that containers in low QoS pods that consume a large amount of memory
relative to their scheduling requests are killed first.</p><p>Unlike pod eviction, if a container is OOM killed, the kubelet can restart it
based on its <code>restartPolicy</code>.</p><h2 id="node-pressure-eviction-good-practices">Good practices</h2><p>The following sections describe good practice for eviction configuration.</p><h3 id="schedulable-resources-and-eviction-policies">Schedulable resources and eviction policies</h3><p>When you configure the kubelet with an eviction policy, you should make sure that
the scheduler will not schedule pods if they will trigger eviction because they
immediately induce memory pressure.</p><p>Consider the following scenario:</p><ul><li>Node memory capacity: 10GiB</li><li>Operator wants to reserve 10% of memory capacity for system daemons (kernel, <code>kubelet</code>, etc.)</li><li>Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.</li></ul><p>For this to work, the kubelet is launched as follows:</p><pre tabindex="0"><code class="language-none" data-lang="none">--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><p>In this configuration, the <code>--system-reserved</code> flag reserves 1.5GiB of memory
for the system, which is <code>10% of the total memory + the eviction threshold amount</code>.</p><p>The node can reach the eviction threshold if a pod is using more than its request,
or if the system is using more than 1GiB of memory, which makes the <code>memory.available</code>
signal fall below 500MiB and triggers the threshold.</p><h3 id="daemonset">DaemonSets and node-pressure eviction</h3><p>Pod priority is a major factor in making eviction decisions. If you do not want
the kubelet to evict pods that belong to a DaemonSet, give those pods a high
enough priority by specifying a suitable <code>priorityClassName</code> in the pod spec.
You can also use a lower priority, or the default, to only allow pods from that
DaemonSet to run when there are enough resources.</p><h2 id="known-issues">Known issues</h2><p>The following sections describe known issues related to out of resource handling.</p><h3 id="kubelet-may-not-observe-memory-pressure-right-away">kubelet may not observe memory pressure right away</h3><p>By default, the kubelet polls cAdvisor to collect memory usage stats at a
regular interval. If memory usage increases within that window rapidly, the
kubelet may not observe <code>MemoryPressure</code> fast enough, and the OOM killer
will still be invoked.</p><p>You can use the <code>--kernel-memcg-notification</code> flag to enable the <code>memcg</code>
notification API on the kubelet to get notified immediately when a threshold
is crossed.</p><p>If you are not trying to achieve extreme utilization, but a sensible measure of
overcommit, a viable workaround for this issue is to use the <code>--kube-reserved</code>
and <code>--system-reserved</code> flags to allocate memory for the system.</p><h3 id="active-file-memory-is-not-considered-as-available-memory">active_file memory is not considered as available memory</h3><p>On Linux, the kernel tracks the number of bytes of file-backed memory on active
least recently used (LRU) list as the <code>active_file</code> statistic. The kubelet treats <code>active_file</code> memory
areas as not reclaimable. For workloads that make intensive use of block-backed
local storage, including ephemeral local storage, kernel-level caches of file
and block data means that many recently accessed cache pages are likely to be
counted as <code>active_file</code>. If enough of these kernel block buffers are on the
active LRU list, the kubelet is liable to observe this as high resource use and
taint the node as experiencing memory pressure - triggering pod eviction.</p><p>For more details, see <a href="https://github.com/kubernetes/kubernetes/issues/43916">https://github.com/kubernetes/kubernetes/issues/43916</a></p><p>You can work around that behavior by setting the memory limit and memory request
the same for containers likely to perform intensive I/O activity. You will need
to estimate or measure an optimal memory limit value for that container.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated Eviction</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority and Preemption</a></li><li>Learn about <a href="/docs/tasks/run-application/configure-pdb/">PodDisruptionBudgets</a></li><li>Learn about <a href="/docs/tasks/configure-pod-container/quality-service-pod/">Quality of Service</a> (QoS)</li><li>Check out the <a href="/docs/reference/generated/kubernetes-api/v1.34/#create-eviction-pod-v1-core">Eviction API</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Services, Load Balancing, and Networking</h1><div class="lead">Concepts and resources behind networking in Kubernetes.</div><h2 id="the-kubernetes-network-model">The Kubernetes network model</h2><p>The Kubernetes network model is built out of several pieces:</p><ul><li><p>Each <a href="/docs/concepts/workloads/pods/">pod</a> in a cluster gets its
own unique cluster-wide IP address.</p><ul><li>A pod has its own private network namespace which is shared by
all of the containers within the pod. Processes running in
different containers in the same pod can communicate with each
other over <code>localhost</code>.</li></ul></li><li><p>The <em>pod network</em> (also called a cluster network) handles communication
between pods. It ensures that (barring intentional network segmentation):</p><ul><li><p>All pods can communicate with all other pods, whether they are
on the same <a href="/docs/concepts/architecture/nodes/">node</a> or on
different nodes. Pods can communicate with each other
directly, without the use of proxies or address translation (NAT).</p><p>On Windows, this rule does not apply to host-network pods.</p></li><li><p>Agents on a node (such as system daemons, or kubelet) can
communicate with all pods on that node.</p></li></ul></li><li><p>The <a href="/docs/concepts/services-networking/service/">Service</a> API
lets you provide a stable (long lived) IP address or hostname for a service implemented
by one or more backend pods, where the individual pods making up
the service can change over time.</p><ul><li><p>Kubernetes automatically manages
<a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlice</a>
objects to provide information about the pods currently backing a Service.</p></li><li><p>A service proxy implementation monitors the set of Service and
EndpointSlice objects, and programs the data plane to route
service traffic to its backends, by using operating system or
cloud provider APIs to intercept or rewrite packets.</p></li></ul></li><li><p>The <a href="/docs/concepts/services-networking/gateway/">Gateway</a> API
(or its predecessor, <a href="/docs/concepts/services-networking/ingress/">Ingress</a>)
allows you to make Services accessible to clients that are outside the cluster.</p><ul><li>A simpler, but less-configurable, mechanism for cluster
ingress is available via the Service API's
<a href="/docs/concepts/services-networking/service/#loadbalancer"><code>type: LoadBalancer</code></a>,
when using a supported <a class="glossary-tooltip" title="An organization that offers a cloud computing platform." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-cloud-provider" target="_blank" aria-label="Cloud Provider">Cloud Provider</a>.</li></ul></li><li><p><a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> is a built-in
Kubernetes API that allows you to control traffic between pods, or between pods and
the outside world.</p></li></ul><p>In older container systems, there was no automatic connectivity
between containers on different hosts, and so it was often necessary
to explicitly create links between containers, or to map container
ports to host ports to make them reachable by containers on other
hosts. This is not needed in Kubernetes; Kubernetes's model is that
pods can be treated much like VMs or physical hosts from the
perspectives of port allocation, naming, service discovery, load
balancing, application configuration, and migration.</p><p>Only a few parts of this model are implemented by Kubernetes itself.
For the other parts, Kubernetes defines the APIs, but the
corresponding functionality is provided by external components, some
of which are optional:</p><ul><li><p>Pod network namespace setup is handled by system-level software implementing the
<a href="/docs/concepts/containers/cri/">Container Runtime Interface</a>.</p></li><li><p>The pod network itself is managed by a
<a href="/docs/concepts/cluster-administration/addons/#networking-and-network-policy">pod network implementation</a>.
On Linux, most container runtimes use the
<a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" aria-label="Container Networking Interface (CNI)">Container Networking Interface (CNI)</a>
to interact with the pod network implementation, so these
implementations are often called <em>CNI plugins</em>.</p></li><li><p>Kubernetes provides a default implementation of service proxying,
called <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a>, but some pod
network implementations instead use their own service proxy that
is more tightly integrated with the rest of the implementation.</p></li><li><p>NetworkPolicy is generally also implemented by the pod network
implementation. (Some simpler pod network implementations don't
implement NetworkPolicy, or an administrator may choose to
configure the pod network without NetworkPolicy support. In these
cases, the API will still be present, but it will have no effect.)</p></li><li><p>There are many <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations of the Gateway API</a>,
some of which are specific to particular cloud environments, some more
focused on "bare metal" environments, and others more generic.</p></li></ul><h2 id="what-s-next">What's next</h2><p>The <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a>
tutorial lets you learn about Services and Kubernetes networking with a hands-on example.</p><p><a href="/docs/concepts/cluster-administration/networking/">Cluster Networking</a> explains how to set
up networking for your cluster, and also provides an overview of the technologies involved.</p><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/concepts/services-networking/service/">Service</a></h5><p>Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/ingress/">Ingress</a></h5><p>Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</a></h5><p>In order for an <a href="/docs/concepts/services-networking/ingress/">Ingress</a> to work in your cluster, there must be an <em>ingress controller</em> running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/gateway/">Gateway API</a></h5><p>Gateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a></h5><p>The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/network-policies/">Network Policies</a></h5><p>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a></h5><p>Your workload can discover Services within your cluster using DNS; this page explains how that works.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/dual-stack/">IPv4/IPv6 dual-stack</a></h5><p>Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/topology-aware-routing/">Topology Aware Routing</a></h5><p><em>Topology Aware Routing</em> provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/windows-networking/">Networking on Windows</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/services-networking/cluster-ip-allocation/">Service ClusterIP allocation</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/services-networking/service-traffic-policy/">Service Internal Traffic Policy</a></h5><p>If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use <em>Service Internal Traffic Policy</em> to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.</p></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Storage Capacity</h1><p>Storage capacity is limited and may vary depending on the node on
which a pod runs: network-attached storage might not be accessible by
all nodes, or storage is local to a node to begin with.</p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>This page describes how Kubernetes keeps track of storage capacity and
how the scheduler uses that information to <a href="/docs/concepts/scheduling-eviction/">schedule Pods</a> onto nodes
that have access to enough storage capacity for the remaining missing
volumes. Without storage capacity tracking, the scheduler may choose a
node that doesn't have enough capacity to provision a volume and
multiple scheduling retries will be needed.</p><h2 id="before-you-begin">Before you begin</h2><p>Kubernetes v1.34 includes cluster-level API support for
storage capacity tracking. To use this you must also be using a CSI driver that
supports capacity tracking. Consult the documentation for the CSI drivers that
you use to find out whether this support is available and, if so, how to use
it. If you are not running Kubernetes v1.34, check the
documentation for that version of Kubernetes.</p><h2 id="api">API</h2><p>There are two API extensions for this feature:</p><ul><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1/">CSIStorageCapacity</a> objects:
these get produced by a CSI driver in the namespace
where the driver is installed. Each object contains capacity
information for one storage class and defines which nodes have
access to that storage.</li><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/#CSIDriverSpec">The <code>CSIDriverSpec.StorageCapacity</code> field</a>:
when set to <code>true</code>, the Kubernetes scheduler will consider storage
capacity for volumes that use the CSI driver.</li></ul><h2 id="scheduling">Scheduling</h2><p>Storage capacity information is used by the Kubernetes scheduler if:</p><ul><li>a Pod uses a volume that has not been created yet,</li><li>that volume uses a <a class="glossary-tooltip" title="A StorageClass provides a way for administrators to describe different available storage types." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/storage-classes" target="_blank" aria-label="StorageClass">StorageClass</a> which references a CSI driver and
uses <code>WaitForFirstConsumer</code> <a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">volume binding
mode</a>,
and</li><li>the <code>CSIDriver</code> object for the driver has <code>StorageCapacity</code> set to
true.</li></ul><p>In that case, the scheduler only considers nodes for the Pod which
have enough storage available to them. This check is very
simplistic and only compares the size of the volume against the
capacity listed in <code>CSIStorageCapacity</code> objects with a topology that
includes the node.</p><p>For volumes with <code>Immediate</code> volume binding mode, the storage driver
decides where to create the volume, independently of Pods that will
use the volume. The scheduler then schedules Pods onto nodes where the
volume is available after the volume has been created.</p><p>For <a href="/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">CSI ephemeral volumes</a>,
scheduling always happens without considering storage capacity. This
is based on the assumption that this volume type is only used by
special CSI drivers which are local to a node and do not need
significant resources there.</p><h2 id="rescheduling">Rescheduling</h2><p>When a node has been selected for a Pod with <code>WaitForFirstConsumer</code>
volumes, that decision is still tentative. The next step is that the
CSI storage driver gets asked to create the volume with a hint that the
volume is supposed to be available on the selected node.</p><p>Because Kubernetes might have chosen a node based on out-dated
capacity information, it is possible that the volume cannot really be
created. The node selection is then reset and the Kubernetes scheduler
tries again to find a node for the Pod.</p><h2 id="limitations">Limitations</h2><p>Storage capacity tracking increases the chance that scheduling works
on the first try, but cannot guarantee this because the scheduler has
to decide based on potentially out-dated information. Usually, the
same retry mechanism as for scheduling without any storage capacity
information handles scheduling failures.</p><p>One situation where scheduling can fail permanently is when a Pod uses
multiple volumes: one volume might have been created already in a
topology segment which then does not have enough capacity left for
another volume. Manual intervention is necessary to recover from this,
for example by increasing capacity or deleting the volume that was
already created.</p><h2 id="what-s-next">What's next</h2><ul><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md">Storage Capacity Constraints for Pod Scheduling KEP</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Kubernetes Scheduler</h1><p>In Kubernetes, <em>scheduling</em> refers to making sure that <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>
are matched to <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Nodes">Nodes</a> so that
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="Kubelet">Kubelet</a> can run them.</p><h2 id="scheduling">Scheduling overview</h2><p>A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.</p><p>If you want to understand why Pods are placed onto a particular Node,
or if you're planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.</p><h2 id="kube-scheduler">kube-scheduler</h2><p><a href="/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a>
is the default scheduler for Kubernetes and runs as part of the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.</p><p>Kube-scheduler selects an optimal node to run newly created or not yet
scheduled (unscheduled) pods. Since containers in pods - and pods themselves -
can have different requirements, the scheduler filters out any nodes that
don't meet a Pod's specific scheduling needs. Alternatively, the API lets
you specify a node for a Pod when you create it, but this is unusual
and is only done in special cases.</p><p>In a cluster, Nodes that meet the scheduling requirements for a Pod
are called <em>feasible</em> nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.</p><p>The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called <em>binding</em>.</p><p>Factors that need to be taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.</p><h3 id="kube-scheduler-implementation">Node selection in kube-scheduler</h3><p>kube-scheduler selects a node for the pod in a 2-step operation:</p><ol><li>Filtering</li><li>Scoring</li></ol><p>The <em>filtering</em> step finds the set of Nodes where it's feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resources to meet a Pod's specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn't (yet) schedulable.</p><p>In the <em>scoring</em> step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.</p><p>Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.</p><p>There are two supported ways to configure the filtering and scoring behavior
of the scheduler:</p><ol><li><a href="/docs/reference/scheduling/policies/">Scheduling Policies</a> allow you to configure <em>Predicates</em> for filtering and <em>Priorities</em> for scoring.</li><li><a href="/docs/reference/scheduling/config/#profiles">Scheduling Profiles</a> allow you to configure Plugins that implement different scheduling stages, including: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, and others. You can also configure the kube-scheduler to run different profiles.</li></ol><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/scheduling-eviction/scheduler-perf-tuning/">scheduler performance tuning</a></li><li>Read about <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a></li><li>Read the <a href="/docs/reference/command-line-tools-reference/kube-scheduler/">reference documentation</a> for kube-scheduler</li><li>Read the <a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler config (v1)</a> reference</li><li>Learn about <a href="/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">configuring multiple schedulers</a></li><li>Learn about <a href="/docs/tasks/administer-cluster/topology-manager/">topology management policies</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-overhead/">Pod Overhead</a></li><li>Learn about scheduling of Pods that use volumes in:<ul><li><a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">Volume Topology Support</a></li><li><a href="/docs/concepts/storage/storage-capacity/">Storage Capacity Tracking</a></li><li><a href="/docs/concepts/storage/storage-limits/">Node-specific Volume Limits</a></li></ul></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Node-specific Volume Limits</h1><p>This page describes the maximum number of volumes that can be attached
to a Node for various cloud providers.</p><p>Cloud providers like Google, Amazon, and Microsoft typically have a limit on
how many volumes can be attached to a Node. It is important for Kubernetes to
respect those limits. Otherwise, Pods scheduled on a Node could get stuck
waiting for volumes to attach.</p><h2 id="kubernetes-default-limits">Kubernetes default limits</h2><p>The Kubernetes scheduler has default limits on the number of volumes
that can be attached to a Node:</p><table><tr><th>Cloud service</th><th>Maximum volumes per Node</th></tr><tr><td><a href="https://aws.amazon.com/ebs/">Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr><tr><td><a href="https://cloud.google.com/persistent-disk/">Google Persistent Disk</a></td><td>16</td></tr><tr><td><a href="https://azure.microsoft.com/en-us/services/storage/main-disks/">Microsoft Azure Disk Storage</a></td><td>16</td></tr></table><h2 id="dynamic-volume-limits">Dynamic volume limits</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>Dynamic volume limits are supported for following volume types.</p><ul><li>Amazon EBS</li><li>Google Persistent Disk</li><li>Azure Disk</li><li>CSI</li></ul><p>For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node
type and enforces the appropriate maximum number of volumes for the node. For example:</p><ul><li><p>On
<a href="https://cloud.google.com/compute/">Google Compute Engine</a>,
up to 127 volumes can be attached to a node, <a href="https://cloud.google.com/compute/docs/disks/#pdnumberlimits">depending on the node
type</a>.</p></li><li><p>For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25
volumes to be attached to a Node. For other instance types on
<a href="https://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud (EC2)</a>,
Kubernetes allows 39 volumes to be attached to a Node.</p></li><li><p>On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes">Sizes for virtual machines in Azure</a>.</p></li><li><p>If a CSI storage driver advertises a maximum number of volumes for a Node (using <code>NodeGetInfo</code>), the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a> honors that limit.
Refer to the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo">CSI specifications</a> for details.</p></li><li><p>For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.</p></li></ul><h3 id="mutable-csi-node-allocatable-count">Mutable CSI Node Allocatable Count</h3><div class="feature-state-notice feature-beta" title="Feature Gate: MutableCSINodeAllocatableCount"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: false)</div><p>CSI drivers can dynamically adjust the maximum number of volumes that can be attached to a Node at runtime. This enhances scheduling accuracy and reduces pod scheduling failures due to changes in resource availability.</p><p>To use this feature, you must enable the <code>MutableCSINodeAllocatableCount</code> feature gate on the following components:</p><ul><li><code>kube-apiserver</code></li><li><code>kubelet</code></li></ul><h4 id="periodic-updates">Periodic Updates</h4><p>When enabled, CSI drivers can request periodic updates to their volume limits by setting the <code>nodeAllocatableUpdatePeriodSeconds</code> field in the <code>CSIDriver</code> specification. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>CSIDriver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeAllocatableUpdatePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Kubelet will periodically call the corresponding CSI driverâ€™s <code>NodeGetInfo</code> endpoint to refresh the maximum number of attachable volumes, using the interval specified in <code>nodeAllocatableUpdatePeriodSeconds</code>. The minimum allowed value for this field is 10 seconds.</p><p>If a volume attachment operation fails with a <code>ResourceExhausted</code> error (gRPC code 8), Kubernetes triggers an immediate update to the allocatable volume count for that Node. Additionally, kubelet marks affected pods as Failed, allowing their controllers to handle recreation. This prevents pods from getting stuck indefinitely in the <code>ContainerCreating</code> state.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Ephemeral Volumes</h1><p>This document describes <em>ephemeral volumes</em> in Kubernetes. Familiarity
with <a href="/docs/concepts/storage/volumes/">volumes</a> is suggested, in
particular PersistentVolumeClaim and PersistentVolume.</p><p>Some applications need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance.</p><p>Other applications expect some read-only input data to be present in
files, like configuration data or secret keys.</p><p><em>Ephemeral volumes</em> are designed for these use cases. Because volumes
follow the Pod's lifetime and get created and deleted along with the
Pod, Pods can be stopped and restarted without being limited to where
some persistent volume is available.</p><p>Ephemeral volumes are specified <em>inline</em> in the Pod spec, which
simplifies application deployment and management.</p><h3 id="types-of-ephemeral-volumes">Types of ephemeral volumes</h3><p>Kubernetes supports several different kinds of ephemeral volumes for
different purposes:</p><ul><li><a href="/docs/concepts/storage/volumes/#emptydir">emptyDir</a>: empty at Pod startup,
with storage coming locally from the kubelet base directory (usually
the root disk) or RAM</li><li><a href="/docs/concepts/storage/volumes/#configmap">configMap</a>,
<a href="/docs/concepts/storage/volumes/#downwardapi">downwardAPI</a>,
<a href="/docs/concepts/storage/volumes/#secret">secret</a>: inject different
kinds of Kubernetes data into a Pod</li><li><a href="/docs/concepts/storage/volumes/#image">image</a>: allows mounting container image files or artifacts,
directly to a Pod.</li><li><a href="#csi-ephemeral-volumes">CSI ephemeral volumes</a>:
similar to the previous volume kinds, but provided by special <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> drivers
which specifically <a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">support this feature</a></li><li><a href="#generic-ephemeral-volumes">generic ephemeral volumes</a>, which
can be provided by all storage drivers that also support persistent volumes</li></ul><p><code>emptyDir</code>, <code>configMap</code>, <code>downwardAPI</code>, <code>secret</code> are provided as
<a href="/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local ephemeral
storage</a>.
They are managed by kubelet on each node.</p><p>CSI ephemeral volumes <em>must</em> be provided by third-party CSI storage
drivers.</p><p>Generic ephemeral volumes <em>can</em> be provided by third-party CSI storage
drivers, but also by any other storage driver that supports dynamic
provisioning. Some CSI drivers are written specifically for CSI
ephemeral volumes and do not support dynamic provisioning: those then
cannot be used for generic ephemeral volumes.</p><p>The advantage of using third-party drivers is that they can offer
functionality that Kubernetes itself does not support, for example
storage with different performance characteristics than the disk that
is managed by kubelet, or injecting different data.</p><h3 id="csi-ephemeral-volumes">CSI ephemeral volumes</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI <a href="https://kubernetes-csi.github.io/docs/drivers.html">Drivers list</a>
shows which drivers support ephemeral volumes.</div><p>Conceptually, CSI ephemeral volumes are similar to <code>configMap</code>,
<code>downwardAPI</code> and <code>secret</code> volume types: the storage is managed locally on each
node and is created together with other local resources after a Pod has been
scheduled onto a node. Kubernetes has no concept of rescheduling Pods
anymore at this stage. Volume creation has to be unlikely to fail,
otherwise Pod startup gets stuck. In particular, <a href="/docs/concepts/storage/storage-capacity/">storage capacity
aware Pod scheduling</a> is <em>not</em>
supported for these volumes. They are currently also not covered by
the storage resource usage limits of a Pod, because that is something
that kubelet can only enforce for storage that it manages itself.</p><p>Here's an example manifest for a Pod that uses CSI ephemeral storage:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-csi-app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/data"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-csi-inline-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"1000000"</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-csi-inline-vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">csi</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>inline.storage.kubernetes.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">volumeAttributes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>volumeAttributes</code> determine what volume is prepared by the
driver. These attributes are specific to each driver and not
standardized. See the documentation of each CSI driver for further
instructions.</p><h3 id="csi-driver-restrictions">CSI driver restrictions</h3><p>CSI ephemeral volumes allow users to provide <code>volumeAttributes</code>
directly to the CSI driver as part of the Pod spec. A CSI driver
allowing <code>volumeAttributes</code> that are typically restricted to
administrators is NOT suitable for use in an inline ephemeral volume.
For example, parameters that are normally defined in the StorageClass
should not be exposed to users through the use of inline ephemeral volumes.</p><p>Cluster administrators who need to restrict the CSI drivers that are
allowed to be used as inline volumes within a Pod spec may do so by:</p><ul><li>Removing <code>Ephemeral</code> from <code>volumeLifecycleModes</code> in the CSIDriver spec, which prevents the
driver from being used as an inline ephemeral volume.</li><li>Using an <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>
to restrict how this driver is used.</li></ul><h3 id="generic-ephemeral-volumes">Generic ephemeral volumes</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>Generic ephemeral volumes are similar to <code>emptyDir</code> volumes in the
sense that they provide a per-pod directory for scratch data that is
usually empty after provisioning. But they may also have additional
features:</p><ul><li>Storage can be local or network-attached.</li><li>Volumes can have a fixed size that Pods are not able to exceed.</li><li>Volumes may have some initial data, depending on the driver and
parameters.</li><li>Typical operations on volumes are supported assuming that the driver
supports them, including
<a href="/docs/concepts/storage/volume-snapshots/">snapshotting</a>,
<a href="/docs/concepts/storage/volume-pvc-datasource/">cloning</a>,
<a href="/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">resizing</a>,
and <a href="/docs/concepts/storage/storage-capacity/">storage capacity tracking</a>.</li></ul><p>Example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/scratch"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>scratch-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"1000000"</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>scratch-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">ephemeral</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">volumeClaimTemplate</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>my-frontend-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">"ReadWriteOnce"</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">"scratch-storage-class"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="lifecycle-and-persistentvolumeclaim">Lifecycle and PersistentVolumeClaim</h3><p>The key design idea is that the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#ephemeralvolumesource-v1-core">parameters for a volume claim</a>
are allowed inside a volume source of the Pod. Labels, annotations and
the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets
created, the ephemeral volume controller then creates an actual PersistentVolumeClaim
object in the same namespace as the Pod and ensures that the PersistentVolumeClaim
gets deleted when the Pod gets deleted.</p><p>That triggers volume binding and/or provisioning, either immediately if
the <a class="glossary-tooltip" title="A StorageClass provides a way for administrators to describe different available storage types." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/storage-classes" target="_blank" aria-label="StorageClass">StorageClass</a> uses immediate volume binding or when the Pod is
tentatively scheduled onto a node (<code>WaitForFirstConsumer</code> volume
binding mode). The latter is recommended for generic ephemeral volumes
because then the scheduler is free to choose a suitable node for
the Pod. With immediate binding, the scheduler is forced to select a node that has
access to the volume once it is available.</p><p>In terms of <a href="/docs/concepts/architecture/garbage-collection/#owners-dependents">resource ownership</a>,
a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)
that provide that ephemeral storage. When the Pod is deleted,
the Kubernetes garbage collector deletes the PVC, which then usually
triggers deletion of the volume because the default reclaim policy of
storage classes is to delete volumes. You can create quasi-ephemeral local storage
using a StorageClass with a reclaim policy of <code>retain</code>: the storage outlives the Pod,
and in this case you need to ensure that volume clean up happens separately.</p><p>While these PVCs exist, they can be used like any other PVC. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.</p><h3 id="persistentvolumeclaim-naming">PersistentVolumeClaim naming</h3><p>Naming of the automatically created PVCs is deterministic: the name is
a combination of the Pod name and volume name, with a hyphen (<code>-</code>) in the
middle. In the example above, the PVC name will be
<code>my-app-scratch-volume</code>. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known.</p><p>The deterministic naming also introduces a potential conflict between different
Pods (a Pod "pod-a" with volume "scratch" and another Pod with name
"pod" and volume "a-scratch" both end up with the same PVC name
"pod-a-scratch") and between Pods and manually created PVCs.</p><p>Such conflicts are detected: a PVC is only used for an ephemeral
volume if it was created for the Pod. This check is based on the
ownership relationship. An existing PVC is not overwritten or
modified. But this does not resolve the conflict because without the
right PVC, the Pod cannot start.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Take care when naming Pods and volumes inside the
same namespace, so that these conflicts can't occur.</div><h3 id="security">Security</h3><p>Using generic ephemeral volumes allows users to create PVCs indirectly
if they can create Pods, even if they do not have permission to create PVCs directly.
Cluster administrators must be aware of this. If this does not fit their security model,
they should use an <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>
that rejects objects like Pods that have a generic ephemeral volume.</p><p>The normal <a href="/docs/concepts/policy/resource-quotas/#storage-resource-quota">namespace quota for PVCs</a>
still applies, so even if users are allowed to use this new mechanism, they cannot use
it to circumvent other policies.</p><h2 id="what-s-next">What's next</h2><h3 id="ephemeral-volumes-managed-by-kubelet">Ephemeral volumes managed by kubelet</h3><p>See <a href="/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local ephemeral storage</a>.</p><h3 id="csi-ephemeral-volumes-1">CSI ephemeral volumes</h3><ul><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md">Ephemeral Inline CSI volumes KEP</a>.</li><li>For more information on further development of this feature, see the
<a href="https://github.com/kubernetes/enhancements/issues/596">enhancement tracking issue #596</a>.</li></ul><h3 id="generic-ephemeral-volumes-1">Generic ephemeral volumes</h3><ul><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md">Generic ephemeral inline volumes KEP</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Application Security Checklist</h1><div class="lead">Baseline guidelines around ensuring application security on Kubernetes, aimed at application developers</div><p>This checklist aims to provide basic guidelines on securing applications
running in Kubernetes from a developer's perspective.
This list is not meant to be exhaustive and is intended to evolve over time.</p><p>On how to read and use this document:</p><ul><li>The order of topics does not reflect an order of priority.</li><li>Some checklist items are detailed in the paragraph below the list of each section.</li><li>This checklist assumes that a <code>developer</code> is a Kubernetes cluster user who
interacts with namespaced scope objects.</li></ul><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Checklists are <strong>not</strong> sufficient for attaining a good security posture on their own.
A good security posture requires constant attention and improvement, but a checklist
can be the first step on the never-ending journey towards security preparedness.
Some recommendations in this checklist may be too restrictive or too lax for
your specific security needs. Since Kubernetes security is not "one size fits all",
each category of checklist items should be evaluated on its merits.</div><h2 id="base-security-hardening">Base security hardening</h2><p>The following checklist provides base security hardening recommendations that
would apply to most applications deploying to Kubernetes.</p><h3 id="application-design">Application design</h3><ul><li><input disabled="" type="checkbox"/> Follow the right
<a href="https://www.cncf.io/wp-content/uploads/2022/06/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">security principles</a>
when designing applications.</li><li><input disabled="" type="checkbox"/> Application configured with appropriate <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/pod-qos/" target="_blank" aria-label="QoS class">QoS class</a>
through resource request and limits.<ul><li><input disabled="" type="checkbox"/> Memory limit is set for the workloads with a limit equal to or greater than the request.</li><li><input disabled="" type="checkbox"/> CPU limit might be set on sensitive workloads.</li></ul></li></ul><h3 id="service-account">Service account</h3><ul><li><input disabled="" type="checkbox"/> Avoid using the <code>default</code> ServiceAccount. Instead, create ServiceAccounts for
each workload or microservice.</li><li><input disabled="" type="checkbox"/> <code>automountServiceAccountToken</code> should be set to <code>false</code> unless the pod
specifically requires access to the Kubernetes API to operate.</li></ul><h3 id="security-context-pod">Pod-level <code>securityContext</code> recommendations</h3><ul><li><input disabled="" type="checkbox"/> Set <code>runAsNonRoot: true</code>.</li><li><input disabled="" type="checkbox"/> Configure the container to execute as a less privileged user
(for example, using <code>runAsUser</code> and <code>runAsGroup</code>), and configure appropriate
permissions on files or directories inside the container image.</li><li><input disabled="" type="checkbox"/> Optionally add a supplementary group with <code>fsGroup</code> to access persistent volumes.</li><li><input disabled="" type="checkbox"/> The application deploys into a namespace that enforces an appropriate
<a href="/docs/concepts/security/pod-security-standards/">Pod security standard</a>.
If you cannot control this enforcement for the cluster(s) where the application is
deployed, take this into account either through documentation or additional defense in depth.</li></ul><h3 id="security-context-container">Container-level <code>securityContext</code> recommendations</h3><ul><li><input disabled="" type="checkbox"/> Disable privilege escalations using <code>allowPrivilegeEscalation: false</code>.</li><li><input disabled="" type="checkbox"/> Configure the root filesystem to be read-only with <code>readOnlyRootFilesystem: true</code>.</li><li><input disabled="" type="checkbox"/> Avoid running privileged containers (set <code>privileged: false</code>).</li><li><input disabled="" type="checkbox"/> Drop all capabilities from the containers and add back only specific ones
that are needed for operation of the container.</li></ul><h3 id="rbac">Role Based Access Control (RBAC)</h3><ul><li><input disabled="" type="checkbox"/> Permissions such as <strong>create</strong>, <strong>patch</strong>, <strong>update</strong> and <strong>delete</strong>
should be only granted if necessary.</li><li><input disabled="" type="checkbox"/> Avoid creating RBAC permissions to create or update roles which can lead to
<a href="/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping">privilege escalation</a>.</li><li><input disabled="" type="checkbox"/> Review bindings for the <code>system:unauthenticated</code> group and remove them where
possible, as this gives access to anyone who can contact the API server at a network level.</li></ul><p>The <strong>create</strong>, <strong>update</strong> and <strong>delete</strong> verbs should be permitted judiciously.
The <strong>patch</strong> verb if allowed on a Namespace can
<a href="/docs/concepts/security/rbac-good-practices/#namespace-modification">allow users to update labels on the namespace or deployments</a>
which can increase the attack surface.</p><p>For sensitive workloads, consider providing a recommended ValidatingAdmissionPolicy
that further restricts the permitted write actions.</p><h3 id="image-security">Image security</h3><ul><li><input disabled="" type="checkbox"/> Using an image scanning tool to scan an image before deploying containers in the Kubernetes cluster.</li><li><input disabled="" type="checkbox"/> Use container signing to validate the container image signature before deploying to the Kubernetes cluster.</li></ul><h3 id="network-policies">Network policies</h3><ul><li><input disabled="" type="checkbox"/> Configure <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicies</a>
to only allow expected ingress and egress traffic from the pods.</li></ul><p>Make sure that your cluster provides and enforces NetworkPolicy.
If you are writing an application that users will deploy to different clusters,
consider whether you can assume that NetworkPolicy is available and enforced.</p><h2 id="advanced">Advanced security hardening</h2><p>This section of this guide covers some advanced security hardening points
which might be valuable based on different Kubernetes environment setup.</p><h3 id="linux-container-security">Linux container security</h3><p>Configure <a class="glossary-tooltip" title="The securityContext field defines privilege and access control settings for a Pod or container." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/security-context/" target="_blank" aria-label="Security Context">Security Context</a>
for the pod-container.</p><ul><li><input disabled="" type="checkbox"/> <a href="/docs/tasks/configure-pod-container/security-context/#set-the-seccomp-profile-for-a-container">Set the Seccomp Profile for a Container</a>.</li><li><input disabled="" type="checkbox"/> <a href="/docs/tutorials/security/apparmor/">Restrict a Container's Access to Resources with AppArmor</a>.</li><li><input disabled="" type="checkbox"/> <a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">Assign SELinux Labels to a Container</a>.</li></ul><h3 id="runtime-classes">Runtime classes</h3><ul><li><input disabled="" type="checkbox"/> Configure appropriate runtime classes for containers.</li></ul><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Some containers may require a different isolation level from what is provided by
the default runtime of the cluster. <code>runtimeClassName</code> can be used in a podspec
to define a different runtime class.</p><p>For sensitive workloads consider using kernel emulation tools like
<a href="https://gvisor.dev/docs/">gVisor</a>, or virtualized isolation using a mechanism
such as <a href="https://katacontainers.io/">kata-containers</a>.</p><p>In high trust environments, consider using
<a href="/blog/2023/07/06/confidential-kubernetes/">confidential virtual machines</a>
to improve cluster security even further.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Windows Storage</h1><p>This page provides an storage overview specific to the Windows operating system.</p><h2 id="storage">Persistent storage</h2><p>Windows has a layered filesystem driver to mount container layers and create a copy
filesystem based on NTFS. All file paths in the container are resolved only within
the context of that container.</p><ul><li>With Docker, volume mounts can only target a directory in the container, and not
an individual file. This limitation does not apply to containerd.</li><li>Volume mounts cannot project files or directories back to the host filesystem.</li><li>Read-only filesystems are not supported because write access is always required
for the Windows registry and SAM database. However, read-only volumes are supported.</li><li>Volume user-masks and permissions are not available. Because the SAM is not shared
between the host &amp; container, there's no mapping between them. All permissions are
resolved within the context of the container.</li></ul><p>As a result, the following storage functionality is not supported on Windows nodes:</p><ul><li>Volume subpath mounts: only the entire volume can be mounted in a Windows container</li><li>Subpath volume mounting for Secrets</li><li>Host mount projection</li><li>Read-only root filesystem (mapped volumes still support <code>readOnly</code>)</li><li>Block device mapping</li><li>Memory as the storage medium (for example, <code>emptyDir.medium</code> set to <code>Memory</code>)</li><li>File system features like uid/gid; per-user Linux filesystem permissions</li><li>Setting <a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#set-posix-permissions-for-secret-keys">secret permissions with DefaultMode</a> (due to UID/GID dependency)</li><li>NFS based storage/volume support</li><li>Expanding the mounted volume (resizefs)</li></ul><p>Kubernetes <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volumes">volumes</a> enable complex
applications, with data persistence and Pod volume sharing requirements, to be deployed
on Kubernetes. Management of persistent volumes associated with a specific storage
back-end or protocol includes actions such as provisioning/de-provisioning/resizing
of volumes, attaching/detaching a volume to/from a Kubernetes node and
mounting/dismounting a volume to/from individual containers in a pod that needs to
persist data.</p><p>Volume management components are shipped as Kubernetes volume
<a href="/docs/concepts/storage/volumes/#volume-types">plugin</a>.
The following broad classes of Kubernetes volume plugins are supported on Windows:</p><ul><li><a href="/docs/concepts/storage/volumes/#flexvolume"><code>FlexVolume plugins</code></a><ul><li>Please note that FlexVolumes have been deprecated as of 1.23</li></ul></li><li><a href="/docs/concepts/storage/volumes/#csi"><code>CSI Plugins</code></a></li></ul><h5 id="in-tree-volume-plugins">In-tree volume plugins</h5><p>The following in-tree plugins support persistent storage on Windows nodes:</p><ul><li><a href="/docs/concepts/storage/volumes/#azurefile"><code>azureFile</code></a></li><li><a href="/docs/concepts/storage/volumes/#vspherevolume"><code>vsphereVolume</code></a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Scheduling Readiness</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [stable]</code></div><p>Pods were considered ready for scheduling once created. Kubernetes scheduler
does its due diligence to find nodes to place all pending Pods. However, in a
real-world case, some Pods may stay in a "miss-essential-resources" state for a long period.
These Pods actually churn the scheduler (and downstream integrators like Cluster AutoScaler)
in an unnecessary manner.</p><p>By specifying/removing a Pod's <code>.spec.schedulingGates</code>, you can control when a Pod is ready
to be considered for scheduling.</p><h2 id="configuring-pod-schedulinggates">Configuring Pod schedulingGates</h2><p>The <code>schedulingGates</code> field contains a list of strings, and each string literal is perceived as a
criteria that Pod should be satisfied before considered schedulable. This field can be initialized
only when a Pod is created (either by the client, or mutated during admission). After creation,
each schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is disallowed.</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNplkktTwyAUhf8KgzuHWpukaYszutGlK3caFxQuCVMCGSDVTKf_XfKyPlhxz4HDB9wT5lYAptgHFuBRsdKxenFMClMYFIdfUdRYgbiD6ItJTEbR8wpEq5UpUfnDTf-5cbPoJjcbXdcaE61RVJIiqJvQ_Y30D-OCt-t3tFjcR5wZayiVnIGmkv4NiEfX9jijKTmmRH5jf0sRugOP0HyHUc1m6KGMFP27cM28fwSJDluPpNKaXqVJzmFNfHD2APRKSjnNFx9KhIpmzSfhVls3eHdTRrwG8QnxKfEZUUNeYTDBNbiaKRF_5dSfX-BQQQ0FpnEqQLJWhwIX5hyXsjbYl85wTINrgeC2EZd_xFQy7b_VJ6GCdd-itkxALE84dE3fAqXyIUZya6Qqe711OspVCI2ny2Vv35QqVO3-htt66ZWomAvVcZcv8yTfsiSFfJOydZoKvl_ttjLJVlJsblcJw-czwQ0zr9ZeqGDgeR77b2jD8xdtjtDn"><img src="/docs/images/podSchedulingGates.svg" alt="pod-scheduling-gates-diagram"/></a><figcaption><p>Figure. Pod SchedulingGates</p></figcaption></figure><h2 id="usage-example">Usage example</h2><p>To mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates like this:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-scheduling-gates.yaml" download="pods/pod-with-scheduling-gates.yaml"><code>pods/pod-with-scheduling-gates.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-with-scheduling-gates-yaml&quot;)" title="Copy pods/pod-with-scheduling-gates.yaml to clipboard"/></div><div class="includecode" id="pods-pod-with-scheduling-gates-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">schedulingGates</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example.com/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example.com/bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.6<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>After the Pod's creation, you can check its state using:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pod test-pod
</span></span></code></pre></div><p>The output reveals it's in <code>SchedulingGated</code> state:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME       READY   STATUS            RESTARTS   AGE
test-pod   0/1     SchedulingGated   0          7s
</code></pre><p>You can also check its <code>schedulingGates</code> field by running:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pod test-pod -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.spec.schedulingGates}'</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code class="language-none" data-lang="none">[{"name":"example.com/foo"},{"name":"example.com/bar"}]
</code></pre><p>To inform scheduler this Pod is ready for scheduling, you can remove its <code>schedulingGates</code> entirely
by reapplying a modified manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-without-scheduling-gates.yaml" download="pods/pod-without-scheduling-gates.yaml"><code>pods/pod-without-scheduling-gates.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-without-scheduling-gates-yaml&quot;)" title="Copy pods/pod-without-scheduling-gates.yaml to clipboard"/></div><div class="includecode" id="pods-pod-without-scheduling-gates-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.6<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>You can check if the <code>schedulingGates</code> is cleared by running:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pod test-pod -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.spec.schedulingGates}'</span>
</span></span></code></pre></div><p>The output is expected to be empty. And you can check its latest status by running:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pod test-pod -o wide
</span></span></code></pre></div><p>Given the test-pod doesn't request any CPU/memory resources, it's expected that this Pod's state get
transited from previous <code>SchedulingGated</code> to <code>Running</code>:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME       READY   STATUS    RESTARTS   AGE   IP         NODE
test-pod   1/1     Running   0          15s   10.0.0.4   node-2
</code></pre><h2 id="observability">Observability</h2><p>The metric <code>scheduler_pending_pods</code> comes with a new label <code>"gated"</code> to distinguish whether a Pod
has been tried scheduling but claimed as unschedulable, or explicitly marked as not ready for
scheduling. You can use <code>scheduler_pending_pods{queue="gated"}</code> to check the metric result.</p><h2 id="mutable-pod-scheduling-directives">Mutable Pod scheduling directives</h2><p>You can mutate scheduling directives of Pods while they have scheduling gates, with certain constraints.
At a high level, you can only tighten the scheduling directives of a Pod. In other words, the updated
directives would cause the Pods to only be able to be scheduled on a subset of the nodes that it would
previously match. More concretely, the rules for updating a Pod's scheduling directives are as follows:</p><ol><li><p>For <code>.spec.nodeSelector</code>, only additions are allowed. If absent, it will be allowed to be set.</p></li><li><p>For <code>spec.affinity.nodeAffinity</code>, if nil, then setting anything is allowed.</p></li><li><p>If <code>NodeSelectorTerms</code> was empty, it will be allowed to be set.
If not empty, then only additions of <code>NodeSelectorRequirements</code> to <code>matchExpressions</code>
or <code>fieldExpressions</code> are allowed, and no changes to existing <code>matchExpressions</code>
and <code>fieldExpressions</code> will be allowed. This is because the terms in
<code>.requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms</code>, are ORed
while the expressions in <code>nodeSelectorTerms[].matchExpressions</code> and
<code>nodeSelectorTerms[].fieldExpressions</code> are ANDed.</p></li><li><p>For <code>.preferredDuringSchedulingIgnoredDuringExecution</code>, all updates are allowed.
This is because preferred terms are not authoritative, and so policy controllers
don't validate those terms.</p></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/3521-pod-scheduling-readiness">PodSchedulingReadiness KEP</a> for more details</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Cloud Native Security and Kubernetes</h1><div class="lead">Concepts for keeping your cloud-native workload secure.</div><p>Kubernetes is based on a cloud-native architecture, and draws on advice from the
<a class="glossary-tooltip" title="Cloud Native Computing Foundation" data-toggle="tooltip" data-placement="top" href="https://cncf.io/" target="_blank" aria-label="CNCF">CNCF</a> about good practice for
cloud native information security.</p><p>Read on through this page for an overview of how Kubernetes is designed to
help you deploy a secure cloud native platform.</p><h2 id="cloud-native-information-security">Cloud native information security</h2><p>The CNCF <a href="https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">white paper</a>
on cloud native security defines security controls and practices that are
appropriate to different <em>lifecycle phases</em>.</p><h2 id="lifecycle-phase-develop"><em>Develop</em> lifecycle phase</h2><ul><li>Ensure the integrity of development environments.</li><li>Design applications following good practice for information security,
appropriate for your context.</li><li>Consider end user security as part of solution design.</li></ul><p>To achieve this, you can:</p><ol><li>Adopt an architecture, such as <a href="https://glossary.cncf.io/zero-trust-architecture/">zero trust</a>,
that minimizes attack surfaces, even for internal threats.</li><li>Define a code review process that considers security concerns.</li><li>Build a <em>threat model</em> of your system or application that identifies
trust boundaries. Use that to model to identify risks and to help find
ways to treat those risks.</li><li>Incorporate advanced security automation, such as <em>fuzzing</em> and
<a href="https://glossary.cncf.io/security-chaos-engineering/">security chaos engineering</a>,
where it's justified.</li></ol><h2 id="lifecycle-phase-distribute"><em>Distribute</em> lifecycle phase</h2><ul><li>Ensure the security of the supply chain for container images you execute.</li><li>Ensure the security of the supply chain for the cluster and other components
that execute your application. An example of another component might be an
external database that your cloud-native application uses for persistence.</li></ul><p>To achieve this, you can:</p><ol><li>Scan container images and other artifacts for known vulnerabilities.</li><li>Ensure that software distribution uses encryption in transit, with
a chain of trust for the software source.</li><li>Adopt and follow processes to update dependencies when updates are
available, especially in response to security announcements.</li><li>Use validation mechanisms such as digital certificates for supply
chain assurance.</li><li>Subscribe to feeds and other mechanisms to alert you to security
risks.</li><li>Restrict access to artifacts. Place container images in a
<a href="/docs/concepts/containers/images/#using-a-private-registry">private registry</a>
that only allows authorized clients to pull images.</li></ol><h2 id="lifecycle-phase-deploy"><em>Deploy</em> lifecycle phase</h2><p>Ensure appropriate restrictions on what can be deployed, who can deploy it,
and where it can be deployed to.
You can enforce measures from the <em>distribute</em> phase, such as verifying the
cryptographic identity of container image artifacts.</p><p>You can deploy different applications and cluster components into different
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespaces">namespaces</a>. Containers
themselves, and namespaces, both provide isolation mechanisms that are
relevant to information security.</p><p>When you deploy Kubernetes, you also set the foundation for your
applications' runtime environment: a Kubernetes cluster (or
multiple clusters).
That IT infrastructure must provide the security guarantees that higher
layers expect.</p><h2 id="lifecycle-phase-runtime"><em>Runtime</em> lifecycle phase</h2><p>The Runtime phase comprises three critical areas: <a href="#protection-runtime-access">access</a>,
<a href="#protection-runtime-compute">compute</a>, and <a href="#protection-runtime-storage">storage</a>.</p><h3 id="protection-runtime-access">Runtime protection: access</h3><p>The Kubernetes API is what makes your cluster work. Protecting this API is key
to providing effective cluster security.</p><p>Other pages in the Kubernetes documentation have more detail about how to set up
specific aspects of access control. The <a href="/docs/concepts/security/security-checklist/">security checklist</a>
has a set of suggested basic checks for your cluster.</p><p>Beyond that, securing your cluster means implementing effective
<a href="/docs/concepts/security/controlling-access/#authentication">authentication</a> and
<a href="/docs/concepts/security/controlling-access/#authorization">authorization</a> for API access. Use <a href="/docs/concepts/security/service-accounts/">ServiceAccounts</a> to
provide and manage security identities for workloads and cluster
components.</p><p>Kubernetes uses TLS to protect API traffic; make sure to deploy the cluster using
TLS (including for traffic between nodes and the control plane), and protect the
encryption keys. If you use Kubernetes' own API for
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/#certificate-signing-requests">CertificateSigningRequests</a>,
pay special attention to restricting misuse there.</p><h3 id="protection-runtime-compute">Runtime protection: compute</h3><p><a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/" target="_blank" aria-label="Containers">Containers</a> provide two
things: isolation between different applications, and a mechanism to combine
those isolated applications to run on the same host computer. Those two
aspects, isolation and aggregation, mean that runtime security involves
identifying trade-offs and finding an appropriate balance.</p><p>Kubernetes relies on a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>
to actually set up and run containers. The Kubernetes project does
not recommend a specific container runtime and you should make sure that
the runtime(s) that you choose meet your information security needs.</p><p>To protect your compute at runtime, you can:</p><ol><li><p>Enforce <a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a>
for applications, to help ensure they run with only the necessary privileges.</p></li><li><p>Run a specialized operating system on your nodes that is designed specifically
for running containerized workloads. This is typically based on a read-only
operating system (<em>immutable image</em>) that provides only the services
essential for running containers.</p><p>Container-specific operating systems help to isolate system components and
present a reduced attack surface in case of a container escape.</p></li><li><p>Define <a href="/docs/concepts/policy/resource-quotas/">ResourceQuotas</a> to
fairly allocate shared resources, and use
mechanisms such as <a href="/docs/concepts/policy/limit-range/">LimitRanges</a>
to ensure that Pods specify their resource requirements.</p></li><li><p>Partition workloads across different nodes.
Use <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-isolation-restriction">node isolation</a>
mechanisms, either from Kubernetes itself or from the ecosystem, to ensure that
Pods with different trust contexts are run on separate sets of nodes.</p></li><li><p>Use a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>
that provides security restrictions.</p></li><li><p>On Linux nodes, use a Linux security module such as <a href="/docs/tutorials/security/apparmor/">AppArmor</a>
or <a href="/docs/tutorials/security/seccomp/">seccomp</a>.</p></li></ol><h3 id="protection-runtime-storage">Runtime protection: storage</h3><p>To protect storage for your cluster and the applications that run there, you can:</p><ol><li>Integrate your cluster with an external storage plugin that provides encryption at
rest for volumes.</li><li>Enable <a href="/docs/tasks/administer-cluster/encrypt-data/">encryption at rest</a> for
API objects.</li><li>Protect data durability using backups. Verify that you can restore these, whenever you need to.</li><li>Authenticate connections between cluster nodes and any network storage they rely
upon.</li><li>Implement data encryption within your own application.</li></ol><p>For encryption keys, generating these within specialized hardware provides
the best protection against disclosure risks. A <em>hardware security module</em>
can let you perform cryptographic operations without allowing the security
key to be copied elsewhere.</p><h3 id="networking-and-security">Networking and security</h3><p>You should also consider network security measures, such as
<a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> or a
<a href="https://glossary.cncf.io/service-mesh/">service mesh</a>.
Some network plugins for Kubernetes provide encryption for your
cluster network, using technologies such as a virtual
private network (VPN) overlay.
By design, Kubernetes lets you use your own networking plugin for your
cluster (if you use managed Kubernetes, the person or organization
managing your cluster may have chosen a network plugin for you).</p><p>The network plugin you choose and the way you integrate it can have a
strong impact on the security of information in transit.</p><h3 id="observability-and-runtime-security">Observability and runtime security</h3><p>Kubernetes lets you extend your cluster with extra tooling. You can set up third
party solutions to help you monitor or troubleshoot your applications and the
clusters they are running. You also get some basic observability features built
in to Kubernetes itself. Your code running in containers can generate logs,
publish metrics or provide other observability data; at deploy time, you need to
make sure your cluster provides an appropriate level of protection there.</p><p>If you set up a metrics dashboard or something similar, review the chain of components
that populate data into that dashboard, as well as the dashboard itself. Make sure
that the whole chain is designed with enough resilience and enough integrity protection
that you can rely on it even during an incident where your cluster might be degraded.</p><p>Where appropriate, deploy security measures below the level of Kubernetes
itself, such as cryptographically measured boot, or authenticated distribution
of time (which helps ensure the fidelity of logs and audit records).</p><p>For a high assurance environment, deploy cryptographic protections to ensure that
logs are both tamper-proof and confidential.</p><h2 id="what-s-next">What's next</h2><h3 id="further-reading-cloud-native">Cloud native security</h3><ul><li>CNCF <a href="https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">white paper</a>
on cloud native security.</li><li>CNCF <a href="https://github.com/cncf/tag-security/blob/f80844baaea22a358f5b20dca52cd6f72a32b066/supply-chain-security/supply-chain-security-paper/CNCF_SSCP_v1.pdf">white paper</a>
on good practices for securing a software supply chain.</li><li><a href="https://archive.fosdem.org/2020/schedule/event/kubernetes/">Fixing the Kubernetes clusterf**k: Understanding security from the kernel up</a> (FOSDEM 2020)</li><li><a href="https://www.youtube.com/watch?v=wqsUfvRyYpw">Kubernetes Security Best Practices</a> (Kubernetes Forum Seoul 2019)</li><li><a href="https://www.youtube.com/watch?v=EzSkU3Oecuw">Towards Measured Boot Out of the Box</a> (Linux Security Summit 2016)</li></ul><h3 id="further-reading-k8s">Kubernetes and information security</h3><ul><li><a href="/docs/concepts/security/">Kubernetes security</a></li><li><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing your cluster</a></li><li><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Data encryption in transit</a> for the control plane</li><li><a href="/docs/tasks/administer-cluster/encrypt-data/">Data encryption at rest</a></li><li><a href="/docs/concepts/configuration/secret/">Secrets in Kubernetes</a></li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a></li><li><a href="/docs/concepts/services-networking/network-policies/">Network policies</a> for Pods</li><li><a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a></li><li><a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Security</h1><div class="lead">Concepts for keeping your cloud-native workload secure.</div><p>This section of the Kubernetes documentation aims to help you learn to run
workloads more securely, and about the essential aspects of keeping a
Kubernetes cluster secure.</p><p>Kubernetes is based on a cloud-native architecture, and draws on advice from the
<a class="glossary-tooltip" title="Cloud Native Computing Foundation" data-toggle="tooltip" data-placement="top" href="https://cncf.io/" target="_blank" aria-label="CNCF">CNCF</a> about good practice for
cloud native information security.</p><p>Read <a href="/docs/concepts/security/cloud-native-security/">Cloud Native Security and Kubernetes</a>
for the broader context about how to secure your cluster and the applications that
you're running on it.</p><h2 id="security-mechanisms">Kubernetes security mechanisms</h2><p>Kubernetes includes several APIs and security controls, as well as ways to
define <a href="#policies">policies</a> that can form part of how you manage information security.</p><h3 id="control-plane-protection">Control plane protection</h3><p>A key security mechanism for any Kubernetes cluster is to
<a href="/docs/concepts/security/controlling-access/">control access to the Kubernetes API</a>.</p><p>Kubernetes expects you to configure and use TLS to provide
<a href="/docs/tasks/tls/managing-tls-in-a-cluster/">data encryption in transit</a>
within the control plane, and between the control plane and its clients.
You can also enable <a href="/docs/tasks/administer-cluster/encrypt-data/">encryption at rest</a>
for the data stored within Kubernetes control plane; this is separate from using
encryption at rest for your own workloads' data, which might also be a good idea.</p><h3 id="secrets">Secrets</h3><p>The <a href="/docs/concepts/configuration/secret/">Secret</a> API provides basic protection for
configuration values that require confidentiality.</p><h3 id="workload-protection">Workload protection</h3><p>Enforce <a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a> to
ensure that Pods and their containers are isolated appropriately. You can also use
<a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a> to define custom isolation
if you need it.</p><p><a href="/docs/concepts/services-networking/network-policies/">Network policies</a> let you control
network traffic between Pods, or between Pods and the network outside your cluster.</p><p>You can deploy security controls from the wider ecosystem to implement preventative
or detective controls around Pods, their containers, and the images that run in them.</p><h3 id="admission-control">Admission control</h3><p><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission controllers</a>
are plugins that intercept Kubernetes API requests and can validate or mutate
the requests based on specific fields in the request. Thoughtfully designing
these controllers helps to avoid unintended disruptions as Kubernetes APIs
change across version updates. For design considerations, see
<a href="/docs/concepts/cluster-administration/admission-webhooks-good-practices/">Admission Webhook Good Practices</a>.</p><h3 id="auditing">Auditing</h3><p>Kubernetes <a href="/docs/tasks/debug/debug-cluster/audit/">audit logging</a> provides a
security-relevant, chronological set of records documenting the sequence of actions
in a cluster. The cluster audits the activities generated by users, by applications
that use the Kubernetes API, and by the control plane itself.</p><h2 id="cloud-provider-security">Cloud provider security</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆItems on this page refer to vendors external to Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. To add a vendor, product or project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>If you are running a Kubernetes cluster on your own hardware or a different cloud provider,
consult your documentation for security best practices.
Here are links to some of the popular cloud providers' security documentation:</p><table><caption style="display:none">Cloud provider security</caption><thead><tr><th>IaaS Provider</th><th>Link</th></tr></thead><tbody><tr><td>Alibaba Cloud</td><td><a href="https://www.alibabacloud.com/trust-center">https://www.alibabacloud.com/trust-center</a></td></tr><tr><td>Amazon Web Services</td><td><a href="https://aws.amazon.com/security">https://aws.amazon.com/security</a></td></tr><tr><td>Google Cloud Platform</td><td><a href="https://cloud.google.com/security">https://cloud.google.com/security</a></td></tr><tr><td>Huawei Cloud</td><td><a href="https://www.huaweicloud.com/intl/en-us/securecenter/overallsafety">https://www.huaweicloud.com/intl/en-us/securecenter/overallsafety</a></td></tr><tr><td>IBM Cloud</td><td><a href="https://www.ibm.com/cloud/security">https://www.ibm.com/cloud/security</a></td></tr><tr><td>Microsoft Azure</td><td><a href="https://docs.microsoft.com/en-us/azure/security/azure-security">https://docs.microsoft.com/en-us/azure/security/azure-security</a></td></tr><tr><td>Oracle Cloud Infrastructure</td><td><a href="https://www.oracle.com/security">https://www.oracle.com/security</a></td></tr><tr><td>Tencent Cloud</td><td><a href="https://www.tencentcloud.com/solutions/data-security-and-information-protection">https://www.tencentcloud.com/solutions/data-security-and-information-protection</a></td></tr><tr><td>VMware vSphere</td><td><a href="https://www.vmware.com/solutions/security/hardening-guides">https://www.vmware.com/solutions/security/hardening-guides</a></td></tr></tbody></table><h2 id="policies">Policies</h2><p>You can define security policies using Kubernetes-native mechanisms,
such as <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a>
(declarative control over network packet filtering) or
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicy</a> (declarative restrictions on what changes
someone can make using the Kubernetes API).</p><p>However, you can also rely on policy implementations from the wider
ecosystem around Kubernetes. Kubernetes provides extension mechanisms
to let those ecosystem projects implement their own policy controls
on source code review, container image approval, API access controls,
networking, and more.</p><p>For more information about policy mechanisms and Kubernetes,
read <a href="/docs/concepts/policy/">Policies</a>.</p><h2 id="what-s-next">What's next</h2><p>Learn about related Kubernetes security topics:</p><ul><li><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing your cluster</a></li><li><a href="/docs/reference/issues-security/official-cve-feed/">Known vulnerabilities</a>
in Kubernetes (and links to further information)</li><li><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Data encryption in transit</a> for the control plane</li><li><a href="/docs/tasks/administer-cluster/encrypt-data/">Data encryption at rest</a></li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a></li><li><a href="/docs/concepts/services-networking/network-policies/">Network policies</a> for Pods</li><li><a href="/docs/concepts/configuration/secret/">Secrets in Kubernetes</a></li><li><a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a></li><li><a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a></li></ul><p>Learn the context:</p><ul><li><a href="/docs/concepts/security/cloud-native-security/">Cloud Native Security and Kubernetes</a></li></ul><p>Get certified:</p><ul><li><a href="https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/">Certified Kubernetes Security Specialist</a>
certification and official training course.</li></ul><p>Read more in this section:</p><div class="section-index"><ul><li><a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a></li><li><a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission</a></li><li><a href="/docs/concepts/security/service-accounts/">Service Accounts</a></li><li><a href="/docs/concepts/security/pod-security-policy/">Pod Security Policies</a></li><li><a href="/docs/concepts/security/linux-security/">Security For Linux Nodes</a></li><li><a href="/docs/concepts/security/windows-security/">Security For Windows Nodes</a></li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a></li><li><a href="/docs/concepts/security/rbac-good-practices/">Role Based Access Control Good Practices</a></li><li><a href="/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a></li><li><a href="/docs/concepts/security/multi-tenancy/">Multi-tenancy</a></li><li><a href="/docs/concepts/security/hardening-guide/authentication-mechanisms/">Hardening Guide - Authentication Mechanisms</a></li><li><a href="/docs/concepts/security/hardening-guide/scheduler/">Hardening Guide - Scheduler Configuration</a></li><li><a href="/docs/concepts/security/api-server-bypass-risks/">Kubernetes API Server Bypass Risks</a></li><li><a href="/docs/concepts/security/linux-kernel-security-constraints/">Linux kernel security constraints for Pods and containers</a></li><li><a href="/docs/concepts/security/security-checklist/">Security Checklist</a></li><li><a href="/docs/concepts/security/application-security-checklist/">Application Security Checklist</a></li></ul></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Storage Classes</h1><p>This document describes the concept of a StorageClass in Kubernetes. Familiarity
with <a href="/docs/concepts/storage/volumes/">volumes</a> and
<a href="/docs/concepts/storage/persistent-volumes/">persistent volumes</a> is suggested.</p><p>A StorageClass provides a way for administrators to describe the <em>classes</em> of
storage they offer. Different classes might map to quality-of-service levels,
or to backup policies, or to arbitrary policies determined by the cluster
administrators. Kubernetes itself is unopinionated about what classes
represent.</p><p>The Kubernetes concept of a storage class is similar to â€œprofilesâ€ in some other
storage system designs.</p><h2 id="storageclass-objects">StorageClass objects</h2><p>Each StorageClass contains the fields <code>provisioner</code>, <code>parameters</code>, and
<code>reclaimPolicy</code>, which are used when a PersistentVolume belonging to the
class needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).</p><p>The name of a StorageClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating StorageClass objects.</p><p>As an administrator, you can specify a default StorageClass that applies to any PVCs that
don't request a specific class. For more details, see the
<a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim concept</a>.</p><p>Here's an example of a StorageClass:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass-low-latency.yaml" download="storage/storageclass-low-latency.yaml"><code>storage/storageclass-low-latency.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-low-latency-yaml&quot;)" title="Copy storage/storageclass-low-latency.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-low-latency-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>low-latency<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">storageclass.kubernetes.io/is-default-class</span>:<span style="color:#bbb"> </span><span style="color:#b44">"false"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>csi-driver.example-vendor.example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">reclaimPolicy</span>:<span style="color:#bbb"> </span>Retain<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># default value is Delete</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">allowVolumeExpansion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">mountOptions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- discard<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># this might enable UNMAP / TRIM at the block storage layer</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">volumeBindingMode</span>:<span style="color:#bbb"> </span>WaitForFirstConsumer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">guaranteedReadWriteLatency</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># provider-specific</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="default-storageclass">Default StorageClass</h2><p>You can mark a StorageClass as the default for your cluster.
For instructions on setting the default StorageClass, see
<a href="/docs/tasks/administer-cluster/change-default-storage-class/">Change the default StorageClass</a>.</p><p>When a PVC does not specify a <code>storageClassName</code>, the default StorageClass is
used.</p><p>If you set the
<a href="/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class"><code>storageclass.kubernetes.io/is-default-class</code></a>
annotation to true on more than one StorageClass in your cluster, and you then
create a PersistentVolumeClaim with no <code>storageClassName</code> set, Kubernetes
uses the most recently created default StorageClass.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You should try to only have one StorageClass in your cluster that is
marked as the default. The reason that Kubernetes allows you to have
multiple default StorageClasses is to allow for seamless migration.</div><p>You can create a PersistentVolumeClaim without specifying a <code>storageClassName</code>
for the new PVC, and you can do so even when no default StorageClass exists
in your cluster. In this case, the new PVC creates as you defined it, and the
<code>storageClassName</code> of that PVC remains unset until a default becomes available.</p><p>You can have a cluster without any default StorageClass. If you don't mark any
StorageClass as default (and one hasn't been set for you by, for example, a cloud provider),
then Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need
it.</p><p>If or when a default StorageClass becomes available, the control plane identifies any
existing PVCs without <code>storageClassName</code>. For the PVCs that either have an empty
value for <code>storageClassName</code> or do not have this key, the control plane then
updates those PVCs to set <code>storageClassName</code> to match the new default StorageClass.
If you have an existing PVC where the <code>storageClassName</code> is <code>""</code>, and you configure
a default StorageClass, then this PVC will not get updated.</p><p>In order to keep binding to PVs with <code>storageClassName</code> set to <code>""</code>
(while a default StorageClass is present), you need to set the <code>storageClassName</code>
of the associated PVC to <code>""</code>.</p><h2 id="provisioner">Provisioner</h2><p>Each StorageClass has a provisioner that determines what volume plugin is used
for provisioning PVs. This field must be specified.</p><table><thead><tr><th style="text-align:left">Volume Plugin</th><th style="text-align:center">Internal Provisioner</th><th style="text-align:center">Config Example</th></tr></thead><tbody><tr><td style="text-align:left">AzureFile</td><td style="text-align:center">âœ“</td><td style="text-align:center"><a href="#azure-file">Azure File</a></td></tr><tr><td style="text-align:left">CephFS</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">FC</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">FlexVolume</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">iSCSI</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">Local</td><td style="text-align:center">-</td><td style="text-align:center"><a href="#local">Local</a></td></tr><tr><td style="text-align:left">NFS</td><td style="text-align:center">-</td><td style="text-align:center"><a href="#nfs">NFS</a></td></tr><tr><td style="text-align:left">PortworxVolume</td><td style="text-align:center">âœ“</td><td style="text-align:center"><a href="#portworx-volume">Portworx Volume</a></td></tr><tr><td style="text-align:left">RBD</td><td style="text-align:center">-</td><td style="text-align:center"><a href="#ceph-rbd">Ceph RBD</a></td></tr><tr><td style="text-align:left">VsphereVolume</td><td style="text-align:center">âœ“</td><td style="text-align:center"><a href="#vsphere">vSphere</a></td></tr></tbody></table><p>You are not restricted to specifying the "internal" provisioners
listed here (whose names are prefixed with "kubernetes.io" and shipped
alongside Kubernetes). You can also run and specify external provisioners,
which are independent programs that follow a <a href="https://git.k8s.io/design-proposals-archive/storage/volume-provisioning.md">specification</a>
defined by Kubernetes. Authors of external provisioners have full discretion
over where their code lives, how the provisioner is shipped, how it needs to be
run, what volume plugin it uses (including Flex), etc. The repository
<a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">kubernetes-sigs/sig-storage-lib-external-provisioner</a>
houses a library for writing external provisioners that implements the bulk of
the specification. Some external provisioners are listed under the repository
<a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">kubernetes-sigs/sig-storage-lib-external-provisioner</a>.</p><p>For example, NFS doesn't provide an internal provisioner, but an external
provisioner can be used. There are also cases when 3rd party storage
vendors provide their own external provisioner.</p><h2 id="reclaim-policy">Reclaim policy</h2><p>PersistentVolumes that are dynamically created by a StorageClass will have the
<a href="/docs/concepts/storage/persistent-volumes/#reclaiming">reclaim policy</a>
specified in the <code>reclaimPolicy</code> field of the class, which can be
either <code>Delete</code> or <code>Retain</code>. If no <code>reclaimPolicy</code> is specified when a
StorageClass object is created, it will default to <code>Delete</code>.</p><p>PersistentVolumes that are created manually and managed via a StorageClass will have
whatever reclaim policy they were assigned at creation.</p><h2 id="allow-volume-expansion">Volume expansion</h2><p>PersistentVolumes can be configured to be expandable. This allows you to resize the
volume by editing the corresponding PVC object, requesting a new larger amount of
storage.</p><p>The following types of volumes support volume expansion, when the underlying
StorageClass has the field <code>allowVolumeExpansion</code> set to true.</p><table><caption style="display:none">Table of Volume types and the version of Kubernetes they require</caption><thead><tr><th style="text-align:left">Volume type</th><th style="text-align:left">Required Kubernetes version for volume expansion</th></tr></thead><tbody><tr><td style="text-align:left">Azure File</td><td style="text-align:left">1.11</td></tr><tr><td style="text-align:left">CSI</td><td style="text-align:left">1.24</td></tr><tr><td style="text-align:left">FlexVolume</td><td style="text-align:left">1.13</td></tr><tr><td style="text-align:left">Portworx</td><td style="text-align:left">1.11</td></tr><tr><td style="text-align:left">rbd</td><td style="text-align:left">1.11</td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You can only use the volume expansion feature to grow a Volume, not to shrink it.</div><h2 id="mount-options">Mount options</h2><p>PersistentVolumes that are dynamically created by a StorageClass will have the
mount options specified in the <code>mountOptions</code> field of the class.</p><p>If the volume plugin does not support mount options but mount options are
specified, provisioning will fail. Mount options are <strong>not</strong> validated on either
the class or PV. If a mount option is invalid, the PV mount fails.</p><h2 id="volume-binding-mode">Volume binding mode</h2><p>The <code>volumeBindingMode</code> field controls when
<a href="/docs/concepts/storage/persistent-volumes/#provisioning">volume binding and dynamic provisioning</a>
should occur. When unset, <code>Immediate</code> mode is used by default.</p><p>The <code>Immediate</code> mode indicates that volume binding and dynamic
provisioning occurs once the PersistentVolumeClaim is created. For storage
backends that are topology-constrained and not globally accessible from all Nodes
in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling
requirements. This may result in unschedulable Pods.</p><p>A cluster administrator can address this issue by specifying the <code>WaitForFirstConsumer</code> mode which
will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
PersistentVolumes will be selected or provisioned conforming to the topology that is
specified by the Pod's scheduling constraints. These include, but are not limited to, <a href="/docs/concepts/configuration/manage-resources-containers/">resource
requirements</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">node selectors</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod affinity and
anti-affinity</a>,
and <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a>.</p><p>The following plugins support <code>WaitForFirstConsumer</code> with dynamic provisioning:</p><ul><li>CSI volumes, provided that the specific CSI driver supports this</li></ul><p>The following plugins support <code>WaitForFirstConsumer</code> with pre-created PersistentVolume binding:</p><ul><li>CSI volumes, provided that the specific CSI driver supports this</li><li><a href="#local"><code>local</code></a></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you choose to use <code>WaitForFirstConsumer</code>, do not use <code>nodeName</code> in the Pod spec
to specify node affinity.
If <code>nodeName</code> is used in this case, the scheduler will be bypassed and PVC will remain in <code>pending</code> state.</p><p>Instead, you can use node selector for <code>kubernetes.io/hostname</code>:</p></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/pod-volume-binding.yaml" download="storage/storageclass/pod-volume-binding.yaml"><code>storage/storageclass/pod-volume-binding.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-pod-volume-binding-yaml&quot;)" title="Copy storage/storageclass/pod-volume-binding.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-pod-volume-binding-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>task-pv-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/hostname</span>:<span style="color:#bbb"> </span>kube-01<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">persistentVolumeClaim</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">claimName</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>task-pv-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"http-server"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/usr/share/nginx/html"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="allowed-topologies">Allowed topologies</h2><p>When a cluster operator specifies the <code>WaitForFirstConsumer</code> volume binding mode, it is no longer necessary
to restrict provisioning to specific topologies in most situations. However,
if still required, <code>allowedTopologies</code> can be specified.</p><p>This example demonstrates how to restrict the topology of provisioned volumes to specific
zones and should be used as a replacement for the <code>zone</code> and <code>zones</code> parameters for the
supported plugins.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-topology.yaml" download="storage/storageclass/storageclass-topology.yaml"><code>storage/storageclass/storageclass-topology.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-topology-yaml&quot;)" title="Copy storage/storageclass/storageclass-topology.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-topology-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>standard<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb">  </span>example.com/example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>pd-standard<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">volumeBindingMode</span>:<span style="color:#bbb"> </span>WaitForFirstConsumer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">allowedTopologies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">matchLabelExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- us-central-1a<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- us-central-1b<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="parameters">Parameters</h2><p>StorageClasses have parameters that describe volumes belonging to the storage
class. Different parameters may be accepted depending on the <code>provisioner</code>.
When a parameter is omitted, some default is used.</p><p>There can be at most 512 parameters defined for a StorageClass.
The total length of the parameters object including its keys and values cannot
exceed 256 KiB.</p><h3 id="aws-ebs">AWS EBS</h3><p>Kubernetes 1.34 does not include a <code>awsElasticBlockStore</code> volume type.</p><p>The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS</a>
out-of-tree storage driver instead.</p><p>Here is an example StorageClass for the AWS EBS CSI driver:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-aws-ebs.yaml" download="storage/storageclass/storageclass-aws-ebs.yaml"><code>storage/storageclass/storageclass-aws-ebs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-aws-ebs-yaml&quot;)" title="Copy storage/storageclass/storageclass-aws-ebs.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-aws-ebs-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ebs-sc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>ebs.csi.aws.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">volumeBindingMode</span>:<span style="color:#bbb"> </span>WaitForFirstConsumer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">csi.storage.k8s.io/fstype</span>:<span style="color:#bbb"> </span>xfs<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>io1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">iopsPerGB</span>:<span style="color:#bbb"> </span><span style="color:#b44">"50"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">encrypted</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tagSpecification_1</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key1=value1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tagSpecification_2</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key2=value2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">allowedTopologies</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">matchLabelExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>topology.ebs.csi.aws.com/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- us-east-2c<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p><code>tagSpecification</code>: Tags with this prefix are applied to dynamically provisioned EBS volumes.</p><h3 id="aws-efs">AWS EFS</h3><p>To configure AWS EFS storage, you can use the out-of-tree <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver">AWS_EFS_CSI_DRIVER</a>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-aws-efs.yaml" download="storage/storageclass/storageclass-aws-efs.yaml"><code>storage/storageclass/storageclass-aws-efs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-aws-efs-yaml&quot;)" title="Copy storage/storageclass/storageclass-aws-efs.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-aws-efs-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>efs-sc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>efs.csi.aws.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">provisioningMode</span>:<span style="color:#bbb"> </span>efs-ap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">fileSystemId</span>:<span style="color:#bbb"> </span>fs-92107410<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">directoryPerms</span>:<span style="color:#bbb"> </span><span style="color:#b44">"700"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><ul><li><code>provisioningMode</code>: The type of volume to be provisioned by Amazon EFS. Currently, only access point based provisioning is supported (<code>efs-ap</code>).</li><li><code>fileSystemId</code>: The file system under which the access point is created.</li><li><code>directoryPerms</code>: The directory permissions of the root directory created by the access point.</li></ul><p>For more details, refer to the <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md">AWS_EFS_CSI_Driver Dynamic Provisioning</a> documentation.</p><h3 id="nfs">NFS</h3><p>To configure NFS storage, you can use the in-tree driver or the
<a href="https://github.com/kubernetes-csi/csi-driver-nfs#readme">NFS CSI driver for Kubernetes</a>
(recommended).</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-nfs.yaml" download="storage/storageclass/storageclass-nfs.yaml"><code>storage/storageclass/storageclass-nfs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-nfs-yaml&quot;)" title="Copy storage/storageclass/storageclass-nfs.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-nfs-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-nfs<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>example.com/external-nfs<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span>nfs-server.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/share<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#b44">"false"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><ul><li><code>server</code>: Server is the hostname or IP address of the NFS server.</li><li><code>path</code>: Path that is exported by the NFS server.</li><li><code>readOnly</code>: A flag indicating whether the storage will be mounted as read only (default false).</li></ul><p>Kubernetes doesn't include an internal NFS provisioner.
You need to use an external provisioner to create a StorageClass for NFS.
Here are some examples:</p><ul><li><a href="https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner">NFS Ganesha server and external provisioner</a></li><li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">NFS subdir external provisioner</a></li></ul><h3 id="vsphere">vSphere</h3><p>There are two types of provisioners for vSphere storage classes:</p><ul><li><a href="#vsphere-provisioner-csi">CSI provisioner</a>: <code>csi.vsphere.vmware.com</code></li><li><a href="#vcp-provisioner">vCP provisioner</a>: <code>kubernetes.io/vsphere-volume</code></li></ul><p>In-tree provisioners are <a href="/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi">deprecated</a>.
For more information on the CSI provisioner, see
<a href="https://vsphere-csi-driver.sigs.k8s.io/">Kubernetes vSphere CSI Driver</a> and
<a href="/docs/concepts/storage/volumes/#vsphere-csi-migration">vSphereVolume CSI migration</a>.</p><h4 id="vsphere-provisioner-csi">CSI Provisioner</h4><p>The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters.
For an example, refer to the <a href="https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml">vSphere CSI repository</a>.</p><h4 id="vcp-provisioner">vCP Provisioner</h4><p>The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.</p><ol><li><p>Create a StorageClass with a user specified disk format.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/vsphere-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">diskformat</span>:<span style="color:#bbb"> </span>zeroedthick<span style="color:#bbb">
</span></span></span></code></pre></div><p><code>diskformat</code>: <code>thin</code>, <code>zeroedthick</code> and <code>eagerzeroedthick</code>. Default: <code>"thin"</code>.</p></li><li><p>Create a StorageClass with a disk format on a user specified datastore.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/vsphere-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">diskformat</span>:<span style="color:#bbb"> </span>zeroedthick<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">datastore</span>:<span style="color:#bbb"> </span>VSANDatastore<span style="color:#bbb">
</span></span></span></code></pre></div><p><code>datastore</code>: The user can also specify the datastore in the StorageClass.
The volume will be created on the datastore specified in the StorageClass,
which in this case is <code>VSANDatastore</code>. This field is optional. If the
datastore is not specified, then the volume will be created on the datastore
specified in the vSphere config file used to initialize the vSphere Cloud
Provider.</p></li><li><p>Storage Policy Management inside kubernetes</p><ul><li><p>Using existing vCenter SPBM policy</p><p>One of the most important features of vSphere for Storage Management is
policy based Management. Storage Policy Based Management (SPBM) is a
storage policy framework that provides a single unified control plane
across a broad range of data services and storage solutions. SPBM enables
vSphere administrators to overcome upfront storage provisioning challenges,
such as capacity planning, differentiated service levels and managing
capacity headroom.</p><p>The SPBM policies can be specified in the StorageClass using the
<code>storagePolicyName</code> parameter.</p></li><li><p>Virtual SAN policy support inside Kubernetes</p><p>Vsphere Infrastructure (VI) Admins will have the ability to specify custom
Virtual SAN Storage Capabilities during dynamic volume provisioning. You
can now define storage requirements, such as performance and availability,
in the form of storage capabilities during dynamic volume provisioning.
The storage capability requirements are converted into a Virtual SAN
policy which are then pushed down to the Virtual SAN layer when a
persistent volume (virtual disk) is being created. The virtual disk is
distributed across the Virtual SAN datastore to meet the requirements.</p><p>You can see <a href="https://github.com/vmware-archive/vsphere-storage-for-kubernetes/blob/fa4c8b8ad46a85b6555d715dd9d27ff69839df53/documentation/policy-based-mgmt.md">Storage Policy Based Management for dynamic provisioning of volumes</a>
for more details on how to use storage policies for persistent volumes
management.</p></li></ul></li></ol><h3 id="ceph-rbd">Ceph RBD (deprecated)</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [deprecated]</code></div><p>This internal provisioner of Ceph RBD is deprecated. Please use
<a href="https://github.com/ceph/ceph-csi">CephFS RBD CSI driver</a>.</p></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-ceph-rbd.yaml" download="storage/storageclass/storageclass-ceph-rbd.yaml"><code>storage/storageclass/storageclass-ceph-rbd.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-ceph-rbd-yaml&quot;)" title="Copy storage/storageclass/storageclass-ceph-rbd.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-ceph-rbd-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/rbd<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This provisioner is deprecated</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">monitors</span>:<span style="color:#bbb"> </span><span style="color:#666">198.19.254.105</span>:<span style="color:#666">6789</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">adminId</span>:<span style="color:#bbb"> </span>kube<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">adminSecretName</span>:<span style="color:#bbb"> </span>ceph-secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">adminSecretNamespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">pool</span>:<span style="color:#bbb"> </span>kube<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">userId</span>:<span style="color:#bbb"> </span>kube<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">userSecretName</span>:<span style="color:#bbb"> </span>ceph-secret-user<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">userSecretNamespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">imageFormat</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">imageFeatures</span>:<span style="color:#bbb"> </span><span style="color:#b44">"layering"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><ul><li><p><code>monitors</code>: Ceph monitors, comma delimited. This parameter is required.</p></li><li><p><code>adminId</code>: Ceph client ID that is capable of creating images in the pool.
Default is "admin".</p></li><li><p><code>adminSecretName</code>: Secret Name for <code>adminId</code>. This parameter is required.
The provided secret must have type "kubernetes.io/rbd".</p></li><li><p><code>adminSecretNamespace</code>: The namespace for <code>adminSecretName</code>. Default is "default".</p></li><li><p><code>pool</code>: Ceph RBD pool. Default is "rbd".</p></li><li><p><code>userId</code>: Ceph client ID that is used to map the RBD image. Default is the
same as <code>adminId</code>.</p></li><li><p><code>userSecretName</code>: The name of Ceph Secret for <code>userId</code> to map RBD image. It
must exist in the same namespace as PVCs. This parameter is required.
The provided secret must have type "kubernetes.io/rbd", for example created in this
way:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret generic ceph-secret --type<span style="color:#666">=</span><span style="color:#b44">"kubernetes.io/rbd"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --from-literal<span style="color:#666">=</span><span style="color:#b8860b">key</span><span style="color:#666">=</span><span style="color:#b44">'QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ=='</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --namespace<span style="color:#666">=</span>kube-system
</span></span></code></pre></div></li><li><p><code>userSecretNamespace</code>: The namespace for <code>userSecretName</code>.</p></li><li><p><code>fsType</code>: fsType that is supported by kubernetes. Default: <code>"ext4"</code>.</p></li><li><p><code>imageFormat</code>: Ceph RBD image format, "1" or "2". Default is "2".</p></li><li><p><code>imageFeatures</code>: This parameter is optional and should only be used if you
set <code>imageFormat</code> to "2". Currently supported features are <code>layering</code> only.
Default is "", and no features are turned on.</p></li></ul><h3 id="azure-disk">Azure Disk</h3><p>Kubernetes 1.34 does not include a <code>azureDisk</code> volume type.</p><p>The <code>azureDisk</code> in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver">Azure Disk</a> third party
storage driver instead.</p><h3 id="azure-file">Azure File (deprecated)</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-azure-file.yaml" download="storage/storageclass/storageclass-azure-file.yaml"><code>storage/storageclass/storageclass-azure-file.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-azure-file-yaml&quot;)" title="Copy storage/storageclass/storageclass-azure-file.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-azure-file-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>azurefile<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/azure-file<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">skuName</span>:<span style="color:#bbb"> </span>Standard_LRS<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">location</span>:<span style="color:#bbb"> </span>eastus<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageAccount</span>:<span style="color:#bbb"> </span>azure_storage_account_name<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># example value</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><ul><li><code>skuName</code>: Azure storage account SKU tier. Default is empty.</li><li><code>location</code>: Azure storage account location. Default is empty.</li><li><code>storageAccount</code>: Azure storage account name. Default is empty. If a storage
account is not provided, all storage accounts associated with the resource
group are searched to find one that matches <code>skuName</code> and <code>location</code>. If a
storage account is provided, it must reside in the same resource group as the
cluster, and <code>skuName</code> and <code>location</code> are ignored.</li><li><code>secretNamespace</code>: the namespace of the secret that contains the Azure Storage
Account Name and Key. Default is the same as the Pod.</li><li><code>secretName</code>: the name of the secret that contains the Azure Storage Account Name and
Key. Default is <code>azure-storage-account-&lt;accountName&gt;-secret</code></li><li><code>readOnly</code>: a flag indicating whether the storage will be mounted as read only.
Defaults to false which means a read/write mount. This setting will impact the
<code>ReadOnly</code> setting in VolumeMounts as well.</li></ul><p>During storage provisioning, a secret named by <code>secretName</code> is created for the
mounting credentials. If the cluster has enabled both
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> and
<a href="/docs/reference/access-authn-authz/rbac/#controller-roles">Controller Roles</a>,
add the <code>create</code> permission of resource <code>secret</code> for clusterrole
<code>system:controller:persistent-volume-binder</code>.</p><p>In a multi-tenancy context, it is strongly recommended to set the value for
<code>secretNamespace</code> explicitly, otherwise the storage account credentials may
be read by other users.</p><h3 id="portworx-volume">Portworx volume (deprecated)</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-portworx-volume.yaml" download="storage/storageclass/storageclass-portworx-volume.yaml"><code>storage/storageclass/storageclass-portworx-volume.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-portworx-volume-yaml&quot;)" title="Copy storage/storageclass/storageclass-portworx-volume.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-portworx-volume-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>portworx-io-priority-high<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/portworx-volume<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This provisioner is deprecated</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">repl</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">snap_interval</span>:<span style="color:#bbb"> </span><span style="color:#b44">"70"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">priority_io</span>:<span style="color:#bbb"> </span><span style="color:#b44">"high"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><ul><li><code>fs</code>: filesystem to be laid out: <code>none/xfs/ext4</code> (default: <code>ext4</code>).</li><li><code>block_size</code>: block size in Kbytes (default: <code>32</code>).</li><li><code>repl</code>: number of synchronous replicas to be provided in the form of
replication factor <code>1..3</code> (default: <code>1</code>) A string is expected here i.e.
<code>"1"</code> and not <code>1</code>.</li><li><code>priority_io</code>: determines whether the volume will be created from higher
performance or a lower priority storage <code>high/medium/low</code> (default: <code>low</code>).</li><li><code>snap_interval</code>: clock/time interval in minutes for when to trigger snapshots.
Snapshots are incremental based on difference with the prior snapshot, 0
disables snaps (default: <code>0</code>). A string is expected here i.e.
<code>"70"</code> and not <code>70</code>.</li><li><code>aggregation_level</code>: specifies the number of chunks the volume would be
distributed into, 0 indicates a non-aggregated volume (default: <code>0</code>). A string
is expected here i.e. <code>"0"</code> and not <code>0</code></li><li><code>ephemeral</code>: specifies whether the volume should be cleaned-up after unmount
or should be persistent. <code>emptyDir</code> use case can set this value to true and
<code>persistent volumes</code> use case such as for databases like Cassandra should set
to false, <code>true/false</code> (default <code>false</code>). A string is expected here i.e.
<code>"true"</code> and not <code>true</code>.</li></ul><h3 id="local">Local</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-local.yaml" download="storage/storageclass/storageclass-local.yaml"><code>storage/storageclass/storageclass-local.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;storage-storageclass-storageclass-local-yaml&quot;)" title="Copy storage/storageclass/storageclass-local.yaml to clipboard"/></div><div class="includecode" id="storage-storageclass-storageclass-local-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>local-storage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/no-provisioner<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># indicates that this StorageClass does not support automatic provisioning</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">volumeBindingMode</span>:<span style="color:#bbb"> </span>WaitForFirstConsumer<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Local volumes do not support dynamic provisioning in Kubernetes 1.34;
however a StorageClass should still be created to delay volume binding until a Pod is actually
scheduled to the appropriate node. This is specified by the <code>WaitForFirstConsumer</code> volume
binding mode.</p><p>Delaying volume binding allows the scheduler to consider all of a Pod's
scheduling constraints when choosing an appropriate PersistentVolume for a
PersistentVolumeClaim.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Security Policies</h1><div class="alert alert-warning" role="alert"><h4 class="alert-heading">Removed feature</h4>PodSecurityPolicy was <a href="/blog/2021/04/08/kubernetes-1-21-release-announcement/#podsecuritypolicy-deprecation">deprecated</a>
in Kubernetes v1.21, and removed from Kubernetes in v1.25.</div><p>Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using
either or both:</p><ul><li><a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission</a></li><li>a 3rd party admission plugin, that you deploy and configure yourself</li></ul><p>For a migration guide, see <a href="/docs/tasks/configure-pod-container/migrate-from-psp/">Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.
For more information on the removal of this API,
see <a href="/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future</a>.</p><p>If you are not running Kubernetes v1.34, check the documentation for
your version of Kubernetes.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Service Internal Traffic Policy</h1><div class="lead">If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use <em>Service Internal Traffic Policy</em> to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p><em>Service Internal Traffic Policy</em> enables internal traffic restrictions to only route
internal traffic to endpoints within the node the traffic originated from. The
"internal" traffic here refers to traffic originated from Pods in the current
cluster. This can help to reduce costs and improve performance.</p><h2 id="using-service-internal-traffic-policy">Using Service Internal Traffic Policy</h2><p>You can enable the internal-only traffic policy for a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>, by setting its
<code>.spec.internalTrafficPolicy</code> to <code>Local</code>. This tells kube-proxy to only use node local
endpoints for cluster internal traffic.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>For pods on nodes with no endpoints for a given Service, the Service
behaves as if it has zero endpoints (for Pods on this node) even if the service
does have endpoints on other nodes.</div><p>The following example shows what a Service looks like when you set
<code>.spec.internalTrafficPolicy</code> to <code>Local</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">internalTrafficPolicy</span>:<span style="color:#bbb"> </span>Local<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="how-it-works">How it works</h2><p>The kube-proxy filters the endpoints it routes to based on the
<code>spec.internalTrafficPolicy</code> setting. When it's set to <code>Local</code>, only node local
endpoints are considered. When it's <code>Cluster</code> (the default), or is not set,
Kubernetes considers all endpoints.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/services-networking/topology-aware-routing/">Topology Aware Routing</a></li><li>Read about <a href="/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">Service External Traffic Policy</a></li><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">DNS for Services and Pods</h1><div class="lead">Your workload can discover Services within your cluster using DNS; this page explains how that works.</div><p>Kubernetes creates DNS records for Services and Pods. You can contact
Services with consistent DNS names instead of IP addresses.</p><p>Kubernetes publishes information about Pods and Services which is used
to program DNS. kubelet configures Pods' DNS so that running containers
can look up Services by name rather than IP.</p><p>Services defined in the cluster are assigned DNS names. By default, a
client Pod's DNS search list includes the Pod's own namespace and the
cluster's default domain.</p><h3 id="namespaces-of-services">Namespaces of Services</h3><p>A DNS query may return different results based on the namespace of the Pod making
it. DNS queries that don't specify a namespace are limited to the Pod's
namespace. Access Services in other namespaces by specifying it in the DNS query.</p><p>For example, consider a Pod in a <code>test</code> namespace. A <code>data</code> Service is in
the <code>prod</code> namespace.</p><p>A query for <code>data</code> returns no results, because it uses the Pod's <code>test</code> namespace.</p><p>A query for <code>data.prod</code> returns the intended result, because it specifies the
namespace.</p><p>DNS queries may be expanded using the Pod's <code>/etc/resolv.conf</code>. kubelet
configures this file for each Pod. For example, a query for just <code>data</code> may be
expanded to <code>data.test.svc.cluster.local</code>. The values of the <code>search</code> option
are used to expand queries. To learn more about DNS queries, see
<a href="https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html">the <code>resolv.conf</code> manual page</a>.</p><pre tabindex="0"><code>nameserver 10.32.0.10
search &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre><p>In summary, a Pod in the <em>test</em> namespace can successfully resolve either
<code>data.prod</code> or <code>data.prod.svc.cluster.local</code>.</p><h3 id="dns-records">DNS Records</h3><p>What objects get DNS records?</p><ol><li>Services</li><li>Pods</li></ol><p>The following sections detail the supported DNS record types and layout that is
supported. Any other layout or names or queries that happen to work are
considered implementation details and are subject to change without warning.
For more up-to-date specification, see
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS-Based Service Discovery</a>.</p><h2 id="services">Services</h2><h3 id="a-aaaa-records">A/AAAA records</h3><p>"Normal" (not headless) Services are assigned DNS A and/or AAAA records,
depending on the IP family or families of the Service, with a name of the form
<code>my-svc.my-namespace.svc.cluster-domain.example</code>. This resolves to the cluster IP
of the Service.</p><p><a href="/docs/concepts/services-networking/service/#headless-services">Headless Services</a>
(without a cluster IP) are also assigned DNS A and/or AAAA records,
with a name of the form <code>my-svc.my-namespace.svc.cluster-domain.example</code>. Unlike normal
Services, this resolves to the set of IPs of all of the Pods selected by the Service.
Clients are expected to consume the set or else use standard round-robin
selection from the set.</p><h3 id="srv-records">SRV records</h3><p>SRV Records are created for named ports that are part of normal or headless
services.</p><ul><li>For each named port, the SRV record has the form
<code>_port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example</code>.</li><li>For a regular Service, this resolves to the port number and the domain name:
<code>my-svc.my-namespace.svc.cluster-domain.example</code>.</li><li>For a headless Service, this resolves to multiple answers, one for each Pod
that is backing the Service, and contains the port number and the domain name of the Pod
of the form <code>hostname.my-svc.my-namespace.svc.cluster-domain.example</code>.</li></ul><h2 id="pods">Pods</h2><h3 id="a-aaaa-records-1">A/AAAA records</h3><p>Kube-DNS versions, prior to the implementation of the
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS specification</a>,
had the following DNS resolution:</p><pre tabindex="0"><code>&lt;pod-IPv4-address&gt;.&lt;namespace&gt;.pod.&lt;cluster-domain&gt;
</code></pre><p>For example, if a Pod in the <code>default</code> namespace has the IP address 172.17.0.3,
and the domain name for your cluster is <code>cluster.local</code>, then the Pod has a DNS name:</p><pre tabindex="0"><code>172-17-0-3.default.pod.cluster.local
</code></pre><p>Some cluster DNS mechanisms, like <a href="https://coredns.io/">CoreDNS</a>, also provide <code>A</code> records for:</p><pre tabindex="0"><code>&lt;pod-ipv4-address&gt;.&lt;service-name&gt;.&lt;my-namespace&gt;.svc.&lt;cluster-domain.example&gt;
</code></pre><p>For example, if a Pod in the <code>cafe</code> namespace has the IP address 172.17.0.3,
is an endpoint of a Service named <code>barista</code>, and the domain name for your cluster is
<code>cluster.local</code>, then the Pod would have this service-scoped DNS <code>A</code> record.</p><pre tabindex="0"><code>172-17-0-3.barista.cafe.svc.cluster.local
</code></pre><h3 id="pod-hostname-and-subdomain-field">Pod's hostname and subdomain fields</h3><p>Currently when a Pod is created, its hostname (as observed from within the Pod)
is the Pod's <code>metadata.name</code> value.</p><p>The Pod spec has an optional <code>hostname</code> field, which can be used to specify a
different hostname. When specified, it takes precedence over the Pod's name to be
the hostname of the Pod (again, as observed from within the Pod). For example,
given a Pod with <code>spec.hostname</code> set to <code>"my-host"</code>, the Pod will have its
hostname set to <code>"my-host"</code>.</p><p>The Pod spec also has an optional <code>subdomain</code> field which can be used to indicate
that the pod is part of sub-group of the namespace. For example, a Pod with <code>spec.hostname</code>
set to <code>"foo"</code>, and <code>spec.subdomain</code> set to <code>"bar"</code>, in namespace <code>"my-namespace"</code>, will
have its hostname set to <code>"foo"</code> and its fully qualified domain name (FQDN) set to
<code>"foo.bar.my-namespace.svc.cluster.local"</code> (once more, as observed from within
the Pod).</p><p>If there exists a headless Service in the same namespace as the Pod, with
the same name as the subdomain, the cluster's DNS Server also returns A and/or AAAA
records for the Pod's fully qualified hostname.</p><p>Example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox-subdomain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># name is not required for single-port Services</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">1234</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostname</span>:<span style="color:#bbb"> </span>busybox-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">subdomain</span>:<span style="color:#bbb"> </span>busybox-subdomain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"3600"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostname</span>:<span style="color:#bbb"> </span>busybox-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">subdomain</span>:<span style="color:#bbb"> </span>busybox-subdomain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"3600"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span></code></pre></div><p>Given the above Service <code>"busybox-subdomain"</code> and the Pods which set <code>spec.subdomain</code>
to <code>"busybox-subdomain"</code>, the first Pod will see its own FQDN as
<code>"busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example"</code>. DNS serves
A and/or AAAA records at that name, pointing to the Pod's IP. Both Pods "<code>busybox1</code>" and
"<code>busybox2</code>" will have their own address records.</p><p>An <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/endpoint-slices/" target="_blank" aria-label="EndpointSlice">EndpointSlice</a> can specify
the DNS hostname for any endpoint addresses, along with its IP.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A and AAAA records are not created for Pod names since <code>hostname</code> is missing for the Pod.
A Pod with no <code>hostname</code> but with <code>subdomain</code> will only create the
A or AAAA record for the headless Service (<code>busybox-subdomain.my-namespace.svc.cluster-domain.example</code>),
pointing to the Pods' IP addresses. Also, the Pod needs to be ready in order to have a
record unless <code>publishNotReadyAddresses=True</code> is set on the Service.</div><h3 id="pod-sethostnameasfqdn-field">Pod's setHostnameAsFQDN field</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [stable]</code></div><p>When a Pod is configured to have fully qualified domain name (FQDN), its
hostname is the short hostname. For example, if you have a Pod with the fully
qualified domain name <code>busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example</code>,
then by default the <code>hostname</code> command inside that Pod returns <code>busybox-1</code> and the
<code>hostname --fqdn</code> command returns the FQDN.</p><p>When you set <code>setHostnameAsFQDN: true</code> in the Pod spec, the kubelet writes the Pod's FQDN
into the hostname for that Pod's namespace. In this case, both <code>hostname</code> and <code>hostname --fqdn</code>
return the Pod's FQDN.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>In Linux, the hostname field of the kernel (the <code>nodename</code> field of <code>struct utsname</code>) is limited to 64 characters.</p><p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start.
The Pod will remain in <code>Pending</code> status (<code>ContainerCreating</code> as seen by <code>kubectl</code>) generating
error events, such as Failed to construct FQDN from Pod hostname and cluster domain,
FQDN <code>long-FQDN</code> is too long (64 characters is the max, 70 characters requested).
One way of improving user experience for this scenario is to create an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission webhook controller</a>
to control FQDN size when users create top level objects, for example, Deployment.</p></div><h3 id="pod-s-dns-policy">Pod's DNS Policy</h3><p>DNS policies can be set on a per-Pod basis. Currently Kubernetes supports the
following Pod-specific DNS policies. These policies are specified in the
<code>dnsPolicy</code> field of a Pod Spec.</p><ul><li><p>"<code>Default</code>": The Pod inherits the name resolution configuration from the node
that the Pods run on.
See <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">related discussion</a>
for more details.</p></li><li><p>"<code>ClusterFirst</code>": Any DNS query that does not match the configured cluster
domain suffix, such as "<code>www.kubernetes.io</code>", is forwarded to an upstream
nameserver by the DNS server. Cluster administrators may have extra
stub-domain and upstream DNS servers configured.
See <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">related discussion</a>
for details on how DNS queries are handled in those cases.</p></li><li><p>"<code>ClusterFirstWithHostNet</code>": For Pods running with hostNetwork, you should
explicitly set its DNS policy to "<code>ClusterFirstWithHostNet</code>". Otherwise, Pods
running with hostNetwork and <code>"ClusterFirst"</code> will fallback to the behavior
of the <code>"Default"</code> policy.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This is not supported on Windows. See <a href="#dns-windows">below</a> for details.</div></li><li><p>"<code>None</code>": It allows a Pod to ignore DNS settings from the Kubernetes
environment. All DNS settings are supposed to be provided using the
<code>dnsConfig</code> field in the Pod Spec.
See <a href="#pod-dns-config">Pod's DNS config</a> subsection below.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>"Default" is not the default DNS policy. If <code>dnsPolicy</code> is not
explicitly specified, then "ClusterFirst" is used.</div><p>The example below shows a Pod with its DNS policy set to
"<code>ClusterFirstWithHostNet</code>" because it has <code>hostNetwork</code> set to <code>true</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"3600"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dnsPolicy</span>:<span style="color:#bbb"> </span>ClusterFirstWithHostNet<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="pod-dns-config">Pod's DNS Config</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code></div><p>Pod's DNS Config allows users more control on the DNS settings for a Pod.</p><p>The <code>dnsConfig</code> field is optional and it can work with any <code>dnsPolicy</code> settings.
However, when a Pod's <code>dnsPolicy</code> is set to "<code>None</code>", the <code>dnsConfig</code> field has
to be specified.</p><p>Below are the properties a user can specify in the <code>dnsConfig</code> field:</p><ul><li><code>nameservers</code>: a list of IP addresses that will be used as DNS servers for the
Pod. There can be at most 3 IP addresses specified. When the Pod's <code>dnsPolicy</code>
is set to "<code>None</code>", the list must contain at least one IP address, otherwise
this property is optional.
The servers listed will be combined to the base nameservers generated from the
specified DNS policy with duplicate addresses removed.</li><li><code>searches</code>: a list of DNS search domains for hostname lookup in the Pod.
This property is optional. When specified, the provided list will be merged
into the base search domain names generated from the chosen DNS policy.
Duplicate domain names are removed.
Kubernetes allows up to 32 search domains.</li><li><code>options</code>: an optional list of objects where each object may have a <code>name</code>
property (required) and a <code>value</code> property (optional). The contents in this
property will be merged to the options generated from the specified DNS policy.
Duplicate entries are removed.</li></ul><p>The following is an example Pod with custom DNS settings:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/custom-dns.yaml" download="service/networking/custom-dns.yaml"><code>service/networking/custom-dns.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-custom-dns-yaml&quot;)" title="Copy service/networking/custom-dns.yaml to clipboard"/></div><div class="includecode" id="service-networking-custom-dns-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dns-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dnsPolicy</span>:<span style="color:#bbb"> </span><span style="color:#b44">"None"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dnsConfig</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nameservers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#666">192.0.2.1</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># this is an example</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">searches</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- ns1.svc.cluster-domain.example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- my.dns.search.suffix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">options</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ndots<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>edns0<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>When the Pod above is created, the container <code>test</code> gets the following contents
in its <code>/etc/resolv.conf</code> file:</p><pre tabindex="0"><code>nameserver 192.0.2.1
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
</code></pre><p>For IPv6 setup, search path and name server should be set up like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">exec</span> -it dns-example -- cat /etc/resolv.conf
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>nameserver 2001:db8:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
</code></pre><h2 id="dns-search-domain-list-limits">DNS search domain list limits</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes 1.28 [stable]</code></div><p>Kubernetes itself does not limit the DNS Config until the length of the search
domain list exceeds 32 or the total length of all search domains exceeds 2048.
This limit applies to the node's resolver configuration file, the Pod's DNS
Config, and the merged DNS Config respectively.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Some container runtimes of earlier versions may have their own restrictions on
the number of DNS search domains. Depending on the container runtime
environment, the pods with a large number of DNS search domains may get stuck in
the pending state.</p><p>It is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have
this problem.</p></div><h2 id="dns-windows">DNS resolution on Windows nodes</h2><ul><li><code>ClusterFirstWithHostNet</code> is not supported for Pods that run on Windows nodes.
Windows treats all names with a <code>.</code> as a FQDN and skips FQDN resolution.</li><li>On Windows, there are multiple DNS resolvers that can be used. As these come with
slightly different behaviors, using the
<a href="https://docs.microsoft.com/powershell/module/dnsclient/resolve-dnsname"><code>Resolve-DNSName</code></a>
powershell cmdlet for name query resolutions is recommended.</li><li>On Linux, you have a DNS suffix list, which is used after resolution of a name as fully
qualified has failed.
On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with that
Pod's namespace (example: <code>mydns.svc.cluster.local</code>). Windows can resolve FQDNs, Services,
or network name which can be resolved with this single suffix. For example, a Pod spawned
in the <code>default</code> namespace, will have the DNS suffix <code>default.svc.cluster.local</code>.
Inside a Windows Pod, you can resolve both <code>kubernetes.default.svc.cluster.local</code>
and <code>kubernetes</code>, but not the partially qualified names (<code>kubernetes.default</code> or
<code>kubernetes.default.svc</code>).</li></ul><h2 id="what-s-next">What's next</h2><p>For guidance on administering DNS configurations, check
<a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Configure DNS Service</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Overhead</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>When you run a Pod on a Node, the Pod itself takes an amount of system resources. These
resources are additional to the resources needed to run the container(s) inside the Pod.
In Kubernetes, <em>Pod Overhead</em> is a way to account for the resources consumed by the Pod
infrastructure on top of the container requests &amp; limits.</p><p>In Kubernetes, the Pod's overhead is set at
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission</a>
time according to the overhead associated with the Pod's
<a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a>.</p><p>A pod's overhead is considered in addition to the sum of container resource requests when
scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup,
and when carrying out Pod eviction ranking.</p><h2 id="set-up">Configuring Pod overhead</h2><p>You need to make sure a <code>RuntimeClass</code> is utilized which defines the <code>overhead</code> field.</p><h2 id="usage-example">Usage example</h2><p>To work with Pod overhead, you need a RuntimeClass that defines the <code>overhead</code> field. As
an example, you could use the following RuntimeClass definition with a virtualization container
runtime (in this example, Kata Containers combined with the Firecracker virtual machine monitor)
that uses around 120MiB per Pod for the virtual machine and the guest OS:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># You need to change this example to match the actual runtime name, and per-Pod</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># resource overhead, that the container runtime is adding in your cluster.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>node.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>RuntimeClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>kata-fc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">handler</span>:<span style="color:#bbb"> </span>kata-fc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">overhead</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podFixed</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"120Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"250m"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Workloads which are created which specify the <code>kata-fc</code> RuntimeClass handler will take the memory and
cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.</p><p>Consider running the given example workload, test-pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">runtimeClassName</span>:<span style="color:#bbb"> </span>kata-fc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">stdin</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tty</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-ctr<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>1500m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If only <code>limits</code> are specified in the pod definition, kubelet will deduce <code>requests</code> from those limits and set them to be the same as the defined <code>limits</code>.</div><p>At admission time the RuntimeClass <a href="/docs/reference/access-authn-authz/admission-controllers/">admission controller</a>
updates the workload's PodSpec to include the <code>overhead</code> as described in the RuntimeClass. If the PodSpec already has this field defined,
the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod
to include an <code>overhead</code>.</p><p>After the RuntimeClass admission controller has made modifications, you can check the updated
Pod overhead value:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pod test-pod -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.spec.overhead}'</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>map[cpu:250m memory:120Mi]
</code></pre><p>If a <a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a> is defined, the sum of container requests as well as the
<code>overhead</code> field are counted.</p><p>When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's
<code>overhead</code> as well as the sum of container requests for that Pod. For this example, the scheduler adds the
requests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.</p><p>Once a Pod is scheduled to a node, the kubelet on that node creates a new <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank" aria-label="cgroup">cgroup</a> for the Pod. It is within this pod that the underlying
container runtime will create containers.</p><p>If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),
the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU
and memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the <code>overhead</code>
defined in the PodSpec.</p><p>For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set <code>cpu.shares</code> based on the
sum of container requests plus the <code>overhead</code> defined in the PodSpec.</p><p>Looking at our example, verify the container requests for the workload:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pod test-pod -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.spec.containers[*].resources.limits}'</span>
</span></span></code></pre></div><p>The total container requests are 2000m CPU and 200MiB of memory:</p><pre tabindex="0"><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>Check this against what is observed by the node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl describe node | grep test-pod -B2
</span></span></code></pre></div><p>The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead:</p><pre tabindex="0"><code>  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------    ----       ------------  ----------   ---------------  -------------  ---
  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id="verify-pod-cgroup-limits">Verify Pod cgroup limits</h2><p>Check the Pod's memory cgroups on the node where the workload is running. In the following example,
<a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md"><code>crictl</code></a>
is used on the node, which provides a CLI for CRI-compatible container runtimes. This is an
advanced example to show Pod overhead behavior, and it is not expected that users should need to check
cgroups directly on the node.</p><p>First, on the particular node, determine the Pod identifier:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on the node where the Pod is scheduled</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">POD_ID</span><span style="color:#666">=</span><span style="color:#b44">"</span><span style="color:#a2f;font-weight:700">$(</span>sudo crictl pods --name test-pod -q<span style="color:#a2f;font-weight:700">)</span><span style="color:#b44">"</span>
</span></span></code></pre></div><p>From this, you can determine the cgroup path for the Pod:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on the node where the Pod is scheduled</span>
</span></span><span style="display:flex"><span>sudo crictl inspectp -o<span style="color:#666">=</span>json <span style="color:#b8860b">$POD_ID</span> | grep cgroupsPath
</span></span></code></pre></div><p>The resulting cgroup path includes the Pod's <code>pause</code> container. The Pod level cgroup is one directory above.</p><pre tabindex="0"><code>  "cgroupsPath": "/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a"
</code></pre><p>In this specific case, the pod cgroup path is <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>.
Verify the Pod level cgroup setting for memory:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Run this on the node where the Pod is scheduled.</span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Also, change the name of the cgroup to match the cgroup allocated for your pod.</span>
</span></span><span style="display:flex"><span> cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</span></span></code></pre></div><p>This is 320 MiB, as expected:</p><pre tabindex="0"><code>335544320
</code></pre><h3 id="observability">Observability</h3><p>Some <code>kube_pod_overhead_*</code> metrics are available in <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a>
to help identify when Pod overhead is being utilized and to help observe stability of workloads
running with a defined overhead.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a></li><li>Read the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead">PodOverhead Design</a>
enhancement proposal for extra context</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Topology Spread Constraints</h1><p>You can use <em>topology spread constraints</em> to control how
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> are spread across your cluster
among failure-domains such as regions, zones, nodes, and other user-defined topology
domains. This can help to achieve high availability as well as efficient resource
utilization.</p><p>You can set <a href="#cluster-level-default-constraints">cluster-level constraints</a> as a default,
or configure topology spread constraints for individual workloads.</p><h2 id="motivation">Motivation</h2><p>Imagine that you have a cluster of up to twenty nodes, and you want to run a
<a class="glossary-tooltip" title="A workload is an application running on Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/" target="_blank" aria-label="workload">workload</a>
that automatically scales how many replicas it uses. There could be as few as
two Pods or as many as fifteen.
When there are only two Pods, you'd prefer not to have both of those Pods run on the
same node: you would run the risk that a single node failure takes your workload
offline.</p><p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.</p><p>As you scale up and run more Pods, a different concern becomes important. Imagine
that you have three nodes running five Pods each. The nodes have enough capacity
to run that many replicas; however, the clients that interact with this workload
are split across three different datacenters (or infrastructure zones). Now you
have less concern about a single node failure, but you notice that latency is
higher than you'd like, and you are paying for network costs associated with
sending network traffic between the different zones.</p><p>You decide that under normal operation you'd prefer to have a similar number of replicas
<a href="/docs/concepts/scheduling-eviction/">scheduled</a> into each infrastructure zone,
and you'd like the cluster to self-heal in the case that there is a problem.</p><p>Pod topology spread constraints offer you a declarative way to configure that.</p><h2 id="topologyspreadconstraints-field"><code>topologySpreadConstraints</code> field</h2><p>The Pod API includes a field, <code>spec.topologySpreadConstraints</code>. The usage of this field looks like
the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Configure a topology spread constraint</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">topologySpreadConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span>&lt;integer&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">minDomains</span>:<span style="color:#bbb"> </span>&lt;integer&gt;<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>&lt;string&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>&lt;string&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb"> </span>&lt;object&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">matchLabelKeys</span>:<span style="color:#bbb"> </span>&lt;list&gt;<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># optional; beta since v1.27</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">nodeAffinityPolicy</span>:<span style="color:#bbb"> </span>[Honor|Ignore]<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># optional; beta since v1.26</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">nodeTaintsPolicy</span>:<span style="color:#bbb"> </span>[Honor|Ignore]<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># optional; beta since v1.26</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">### other Pod fields go here</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There can only be one <code>topologySpreadConstraint</code> for a given <code>topologyKey</code> and <code>whenUnsatisfiable</code> value. For example, if you have defined a <code>topologySpreadConstraint</code> that uses the <code>topologyKey</code> "kubernetes.io/hostname" and <code>whenUnsatisfiable</code> value "DoNotSchedule", you can only add another <code>topologySpreadConstraint</code> for the <code>topologyKey</code> "kubernetes.io/hostname" if you use a different <code>whenUnsatisfiable</code> value.</div><p>You can read more about this field by running <code>kubectl explain Pod.spec.topologySpreadConstraints</code> or
refer to the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling">scheduling</a> section of the API reference for Pod.</p><h3 id="spread-constraint-definition">Spread constraint definition</h3><p>You can define one or multiple <code>topologySpreadConstraints</code> entries to instruct the
kube-scheduler how to place each incoming Pod in relation to the existing Pods across
your cluster. Those fields are:</p><ul><li><p><strong>maxSkew</strong> describes the degree to which Pods may be unevenly distributed. You must
specify this field and the number must be greater than zero. Its semantics differ
according to the value of <code>whenUnsatisfiable</code>:</p><ul><li>if you select <code>whenUnsatisfiable: DoNotSchedule</code>, then <code>maxSkew</code> defines the
maximum permitted difference between the number of matching pods in the target
topology and the <em>global minimum</em>
(the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains).
For example, if you have 3 zones with 2, 2 and 1 matching pods respectively,
<code>MaxSkew</code> is set to 1 then the global minimum is 1.</li><li>if you select <code>whenUnsatisfiable: ScheduleAnyway</code>, the scheduler gives higher
precedence to topologies that would help reduce the skew.</li></ul></li><li><p><strong>minDomains</strong> indicates a minimum number of eligible domains. This field is optional.
A domain is a particular instance of a topology. An eligible domain is a domain whose
nodes match the node selector.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Before Kubernetes v1.30, the <code>minDomains</code> field was only available if the
<code>MinDomainsInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates-removed/">feature gate</a>
was enabled (default since v1.28). In older Kubernetes clusters it might be explicitly
disabled or the field might not be available.</div><ul><li>The value of <code>minDomains</code> must be greater than 0, when specified.
You can only specify <code>minDomains</code> in conjunction with <code>whenUnsatisfiable: DoNotSchedule</code>.</li><li>When the number of eligible domains with match topology keys is less than <code>minDomains</code>,
Pod topology spread treats global minimum as 0, and then the calculation of <code>skew</code> is performed.
The global minimum is the minimum number of matching Pods in an eligible domain,
or zero if the number of eligible domains is less than <code>minDomains</code>.</li><li>When the number of eligible domains with matching topology keys equals or is greater than
<code>minDomains</code>, this value has no effect on scheduling.</li><li>If you do not specify <code>minDomains</code>, the constraint behaves as if <code>minDomains</code> is 1.</li></ul></li><li><p><strong>topologyKey</strong> is the key of <a href="#node-labels">node labels</a>. Nodes that have a label with this key
and identical values are considered to be in the same topology.
We call each instance of a topology (in other words, a &lt;key, value&gt; pair) a domain. The scheduler
will try to put a balanced number of pods into each domain.
Also, we define an eligible domain as a domain whose nodes meet the requirements of
nodeAffinityPolicy and nodeTaintsPolicy.</p></li><li><p><strong>whenUnsatisfiable</strong> indicates how to deal with a Pod if it doesn't satisfy the spread constraint:</p><ul><li><code>DoNotSchedule</code> (default) tells the scheduler not to schedule it.</li><li><code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.</li></ul></li><li><p><strong>labelSelector</strong> is used to find matching Pods. Pods
that match this label selector are counted to determine the
number of Pods in their corresponding topology domain.
See <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">Label Selectors</a>
for more details.</p></li><li><p><strong>matchLabelKeys</strong> is a list of pod label keys to select the group of pods over which
the spreading skew will be calculated. At a pod creation,
the kube-apiserver uses those keys to lookup values from the incoming pod labels,
and those key-value labels will be merged with any existing <code>labelSelector</code>.
The same key is forbidden to exist in both <code>matchLabelKeys</code> and <code>labelSelector</code>.
<code>matchLabelKeys</code> cannot be set when <code>labelSelector</code> isn't set.
Keys that don't exist in the pod labels will be ignored.
A null or empty list means only match against the <code>labelSelector</code>.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>It's not recommended to use <code>matchLabelKeys</code> with labels that might be updated directly on pods.
Even if you edit the pod's label that is specified at <code>matchLabelKeys</code> <strong>directly</strong>,
(that is, you edit the Pod and not a Deployment),
kube-apiserver doesn't reflect the label update onto the merged <code>labelSelector</code>.</div><p>With <code>matchLabelKeys</code>, you don't need to update the <code>pod.spec</code> between different revisions.
The controller/operator just needs to set different values to the same label key for different
revisions. For example, if you are configuring a Deployment, you can use the label keyed with
<a href="/docs/concepts/workloads/controllers/deployment/#pod-template-hash-label">pod-template-hash</a>, which
is added automatically by the Deployment controller, to distinguish between different revisions
in a single Deployment.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologySpreadConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>kubernetes.io/hostname<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchLabelKeys</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- pod-template-hash<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The <code>matchLabelKeys</code> field is a beta-level field and enabled by default in 1.27. You can disable it by disabling the
<code>MatchLabelKeysInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>Before v1.34, <code>matchLabelKeys</code> was handled implicitly.
Since v1.34, key-value labels corresponding to <code>matchLabelKeys</code> are explicitly merged into <code>labelSelector</code>.
You can disable it and revert to the previous behavior by disabling the <code>MatchLabelKeysInPodTopologySpreadSelectorMerge</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> of kube-apiserver.</p></div></li><li><p><strong>nodeAffinityPolicy</strong> indicates how we will treat Pod's nodeAffinity/nodeSelector
when calculating pod topology spread skew. Options are:</p><ul><li>Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.</li><li>Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.</li></ul><p>If this value is null, the behavior is equivalent to the Honor policy.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>nodeAffinityPolicy</code> became beta in 1.26 and graduated to GA in 1.33.
It's enabled by default in beta, you can disable it by disabling the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</div></li><li><p><strong>nodeTaintsPolicy</strong> indicates how we will treat node taints when calculating
pod topology spread skew. Options are:</p><ul><li>Honor: nodes without taints, along with tainted nodes for which the incoming pod
has a toleration, are included.</li><li>Ignore: node taints are ignored. All nodes are included.</li></ul><p>If this value is null, the behavior is equivalent to the Ignore policy.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>nodeTaintsPolicy</code> became beta in 1.26 and graduated to GA in 1.33.
It's enabled by default in beta, you can disable it by disabling the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</div></li></ul><p>When a Pod defines more than one <code>topologySpreadConstraint</code>, those constraints are
combined using a logical AND operation: the kube-scheduler looks for a node for the incoming Pod
that satisfies all the configured constraints.</p><h3 id="node-labels">Node labels</h3><p>Topology spread constraints rely on node labels to identify the topology
domain(s) that each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a> is in.
For example, a node might have labels:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">region</span>:<span style="color:#bbb"> </span>us-east-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">zone</span>:<span style="color:#bbb"> </span>us-east-1a<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>For brevity, this example doesn't use the
<a href="/docs/reference/labels-annotations-taints/">well-known</a> label keys
<code>topology.kubernetes.io/zone</code> and <code>topology.kubernetes.io/region</code>. However,
those registered label keys are nonetheless recommended rather than the private
(unqualified) label keys <code>region</code> and <code>zone</code> that are used here.</p><p>You can't make a reliable assumption about the meaning of a private label key
between different contexts.</p></div><p>Suppose you have a 4-node cluster with the following labels:</p><pre tabindex="0"><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>Then the cluster is logically viewed as below:</p><figure><div class="mermaid">graph TB
subgraph "zoneB"
n3(Node3)
n4(Node4)
end
subgraph "zoneA"
n1(Node1)
n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h2 id="consistency">Consistency</h2><p>You should set the same Pod topology spread constraints on all pods in a group.</p><p>Usually, if you are using a workload controller such as a Deployment, the pod template
takes care of this for you. If you mix different spread constraints then Kubernetes
follows the API definition of the field; however, the behavior is more likely to become
confusing and troubleshooting is less straightforward.</p><p>You need a mechanism to ensure that all the nodes in a topology domain (such as a
cloud provider region) are labeled consistently.
To avoid you needing to manually label nodes, most clusters automatically
populate well-known labels such as <code>kubernetes.io/hostname</code>. Check whether
your cluster supports this.</p><h2 id="topology-spread-constraint-examples">Topology spread constraint examples</h2><h3 id="example-one-topologyspreadconstraint">Example: one topology spread constraint</h3><p>Suppose you have a 4-node cluster where 3 Pods labeled <code>foo: bar</code> are located in
node1, node2 and node3 respectively:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>If you want an incoming Pod to be evenly spread with existing Pods across zones, you
can use a manifest similar to:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint.yaml" download="pods/topology-spread-constraints/one-constraint.yaml"><code>pods/topology-spread-constraints/one-constraint.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-topology-spread-constraints-one-constraint-yaml&quot;)" title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard"/></div><div class="includecode" id="pods-topology-spread-constraints-one-constraint-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">topologySpreadConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>From that manifest, <code>topologyKey: zone</code> implies the even distribution will only be applied
to nodes that are labeled <code>zone: &lt;any value&gt;</code> (nodes that don't have a <code>zone</code> label
are skipped). The field <code>whenUnsatisfiable: DoNotSchedule</code> tells the scheduler to let the
incoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.</p><p>If the scheduler placed this incoming Pod into zone <code>A</code>, the distribution of Pods would
become <code>[3, 1]</code>. That means the actual skew is then 2 (calculated as <code>3 - 1</code>), which
violates <code>maxSkew: 1</code>. To satisfy the constraints and context for this example, the
incoming Pod can only be placed onto a node in zone <code>B</code>:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
p4(mypod) --&gt; n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>OR</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
p4(mypod) --&gt; n3
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>You can tweak the Pod spec to meet various kinds of requirements:</p><ul><li>Change <code>maxSkew</code> to a bigger value - such as <code>2</code> - so that the incoming Pod can
be placed into zone <code>A</code> as well.</li><li>Change <code>topologyKey</code> to <code>node</code> so as to distribute the Pods evenly across nodes
instead of zones. In the above example, if <code>maxSkew</code> remains <code>1</code>, the incoming
Pod can only be placed onto the node <code>node4</code>.</li><li>Change <code>whenUnsatisfiable: DoNotSchedule</code> to <code>whenUnsatisfiable: ScheduleAnyway</code>
to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs
are satisfied). However, it's preferred to be placed into the topology domain which
has fewer matching Pods. (Be aware that this preference is jointly normalized
with other internal scheduling priorities such as resource usage ratio).</li></ul><h3 id="example-multiple-topologyspreadconstraints">Example: multiple topology spread constraints</h3><p>This builds upon the previous example. Suppose you have a 4-node cluster where 3
existing Pods labeled <code>foo: bar</code> are located on node1, node2 and node3 respectively:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>You can combine two topology spread constraints to control the spread of Pods both
by node and by zone:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml" download="pods/topology-spread-constraints/two-constraints.yaml"><code>pods/topology-spread-constraints/two-constraints.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-topology-spread-constraints-two-constraints-yaml&quot;)" title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard"/></div><div class="includecode" id="pods-topology-spread-constraints-two-constraints-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">topologySpreadConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>node<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>In this case, to match the first constraint, the incoming Pod can only be placed onto
nodes in zone <code>B</code>; while in terms of the second constraint, the incoming Pod can only be
scheduled to the node <code>node4</code>. The scheduler only considers options that satisfy all
defined constraints, so the only valid placement is onto node <code>node4</code>.</p><h3 id="example-conflicting-topologyspreadconstraints">Example: conflicting topology spread constraints</h3><p>Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p4(Pod) --&gt; n3(Node3)
p5(Pod) --&gt; n3
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n1
p3(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>If you were to apply
<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml"><code>two-constraints.yaml</code></a>
(the manifest from the previous example)
to <strong>this</strong> cluster, you would see that the Pod <code>mypod</code> stays in the <code>Pending</code> state.
This happens because: to satisfy the first constraint, the Pod <code>mypod</code> can only
be placed into zone <code>B</code>; while in terms of the second constraint, the Pod <code>mypod</code>
can only schedule to node <code>node2</code>. The intersection of the two constraints returns
an empty set, and the scheduler cannot place the Pod.</p><p>To overcome this situation, you can either increase the value of <code>maxSkew</code> or modify
one of the constraints to use <code>whenUnsatisfiable: ScheduleAnyway</code>. Depending on
circumstances, you might also decide to delete an existing Pod manually - for example,
if you are troubleshooting why a bug-fix rollout is not making progress.</p><h4 id="interaction-with-node-affinity-and-node-selectors">Interaction with node affinity and node selectors</h4><p>The scheduler will skip the non-matching nodes from the skew calculations if the
incoming Pod has <code>spec.nodeSelector</code> or <code>spec.affinity.nodeAffinity</code> defined.</p><h3 id="example-topologyspreadconstraints-with-nodeaffinity">Example: topology spread constraints with node affinity</h3><p>Suppose you have a 5-node cluster ranging across zones A to C:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><figure><div class="mermaid">graph BT
subgraph "zoneC"
n5(Node5)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>and you know that zone <code>C</code> must be excluded. In this case, you can compose a manifest
as below, so that Pod <code>mypod</code> will be placed into zone <code>B</code> instead of zone <code>C</code>.
Similarly, Kubernetes also respects <code>spec.nodeSelector</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml" download="pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml"><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml&quot;)" title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard"/></div><div class="includecode" id="pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">topologySpreadConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">labelSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">affinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodeAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>NotIn<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- zoneC<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><h2 id="implicit-conventions">Implicit conventions</h2><p>There are some implicit conventions worth noting here:</p><ul><li><p>Only the Pods holding the same namespace as the incoming Pod can be matching candidates.</p></li><li><p>The scheduler only considers nodes that have all <code>topologySpreadConstraints[*].topologyKey</code> present at the same time.
Nodes missing any of these <code>topologyKeys</code> are bypassed. This implies that:</p><ol><li>any Pods located on those bypassed nodes do not impact <code>maxSkew</code> calculation - in the
above <a href="#example-conflicting-topologyspreadconstraints">example</a>, suppose the node <code>node1</code>
does not have a label "zone", then the 2 Pods will
be disregarded, hence the incoming Pod will be scheduled into zone <code>A</code>.</li><li>the incoming Pod has no chances to be scheduled onto this kind of nodes -
in the above example, suppose a node <code>node5</code> has the <strong>mistyped</strong> label <code>zone-typo: zoneC</code>
(and no <code>zone</code> label set). After node <code>node5</code> joins the cluster, it will be bypassed and
Pods for this workload aren't scheduled there.</li></ol></li><li><p>Be aware of what will happen if the incoming Pod's
<code>topologySpreadConstraints[*].labelSelector</code> doesn't match its own labels. In the
above example, if you remove the incoming Pod's labels, it can still be placed onto
nodes in zone <code>B</code>, since the constraints are still satisfied. However, after that
placement, the degree of imbalance of the cluster remains unchanged - it's still zone <code>A</code>
having 2 Pods labeled as <code>foo: bar</code>, and zone <code>B</code> having 1 Pod labeled as
<code>foo: bar</code>. If this is not what you expect, update the workload's
<code>topologySpreadConstraints[*].labelSelector</code> to match the labels in the pod template.</p></li></ul><h2 id="cluster-level-default-constraints">Cluster-level default constraints</h2><p>It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:</p><ul><li>It doesn't define any constraints in its <code>.spec.topologySpreadConstraints</code>.</li><li>It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.</li></ul><p>Default constraints can be set as part of the <code>PodTopologySpread</code> plugin
arguments in a <a href="/docs/reference/scheduling/config/#profiles">scheduling profile</a>.
The constraints are specified with the same <a href="#topologyspreadconstraints-field">API above</a>, except that
<code>labelSelector</code> must be empty. The selectors are calculated from the Services,
ReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.</p><p>An example configuration might look like follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">profiles</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pluginConfig</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>PodTopologySpread<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">defaultConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">defaultingType</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="internal-default-constraints">Built-in default constraints</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>If you don't configure any cluster-level default constraints for pod topology spreading,
then kube-scheduler acts as if you specified the following default topology constraints:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">defaultConstraints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">"kubernetes.io/hostname"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">"topology.kubernetes.io/zone"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span></span></span></code></pre></div><p>Also, the legacy <code>SelectorSpread</code> plugin, which provides an equivalent behavior,
is disabled by default.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The <code>PodTopologySpread</code> plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy <code>SelectorSpread</code> plugin when
using the default topology constraints.</p><p>If your nodes are not expected to have <strong>both</strong> <code>kubernetes.io/hostname</code> and
<code>topology.kubernetes.io/zone</code> labels set, define your own constraints
instead of using the Kubernetes defaults.</p></div><p>If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting <code>defaultingType</code> to <code>List</code> and leaving
empty <code>defaultConstraints</code> in the <code>PodTopologySpread</code> plugin configuration:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">profiles</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">pluginConfig</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>PodTopologySpread<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">defaultConstraints</span>:<span style="color:#bbb"> </span>[]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">defaultingType</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="comparison-with-podaffinity-podantiaffinity">Comparison with podAffinity and podAntiAffinity</h2><p>In Kubernetes, <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">inter-Pod affinity and anti-affinity</a>
control how Pods are scheduled in relation to one another - either more packed
or more scattered.</p><dl><dt><code>podAffinity</code></dt><dd>attracts Pods; you can try to pack any number of Pods into qualifying
topology domain(s).</dd><dt><code>podAntiAffinity</code></dt><dd>repels Pods. If you set this to <code>requiredDuringSchedulingIgnoredDuringExecution</code> mode then
only a single Pod can be scheduled into a single topology domain; if you choose
<code>preferredDuringSchedulingIgnoredDuringExecution</code> then you lose the ability to enforce the
constraint.</dd></dl><p>For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly.</p><p>For more context, see the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation">Motivation</a>
section of the enhancement proposal about Pod topology spread constraints.</p><h2 id="known-limitations">Known limitations</h2><ul><li><p>There's no guarantee that the constraints remain satisfied when Pods are removed. For
example, scaling down a Deployment may result in imbalanced Pods distribution.</p><p>You can use a tool such as the <a href="https://github.com/kubernetes-sigs/descheduler">Descheduler</a>
to rebalance the Pods distribution.</p></li><li><p>Pods matched on tainted nodes are respected.
See <a href="https://github.com/kubernetes/kubernetes/issues/80921">Issue 80921</a>.</p></li><li><p>The scheduler doesn't have prior knowledge of all the zones or other topology
domains that a cluster has. They are determined from the existing nodes in the
cluster. This could lead to a problem in autoscaled clusters, when a node pool (or
node group) is scaled to zero nodes, and you're expecting the cluster to scale up,
because, in this case, those topology domains won't be considered until there is
at least one node in them.</p><p>You can work around this by using a Node autoscaler that is aware of
Pod topology spread constraints and is also aware of the overall set of topology
domains.</p></li><li><p>Pods that don't match their own labelSelector create "ghost pods". If a pod's
labels don't match the <code>labelSelector</code> in its topology spread constraint, the pod
won't count itself in spread calculations. This means:</p><ul><li>Multiple such pods can just accumulate on the same topology (until matching pods are newly created/deleted) because those pod's schedule don't change a spreading calculation result.</li><li>The spreading constraint works in an unintended way, most likely not matching your expectations</li></ul><p>Ensure your pod's labels match the <code>labelSelector</code> in your spread constraints.
Typically, a pod should match its own topology spread constraint selector.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>The blog article <a href="/blog/2020/05/introducing-podtopologyspread/">Introducing PodTopologySpread</a>
explains <code>maxSkew</code> in some detail, as well as covering some advanced usage examples.</li><li>Read the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling">scheduling</a> section of
the API reference for Pod.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Network Policies</h1><div class="lead">If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.</div><p>If you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols,
then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.
NetworkPolicies are an application-centric construct which allow you to specify how a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="pod">pod</a> is allowed to communicate with various network
"entities" (we use the word "entity" here to avoid overloading the more common terms such as
"endpoints" and "services", which have specific Kubernetes connotations) over the network.
NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to
other connections.</p><p>The entities that a Pod can communicate with are identified through a combination of the following
three identifiers:</p><ol><li>Other pods that are allowed (exception: a pod cannot block access to itself)</li><li>Namespaces that are allowed</li><li>IP blocks (exception: traffic to and from the node where a Pod is running is always allowed,
regardless of the IP address of the Pod or the node)</li></ol><p>When defining a pod- or namespace-based NetworkPolicy, you use a
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels/" target="_blank" aria-label="selector">selector</a> to specify what traffic is allowed to
and from the Pod(s) that match the selector.</p><p>Meanwhile, when IP-based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).</p><h2 id="prerequisites">Prerequisites</h2><p>Network policies are implemented by the <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a>.
To use network policies, you must be using a networking solution which supports NetworkPolicy.
Creating a NetworkPolicy resource without a controller that implements it will have no effect.</p><h2 id="the-two-sorts-of-pod-isolation">The two sorts of pod isolation</h2><p>There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress.
They concern what connections may be established. "Isolation" here is not absolute, rather it
means "some restrictions apply". The alternative, "non-isolated for $direction", means that no
restrictions apply in the stated direction. The two sorts of isolation (or not) are declared
independently, and are both relevant for a connection from one pod to another.</p><p>By default, a pod is non-isolated for egress; all outbound connections are allowed.
A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has
"Egress" in its <code>policyTypes</code>; we say that such a policy applies to the pod for egress.
When a pod is isolated for egress, the only allowed connections from the pod are those allowed by
the <code>egress</code> list of some NetworkPolicy that applies to the pod for egress. Reply traffic for those
allowed connections will also be implicitly allowed.
The effects of those <code>egress</code> lists combine additively.</p><p>By default, a pod is non-isolated for ingress; all inbound connections are allowed.
A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and
has "Ingress" in its <code>policyTypes</code>; we say that such a policy applies to the pod for ingress.
When a pod is isolated for ingress, the only allowed connections into the pod are those from
the pod's node and those allowed by the <code>ingress</code> list of some NetworkPolicy that applies to
the pod for ingress. Reply traffic for those allowed connections will also be implicitly allowed.
The effects of those <code>ingress</code> lists combine additively.</p><p>Network policies do not conflict; they are additive. If any policy or policies apply to a given
pod for a given direction, the connections allowed in that direction from that pod is the union of
what the applicable policies allow. Thus, order of evaluation does not affect the policy result.</p><p>For a connection from a source pod to a destination pod to be allowed, both the egress policy on
the source pod and the ingress policy on the destination pod need to allow the connection. If
either side does not allow the connection, it will not happen.</p><h2 id="networkpolicy-resource">The NetworkPolicy resource</h2><p>See the <a href="/docs/reference/generated/kubernetes-api/v1.34/#networkpolicy-v1-networking-k8s-io">NetworkPolicy</a>
reference for a full definition of the resource.</p><p>An example NetworkPolicy might look like this:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/networkpolicy.yaml" download="service/networking/networkpolicy.yaml"><code>service/networking/networkpolicy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-networkpolicy-yaml&quot;)" title="Copy service/networking/networkpolicy.yaml to clipboard"/></div><div class="includecode" id="service-networking-networkpolicy-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-network-policy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>db<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">from</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">ipBlock</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cidr</span>:<span style="color:#bbb"> </span><span style="color:#666">172.17.0.0</span>/16<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">except</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:#666">172.17.1.0</span>/24<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">namespaceSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">project</span>:<span style="color:#bbb"> </span>myproject<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">egress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">to</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">ipBlock</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cidr</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.0.0</span>/24<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">5978</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>POSTing this to the API server for your cluster will have no effect unless your chosen networking
solution supports network policy.</div><p><strong>Mandatory Fields</strong>: As with all other Kubernetes config, a NetworkPolicy needs <code>apiVersion</code>,
<code>kind</code>, and <code>metadata</code> fields. For general information about working with config files, see
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap</a>,
and <a href="/docs/concepts/overview/working-with-objects/object-management/">Object Management</a>.</p><p><strong>spec</strong>: NetworkPolicy <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status">spec</a>
has all the information needed to define a particular network policy in the given namespace.</p><p><strong>podSelector</strong>: Each NetworkPolicy includes a <code>podSelector</code> which selects the grouping of pods to
which the policy applies. The example policy selects pods with the label "role=db". An empty
<code>podSelector</code> selects all pods in the namespace.</p><p><strong>policyTypes</strong>: Each NetworkPolicy includes a <code>policyTypes</code> list which may include either
<code>Ingress</code>, <code>Egress</code>, or both. The <code>policyTypes</code> field indicates whether or not the given policy
applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no
<code>policyTypes</code> are specified on a NetworkPolicy then by default <code>Ingress</code> will always be set and
<code>Egress</code> will be set if the NetworkPolicy has any egress rules.</p><p><strong>ingress</strong>: Each NetworkPolicy may include a list of allowed <code>ingress</code> rules. Each rule allows
traffic which matches both the <code>from</code> and <code>ports</code> sections. The example policy contains a single
rule, which matches traffic on a single port, from one of three sources, the first specified via
an <code>ipBlock</code>, the second via a <code>namespaceSelector</code> and the third via a <code>podSelector</code>.</p><p><strong>egress</strong>: Each NetworkPolicy may include a list of allowed <code>egress</code> rules. Each rule allows
traffic which matches both the <code>to</code> and <code>ports</code> sections. The example policy contains a single
rule, which matches traffic on a single port to any destination in <code>10.0.0.0/24</code>.</p><p>So, the example NetworkPolicy:</p><ol><li><p>isolates <code>role=db</code> pods in the <code>default</code> namespace for both ingress and egress traffic
(if they weren't already isolated)</p></li><li><p>(Ingress rules) allows connections to all pods in the <code>default</code> namespace with the label
<code>role=db</code> on TCP port 6379 from:</p><ul><li>any pod in the <code>default</code> namespace with the label <code>role=frontend</code></li><li>any pod in a namespace with the label <code>project=myproject</code></li><li>IP addresses in the ranges <code>172.17.0.0</code>â€“<code>172.17.0.255</code> and <code>172.17.2.0</code>â€“<code>172.17.255.255</code>
(ie, all of <code>172.17.0.0/16</code> except <code>172.17.1.0/24</code>)</li></ul></li><li><p>(Egress rules) allows connections from any pod in the <code>default</code> namespace with the label
<code>role=db</code> to CIDR <code>10.0.0.0/24</code> on TCP port 5978</p></li></ol><p>See the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
walkthrough for further examples.</p><h2 id="behavior-of-to-and-from-selectors">Behavior of <code>to</code> and <code>from</code> selectors</h2><p>There are four kinds of selectors that can be specified in an <code>ingress</code> <code>from</code> section or <code>egress</code>
<code>to</code> section:</p><p><strong>podSelector</strong>: This selects particular Pods in the same namespace as the NetworkPolicy which
should be allowed as ingress sources or egress destinations.</p><p><strong>namespaceSelector</strong>: This selects particular namespaces for which all Pods should be allowed as
ingress sources or egress destinations.</p><p><strong>namespaceSelector</strong> <em>and</em> <strong>podSelector</strong>: A single <code>to</code>/<code>from</code> entry that specifies both
<code>namespaceSelector</code> and <code>podSelector</code> selects particular Pods within particular namespaces. Be
careful to use correct YAML syntax. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">from</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">namespaceSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>alice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>This policy contains a single <code>from</code> element allowing connections from Pods with the label
<code>role=client</code> in namespaces with the label <code>user=alice</code>. But the following policy is different:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">from</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">namespaceSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">user</span>:<span style="color:#bbb"> </span>alice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>It contains two elements in the <code>from</code> array, and allows connections from Pods in the local
Namespace with the label <code>role=client</code>, <em>or</em> from any Pod in any namespace with the label
<code>user=alice</code>.</p><p>When in doubt, use <code>kubectl describe</code> to see how Kubernetes has interpreted the policy.</p><p><a name="behavior-of-ipblock-selectors"/><strong>ipBlock</strong>: This selects particular IP CIDR ranges to allow as ingress sources or egress
destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.</p><p>Cluster ingress and egress mechanisms often require rewriting the source or destination IP
of packets. In cases where this happens, it is not defined whether this happens before or
after NetworkPolicy processing, and the behavior may be different for different
combinations of network plugin, cloud provider, <code>Service</code> implementation, etc.</p><p>In the case of ingress, this means that in some cases you may be able to filter incoming
packets based on the actual original source IP, while in other cases, the "source IP" that
the NetworkPolicy acts on may be the IP of a <code>LoadBalancer</code> or of the Pod's node, etc.</p><p>For egress, this means that connections from pods to <code>Service</code> IPs that get rewritten to
cluster-external IPs may or may not be subject to <code>ipBlock</code>-based policies.</p><h2 id="default-policies">Default policies</h2><p>By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to
and from pods in that namespace. The following examples let you change the default behavior
in that namespace.</p><h3 id="default-deny-all-ingress-traffic">Default deny all ingress traffic</h3><p>You can create a "default" ingress isolation policy for a namespace by creating a NetworkPolicy
that selects all pods but does not allow any ingress traffic to those pods.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-ingress.yaml" download="service/networking/network-policy-default-deny-ingress.yaml"><code>service/networking/network-policy-default-deny-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-network-policy-default-deny-ingress-yaml&quot;)" title="Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard"/></div><div class="includecode" id="service-networking-network-policy-default-deny-ingress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-deny-ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated
for ingress. This policy does not affect isolation for egress from any pod.</p><h3 id="allow-all-ingress-traffic">Allow all ingress traffic</h3><p>If you want to allow all incoming connections to all pods in a namespace, you can create a policy
that explicitly allows that.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-allow-all-ingress.yaml" download="service/networking/network-policy-allow-all-ingress.yaml"><code>service/networking/network-policy-allow-all-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-network-policy-allow-all-ingress-yaml&quot;)" title="Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard"/></div><div class="includecode" id="service-networking-network-policy-allow-all-ingress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>allow-all-ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- {}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>With this policy in place, no additional policy or policies can cause any incoming connection to
those pods to be denied. This policy has no effect on isolation for egress from any pod.</p><h3 id="default-deny-all-egress-traffic">Default deny all egress traffic</h3><p>You can create a "default" egress isolation policy for a namespace by creating a NetworkPolicy
that selects all pods but does not allow any egress traffic from those pods.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-egress.yaml" download="service/networking/network-policy-default-deny-egress.yaml"><code>service/networking/network-policy-default-deny-egress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-network-policy-default-deny-egress-yaml&quot;)" title="Copy service/networking/network-policy-default-deny-egress.yaml to clipboard"/></div><div class="includecode" id="service-networking-network-policy-default-deny-egress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-deny-egress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed
egress traffic. This policy does not change the ingress isolation behavior of any pod.</p><h3 id="allow-all-egress-traffic">Allow all egress traffic</h3><p>If you want to allow all connections from all pods in a namespace, you can create a policy that
explicitly allows all outgoing connections from pods in that namespace.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-allow-all-egress.yaml" download="service/networking/network-policy-allow-all-egress.yaml"><code>service/networking/network-policy-allow-all-egress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-network-policy-allow-all-egress-yaml&quot;)" title="Copy service/networking/network-policy-allow-all-egress.yaml to clipboard"/></div><div class="includecode" id="service-networking-network-policy-allow-all-egress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>allow-all-egress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">egress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- {}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>With this policy in place, no additional policy or policies can cause any outgoing connection from
those pods to be denied. This policy has no effect on isolation for ingress to any pod.</p><h3 id="default-deny-all-ingress-and-all-egress-traffic">Default deny all ingress and all egress traffic</h3><p>You can create a "default" policy for a namespace which prevents all ingress AND egress traffic by
creating the following NetworkPolicy in that namespace.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-all.yaml" download="service/networking/network-policy-default-deny-all.yaml"><code>service/networking/network-policy-default-deny-all.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-network-policy-default-deny-all-yaml&quot;)" title="Copy service/networking/network-policy-default-deny-all.yaml to clipboard"/></div><div class="includecode" id="service-networking-network-policy-default-deny-all-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default-deny-all<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed
ingress or egress traffic.</p><h2 id="network-traffic-filtering">Network traffic filtering</h2><p>NetworkPolicy is defined for <a href="https://en.wikipedia.org/wiki/OSI_model#Layer_4:_Transport_layer">layer 4</a>
connections (TCP, UDP, and optionally SCTP). For all the other protocols, the behaviour may vary
across network plugins.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You must be using a <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" aria-label="CNI">CNI</a> plugin that supports SCTP
protocol NetworkPolicies.</div><p>When a <code>deny all</code> network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP
connections. For other protocols, such as ARP or ICMP, the behaviour is undefined.
The same applies to allow rules: when a specific pod is allowed as ingress source or egress destination,
it is undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be allowed by some
network plugins and denied by others.</p><h2 id="targeting-a-range-of-ports">Targeting a range of ports</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>When writing a NetworkPolicy, you can target a range of ports instead of a single port.</p><p>This is achievable with the usage of the <code>endPort</code> field, as the following example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/networkpolicy-multiport-egress.yaml" download="service/networking/networkpolicy-multiport-egress.yaml"><code>service/networking/networkpolicy-multiport-egress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-networkpolicy-multiport-egress-yaml&quot;)" title="Copy service/networking/networkpolicy-multiport-egress.yaml to clipboard"/></div><div class="includecode" id="service-networking-networkpolicy-multiport-egress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>multi-port-egress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>db<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- Egress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">egress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">to</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">ipBlock</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">cidr</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.0.0</span>/24<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">32000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">endPort</span>:<span style="color:#bbb"> </span><span style="color:#666">32768</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The above rule allows any Pod with label <code>role=db</code> on the namespace <code>default</code> to communicate
with any IP within the range <code>10.0.0.0/24</code> over TCP, provided that the target
port is between the range 32000 and 32768.</p><p>The following restrictions apply when using this field:</p><ul><li>The <code>endPort</code> field must be equal to or greater than the <code>port</code> field.</li><li><code>endPort</code> can only be defined if <code>port</code> is also defined.</li><li>Both ports must be numeric.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Your cluster must be using a <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" aria-label="CNI">CNI</a> plugin that
supports the <code>endPort</code> field in NetworkPolicy specifications.
If your <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a>
does not support the <code>endPort</code> field and you specify a NetworkPolicy with that,
the policy will be applied only for the single <code>port</code> field.</div><h2 id="targeting-multiple-namespaces-by-label">Targeting multiple namespaces by label</h2><p>In this scenario, your <code>Egress</code> NetworkPolicy targets more than one namespace using their
label names. For this to work, you need to label the target namespaces. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl label namespace frontend <span style="color:#b8860b">namespace</span><span style="color:#666">=</span>frontend
</span></span><span style="display:flex"><span>kubectl label namespace backend <span style="color:#b8860b">namespace</span><span style="color:#666">=</span>backend
</span></span></code></pre></div><p>Add the labels under <code>namespaceSelector</code> in your NetworkPolicy document. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>egress-namespaces<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">policyTypes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">egress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">to</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">namespaceSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>namespace<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"frontend"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"backend"</span>]<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>It is not possible to directly specify the name of the namespaces in a NetworkPolicy.
You must use a <code>namespaceSelector</code> with <code>matchLabels</code> or <code>matchExpressions</code> to select the
namespaces based on their labels.</div><h2 id="targeting-a-namespace-by-its-name">Targeting a Namespace by its name</h2><p>The Kubernetes control plane sets an immutable label <code>kubernetes.io/metadata.name</code> on all
namespaces, the value of the label is the namespace name.</p><p>While NetworkPolicy cannot target a namespace by its name with some object field, you can use the
standardized label to target a specific namespace.</p><h2 id="pod-lifecycle">Pod lifecycle</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The following applies to clusters with a conformant networking plugin and a conformant implementation of
NetworkPolicy.</div><p>When a new NetworkPolicy object is created, it may take some time for a network plugin
to handle the new object. If a pod that is affected by a NetworkPolicy
is created before the network plugin has completed NetworkPolicy handling,
that pod may be started unprotected, and isolation rules will be applied when
the NetworkPolicy handling is completed.</p><p>Once the NetworkPolicy is handled by a network plugin,</p><ol><li><p>All newly created pods affected by a given NetworkPolicy will be isolated before they are started.
Implementations of NetworkPolicy must ensure that filtering is effective throughout
the Pod lifecycle, even from the very first instant that any container in that Pod is started.
Because they are applied at Pod level, NetworkPolicies apply equally to init containers,
sidecar containers, and regular containers.</p></li><li><p>Allow rules will be applied eventually after the isolation rules (or may be applied at the same time).
In the worst case, a newly created pod may have no network connectivity at all when it is first started, if
isolation rules were already applied, but no allow rules were applied yet.</p></li></ol><p>Every created NetworkPolicy will be handled by a network plugin eventually, but there is no
way to tell from the Kubernetes API when exactly that happens.</p><p>Therefore, pods must be resilient against being started up with different network
connectivity than expected. If you need to make sure the pod can reach certain destinations
before being started, you can use an <a href="/docs/concepts/workloads/pods/init-containers/">init container</a>
to wait for those destinations to be reachable before kubelet starts the app containers.</p><p>Every NetworkPolicy will be applied to all selected pods eventually.
Because the network plugin may implement NetworkPolicy in a distributed manner,
it is possible that pods may see a slightly inconsistent view of network policies
when the pod is first created, or when pods or policies change.
For example, a newly-created pod that is supposed to be able to reach both Pod A
on Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately,
but cannot reach Pod B until a few seconds later.</p><h2 id="networkpolicy-and-hostnetwork-pods">NetworkPolicy and <code>hostNetwork</code> pods</h2><p>NetworkPolicy behaviour for <code>hostNetwork</code> pods is undefined, but it should be limited to 2 possibilities:</p><ul><li>The network plugin can distinguish <code>hostNetwork</code> pod traffic from all other traffic
(including being able to distinguish traffic from different <code>hostNetwork</code> pods on
the same node), and will apply NetworkPolicy to <code>hostNetwork</code> pods just like it does
to pod-network pods.</li><li>The network plugin cannot properly distinguish <code>hostNetwork</code> pod traffic,
and so it ignores <code>hostNetwork</code> pods when matching <code>podSelector</code> and <code>namespaceSelector</code>.
Traffic to/from <code>hostNetwork</code> pods is treated the same as all other traffic to/from the node IP.
(This is the most common implementation.)</li></ul><p>This applies when</p><ol><li><p>a <code>hostNetwork</code> pod is selected by <code>spec.podSelector</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div></li><li><p>a <code>hostNetwork</code> pod is selected by a <code>podSelector</code> or <code>namespaceSelector</code> in an <code>ingress</code> or <code>egress</code> rule.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">from</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">podSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">role</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div></li></ol><p>At the same time, since <code>hostNetwork</code> pods have the same IP addresses as the nodes they reside on,
their connections will be treated as node connections. For example, you can allow traffic
from a <code>hostNetwork</code> Pod using an <code>ipBlock</code> rule.</p><h2 id="what-you-can-t-do-with-network-policies-at-least-not-yet">What you can't do with network policies (at least, not yet)</h2><p>As of Kubernetes 1.34, the following functionality does not exist in the
NetworkPolicy API, but you might be able to implement workarounds using Operating System
components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress
controllers, Service Mesh implementations) or admission controllers. In case you are new to
network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be
implemented using the NetworkPolicy API.</p><ul><li>Forcing internal cluster traffic to go through a common gateway (this might be best served with
a service mesh or other proxy).</li><li>Anything TLS related (use a service mesh or ingress controller for this).</li><li>Node specific policies (you can use CIDR notation for these, but you cannot target nodes by
their Kubernetes identities specifically).</li><li>Targeting of services by name (you can, however, target pods or namespaces by their
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="labels">labels</a>, which is often a viable workaround).</li><li>Creation or management of "Policy requests" that are fulfilled by a third party.</li><li>Default policies which are applied to all namespaces or pods (there are some third party
Kubernetes distributions and projects which can do this).</li><li>Advanced policy querying and reachability tooling.</li><li>The ability to log network security events (for example connections that are blocked or accepted).</li><li>The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by
default, with only the ability to add allow rules).</li><li>The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost
access, nor do they have the ability to block access from their resident node).</li></ul><h2 id="networkpolicy-s-impact-on-existing-connections">NetworkPolicy's impact on existing connections</h2><p>When the set of NetworkPolicies that applies to an existing connection changes - this could happen
either due to a change in NetworkPolicies or if the relevant labels of the namespaces/pods selected by the
policy (both subject and peers) are changed in the middle of an existing connection - it is
implementation defined as to whether the change will take effect for that existing connection or not.
Example: A policy is created that leads to denying a previously allowed connection, the underlying network plugin
implementation is responsible for defining if that new policy will close the existing connections or not.
It is recommended not to modify policies/pods/namespaces in ways that might affect existing connections.</p><h2 id="what-s-next">What's next</h2><ul><li>See the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
walkthrough for further examples.</li><li>See more <a href="https://github.com/ahmetb/kubernetes-network-policy-recipes">recipes</a> for common
scenarios enabled by the NetworkPolicy resource.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Taints and Tolerations</h1><p><a href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity"><em>Node affinity</em></a>
is a property of <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> that <em>attracts</em> them to
a set of <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a> (either as a preference or a
hard requirement). <em>Taints</em> are the opposite -- they allow a node to repel a set of pods.</p><p><em>Tolerations</em> are applied to pods. Tolerations allow the scheduler to schedule pods with matching
taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">evaluates other parameters</a>
as part of its function.</p><p>Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not tolerate the taints.</p><h2 id="concepts">Concepts</h2><p>You add a taint to a node using <a href="/docs/reference/generated/kubectl/kubectl-commands#taint">kubectl taint</a>.
For example,</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoSchedule
</span></span></code></pre></div><p>places a taint on node <code>node1</code>. The taint has key <code>key1</code>, value <code>value1</code>, and taint effect <code>NoSchedule</code>.
This means that no pod will be able to schedule onto <code>node1</code> unless it has a matching toleration.</p><p>To remove the taint added by the command above, you can run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoSchedule-
</span></span></code></pre></div><p>You specify a toleration for a pod in the PodSpec. Both of the following tolerations "match" the
taint created by the <code>kubectl taint</code> line above, and thus a pod with either toleration would be able
to schedule onto <code>node1</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Equal"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"value1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoSchedule"</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Exists"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoSchedule"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The default Kubernetes scheduler takes taints and tolerations into account when
selecting a node to run a particular Pod. However, if you manually specify the
<code>.spec.nodeName</code> for a Pod, that action bypasses the scheduler; the Pod is then
bound onto the node where you assigned it, even if there are <code>NoSchedule</code>
taints on that node that you selected.
If this happens and the node also has a <code>NoExecute</code> taint set, the kubelet will
eject the Pod unless there is an appropriate tolerance set.</p><p>Here's an example of a pod that has some tolerations defined:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-toleration.yaml" download="pods/pod-with-toleration.yaml"><code>pods/pod-with-toleration.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-with-toleration-yaml&quot;)" title="Copy pods/pod-with-toleration.yaml to clipboard"/></div><div class="includecode" id="pods-pod-with-toleration-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"example-key"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Exists"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoSchedule"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The default value for <code>operator</code> is <code>Equal</code>.</p><p>A toleration "matches" a taint if the keys are the same and the effects are the same, and:</p><ul><li>the <code>operator</code> is <code>Exists</code> (in which case no <code>value</code> should be specified), or</li><li>the <code>operator</code> is <code>Equal</code> and the values should be equal.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>There are two special cases:</p><p>If the <code>key</code> is empty, then the <code>operator</code> must be <code>Exists</code>, which matches all keys and values. Note that the <code>effect</code> still needs to be matched at the same time.</p><p>An empty <code>effect</code> matches all effects with key <code>key1</code>.</p></div><p>The above example used the <code>effect</code> of <code>NoSchedule</code>. Alternatively, you can use the <code>effect</code> of <code>PreferNoSchedule</code>.</p><p>The allowed values for the <code>effect</code> field are:</p><dl><dt><code>NoExecute</code></dt><dd>This affects pods that are already running on the node as follows:<ul><li>Pods that do not tolerate the taint are evicted immediately</li><li>Pods that tolerate the taint without specifying <code>tolerationSeconds</code> in
their toleration specification remain bound forever</li><li>Pods that tolerate the taint with a specified <code>tolerationSeconds</code> remain
bound for the specified amount of time. After that time elapses, the node
lifecycle controller evicts the Pods from the node.</li></ul></dd><dt><code>NoSchedule</code></dt><dd>No new Pods will be scheduled on the tainted node unless they have a matching
toleration. Pods currently running on the node are <strong>not</strong> evicted.</dd><dt><code>PreferNoSchedule</code></dt><dd><code>PreferNoSchedule</code> is a "preference" or "soft" version of <code>NoSchedule</code>.
The control plane will <em>try</em> to avoid placing a Pod that does not tolerate
the taint on the node, but it is not guaranteed.</dd></dl><p>You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,</p><ul><li>if there is at least one un-ignored taint with effect <code>NoSchedule</code> then Kubernetes will not schedule
the pod onto that node</li><li>if there is no un-ignored taint with effect <code>NoSchedule</code> but there is at least one un-ignored taint with
effect <code>PreferNoSchedule</code> then Kubernetes will <em>try</em> to not schedule the pod onto the node</li><li>if there is at least one un-ignored taint with effect <code>NoExecute</code> then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).</li></ul><p>For example, imagine you taint a node like this</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoSchedule
</span></span><span style="display:flex"><span>kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoExecute
</span></span><span style="display:flex"><span>kubectl taint nodes node1 <span style="color:#b8860b">key2</span><span style="color:#666">=</span>value2:NoSchedule
</span></span></code></pre></div><p>And a pod has two tolerations:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Equal"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"value1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoSchedule"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Equal"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"value1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoExecute"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.</p><p>Normally, if a taint with effect <code>NoExecute</code> is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and pods that do tolerate the
taint will never be evicted. However, a toleration with <code>NoExecute</code> effect can specify
an optional <code>tolerationSeconds</code> field that dictates how long the pod will stay bound
to the node after the taint is added. For example,</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"key1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Equal"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"value1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoExecute"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tolerationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3600</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted.</p><h2 id="example-use-cases">Example Use Cases</h2><p>Taints and tolerations are a flexible way to steer pods <em>away</em> from nodes or evict
pods that shouldn't be running. A few of the use cases are</p><ul><li><p><strong>Dedicated Nodes</strong>: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
<a href="/docs/reference/access-authn-authz/admission-controllers/">admission controller</a>).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them <em>and</em>
ensure they <em>only</em> use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. <code>dedicated=groupName</code>), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with <code>dedicated=groupName</code>.</p></li><li><p><strong>Nodes with Special Hardware</strong>: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don't need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. <code>kubectl taint nodes nodename special=true:NoSchedule</code> or
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
<a href="/docs/reference/access-authn-authz/admission-controllers/">admission controller</a>.
For example, it is recommended to use <a href="/docs/concepts/configuration/manage-resources-containers/#extended-resources">Extended
Resources</a>
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
<a href="/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration">ExtendedResourceToleration</a>
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the <code>ExtendedResourceToleration</code> admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don't have to
manually add tolerations to your pods.</p></li><li><p><strong>Taint based Evictions</strong>: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.</p></li></ul><h2 id="taint-based-evictions">Taint based Evictions</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>The node controller automatically taints a Node when certain conditions
are true. The following taints are built in:</p><ul><li><code>node.kubernetes.io/not-ready</code>: Node is not ready. This corresponds to
the NodeCondition <code>Ready</code> being "<code>False</code>".</li><li><code>node.kubernetes.io/unreachable</code>: Node is unreachable from the node
controller. This corresponds to the NodeCondition <code>Ready</code> being "<code>Unknown</code>".</li><li><code>node.kubernetes.io/memory-pressure</code>: Node has memory pressure.</li><li><code>node.kubernetes.io/disk-pressure</code>: Node has disk pressure.</li><li><code>node.kubernetes.io/pid-pressure</code>: Node has PID pressure.</li><li><code>node.kubernetes.io/network-unavailable</code>: Node's network is unavailable.</li><li><code>node.kubernetes.io/unschedulable</code>: Node is unschedulable.</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>: When the kubelet is started
with an "external" cloud provider, this taint is set on a node to mark it
as unusable. After a controller from the cloud-controller-manager initializes
this node, the kubelet removes this taint.</li></ul><p>In case a node is to be drained, the node controller or the kubelet adds relevant taints
with <code>NoExecute</code> effect. This effect is added by default for the
<code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code> taints.
If the fault condition returns to normal, the kubelet or node
controller can remove the relevant taint(s).</p><p>In some cases when the node is unreachable, the API server is unable to communicate
with the kubelet on the node. The decision to delete the pods cannot be communicated to
the kubelet until communication with the API server is re-established. In the meantime,
the pods that are scheduled for deletion may continue to run on the partitioned node.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The control plane limits the rate of adding new taints to nodes. This rate limiting
manages the number of evictions that are triggered when many nodes become unreachable at
once (for example: if there is a network disruption).</div><p>You can specify <code>tolerationSeconds</code> for a Pod to define how long that Pod stays bound
to a failing or unresponsive Node.</p><p>For example, you might want to keep an application with a lot of local state
bound to node for a long time in the event of network partition, hoping
that the partition will recover and thus the pod eviction can be avoided.
The toleration you set for that Pod might look like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"node.kubernetes.io/unreachable"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Exists"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoExecute"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tolerationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">6000</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Kubernetes automatically adds a toleration for
<code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code>
with <code>tolerationSeconds=300</code>,
unless you, or a controller, set those tolerations explicitly.</p><p>These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.</p></div><p><a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> pods are created with
<code>NoExecute</code> tolerations for the following taints with no <code>tolerationSeconds</code>:</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>This ensures that DaemonSet pods are never evicted due to these problems.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The node controller was responsible for adding taints to nodes and evicting pods. But after 1.29,
the taint-based eviction implementation has been moved out of node controller into a separate,
and independent component called taint-eviction-controller. Users can optionally disable taint-based
eviction by setting <code>--controllers=-taint-eviction-controller</code> in kube-controller-manager.</div><h2 id="taint-nodes-by-condition">Taint Nodes by Condition</h2><p>The control plane, using the node <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>,
automatically creates taints with a <code>NoSchedule</code> effect for
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions">node conditions</a>.</p><p>The scheduler checks taints, not node conditions, when it makes scheduling
decisions. This ensures that node conditions don't directly affect scheduling.
For example, if the <code>DiskPressure</code> node condition is active, the control plane
adds the <code>node.kubernetes.io/disk-pressure</code> taint and does not schedule new pods
onto the affected node. If the <code>MemoryPressure</code> node condition is active, the
control plane adds the <code>node.kubernetes.io/memory-pressure</code> taint.</p><p>You can ignore node conditions for newly created pods by adding the corresponding
Pod tolerations. The control plane also adds the <code>node.kubernetes.io/memory-pressure</code>
toleration on pods that have a <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/pod-qos/" target="_blank" aria-label="QoS class">QoS class</a>
other than <code>BestEffort</code>. This is because Kubernetes treats pods in the <code>Guaranteed</code>
or <code>Burstable</code> QoS classes (even pods with no memory request set) as if they are
able to cope with memory pressure, while new <code>BestEffort</code> pods are not scheduled
onto the affected node.</p><p>The DaemonSet controller automatically adds the following <code>NoSchedule</code>
tolerations to all daemons, to prevent DaemonSets from breaking.</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 or later)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 or later)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>host network only</em>)</li></ul><p>Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.</p><h2 id="device-taints-and-tolerations">Device taints and tolerations</h2><p>Instead of tainting entire nodes, administrators can also <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#device-taints-and-tolerations">taint individual devices</a>
when the cluster uses <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">dynamic resource allocation</a>
to manage special hardware. The advantage is that tainting can be targeted towards exactly the hardware that
is faulty or needs maintenance. Tolerations are also supported and can be specified when requesting
devices. Like taints they apply to all pods which share the same allocated device.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a>
and how you can configure it</li><li>Read about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority</a></li><li>Read about <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#device-taints-and-tolerations">device taints and tolerations</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Security Standards</h1><div class="lead">A detailed look at the different policy levels defined in the Pod Security Standards.</div><p>The Pod Security Standards define three different <em>policies</em> to broadly cover the security
spectrum. These policies are <em>cumulative</em> and range from highly-permissive to highly-restrictive.
This guide outlines the requirements of each policy.</p><table><thead><tr><th>Profile</th><th>Description</th></tr></thead><tbody><tr><td><strong style="white-space:nowrap">Privileged</strong></td><td>Unrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations.</td></tr><tr><td><strong style="white-space:nowrap">Baseline</strong></td><td>Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration.</td></tr><tr><td><strong style="white-space:nowrap">Restricted</strong></td><td>Heavily restricted policy, following current Pod hardening best practices.</td></tr></tbody></table><h2 id="profile-details">Profile Details</h2><h3 id="privileged">Privileged</h3><p><strong>The <em>Privileged</em> policy is purposely-open, and entirely unrestricted.</strong> This type of policy is
typically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.</p><p>The Privileged policy is defined by an absence of restrictions. If you define a Pod where the Privileged
security policy applies, the Pod you define is able to bypass typical container isolation mechanisms.
For example, you can define a Pod that has access to the node's host network.</p><h3 id="baseline">Baseline</h3><p><strong>The <em>Baseline</em> policy is aimed at ease of adoption for common containerized workloads while
preventing known privilege escalations.</strong> This policy is targeted at application operators and
developers of non-critical applications. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption style="display:none">Baseline policy specification</caption><tbody><tr><th>Control</th><th>Policy</th></tr><tr><td style="white-space:nowrap">HostProcess</td><td><p>Windows Pods offer the ability to run <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess containers</a> which enables privileged access to the Windows host machine. Privileged access to the host is disallowed in the Baseline policy.<div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.windowsOptions.hostProcess</code></li><li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style="white-space:nowrap">Host Namespaces</td><td><p>Sharing the host namespaces must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.hostNetwork</code></li><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style="white-space:nowrap">Privileged Containers</td><td><p>Privileged Pods disable most security mechanisms and must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.initContainers[*].securityContext.privileged</code></li><li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style="white-space:nowrap">Capabilities</td><td><p>Adding additional capabilities beyond those listed below must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>AUDIT_WRITE</code></li><li><code>CHOWN</code></li><li><code>DAC_OVERRIDE</code></li><li><code>FOWNER</code></li><li><code>FSETID</code></li><li><code>KILL</code></li><li><code>MKNOD</code></li><li><code>NET_BIND_SERVICE</code></li><li><code>SETFCAP</code></li><li><code>SETGID</code></li><li><code>SETPCAP</code></li><li><code>SETUID</code></li><li><code>SYS_CHROOT</code></li></ul></td></tr><tr><td style="white-space:nowrap">HostPath Volumes</td><td><p>HostPath volumes must be forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*].hostPath</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li></ul></td></tr><tr><td style="white-space:nowrap">Host Ports</td><td><p>HostPorts should be disallowed entirely (recommended) or restricted to a known list</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].ports[*].hostPort</code></li><li><code>spec.initContainers[*].ports[*].hostPort</code></li><li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li>Known list (not supported by the built-in <a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission controller</a>)</li><li><code>0</code></li></ul></td></tr><tr><td>Host Probes / Lifecycle Hooks (v1.34+)</td><td><p>The Host field in probes and lifecycle hooks must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].livenessProbe.httpGet.host</code></li><li><code>spec.containers[*].readinessProbe.httpGet.host</code></li><li><code>spec.containers[*].startupProbe.httpGet.host</code></li><li><code>spec.containers[*].livenessProbe.tcpSocket.host</code></li><li><code>spec.containers[*].readinessProbe.tcpSocket.host</code></li><li><code>spec.containers[*].startupProbe.tcpSocket.host</code></li><li><code>spec.containers[*].lifecycle.postStart.tcpSocket.host</code><li><code>spec.containers[*].lifecycle.preStop.tcpSocket.host</code><li><code>spec.containers[*].lifecycle.postStart.httpGet.host</code></li><li><code>spec.containers[*].lifecycle.preStop.httpGet.host</code></li><li><code>spec.initContainers[*].livenessProbe.httpGet.host</code></li><li><code>spec.initContainers[*].readinessProbe.httpGet.host</code></li><li><code>spec.initContainers[*].startupProbe.httpGet.host</code></li><li><code>spec.initContainers[*].livenessProbe.tcpSocket.host</code></li><li><code>spec.initContainers[*].readinessProbe.tcpSocket.host</code></li><li><code>spec.initContainers[*].startupProbe.tcpSocket.host</code></li><li><code>spec.initContainers[*].lifecycle.postStart.tcpSocket.host</code><li><code>spec.initContainers[*].lifecycle.preStop.tcpSocket.host</code><li><code>spec.initContainers[*].lifecycle.postStart.httpGet.host</code></li><li><code>spec.initContainers[*].lifecycle.preStop.httpGet.host</code></li></li></li></li></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li>""</li></ul></td></tr><tr><td style="white-space:nowrap">AppArmor</td><td><p>On supported hosts, the <code>RuntimeDefault</code> AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.appArmorProfile.type</code></li><li><code>spec.containers[*].securityContext.appArmorProfile.type</code></li><li><code>spec.initContainers[*].securityContext.appArmorProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.appArmorProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul><hr/><ul><li><code>metadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>runtime/default</code></li><li><code>localhost/*</code></li></ul></td></tr><tr><td style="white-space:nowrap">SELinux</td><td><p>Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.type</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li><li><code>container_t</code></li><li><code>container_init_t</code></li><li><code>container_kvm_t</code></li><li><code>container_engine_t</code> (since Kubernetes 1.31)</li></ul><hr/><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.user</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.securityContext.seLinuxOptions.role</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li></ul></td></tr><tr><td style="white-space:nowrap"><code>/proc</code> Mount Type</td><td><p>The default <code>/proc</code> masks are set up to reduce attack surface, and should be required.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.initContainers[*].securityContext.procMount</code></li><li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>Default</code></li></ul></td></tr><tr><td>Seccomp</td><td><p>Seccomp profile must not be explicitly set to <code>Unconfined</code>.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul></td></tr><tr><td style="white-space:nowrap">Sysctls</td><td><p>Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed "safe" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.sysctls[*].name</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>kernel.shm_rmid_forced</code></li><li><code>net.ipv4.ip_local_port_range</code></li><li><code>net.ipv4.ip_unprivileged_port_start</code></li><li><code>net.ipv4.tcp_syncookies</code></li><li><code>net.ipv4.ping_group_range</code></li><li><code>net.ipv4.ip_local_reserved_ports</code> (since Kubernetes 1.27)</li><li><code>net.ipv4.tcp_keepalive_time</code> (since Kubernetes 1.29)</li><li><code>net.ipv4.tcp_fin_timeout</code> (since Kubernetes 1.29)</li><li><code>net.ipv4.tcp_keepalive_intvl</code> (since Kubernetes 1.29)</li><li><code>net.ipv4.tcp_keepalive_probes</code> (since Kubernetes 1.29)</li></ul></td></tr></tbody></table><h3 id="restricted">Restricted</h3><p><strong>The <em>Restricted</em> policy is aimed at enforcing current Pod hardening best practices, at the
expense of some compatibility.</strong> It is targeted at operators and developers of security-critical
applications, as well as lower-trust users. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption style="display:none">Restricted policy specification</caption><tbody><tr><td><strong>Control</strong></td><td><strong>Policy</strong></td></tr><tr><td colspan="2"><em>Everything from the Baseline policy</em></td></tr><tr><td style="white-space:nowrap">Volume Types</td><td><p>The Restricted policy only permits the following volume types.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*]</code></li></ul><p><strong>Allowed Values</strong></p>Every item in the <code>spec.volumes[*]</code> list must set one of the following fields to a non-null value:<ul><li><code>spec.volumes[*].configMap</code></li><li><code>spec.volumes[*].csi</code></li><li><code>spec.volumes[*].downwardAPI</code></li><li><code>spec.volumes[*].emptyDir</code></li><li><code>spec.volumes[*].ephemeral</code></li><li><code>spec.volumes[*].persistentVolumeClaim</code></li><li><code>spec.volumes[*].projected</code></li><li><code>spec.volumes[*].secret</code></li></ul></td></tr><tr><td style="white-space:nowrap">Privilege Escalation (v1.8+)</td><td><p>Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. <em><a href="#os-specific-policy-controls">This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>false</code></li></ul></td></tr><tr><td style="white-space:nowrap">Running as Non-root</td><td><p>Containers must be required to run as non-root users.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsNonRoot</code></li><li><code>spec.containers[*].securityContext.runAsNonRoot</code></li><li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>true</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.runAsNonRoot</code> is set to <code>true</code>.</small></td></tr><tr><td style="white-space:nowrap">Running as Non-root user (v1.23+)</td><td><p>Containers must not set <tt>runAsUser</tt> to 0</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.initContainers[*].securityContext.runAsUser</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>any non-zero value</li><li><code>undefined/null</code></li></ul></td></tr><tr><td style="white-space:nowrap">Seccomp (v1.19+)</td><td><p>Seccomp profile must be explicitly set to one of the allowed values. Both the <code>Unconfined</code> profile and the <em>absence</em> of a profile are prohibited. <em><a href="#os-specific-policy-controls">This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.seccompProfile.type</code> field is set appropriately.
Conversely, the pod-level field may be undefined/<code>nil</code> if _all_ container-
level fields are set.</small></td></tr><tr><td style="white-space:nowrap">Capabilities (v1.22+)</td><td><p>Containers must drop <code>ALL</code> capabilities, and are only permitted to add back
the <code>NET_BIND_SERVICE</code> capability. <em><a href="#os-specific-policy-controls">This is Linux only policy</a> in v1.25+ <code>(.spec.os.name != "windows")</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.drop</code></li><li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Any list of capabilities that includes <code>ALL</code></li></ul><hr/><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>NET_BIND_SERVICE</code></li></ul></td></tr></tbody></table><h2 id="policy-instantiation">Policy Instantiation</h2><p>Decoupling policy definition from policy instantiation allows for a common understanding and
consistent language of policies across clusters, independent of the underlying enforcement
mechanism.</p><p>As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement
of individual policies are not defined here.</p><p><a href="/docs/concepts/security/pod-security-admission/"><strong>Pod Security Admission Controller</strong></a></p><ul><li><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-privileged.yaml" download="security/podsecurity-privileged.yaml">Privileged namespace</a></li><li><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-baseline.yaml" download="security/podsecurity-baseline.yaml">Baseline namespace</a></li><li><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-restricted.yaml" download="security/podsecurity-restricted.yaml">Restricted namespace</a></li></ul><h3 id="alternatives">Alternatives</h3><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:</p><ul><li><a href="https://github.com/kubewarden">Kubewarden</a></li><li><a href="https://kyverno.io/policies/pod-security/">Kyverno</a></li><li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a></li></ul><h2 id="pod-os-field">Pod OS field</h2><p>Kubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of
node in one cluster.
Windows in Kubernetes has some limitations and differentiators from Linux-based
workloads. Specifically, many of the Pod <code>securityContext</code> fields
<a href="/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext">have no effect on Windows</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the Restricted policies should be pinned to a version prior to v1.25.</div><h3 id="restricted-pod-security-standard-changes">Restricted Pod Security Standard changes</h3><p>Another important change, made in Kubernetes v1.25 is that the <em>Restricted</em> policy
has been updated to use the <code>pod.spec.os.name</code> field. Based on the OS name, certain policies that are specific
to a particular OS can be relaxed for the other OS.</p><h4 id="os-specific-policy-controls">OS-specific policy controls</h4><p>Restrictions on the following controls are only required if <code>.spec.os.name</code> is not <code>windows</code>:</p><ul><li>Privilege Escalation</li><li>Seccomp</li><li>Linux Capabilities</li></ul><h2 id="user-namespaces">User namespaces</h2><p>User Namespaces are a Linux-only feature to run workloads with increased
isolation. How they work together with Pod Security Standards is described in
the <a href="/docs/concepts/workloads/pods/user-namespaces/#integration-with-pod-security-admission-checks">documentation</a> for Pods that use user namespaces.</p><h2 id="faq">FAQ</h2><h3 id="why-isn-t-there-a-profile-between-privileged-and-baseline">Why isn't there a profile between Privileged and Baseline?</h3><p>The three profiles defined here have a clear linear progression from most secure (Restricted) to least
secure (Privileged), and cover a broad set of workloads. Privileges required above the Baseline
policy are typically very application specific, so we do not offer a standard profile in this
niche. This is not to say that the privileged profile should always be used in this case, but that
policies in this space need to be defined on a case-by-case basis.</p><p>SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.</p><h3 id="what-s-the-difference-between-a-security-profile-and-a-security-context">What's the difference between a security profile and a security context?</h3><p><a href="/docs/tasks/configure-pod-container/security-context/">Security Contexts</a> configure Pods and
Containers at runtime. Security contexts are defined as part of the Pod and container specifications
in the Pod manifest, and represent parameters to the container runtime.</p><p>Security profiles are control plane mechanisms to enforce specific settings in the Security Context,
as well as other related parameters outside the Security Context. As of July 2021,
<a href="/docs/concepts/security/pod-security-policy/">Pod Security Policies</a> are deprecated in favor of the
built-in <a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission Controller</a>.</p><h3 id="what-about-sandboxed-pods">What about sandboxed Pods?</h3><p>There is currently no API standard that controls whether a Pod is considered sandboxed or
not. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata
Containers), but there is no standard definition of what a sandboxed runtime is.</p><p>The protections necessary for sandboxed workloads can differ from others. For example, the need to
restrict privileged permissions is lessened when the workload is isolated from the underlying
kernel. This allows for workloads requiring heightened permissions to still be isolated.</p><p>Additionally, the protection of sandboxed workloads is highly dependent on the method of
sandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">API-initiated Eviction</h1><p>API-initiated eviction is the process by which you use the <a href="/docs/reference/generated/kubernetes-api/v1.34/#create-eviction-pod-v1-core">Eviction API</a>
to create an <code>Eviction</code> object that triggers graceful pod termination.</p><p>You can request eviction by calling the Eviction API directly, or programmatically
using a client of the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="API server">API server</a>, like the <code>kubectl drain</code> command. This
creates an <code>Eviction</code> object, which causes the API server to terminate the Pod.</p><p>API-initiated evictions respect your configured <a href="/docs/tasks/run-application/configure-pdb/"><code>PodDisruptionBudgets</code></a>
and <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination"><code>terminationGracePeriodSeconds</code></a>.</p><p>Using the API to create an Eviction object for a Pod is like performing a
policy-controlled <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod"><code>DELETE</code> operation</a>
on the Pod.</p><h2 id="calling-the-eviction-api">Calling the Eviction API</h2><p>You can use a <a href="/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api">Kubernetes language client</a>
to access the Kubernetes API and create an <code>Eviction</code> object. To do this, you
POST the attempted operation, similar to the following example:</p><ul class="nav nav-tabs" id="eviction-example" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#eviction-example-0" role="tab" aria-controls="eviction-example-0" aria-selected="true">policy/v1</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#eviction-example-1" role="tab" aria-controls="eviction-example-1">policy/v1beta1</a></li></ul><div class="tab-content" id="eviction-example"><div id="eviction-example-0" class="tab-pane show active" role="tabpanel" aria-labelledby="eviction-example-0"><p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><code>policy/v1</code> Eviction is available in v1.22+. Use <code>policy/v1beta1</code> with prior releases.</div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"apiVersion"</span>: <span style="color:#b44">"policy/v1"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"Eviction"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"metadata"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"name"</span>: <span style="color:#b44">"quux"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"namespace"</span>: <span style="color:#b44">"default"</span>
</span></span><span style="display:flex"><span>  }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></p></div><div id="eviction-example-1" class="tab-pane" role="tabpanel" aria-labelledby="eviction-example-1"><p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Deprecated in v1.22 in favor of <code>policy/v1</code></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"apiVersion"</span>: <span style="color:#b44">"policy/v1beta1"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"Eviction"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"metadata"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"name"</span>: <span style="color:#b44">"quux"</span>,
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"namespace"</span>: <span style="color:#b44">"default"</span>
</span></span><span style="display:flex"><span>  }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div></p></div></div><p>Alternatively, you can attempt an eviction operation by accessing the API using
<code>curl</code> or <code>wget</code>, similar to the following example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>curl -v -H <span style="color:#b44">'Content-type: application/json'</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</span></span></code></pre></div><h2 id="how-api-initiated-eviction-works">How API-initiated eviction works</h2><p>When you request an eviction using the API, the API server performs admission
checks and responds in one of the following ways:</p><ul><li><code>200 OK</code>: the eviction is allowed, the <code>Eviction</code> subresource is created, and
the Pod is deleted, similar to sending a <code>DELETE</code> request to the Pod URL.</li><li><code>429 Too Many Requests</code>: the eviction is not currently allowed because of the
configured <a class="glossary-tooltip" title="An object that limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-pod-disruption-budget" target="_blank" aria-label="PodDisruptionBudget">PodDisruptionBudget</a>.
You may be able to attempt the eviction again later. You might also see this
response because of API rate limiting.</li><li><code>500 Internal Server Error</code>: the eviction is not allowed because there is a
misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.</li></ul><p>If the Pod you want to evict isn't part of a workload that has a
PodDisruptionBudget, the API server always returns <code>200 OK</code> and allows the
eviction.</p><p>If the API server allows the eviction, the Pod is deleted as follows:</p><ol><li>The <code>Pod</code> resource in the API server is updated with a deletion timestamp,
after which the API server considers the <code>Pod</code> resource to be terminated. The
<code>Pod</code> resource is also marked with the configured grace period.</li><li>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> on the node where the local Pod is running notices that the <code>Pod</code>
resource is marked for termination and starts to gracefully shut down the
local Pod.</li><li>While the kubelet is shutting the Pod down, the control plane removes the Pod
from <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/endpoint-slices/" target="_blank" aria-label="EndpointSlice">EndpointSlice</a>
objects. As a result, controllers no longer consider the Pod as a valid object.</li><li>After the grace period for the Pod expires, the kubelet forcefully terminates
the local Pod.</li><li>The kubelet tells the API server to remove the <code>Pod</code> resource.</li><li>The API server deletes the <code>Pod</code> resource.</li></ol><h2 id="troubleshooting-stuck-evictions">Troubleshooting stuck evictions</h2><p>In some cases, your applications may enter a broken state, where the Eviction
API will only return <code>429</code> or <code>500</code> responses until you intervene. This can
happen if, for example, a ReplicaSet creates pods for your application but new
pods do not enter a <code>Ready</code> state. You may also notice this behavior in cases
where the last evicted Pod had a long termination grace period.</p><p>If you notice stuck evictions, try one of the following solutions:</p><ul><li>Abort or pause the automated operation causing the issue. Investigate the stuck
application before you restart the operation.</li><li>Wait a while, then directly delete the Pod from your cluster control plane
instead of using the Eviction API.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn how to protect your applications with a <a href="/docs/tasks/run-application/configure-pdb/">Pod Disruption Budget</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority and Preemption</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Persistent Volumes</h1><p>This document describes <em>persistent volumes</em> in Kubernetes. Familiarity with
<a href="/docs/concepts/storage/volumes/">volumes</a>, <a href="/docs/concepts/storage/storage-classes/">StorageClasses</a>
and <a href="/docs/concepts/storage/volume-attributes-classes/">VolumeAttributesClasses</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>Managing storage is a distinct problem from managing compute instances.
The PersistentVolume subsystem provides an API for users and administrators
that abstracts details of how storage is provided from how it is consumed.
To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.</p><p>A <em>PersistentVolume</em> (PV) is a piece of storage in the cluster that has been
provisioned by an administrator or dynamically provisioned using
<a href="/docs/concepts/storage/storage-classes/">Storage Classes</a>. It is a resource in
the cluster just like a node is a cluster resource. PVs are volume plugins like
Volumes, but have a lifecycle independent of any individual Pod that uses the PV.
This API object captures the details of the implementation of the storage, be that
NFS, iSCSI, or a cloud-provider-specific storage system.</p><p>A <em>PersistentVolumeClaim</em> (PVC) is a request for storage by a user. It is similar
to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can
request specific levels of resources (CPU and Memory). Claims can request specific
size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany,
ReadWriteMany, or ReadWriteOncePod, see <a href="#access-modes">AccessModes</a>).</p><p>While PersistentVolumeClaims allow a user to consume abstract storage resources,
it is common that users need PersistentVolumes with varying properties, such as
performance, for different problems. Cluster administrators need to be able to
offer a variety of PersistentVolumes that differ in more ways than size and access
modes, without exposing users to the details of how those volumes are implemented.
For these needs, there is the <em>StorageClass</em> resource.</p><p>See the <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">detailed walkthrough with working examples</a>.</p><h2 id="lifecycle-of-a-volume-and-claim">Lifecycle of a volume and claim</h2><p>PVs are resources in the cluster. PVCs are requests for those resources and also act
as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:</p><h3 id="provisioning">Provisioning</h3><p>There are two ways PVs may be provisioned: statically or dynamically.</p><h4 id="static">Static</h4><p>A cluster administrator creates a number of PVs. They carry the details of the
real storage, which is available for use by cluster users. They exist in the
Kubernetes API and are available for consumption.</p><h4 id="dynamic">Dynamic</h4><p>When none of the static PVs the administrator created match a user's PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
<a href="/docs/concepts/storage/storage-classes/">storage class</a> and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class <code>""</code> effectively disable
dynamic provisioning for themselves.</p><p>To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the <code>DefaultStorageClass</code>
<a href="/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass">admission controller</a>
on the API server. This can be done, for example, by ensuring that <code>DefaultStorageClass</code> is
among the comma-delimited, ordered list of values for the <code>--enable-admission-plugins</code> flag of
the API server component. For more information on API server command-line flags,
check <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a> documentation.</p><h3 id="binding">Binding</h3><p>A user creates, or in the case of dynamic provisioning, has already created,
a PersistentVolumeClaim with a specific amount of storage requested and with
certain access modes. A control loop in the control plane watches for new PVCs, finds
a matching PV (if possible), and binds them together. If a PV was dynamically
provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise,
the user will always get at least what they asked for, but the volume may be in
excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive,
regardless of how they were bound. A PVC to PV binding is a one-to-one mapping,
using a ClaimRef which is a bi-directional binding between the PersistentVolume
and the PersistentVolumeClaim.</p><p>Claims will remain unbound indefinitely if a matching volume does not exist.
Claims will be bound as matching volumes become available. For example, a
cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi.
The PVC can be bound when a 100Gi PV is added to the cluster.</p><h3 id="using">Using</h3><p>Pods use claims as volumes. The cluster inspects the claim to find the bound
volume and mounts that volume for a Pod. For volumes that support multiple
access modes, the user specifies which mode is desired when using their claim
as a volume in a Pod.</p><p>Once a user has a claim and that claim is bound, the bound PV belongs to the
user for as long as they need it. Users schedule Pods and access their claimed
PVs by including a <code>persistentVolumeClaim</code> section in a Pod's <code>volumes</code> block.
See <a href="#claims-as-volumes">Claims As Volumes</a> for more details on this.</p><h3 id="storage-object-in-use-protection">Storage Object in Use Protection</h3><p>The purpose of the Storage Object in Use Protection feature is to ensure that
PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs)
that are bound to PVCs are not removed from the system, as this may result in data loss.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>PVC is in active use by a Pod when a Pod object exists that is using the PVC.</div><p>If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately.
PVC removal is postponed until the PVC is no longer actively used by any Pods. Also,
if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.
PV removal is postponed until the PV is no longer bound to a PVC.</p><p>You can see that a PVC is protected when the PVC's status is <code>Terminating</code> and the
<code>Finalizers</code> list includes <code>kubernetes.io/pvc-protection</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe pvc hostpath
</span></span><span style="display:flex"><span>Name:          hostpath
</span></span><span style="display:flex"><span>Namespace:     default
</span></span><span style="display:flex"><span>StorageClass:  example-hostpath
</span></span><span style="display:flex"><span>Status:        Terminating
</span></span><span style="display:flex"><span>Volume:
</span></span><span style="display:flex"><span>Labels:        &lt;none&gt;
</span></span><span style="display:flex"><span>Annotations:   volume.beta.kubernetes.io/storage-class<span style="color:#666">=</span>example-hostpath
</span></span><span style="display:flex"><span>               volume.beta.kubernetes.io/storage-provisioner<span style="color:#666">=</span>example.com/hostpath
</span></span><span style="display:flex"><span>Finalizers:    <span style="color:#666">[</span>kubernetes.io/pvc-protection<span style="color:#666">]</span>
</span></span><span style="display:flex"><span>...
</span></span></code></pre></div><p>You can see that a PV is protected when the PV's status is <code>Terminating</code> and
the <code>Finalizers</code> list includes <code>kubernetes.io/pv-protection</code> too:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe pv task-pv-volume
</span></span><span style="display:flex"><span>Name:            task-pv-volume
</span></span><span style="display:flex"><span>Labels:          <span style="color:#b8860b">type</span><span style="color:#666">=</span><span style="color:#a2f">local</span>
</span></span><span style="display:flex"><span>Annotations:     &lt;none&gt;
</span></span><span style="display:flex"><span>Finalizers:      <span style="color:#666">[</span>kubernetes.io/pv-protection<span style="color:#666">]</span>
</span></span><span style="display:flex"><span>StorageClass:    standard
</span></span><span style="display:flex"><span>Status:          Terminating
</span></span><span style="display:flex"><span>Claim:
</span></span><span style="display:flex"><span>Reclaim Policy:  Delete
</span></span><span style="display:flex"><span>Access Modes:    RWO
</span></span><span style="display:flex"><span>Capacity:        1Gi
</span></span><span style="display:flex"><span>Message:
</span></span><span style="display:flex"><span>Source:
</span></span><span style="display:flex"><span>    Type:          HostPath <span style="color:#666">(</span>bare host directory volume<span style="color:#666">)</span>
</span></span><span style="display:flex"><span>    Path:          /tmp/data
</span></span><span style="display:flex"><span>    HostPathType:
</span></span><span style="display:flex"><span>Events:            &lt;none&gt;
</span></span></code></pre></div><h3 id="reclaiming">Reclaiming</h3><p>When a user is done with their volume, they can delete the PVC objects from the
API that allows reclamation of the resource. The reclaim policy for a PersistentVolume
tells the cluster what to do with the volume after it has been released of its claim.
Currently, volumes can either be Retained, Recycled, or Deleted.</p><h4 id="retain">Retain</h4><p>The <code>Retain</code> reclaim policy allows for manual reclamation of the resource.
When the PersistentVolumeClaim is deleted, the PersistentVolume still exists
and the volume is considered "released". But it is not yet available for
another claim because the previous claimant's data remains on the volume.
An administrator can manually reclaim the volume with the following steps.</p><ol><li>Delete the PersistentVolume. The associated storage asset in external infrastructure
still exists after the PV is deleted.</li><li>Manually clean up the data on the associated storage asset accordingly.</li><li>Manually delete the associated storage asset.</li></ol><p>If you want to reuse the same storage asset, create a new PersistentVolume with
the same storage asset definition.</p><h4 id="delete">Delete</h4><p>For volume plugins that support the <code>Delete</code> reclaim policy, deletion removes
both the PersistentVolume object from Kubernetes, as well as the associated
storage asset in the external infrastructure. Volumes that were dynamically provisioned
inherit the <a href="#reclaim-policy">reclaim policy of their StorageClass</a>, which
defaults to <code>Delete</code>. The administrator should configure the StorageClass
according to users' expectations; otherwise, the PV must be edited or
patched after it is created. See
<a href="/docs/tasks/administer-cluster/change-pv-reclaim-policy/">Change the Reclaim Policy of a PersistentVolume</a>.</p><h4 id="recycle">Recycle</h4><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>The <code>Recycle</code> reclaim policy is deprecated. Instead, the recommended approach
is to use dynamic provisioning.</div><p>If supported by the underlying volume plugin, the <code>Recycle</code> reclaim policy performs
a basic scrub (<code>rm -rf /thevolume/*</code>) on the volume and makes it available again for a new claim.</p><p>However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">reference</a>.
The custom recycler Pod template must contain a <code>volumes</code> specification, as
shown in the example below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pv-recycler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/any/path/it/will/be/replaced<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pv-recycler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">"registry.k8s.io/busybox"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"/bin/sh"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-c"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \"$(ls -A /scrub)\" || exit 1"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>vol<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/scrub<span style="color:#bbb">
</span></span></span></code></pre></div><p>However, the particular path specified in the custom recycler Pod template in the
<code>volumes</code> part is replaced with the particular path of the volume that is being recycled.</p><h3 id="persistentvolume-deletion-protection-finalizer">PersistentVolume deletion protection finalizer</h3><div class="feature-state-notice feature-stable" title="Feature Gate: HonorPVReclaimPolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes
having <code>Delete</code> reclaim policy are deleted only after the backing storage are deleted.</p><p>The finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code>(introduced
in v1.31) is added to both dynamically provisioned and statically provisioned
CSI volumes.</p><p>The finalizer <code>kubernetes.io/pv-controller</code>(introduced in v1.31) is added to
dynamically provisioned in-tree plugin volumes and skipped for statically
provisioned in-tree plugin volumes.</p><p>The following is an example of dynamically provisioned in-tree plugin volume:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
</span></span><span style="display:flex"><span>Name:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
</span></span><span style="display:flex"><span>Labels:          &lt;none&gt;
</span></span><span style="display:flex"><span>Annotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner
</span></span><span style="display:flex"><span>                 pv.kubernetes.io/bound-by-controller: yes
</span></span><span style="display:flex"><span>                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume
</span></span><span style="display:flex"><span>Finalizers:      <span style="color:#666">[</span>kubernetes.io/pv-protection kubernetes.io/pv-controller<span style="color:#666">]</span>
</span></span><span style="display:flex"><span>StorageClass:    vcp-sc
</span></span><span style="display:flex"><span>Status:          Bound
</span></span><span style="display:flex"><span>Claim:           default/vcp-pvc-1
</span></span><span style="display:flex"><span>Reclaim Policy:  Delete
</span></span><span style="display:flex"><span>Access Modes:    RWO
</span></span><span style="display:flex"><span>VolumeMode:      Filesystem
</span></span><span style="display:flex"><span>Capacity:        1Gi
</span></span><span style="display:flex"><span>Node Affinity:   &lt;none&gt;
</span></span><span style="display:flex"><span>Message:
</span></span><span style="display:flex"><span>Source:
</span></span><span style="display:flex"><span>    Type:               vSphereVolume <span style="color:#666">(</span>a Persistent Disk resource in vSphere<span style="color:#666">)</span>
</span></span><span style="display:flex"><span>    VolumePath:         <span style="color:#666">[</span>vsanDatastore<span style="color:#666">]</span> d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk
</span></span><span style="display:flex"><span>    FSType:             ext4
</span></span><span style="display:flex"><span>    StoragePolicyName:  vSAN Default Storage Policy
</span></span><span style="display:flex"><span>Events:                 &lt;none&gt;
</span></span></code></pre></div><p>The finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code> is added for CSI volumes.
The following is an example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>Name:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d
</span></span><span style="display:flex"><span>Labels:          &lt;none&gt;
</span></span><span style="display:flex"><span>Annotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com
</span></span><span style="display:flex"><span>Finalizers:      <span style="color:#666">[</span>kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer<span style="color:#666">]</span>
</span></span><span style="display:flex"><span>StorageClass:    fast
</span></span><span style="display:flex"><span>Status:          Bound
</span></span><span style="display:flex"><span>Claim:           demo-app/nginx-logs
</span></span><span style="display:flex"><span>Reclaim Policy:  Delete
</span></span><span style="display:flex"><span>Access Modes:    RWO
</span></span><span style="display:flex"><span>VolumeMode:      Filesystem
</span></span><span style="display:flex"><span>Capacity:        200Mi
</span></span><span style="display:flex"><span>Node Affinity:   &lt;none&gt;
</span></span><span style="display:flex"><span>Message:
</span></span><span style="display:flex"><span>Source:
</span></span><span style="display:flex"><span>    Type:              CSI <span style="color:#666">(</span>a Container Storage Interface <span style="color:#666">(</span>CSI<span style="color:#666">)</span> volume <span style="color:#a2f">source</span><span style="color:#666">)</span>
</span></span><span style="display:flex"><span>    Driver:            csi.vsphere.vmware.com
</span></span><span style="display:flex"><span>    FSType:            ext4
</span></span><span style="display:flex"><span>    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd
</span></span><span style="display:flex"><span>    ReadOnly:          <span style="color:#a2f">false</span>
</span></span><span style="display:flex"><span>    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity<span style="color:#666">=</span>1648442357185-8081-csi.vsphere.vmware.com
</span></span><span style="display:flex"><span>                           <span style="color:#b8860b">type</span><span style="color:#666">=</span>vSphere CNS Block Volume
</span></span><span style="display:flex"><span>Events:                &lt;none&gt;
</span></span></code></pre></div><p>When the <code>CSIMigration{provider}</code> feature flag is enabled for a specific in-tree volume plugin,
the <code>kubernetes.io/pv-controller</code> finalizer is replaced by the
<code>external-provisioner.volume.kubernetes.io/finalizer</code> finalizer.</p><p>The finalizers ensure that the PV object is removed only after the volume is deleted
from the storage backend provided the reclaim policy of the PV is <code>Delete</code>. This
also ensures that the volume is deleted from storage backend irrespective of the
order of deletion of PV and PVC.</p><h3 id="reserving-a-persistentvolume">Reserving a PersistentVolume</h3><p>The control plane can <a href="#binding">bind PersistentVolumeClaims to matching PersistentVolumes</a>
in the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.</p><p>By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding
between that specific PV and PVC. If the PersistentVolume exists and has not reserved
PersistentVolumeClaims through its <code>claimRef</code> field, then the PersistentVolume and
PersistentVolumeClaim will be bound.</p><p>The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that <a href="/docs/concepts/storage/storage-classes/">storage class</a>,
access modes, and requested storage size are valid.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Empty string must be explicitly set otherwise default StorageClass will be set</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeName</span>:<span style="color:#bbb"> </span>foo-pv<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>This method does not guarantee any binding privileges to the PersistentVolume.
If other PersistentVolumeClaims could use the PV that you specify, you first
need to reserve that storage volume. Specify the relevant PersistentVolumeClaim
in the <code>claimRef</code> field of the PV so that other PVCs can not bind to it.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo-pv<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">claimRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>This is useful if you want to consume PersistentVolumes that have their <code>persistentVolumeReclaimPolicy</code> set
to <code>Retain</code>, including cases where you are reusing an existing PV.</p><h3 id="expanding-persistent-volumes-claims">Expanding Persistent Volumes Claims</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand
the following types of volumes:</p><ul><li><a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="csi">csi</a> (including some CSI migrated
volme types)</li><li>flexVolume (deprecated)</li><li>portworxVolume (deprecated)</li></ul><p>You can only expand a PVC if its storage class's <code>allowVolumeExpansion</code> field is set to true.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-vol-default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">provisioner</span>:<span style="color:#bbb"> </span>vendor-name.example/magicstorage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resturl</span>:<span style="color:#bbb"> </span><span style="color:#b44">"http://192.168.10.100:8080"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restuser</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">secretNamespace</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">secretName</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">allowVolumeExpansion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.
If you edit the capacity of a PersistentVolume, and then edit the <code>.spec</code> of a matching
PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,
then no storage resize happens.
The Kubernetes control plane will see that the desired state of both resources matches,
conclude that the backing volume size has been manually
increased and that no resize is necessary.</div><h4 id="csi-volume-expansion">CSI Volume expansion</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Support for expanding CSI volumes is enabled by default but it also requires a
specific CSI driver to support volume expansion. Refer to documentation of the
specific CSI driver for more information.</p><h4 id="resizing-a-volume-containing-a-file-system">Resizing a volume containing a file system</h4><p>You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.</p><p>When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in <code>ReadWrite</code> mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.</p><p>FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the
<code>RequiresFSResize</code> capability to <code>true</code>. The FlexVolume can be resized on Pod restart.</p><h4 id="resizing-an-in-use-persistentvolumeclaim">Resizing an in-use PersistentVolumeClaim</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.</p><p>Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>FlexVolume resize is possible only when the underlying driver supports resize.</div><h4 id="recovering-from-failure-when-expanding-volumes">Recovering from Failure when Expanding Volumes</h4><p>If a user specifies a new size that is too big to be satisfied by underlying
storage system, expansion of PVC will be continuously retried until user or
cluster administrator takes some action. This can be undesirable and hence
Kubernetes provides following methods of recovering from such failures.</p><ul class="nav nav-tabs" id="recovery-methods" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#recovery-methods-0" role="tab" aria-controls="recovery-methods-0" aria-selected="true">Manually with Cluster Administrator access</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#recovery-methods-1" role="tab" aria-controls="recovery-methods-1">By requesting expansion to smaller size</a></li></ul><div class="tab-content" id="recovery-methods"><div id="recovery-methods-0" class="tab-pane show active" role="tabpanel" aria-labelledby="recovery-methods-0"><p><p>If expanding underlying storage fails, the cluster administrator can manually
recover the Persistent Volume Claim (PVC) state and cancel the resize requests.
Otherwise, the resize requests are continuously retried by the controller without
administrator intervention.</p><ol><li>Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC)
with <code>Retain</code> reclaim policy.</li><li>Delete the PVC. Since PV has <code>Retain</code> reclaim policy - we will not lose any data
when we recreate the PVC.</li><li>Delete the <code>claimRef</code> entry from PV specs, so as new PVC can bind to it.
This should make the PV <code>Available</code>.</li><li>Re-create the PVC with smaller size than PV and set <code>volumeName</code> field of the
PVC to the name of the PV. This should bind new PVC to existing PV.</li><li>Don't forget to restore the reclaim policy of the PV.</li></ol></p></div><div id="recovery-methods-1" class="tab-pane" role="tabpanel" aria-labelledby="recovery-methods-1"><p><p>If expansion has failed for a PVC, you can retry expansion with a
smaller size than the previously requested value. To request a new expansion attempt with a
smaller proposed size, edit <code>.spec.resources</code> for that PVC and choose a value that is less than the
value you previously tried.
This is useful if expansion to a higher value did not succeed because of capacity constraint.
If that has happened, or you suspect that it might have, you can retry expansion by specifying a
size that is within the capacity limits of underlying storage provider. You can monitor status of
resize operation by watching <code>.status.allocatedResourceStatuses</code> and events on the PVC.</p><p>Note that,
although you can specify a lower amount of storage than what was requested previously,
the new value must still be higher than <code>.status.capacity</code>.
Kubernetes does not support shrinking a PVC to less than its current size.</p></p></div></div><h2 id="types-of-persistent-volumes">Types of Persistent Volumes</h2><p>PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:</p><ul><li><a href="/docs/concepts/storage/volumes/#csi"><code>csi</code></a> - Container Storage Interface (CSI)</li><li><a href="/docs/concepts/storage/volumes/#fc"><code>fc</code></a> - Fibre Channel (FC) storage</li><li><a href="/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code></a> - HostPath volume
(for single node testing only; WILL NOT WORK in a multi-node cluster;
consider using <code>local</code> volume instead)</li><li><a href="/docs/concepts/storage/volumes/#iscsi"><code>iscsi</code></a> - iSCSI (SCSI over IP) storage</li><li><a href="/docs/concepts/storage/volumes/#local"><code>local</code></a> - local storage devices
mounted on nodes.</li><li><a href="/docs/concepts/storage/volumes/#nfs"><code>nfs</code></a> - Network File System (NFS) storage</li></ul><p>The following types of PersistentVolume are deprecated but still available.
If you are using these volume types except for <code>flexVolume</code>, <code>cephfs</code> and <code>rbd</code>,
please install corresponding CSI drivers.</p><ul><li><a href="/docs/concepts/storage/volumes/#awselasticblockstore"><code>awsElasticBlockStore</code></a> - AWS Elastic Block Store (EBS)
(<strong>migration on by default</strong> starting v1.23)</li><li><a href="/docs/concepts/storage/volumes/#azuredisk"><code>azureDisk</code></a> - Azure Disk
(<strong>migration on by default</strong> starting v1.23)</li><li><a href="/docs/concepts/storage/volumes/#azurefile"><code>azureFile</code></a> - Azure File
(<strong>migration on by default</strong> starting v1.24)</li><li><a href="/docs/concepts/storage/volumes/#cinder"><code>cinder</code></a> - Cinder (OpenStack block storage)
(<strong>migration on by default</strong> starting v1.21)</li><li><a href="/docs/concepts/storage/volumes/#flexvolume"><code>flexVolume</code></a> - FlexVolume
(<strong>deprecated</strong> starting v1.23, no migration plan and no plan to remove support)</li><li><a href="/docs/concepts/storage/volumes/#gcePersistentDisk"><code>gcePersistentDisk</code></a> - GCE Persistent Disk
(<strong>migration on by default</strong> starting v1.23)</li><li><a href="/docs/concepts/storage/volumes/#portworxvolume"><code>portworxVolume</code></a> - Portworx volume
(<strong>migration on by default</strong> starting v1.31)</li><li><a href="/docs/concepts/storage/volumes/#vspherevolume"><code>vsphereVolume</code></a> - vSphere VMDK volume
(<strong>migration on by default</strong> starting v1.25)</li></ul><p>Older versions of Kubernetes also supported the following in-tree PersistentVolume types:</p><ul><li><a href="/docs/concepts/storage/volumes/#cephfs"><code>cephfs</code></a>
(<strong>not available</strong> starting v1.31)</li><li><code>flocker</code> - Flocker storage.
(<strong>not available</strong> starting v1.25)</li><li><code>glusterfs</code> - GlusterFS storage.
(<strong>not available</strong> starting v1.26)</li><li><code>photonPersistentDisk</code> - Photon controller persistent disk.
(<strong>not available</strong> starting v1.15)</li><li><code>quobyte</code> - Quobyte volume.
(<strong>not available</strong> starting v1.25)</li><li><a href="/docs/concepts/storage/volumes/#rbd"><code>rbd</code></a> - Rados Block Device (RBD) volume
(<strong>not available</strong> starting v1.31)</li><li><code>scaleIO</code> - ScaleIO volume.
(<strong>not available</strong> starting v1.21)</li><li><code>storageos</code> - StorageOS volume.
(<strong>not available</strong> starting v1.25)</li></ul><h2 id="persistent-volumes">Persistent Volumes</h2><p>Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pv0003<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>5Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Recycle<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">mountOptions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- hard<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- nfsvers=4.1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nfs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/tmp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span><span style="color:#666">172.17.0.2</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Helper programs relating to the volume type may be required for consumption of
a PersistentVolume within a cluster. In this example, the PersistentVolume is
of type NFS and the helper program /sbin/mount.nfs is required to support the
mounting of NFS filesystems.</div><h3 id="capacity">Capacity</h3><p>Generally, a PV will have a specific storage capacity. This is set using the PV's
<code>capacity</code> attribute which is a <a class="glossary-tooltip" title="A whole-number representation of small or large numbers using SI suffixes." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-quantity" target="_blank" aria-label="Quantity">Quantity</a> value.</p><p>Currently, storage size is the only resource that can be set or requested.
Future attributes may include IOPS, throughput, etc.</p><h3 id="volume-mode">Volume Mode</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>Kubernetes supports two <code>volumeModes</code> of PersistentVolumes: <code>Filesystem</code> and <code>Block</code>.</p><p><code>volumeMode</code> is an optional API parameter.
<code>Filesystem</code> is the default mode used when <code>volumeMode</code> parameter is omitted.</p><p>A volume with <code>volumeMode: Filesystem</code> is <em>mounted</em> into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kubernetes creates a filesystem
on the device before mounting it for the first time.</p><p>You can set the value of <code>volumeMode</code> to <code>Block</code> to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See <a href="#raw-block-volume-support">Raw Block Volume Support</a>
for an example on how to use a volume with <code>volumeMode: Block</code> in a Pod.</p><h3 id="access-modes">Access Modes</h3><p>A PersistentVolume can be mounted on a host in any way supported by the resource
provider. As shown in the table below, providers will have different capabilities
and each PV's access modes are set to the specific modes supported by that particular
volume. For example, NFS can support multiple read/write clients, but a specific
NFS PV might be exported on the server as read-only. Each PV gets its own set of
access modes describing that specific PV's capabilities.</p><p>The access modes are:</p><dl><dt><code>ReadWriteOnce</code></dt><dd>the volume can be mounted as read-write by a single node. ReadWriteOnce access
mode still can allow multiple pods to access (read from or write to) that volume when the pods are
running on the same node. For single pod access, please see ReadWriteOncePod.</dd><dt><code>ReadOnlyMany</code></dt><dd>the volume can be mounted as read-only by many nodes.</dd><dt><code>ReadWriteMany</code></dt><dd>the volume can be mounted as read-write by many nodes.</dd><dt><code>ReadWriteOncePod</code></dt><dd><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div>the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod
access mode if you want to ensure that only one pod across the whole cluster can
read that PVC or write to it.</dd></dl><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The <code>ReadWriteOncePod</code> access mode is only supported for
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> volumes and Kubernetes version
1.22+. To use this feature you will need to update the following
<a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html">CSI sidecars</a>
to these versions or greater:</p><ul><li><a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+</a></li></ul></div><p>In the CLI, the access modes are abbreviated to:</p><ul><li>RWO - ReadWriteOnce</li><li>ROX - ReadOnlyMany</li><li>RWX - ReadWriteMany</li><li>RWOP - ReadWriteOncePod</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.
In some cases, the volume access modes also constrain where the PersistentVolume can be mounted.
Volume access modes do <strong>not</strong> enforce write protection once the storage has been mounted.
Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany,
they don't set any constraints on the volume. For example, even if a PersistentVolume is
created as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes
are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.</div><blockquote><p><strong>Important!</strong> A volume can only be mounted using one access mode at a time,
even if it supports many.</p></blockquote><table><thead><tr><th style="text-align:left">Volume Plugin</th><th style="text-align:center">ReadWriteOnce</th><th style="text-align:center">ReadOnlyMany</th><th style="text-align:center">ReadWriteMany</th><th>ReadWriteOncePod</th></tr></thead><tbody><tr><td style="text-align:left">AzureFile</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td>-</td></tr><tr><td style="text-align:left">CephFS</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td>-</td></tr><tr><td style="text-align:left">CSI</td><td style="text-align:center">depends on the driver</td><td style="text-align:center">depends on the driver</td><td style="text-align:center">depends on the driver</td><td>depends on the driver</td></tr><tr><td style="text-align:left">FC</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">-</td><td>-</td></tr><tr><td style="text-align:left">FlexVolume</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">depends on the driver</td><td>-</td></tr><tr><td style="text-align:left">HostPath</td><td style="text-align:center">âœ“</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td>-</td></tr><tr><td style="text-align:left">iSCSI</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">-</td><td>-</td></tr><tr><td style="text-align:left">NFS</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td>-</td></tr><tr><td style="text-align:left">RBD</td><td style="text-align:center">âœ“</td><td style="text-align:center">âœ“</td><td style="text-align:center">-</td><td>-</td></tr><tr><td style="text-align:left">VsphereVolume</td><td style="text-align:center">âœ“</td><td style="text-align:center">-</td><td style="text-align:center">- (works when Pods are collocated)</td><td>-</td></tr><tr><td style="text-align:left">PortworxVolume</td><td style="text-align:center">âœ“</td><td style="text-align:center">-</td><td style="text-align:center">âœ“</td><td>-</td></tr></tbody></table><h3 id="class">Class</h3><p>A PV can have a class, which is specified by setting the
<code>storageClassName</code> attribute to the name of a
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>.
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no <code>storageClassName</code> has no class and can only be bound
to PVCs that request no particular class.</p><p>In the past, the annotation <code>volume.beta.kubernetes.io/storage-class</code> was used instead
of the <code>storageClassName</code> attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.</p><h3 id="reclaim-policy">Reclaim Policy</h3><p>Current reclaim policies are:</p><ul><li>Retain -- manual reclamation</li><li>Recycle -- basic scrub (<code>rm -rf /thevolume/*</code>)</li><li>Delete -- delete the volume</li></ul><p>For Kubernetes 1.34, only <code>nfs</code> and <code>hostPath</code> volume types support recycling.</p><h3 id="mount-options">Mount Options</h3><p>A Kubernetes administrator can specify additional mount options for when a
Persistent Volume is mounted on a node.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Not all Persistent Volume types support mount options.</div><p>The following volume types support mount options:</p><ul><li><code>csi</code> (including CSI migrated volume types)</li><li><code>iscsi</code></li><li><code>nfs</code></li></ul><p>Mount options are not validated. If a mount option is invalid, the mount fails.</p><p>In the past, the annotation <code>volume.beta.kubernetes.io/mount-options</code> was used instead
of the <code>mountOptions</code> attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.</p><h3 id="node-affinity">Node Affinity</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>For most volume types, you do not need to set this field.
You need to explicitly set this for <a href="/docs/concepts/storage/volumes/#local">local</a> volumes.</div><p>A PV can specify node affinity to define constraints that limit what nodes this
volume can be accessed from. Pods that use a PV will only be scheduled to nodes
that are selected by the node affinity. To specify node affinity, set
<code>nodeAffinity</code> in the <code>.spec</code> of a PV. The
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec">PersistentVolume</a>
API reference has more details on this field.</p><h3 id="phase">Phase</h3><p>A PersistentVolume will be in one of the following phases:</p><dl><dt><code>Available</code></dt><dd>a free resource that is not yet bound to a claim</dd><dt><code>Bound</code></dt><dd>the volume is bound to a claim</dd><dt><code>Released</code></dt><dd>the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster</dd><dt><code>Failed</code></dt><dd>the volume has failed its (automated) reclamation</dd></dl><p>You can see the name of the PVC bound to the PV using <code>kubectl describe persistentvolume &lt;name&gt;</code>.</p><h4 id="phase-transition-timestamp">Phase transition timestamp</h4><div class="feature-state-notice feature-stable" title="Feature Gate: PersistentVolumeLastPhaseTransitionTime"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>The <code>.status</code> field for a PersistentVolume can include an alpha <code>lastPhaseTransitionTime</code> field. This field records
the timestamp of when the volume last transitioned its phase. For newly created
volumes the phase is set to <code>Pending</code> and <code>lastPhaseTransitionTime</code> is set to
the current time.</p><h2 id="persistentvolumeclaims">PersistentVolumeClaims</h2><p>Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myclaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>8Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">release</span>:<span style="color:#bbb"> </span><span style="color:#b44">"stable"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- {<span style="color:green;font-weight:700">key: environment, operator: In, values</span>:<span style="color:#bbb"> </span>[dev]}<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="access-modes-1">Access Modes</h3><p>Claims use <a href="#access-modes">the same conventions as volumes</a> when requesting
storage with specific access modes.</p><h3 id="volume-modes">Volume Modes</h3><p>Claims use <a href="#volume-mode">the same convention as volumes</a> to indicate the
consumption of the volume as either a filesystem or block device.</p><h3 id="volume-name">Volume Name</h3><p>Claims can use the <code>volumeName</code> field to explicitly bind to a specific PersistentVolume. You can also leave
<code>volumeName</code> unset, indicating that you'd like Kubernetes to set up a new PersistentVolume
that matches the claim.
If the specified PV is already bound to another PVC, the binding will be stuck
in a pending state.</p><h3 id="resources">Resources</h3><p>Claims, like Pods, can request specific quantities of a resource. In this case,
the request is for storage. The same
<a href="https://git.k8s.io/design-proposals-archive/scheduling/resources.md">resource model</a>
applies to both volumes and claims.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>For <code>Filesystem</code> volumes, the storage request refers to the "outer" volume size
(i.e. the allocated size from the storage backend).
This means that the writeable size may be slightly lower for providers that
build a filesystem on top of a block device, due to filesystem overhead.
This is especially visible with XFS, where many metadata features are enabled by default.</div><h3 id="selector">Selector</h3><p>Claims can specify a
<a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selector</a>
to further filter the set of volumes.
Only the volumes whose labels match the selector can be bound to the claim.
The selector can consist of two fields:</p><ul><li><code>matchLabels</code> - the volume must have a label with this value</li><li><code>matchExpressions</code> - a list of requirements made by specifying key, list of values,
and operator that relates the key and values.
Valid operators include <code>In</code>, <code>NotIn</code>, <code>Exists</code>, and <code>DoesNotExist</code>.</li></ul><p>All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, are
ANDed together â€“ they must all be satisfied in order to match.</p><h3 id="class-1">Class</h3><p>A claim can request a particular class by specifying the name of a
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>
using the attribute <code>storageClassName</code>.
Only PVs of the requested class, ones with the same <code>storageClassName</code> as the PVC,
can be bound to the PVC.</p><p>PVCs don't necessarily have to request a class. A PVC with its <code>storageClassName</code> set
equal to <code>""</code> is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to <code>""</code>).
A PVC with no <code>storageClassName</code> is not quite the same and is treated differently
by the cluster, depending on whether the
<a href="/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass"><code>DefaultStorageClass</code> admission plugin</a>
is turned on.</p><ul><li>If the admission plugin is turned on, the administrator may specify a default StorageClass.
All PVCs that have no <code>storageClassName</code> can be bound only to PVs of that default.
Specifying a default StorageClass is done by setting the annotation
<code>storageclass.kubernetes.io/is-default-class</code> equal to <code>true</code> in a StorageClass object.
If the administrator does not specify a default, the cluster responds to PVC creation
as if the admission plugin were turned off.
If more than one default StorageClass is specified, the newest default is used when
the PVC is dynamically provisioned.</li><li>If the admission plugin is turned off, there is no notion of a default StorageClass.
All PVCs that have <code>storageClassName</code> set to <code>""</code> can be bound only to PVs
that have <code>storageClassName</code> also set to <code>""</code>.
However, PVCs with missing <code>storageClassName</code> can be updated later once default StorageClass becomes available.
If the PVC gets updated it will no longer bind to PVs that have <code>storageClassName</code> also set to <code>""</code>.</li></ul><p>See <a href="#retroactive-default-storageclass-assignment">retroactive default StorageClass assignment</a> for more details.</p><p>Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.</p><p>When a PVC specifies a <code>selector</code> in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Currently, a PVC with a non-empty <code>selector</code> can't have a PV dynamically provisioned for it.</div><p>In the past, the annotation <code>volume.beta.kubernetes.io/storage-class</code> was used instead
of <code>storageClassName</code> attribute. This annotation is still working; however,
it won't be supported in a future Kubernetes release.</p><h4 id="retroactive-default-storageclass-assignment">Retroactive default StorageClass assignment</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code></div><p>You can create a PersistentVolumeClaim without specifying a <code>storageClassName</code>
for the new PVC, and you can do so even when no default StorageClass exists
in your cluster. In this case, the new PVC creates as you defined it, and the
<code>storageClassName</code> of that PVC remains unset until default becomes available.</p><p>When a default StorageClass becomes available, the control plane identifies any
existing PVCs without <code>storageClassName</code>. For the PVCs that either have an empty
value for <code>storageClassName</code> or do not have this key, the control plane then
updates those PVCs to set <code>storageClassName</code> to match the new default StorageClass.
If you have an existing PVC where the <code>storageClassName</code> is <code>""</code>, and you configure
a default StorageClass, then this PVC will not get updated.</p><p>In order to keep binding to PVs with <code>storageClassName</code> set to <code>""</code>
(while a default StorageClass is present), you need to set the <code>storageClassName</code>
of the associated PVC to <code>""</code>.</p><p>This behavior helps administrators change default StorageClass by removing the
old one first and then creating or setting another one. This brief window while
there is no default causes PVCs without <code>storageClassName</code> created at that time
to not have any default, but due to the retroactive default StorageClass
assignment this way of changing defaults is safe.</p><h2 id="claims-as-volumes">Claims As Volumes</h2><p>Pods access storage by using the claim as a volume. Claims must exist in the
same namespace as the Pod using the claim. The cluster finds the claim in the
Pod's namespace and uses it to get the PersistentVolume backing the claim.
The volume is then mounted to the host and into the Pod.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myfrontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/var/www/html"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypd<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">persistentVolumeClaim</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">claimName</span>:<span style="color:#bbb"> </span>myclaim<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="a-note-on-namespaces">A Note on Namespaces</h3><p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are
namespaced objects, mounting claims with "Many" modes (<code>ROX</code>, <code>RWX</code>) is only
possible within one namespace.</p><h3 id="persistentvolumes-typed-hostpath">PersistentVolumes typed <code>hostPath</code></h3><p>A <code>hostPath</code> PersistentVolume uses a file or directory on the Node to emulate
network-attached storage. See
<a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume">an example of <code>hostPath</code> typed volume</a>.</p><h2 id="raw-block-volume-support">Raw Block Volume Support</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:</p><ul><li>CSI (including some CSI migrated volume types)</li><li>FC (Fibre Channel)</li><li>iSCSI</li><li>Local volume</li></ul><h3 id="persistent-volume-using-a-raw-block-volume">PersistentVolume using a Raw Block Volume</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>block-pv<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeMode</span>:<span style="color:#bbb"> </span>Block<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Retain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">fc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetWWNs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"50060e801049cfd1"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">lun</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="persistent-volume-claim-requesting-a-raw-block-volume">PersistentVolumeClaim requesting a Raw Block Volume</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>block-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeMode</span>:<span style="color:#bbb"> </span>Block<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="pod-specification-adding-raw-block-device-path-in-container">Pod specification adding Raw Block Device path in container</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-with-block-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fc-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>fedora:26<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"/bin/sh"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-c"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">"tail -f /dev/null"</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeDevices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">devicePath</span>:<span style="color:#bbb"> </span>/dev/xvda<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">persistentVolumeClaim</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">claimName</span>:<span style="color:#bbb"> </span>block-pvc<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When adding a raw block device for a Pod, you specify the device path in the
container instead of a mount path.</div><h3 id="binding-block-volumes">Binding Block Volumes</h3><p>If a user requests a raw block volume by indicating this using the <code>volumeMode</code>
field in the PersistentVolumeClaim spec, the binding rules differ slightly from
previous releases that didn't consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for
requesting a raw block device. The table indicates if the volume will be bound or
not given the combinations: Volume binding matrix for statically provisioned volumes:</p><table><thead><tr><th>PV volumeMode</th><th style="text-align:center">PVC volumeMode</th><th style="text-align:right">Result</th></tr></thead><tbody><tr><td>unspecified</td><td style="text-align:center">unspecified</td><td style="text-align:right">BIND</td></tr><tr><td>unspecified</td><td style="text-align:center">Block</td><td style="text-align:right">NO BIND</td></tr><tr><td>unspecified</td><td style="text-align:center">Filesystem</td><td style="text-align:right">BIND</td></tr><tr><td>Block</td><td style="text-align:center">unspecified</td><td style="text-align:right">NO BIND</td></tr><tr><td>Block</td><td style="text-align:center">Block</td><td style="text-align:right">BIND</td></tr><tr><td>Block</td><td style="text-align:center">Filesystem</td><td style="text-align:right">NO BIND</td></tr><tr><td>Filesystem</td><td style="text-align:center">Filesystem</td><td style="text-align:right">BIND</td></tr><tr><td>Filesystem</td><td style="text-align:center">Block</td><td style="text-align:right">NO BIND</td></tr><tr><td>Filesystem</td><td style="text-align:center">unspecified</td><td style="text-align:right">BIND</td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Only statically provisioned volumes are supported for alpha release. Administrators
should take care to consider these values when working with raw block devices.</div><h2 id="volume-snapshot-and-restore-volume-from-snapshot-support">Volume Snapshot and Restore Volume from Snapshot Support</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>Volume snapshots only support the out-of-tree CSI volume plugins.
For details, see <a href="/docs/concepts/storage/volume-snapshots/">Volume Snapshots</a>.
In-tree volume plugins are deprecated. You can read about the deprecated volume
plugins in the
<a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">Volume Plugin FAQ</a>.</p><h3 id="create-persistent-volume-claim-from-volume-snapshot">Create a PersistentVolumeClaim from a Volume Snapshot</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>restore-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>csi-hostpath-sc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dataSource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="volume-cloning">Volume Cloning</h2><p><a href="/docs/concepts/storage/volume-pvc-datasource/">Volume Cloning</a>
only available for CSI volume plugins.</p><h3 id="create-persistent-volume-claim-from-an-existing-pvc">Create PersistentVolumeClaim from an existing PVC</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cloned-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>my-csi-plugin<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dataSource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>existing-src-pvc-name<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="volume-populators-and-data-sources">Volume populators and data sources</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [beta]</code></div><p>Kubernetes supports custom volume populators.
To use custom volume populators, you must enable the <code>AnyVolumeDataSource</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> for
the kube-apiserver and kube-controller-manager.</p><p>Volume populators take advantage of a PVC spec field called <code>dataSourceRef</code>. Unlike the
<code>dataSource</code> field, which can only contain either a reference to another PersistentVolumeClaim
or to a VolumeSnapshot, the <code>dataSourceRef</code> field can contain a reference to any object in the
same namespace, except for core objects other than PVCs. For clusters that have the feature
gate enabled, use of the <code>dataSourceRef</code> is preferred over <code>dataSource</code>.</p><h2 id="cross-namespace-data-sources">Cross namespace data sources</h2><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [alpha]</code></div><p>Kubernetes supports cross namespace volume data sources.
To use cross namespace volume data sources, you must enable the <code>AnyVolumeDataSource</code>
and <code>CrossNamespaceVolumeDataSource</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a> for
the kube-apiserver and kube-controller-manager.
Also, you must enable the <code>CrossNamespaceVolumeDataSource</code> feature gate for the csi-provisioner.</p><p>Enabling the <code>CrossNamespaceVolumeDataSource</code> feature gate allows you to specify
a namespace in the dataSourceRef field.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When you specify a namespace for a volume data source, Kubernetes checks for a
ReferenceGrant in the other namespace before accepting the reference.
ReferenceGrant is part of the <code>gateway.networking.k8s.io</code> extension APIs.
See <a href="https://gateway-api.sigs.k8s.io/api-types/referencegrant/">ReferenceGrant</a>
in the Gateway API documentation for details.
This means that you must extend your Kubernetes cluster with at least ReferenceGrant from the
Gateway API before you can use this mechanism.</div><h2 id="data-source-references">Data source references</h2><p>The <code>dataSourceRef</code> field behaves almost the same as the <code>dataSource</code> field. If one is
specified while the other is not, the API server will give both fields the same value. Neither
field can be changed after creation, and attempting to specify different values for the two
fields will result in a validation error. Therefore the two fields will always have the same
contents.</p><p>There are two differences between the <code>dataSourceRef</code> field and the <code>dataSource</code> field that
users should be aware of:</p><ul><li>The <code>dataSource</code> field ignores invalid values (as if the field was blank) while the
<code>dataSourceRef</code> field never ignores values and will cause an error if an invalid value is
used. Invalid values are any core object (objects with no apiGroup) except for PVCs.</li><li>The <code>dataSourceRef</code> field may contain different types of objects, while the <code>dataSource</code> field
only allows PVCs and VolumeSnapshots.</li></ul><p>When the <code>CrossNamespaceVolumeDataSource</code> feature is enabled, there are additional differences:</p><ul><li>The <code>dataSource</code> field only allows local objects, while the <code>dataSourceRef</code> field allows
objects in any namespaces.</li><li>When namespace is specified, <code>dataSource</code> and <code>dataSourceRef</code> are not synced.</li></ul><p>Users should always use <code>dataSourceRef</code> on clusters that have the feature gate enabled, and
fall back to <code>dataSource</code> on clusters that do not. It is not necessary to look at both fields
under any circumstance. The duplicated values with slightly different semantics exist only for
backwards compatibility. In particular, a mixture of older and newer controllers are able to
interoperate because the fields are the same.</p><h3 id="using-volume-populators">Using volume populators</h3><p>Volume populators are <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controllers">controllers</a> that can
create non-empty volumes, where the contents of the volume are determined by a Custom Resource.
Users create a populated volume by referring to a Custom Resource using the <code>dataSourceRef</code> field:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>populated-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dataSourceRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-name<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ExampleDataSource<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>example.storage.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></span></span></code></pre></div><p>Because volume populators are external components, attempts to create a PVC that uses one
can fail if not all the correct components are installed. External controllers should generate
events on the PVC to provide feedback on the status of the creation, including warnings if
the PVC cannot be created due to some missing component.</p><p>You can install the alpha <a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator</a>
controller into your cluster. That controller generates warning Events on a PVC in the case that no populator
is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the
responsibility of that populator controller to report Events that relate to volume creation and issues during
the process.</p><h3 id="using-a-cross-namespace-volume-data-source">Using a cross-namespace volume data source</h3><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [alpha]</code></div><p>Create a ReferenceGrant to allow the namespace owner to accept the reference.
You define a populated volume by specifying a cross namespace volume data source
using the <code>dataSourceRef</code> field. You must already have a valid ReferenceGrant
in the source namespace:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>gateway.networking.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ReferenceGrant<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>allow-ns1-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">from</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">""</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>ns1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">to</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">group</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-demo<span style="color:#bbb">
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo-pvc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>ns1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">dataSourceRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new-snapshot-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="writing-portable-configuration">Writing Portable Configuration</h2><p>If you're writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:</p><ul><li>Include PersistentVolumeClaim objects in your bundle of config (alongside
Deployments, ConfigMaps, etc).</li><li>Do not include PersistentVolume objects in the config, since the user instantiating
the config may not have permission to create PersistentVolumes.</li><li>Give the user the option of providing a storage class name when instantiating
the template.<ul><li>If the user provides a storage class name, put that value into the
<code>persistentVolumeClaim.storageClassName</code> field.
This will cause the PVC to match the right storage
class if the cluster has StorageClasses enabled by the admin.</li><li>If the user does not provide a storage class name, leave the
<code>persistentVolumeClaim.storageClassName</code> field as nil. This will cause a
PV to be automatically provisioned for the user with the default StorageClass
in the cluster. Many cluster environments have a default StorageClass installed,
or administrators can create their own default StorageClass.</li></ul></li><li>In your tooling, watch for PVCs that are not getting bound after some time
and surface this to the user, as this may indicate that the cluster has no
dynamic storage support (in which case the user should create a matching PV)
or the cluster has no storage system (in which case the user cannot deploy
config requiring PVCs).</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume">Creating a PersistentVolume</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim">Creating a PersistentVolumeClaim</a>.</li><li>Read the <a href="https://git.k8s.io/design-proposals-archive/storage/persistent-storage.md">Persistent Storage design document</a>.</li></ul><h3 id="reference">API references</h3><p>Read about the APIs described in this page:</p><ul><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/"><code>PersistentVolume</code></a></li><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/"><code>PersistentVolumeClaim</code></a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Scheduler Performance Tuning</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [beta]</code></div><p><a href="/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler">kube-scheduler</a>
is the Kubernetes default scheduler. It is responsible for placement of Pods
on Nodes in a cluster.</p><p>Nodes in a cluster that meet the scheduling requirements of a Pod are
called <em>feasible</em> Nodes for the Pod. The scheduler finds feasible Nodes
for a Pod and then runs a set of functions to score the feasible Nodes,
picking a Node with the highest score among the feasible ones to run
the Pod. The scheduler then notifies the API server about this decision
in a process called <em>Binding</em>.</p><p>This page explains performance tuning optimizations that are relevant for
large Kubernetes clusters.</p><p>In large clusters, you can tune the scheduler's behaviour balancing
scheduling outcomes between latency (new Pods are placed quickly) and
accuracy (the scheduler rarely makes poor placement decisions).</p><p>You configure this tuning setting via kube-scheduler setting
<code>percentageOfNodesToScore</code>. This KubeSchedulerConfiguration setting determines
a threshold for scheduling nodes in your cluster.</p><h3 id="setting-the-threshold">Setting the threshold</h3><p>The <code>percentageOfNodesToScore</code> option accepts whole numeric values between 0
and 100. The value 0 is a special number which indicates that the kube-scheduler
should use its compiled-in default.
If you set <code>percentageOfNodesToScore</code> above 100, kube-scheduler acts as if you
had set a value of 100.</p><p>To change the value, edit the
<a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler configuration file</a>
and then restart the scheduler.
In many cases, the configuration file can be found at <code>/etc/kubernetes/config/kube-scheduler.yaml</code>.</p><p>After you have made this change, you can run</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get pods -n kube-system | grep kube-scheduler
</span></span></code></pre></div><p>to verify that the kube-scheduler component is healthy.</p><h2 id="percentage-of-nodes-to-score">Node scoring threshold</h2><p>To improve scheduling performance, the kube-scheduler can stop looking for
feasible nodes once it has found enough of them. In large clusters, this saves
time compared to a naive approach that would consider every node.</p><p>You specify a threshold for how many nodes are enough, as a whole number percentage
of all the nodes in your cluster. The kube-scheduler converts this into an
integer number of nodes. During scheduling, if the kube-scheduler has identified
enough feasible nodes to exceed the configured percentage, the kube-scheduler
stops searching for more feasible nodes and moves on to the
<a href="/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation">scoring phase</a>.</p><p><a href="#how-the-scheduler-iterates-over-nodes">How the scheduler iterates over Nodes</a>
describes the process in detail.</p><h3 id="default-threshold">Default threshold</h3><p>If you don't specify a threshold, Kubernetes calculates a figure using a
linear formula that yields 50% for a 100-node cluster and yields 10%
for a 5000-node cluster. The lower bound for the automatic value is 5%.</p><p>This means that the kube-scheduler always scores at least 5% of your cluster no
matter how large the cluster is, unless you have explicitly set
<code>percentageOfNodesToScore</code> to be smaller than 5.</p><p>If you want the scheduler to score all nodes in your cluster, set
<code>percentageOfNodesToScore</code> to 100.</p><h2 id="example">Example</h2><p>Below is an example configuration that sets <code>percentageOfNodesToScore</code> to 50%.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1alpha1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">algorithmSource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">provider</span>:<span style="color:#bbb"> </span>DefaultProvider<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">percentageOfNodesToScore</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="tuning-percentageofnodestoscore">Tuning percentageOfNodesToScore</h2><p><code>percentageOfNodesToScore</code> must be a value between 1 and 100 with the default
value being calculated based on the cluster size. There is also a hardcoded
minimum value of 100 nodes.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>In clusters with less than 100 feasible nodes, the scheduler still
checks all the nodes because there are not enough feasible nodes to stop
the scheduler's search early.</p><p>In a small cluster, if you set a low value for <code>percentageOfNodesToScore</code>, your
change will have no or little effect, for a similar reason.</p><p>If your cluster has several hundred Nodes or fewer, leave this configuration option
at its default value. Making changes is unlikely to improve the
scheduler's performance significantly.</p></div><p>An important detail to consider when setting this value is that when a smaller
number of nodes in a cluster are checked for feasibility, some nodes are not
sent to be scored for a given Pod. As a result, a Node which could possibly
score a higher value for running the given Pod might not even be passed to the
scoring phase. This would result in a less than ideal placement of the Pod.</p><p>You should avoid setting <code>percentageOfNodesToScore</code> very low so that kube-scheduler
does not make frequent, poor Pod placement decisions. Avoid setting the
percentage to anything below 10%, unless the scheduler's throughput is critical
for your application and the score of nodes is not important. In other words, you
prefer to run the Pod on any Node as long as it is feasible.</p><h2 id="how-the-scheduler-iterates-over-nodes">How the scheduler iterates over Nodes</h2><p>This section is intended for those who want to understand the internal details
of this feature.</p><p>In order to give all the Nodes in a cluster a fair chance of being considered
for running Pods, the scheduler iterates over the nodes in a round robin
fashion. You can imagine that Nodes are in an array. The scheduler starts from
the start of the array and checks feasibility of the nodes until it finds enough
Nodes as specified by <code>percentageOfNodesToScore</code>. For the next Pod, the
scheduler continues from the point in the Node array that it stopped at when
checking feasibility of Nodes for the previous Pod.</p><p>If Nodes are in multiple zones, the scheduler iterates over Nodes in various
zones to ensure that Nodes from different zones are considered in the
feasibility checks. As an example, consider six nodes in two zones:</p><pre tabindex="0"><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><p>The Scheduler evaluates feasibility of the nodes in this order:</p><pre tabindex="0"><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><p>After going over all the Nodes, it goes back to Node 1.</p><h2 id="what-s-next">What's next</h2><ul><li>Check the <a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler configuration reference (v1)</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Dynamic Resource Allocation</h1><div class="feature-state-notice feature-stable" title="Feature Gate: DynamicResourceAllocation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This page describes <em>dynamic resource allocation (DRA)</em> in Kubernetes.</p><h2 id="about-dra">About DRA</h2><p><p>DRA is a Kubernetes feature that lets you request and share resources among Pods.
These resources are often attached
<a class="glossary-tooltip" title="Any resource that's directly or indirectly attached your cluster's nodes, like GPUs or circuit boards." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-device" target="_blank" aria-label="devices">devices</a> like hardware
accelerators.</p></p><p>With DRA, device drivers and cluster admins define device <em>classes</em> that are
available to <em>claim</em> in workloads. Kubernetes allocates matching devices to
specific claims and places the corresponding Pods on nodes that can access the
allocated devices.</p><p>Allocating resources with DRA is a similar experience to
<a href="/docs/concepts/storage/dynamic-provisioning/">dynamic volume provisioning</a>, in
which you use PersistentVolumeClaims to claim storage capacity from storage
classes and request the claimed capacity in your Pods.</p><h3 id="dra-benefits">Benefits of DRA</h3><p>DRA provides a flexible way to categorize, request, and use devices in your
cluster. Using DRA provides benefits like the following:</p><ul><li><strong>Flexible device filtering</strong>: use common expression language (CEL) to perform
fine-grained filtering for specific device attributes.</li><li><strong>Device sharing</strong>: share the same resource with multiple containers or Pods
by referencing the corresponding resource claim.</li><li><strong>Centralized device categorization</strong>: device drivers and cluster admins can
use device classes to provide app operators with hardware categories that are
optimized for various use cases. For example, you can create a cost-optimized
device class for general-purpose workloads, and a high-performance device
class for critical jobs.</li><li><strong>Simplified Pod requests</strong>: with DRA, app operators don't need to specify
device quantities in Pod resource requests. Instead, the Pod references a
resource claim, and the device configuration in that claim applies to the Pod.</li></ul><p>These benefits provide significant improvements in the device allocation
workflow when compared to
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugins</a>,
which require per-container device requests, don't support device sharing, and
don't support expression-based device filtering.</p><h3 id="dra-user-types">Types of DRA users</h3><p>The workflow of using DRA to allocate devices involves the following types of
users:</p><ul><li><p><strong>Device owner</strong>: responsible for devices. Device owners might be commercial
vendors, the cluster operator, or another entity. To use DRA, devices must
have DRA-compatible drivers that do the following:</p><ul><li>Create ResourceSlices that provide Kubernetes with information about
nodes and resources.</li><li>Update ResourceSlices when resource capacity in the cluster changes.</li><li>Optionally, create DeviceClasses that workload operators can use to
claim devices.</li></ul></li><li><p><strong>Cluster admin</strong>: responsible for configuring clusters and nodes,
attaching devices, installing drivers, and similar tasks. To use DRA,
cluster admins do the following:</p><ul><li>Attach devices to nodes.</li><li>Install device drivers that support DRA.</li><li>Optionally, create DeviceClasses that workload operators can use to claim
devices.</li></ul></li><li><p><strong>Workload operator</strong>: responsible for deploying and managing workloads in the
cluster. To use DRA to allocate devices to Pods, workload operators do the
following:</p><ul><li>Create ResourceClaims or ResourceClaimTemplates to request specific
configurations within DeviceClasses.</li><li>Deploy workloads that use specific ResourceClaims or ResourceClaimTemplates.</li></ul></li></ul><h2 id="terminology">DRA terminology</h2><p>DRA uses the following Kubernetes API kinds to provide the core allocation
functionality. All of these API kinds are included in the
<code>resource.k8s.io/v1</code>
<a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank" aria-label="API group">API group</a>.</p><dl><dt>DeviceClass</dt><dd>Defines a category of devices that can be claimed and how to select specific
device attributes in claims. The DeviceClass parameters can match zero or
more devices in ResourceSlices. To claim devices from a DeviceClass,
ResourceClaims select specific device attributes.</dd><dt>ResourceClaim</dt><dd>Describes a request for access to attached resources, such as
devices, in the cluster. ResourceClaims provide Pods with access to
a specific resource. ResourceClaims can be created by workload operators
or generated by Kubernetes based on a ResourceClaimTemplate.</dd><dt>ResourceClaimTemplate</dt><dd>Defines a template that Kubernetes uses to create per-Pod
ResourceClaims for a workload. ResourceClaimTemplates provide Pods with
access to separate, similar resources. Each ResourceClaim that Kubernetes
generates from the template is bound to a specific Pod. When the Pod
terminates, Kubernetes deletes the corresponding ResourceClaim.</dd><dt>ResourceSlice</dt><dd>Represents one or more resources that are attached to nodes, such as devices.
Drivers create and manage ResourceSlices in the cluster. When a ResourceClaim
is created and used in a Pod, Kubernetes uses ResourceSlices to find nodes
that have access to the claimed resources. Kubernetes allocates resources to
the ResourceClaim and schedules the Pod onto a node that can access the
resources.</dd></dl><h3 id="deviceclass">DeviceClass</h3><p>A DeviceClass lets cluster admins or device drivers define categories of devices
in the cluster. DeviceClasses tell operators what devices they can request and
how they can request those devices. You can use
<a href="https://cel.dev">common expression language (CEL)</a> to select devices based on
specific attributes. A ResourceClaim that references the DeviceClass can then
request specific configurations within the DeviceClass.</p><p>To create a DeviceClass, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set Up DRA in a Cluster</a>.</p><h3 id="resourceclaims-templates">ResourceClaims and ResourceClaimTemplates</h3><p>A ResourceClaim defines the resources that a workload needs. Every ResourceClaim
has <em>requests</em> that reference a DeviceClass and select devices from that
DeviceClass. ResourceClaims can also use <em>selectors</em> to filter for devices that
meet specific requirements, and can use <em>constraints</em> to limit the devices that
can satisfy a request. ResourceClaims can be created by workload operators or
can be generated by Kubernetes based on a ResourceClaimTemplate. A
ResourceClaimTemplate defines a template that Kubernetes can use to
auto-generate ResourceClaims for Pods.</p><h4 id="when-to-use-rc-rct">Use cases for ResourceClaims and ResourceClaimTemplates</h4><p>The method that you use depends on your requirements, as follows:</p><ul><li><strong>ResourceClaim</strong>: you want multiple Pods to share access to specific
devices. You manually manage the lifecycle of ResourceClaims that you create.</li><li><strong>ResourceClaimTemplate</strong>: you want Pods to have independent access to
separate, similarly-configured devices. Kubernetes generates ResourceClaims
from the specification in the ResourceClaimTemplate. The lifetime of each
generated ResourceClaim is bound to the lifetime of the corresponding Pod.</li></ul><p>When you define a workload, you can use
<a class="glossary-tooltip" title="An expression language that's designed to be safe for executing user code." data-toggle="tooltip" data-placement="top" href="https://cel.dev" target="_blank" aria-label="Common Expression Language (CEL)">Common Expression Language (CEL)</a>
to filter for specific device attributes or capacity. The available parameters
for filtering depend on the device and the drivers.</p><p>If you directly reference a specific ResourceClaim in a Pod, that ResourceClaim
must already exist in the same namespace as the Pod. If the ResourceClaim
doesn't exist in the namespace, the Pod won't schedule. This behavior is similar
to how a PersistentVolumeClaim must exist in the same namespace as a Pod that
references it.</p><p>You can reference an auto-generated ResourceClaim in a Pod, but this isn't
recommended because auto-generated ResourceClaims are bound to the lifetime of
the Pod that triggered the generation.</p><p>To learn how to claim resources using one of these methods, see
<a href="/docs/tasks/configure-pod-container/assign-resources/allocate-devices-dra/">Allocate Devices to Workloads with DRA</a>.</p><h4 id="prioritized-list">Prioritized list</h4><div class="feature-state-notice feature-beta" title="Feature Gate: DRAPrioritizedList"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>You can provide a prioritized list of subrequests for requests in a ResourceClaim or
ResourceClaimTemplate. The scheduler will then select the first subrequest that can be allocated.
This allows users to specify alternative devices that can be used by the workload if the primary
choice is not available.</p><p>In the example below, the ResourceClaimTemplate requested a device with the color black
and the size large. If a device with those attributes is not available, the pod cannot
be scheduled. With the prioritized list feature, a second alternative can be specified, which
requests two devices with the color white and size small. The large black device will be
allocated if it is available. If it is not, but two small white devices are available,
the pod will still be able to run.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceClaimTemplate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>prioritized-list-claim-template<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>req-0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">firstAvailable</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>large-black<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">deviceClassName</span>:<span style="color:#bbb"> </span>resource.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">selectors</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">cel</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">expression</span>:<span style="color:#bbb"> </span>|-<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">                device.attributes["resource-driver.example.com"].color == "black" &amp;&amp;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">                device.attributes["resource-driver.example.com"].size == "large"</span><span style="color:#bbb">                
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>small-white<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">deviceClassName</span>:<span style="color:#bbb"> </span>resource.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">selectors</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">cel</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">expression</span>:<span style="color:#bbb"> </span>|-<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">                device.attributes["resource-driver.example.com"].color == "white" &amp;&amp;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">                device.attributes["resource-driver.example.com"].size == "small"</span><span style="color:#bbb">                
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">count</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The decision is made on a per-Pod basis, so if the Pod is a member of a ReplicaSet or
similar grouping, you cannot rely on all the members of the group having the same subrequest
chosen. Your workload must be able to accommodate this.</p><p>Prioritized lists is a <em>beta feature</em> and is enabled by default with the
<code>DRAPrioritizedList</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> in
the kube-apiserver and kube-scheduler.</p><h3 id="resourceslice">ResourceSlice</h3><p>Each ResourceSlice represents one or more
<a class="glossary-tooltip" title="Any resource that's directly or indirectly attached your cluster's nodes, like GPUs or circuit boards." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-device" target="_blank" aria-label="devices">devices</a> in a pool. The pool is
managed by a device driver, which creates and manages ResourceSlices. The
resources in a pool might be represented by a single ResourceSlice or span
multiple ResourceSlices.</p><p>ResourceSlices provide useful information to device users and to the scheduler,
and are crucial for dynamic resource allocation. Every ResourceSlice must include
the following information:</p><ul><li><strong>Resource pool</strong>: a group of one or more resources that the driver manages.
The pool can span more than one ResourceSlice. Changes to the resources in a
pool must be propagated across all of the ResourceSlices in that pool. The
device driver that manages the pool is responsible for ensuring that this
propagation happens.</li><li><strong>Devices</strong>: devices in the managed pool. A ResourceSlice can list every
device in a pool or a subset of the devices in a pool. The ResourceSlice
defines device information like attributes, versions, and capacity. Device
users can select devices for allocation by filtering for device information
in ResourceClaims or in DeviceClasses.</li><li><strong>Nodes</strong>: the nodes that can access the resources. Drivers can choose which
nodes can access the resources, whether that's all of the nodes in the
cluster, a single named node, or nodes that have specific node labels.</li></ul><p>Drivers use a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> to
reconcile ResourceSlices in the cluster with the information that the driver has
to publish. This controller overwrites any manual changes, such as cluster users
creating or modifying ResourceSlices.</p><p>Consider the following example ResourceSlice:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cat-slice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span><span style="color:#b44">"resource-driver.example.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">pool</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"black-cat-pool"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resourceSliceCount</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># The allNodes field defines whether any node in the cluster can access the device.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">allNodes</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"large-black-cat"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">attributes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">color</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">string</span>:<span style="color:#bbb"> </span><span style="color:#b44">"black"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">size</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">string</span>:<span style="color:#bbb"> </span><span style="color:#b44">"large"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cat</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">bool</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This ResourceSlice is managed by the <code>resource-driver.example.com</code> driver in the
<code>black-cat-pool</code> pool. The <code>allNodes: true</code> field indicates that any node in the
cluster can access the devices. There's one device in the ResourceSlice, named
<code>large-black-cat</code>, with the following attributes:</p><ul><li><code>color</code>: <code>black</code></li><li><code>size</code>: <code>large</code></li><li><code>cat</code>: <code>true</code></li></ul><p>A DeviceClass could select this ResourceSlice by using these attributes, and a
ResourceClaim could filter for specific devices in that DeviceClass.</p><h2 id="how-it-works">How resource allocation with DRA works</h2><p>The following sections describe the workflow for the various
<a href="#dra-user-types">types of DRA users</a> and for the Kubernetes system during
dynamic resource allocation.</p><h3 id="user-workflow">Workflow for users</h3><ol><li><strong>Driver creation</strong>: device owners or third-party entities create drivers
that can create and manage ResourceSlices in the cluster. These drivers
optionally also create DeviceClasses that define a category of devices and
how to request them.</li><li><strong>Cluster configuration</strong>: cluster admins create clusters, attach devices to
nodes, and install the DRA device drivers. Cluster admins optionally create
DeviceClasses that define categories of devices and how to request them.</li><li><strong>Resource claims</strong>: workload operators create ResourceClaimTemplates or
ResourceClaims that request specific device configurations within a
DeviceClass. In the same step, workload operators modify their Kubernetes
manifests to request those ResourceClaimTemplates or ResourceClaims.</li></ol><h3 id="kubernetes-workflow">Workflow for Kubernetes</h3><ol><li><p><strong>ResourceSlice creation</strong>: drivers in the cluster create ResourceSlices that
represent one or more devices in a managed pool of similar devices.</p></li><li><p><strong>Workload creation</strong>: the cluster control plane checks new workloads for
references to ResourceClaimTemplates or to specific ResourceClaims.</p><ul><li>If the workload uses a ResourceClaimTemplate, a controller named the
<code>resourceclaim-controller</code> generates ResourceClaims for every Pod in the
workload.</li><li>If the workload uses a specific ResourceClaim, Kubernetes checks whether
that ResourceClaim exists in the cluster. If the ResourceClaim doesn't
exist, the Pods won't deploy.</li></ul></li><li><p><strong>ResourceSlice filtering</strong>: for every Pod, Kubernetes checks the
ResourceSlices in the cluster to find a device that satisfies all of the
following criteria:</p><ul><li>The nodes that can access the resources are eligible to run the Pod.</li><li>The ResourceSlice has unallocated resources that match the requirements of
the Pod's ResourceClaim.</li></ul></li><li><p><strong>Resource allocation</strong>: after finding an eligible ResourceSlice for a
Pod's ResourceClaim, the Kubernetes scheduler updates the ResourceClaim
with the allocation details.</p></li><li><p><strong>Pod scheduling</strong>: when resource allocation is complete, the scheduler
places the Pod on a node that can access the allocated resource. The device
driver and the kubelet on that node configure the device and the Pod's access
to the device.</p></li></ol><h2 id="observability-dynamic-resources">Observability of dynamic resources</h2><p>You can check the status of dynamically allocated resources by using any of the
following methods:</p><ul><li><a href="#monitoring-resources">kubelet device metrics</a></li><li><a href="#resourceclaim-device-status">ResourceClaim status</a></li><li><a href="#device-health-monitoring">Device health monitoring</a></li></ul><h3 id="monitoring-resources">kubelet device metrics</h3><p>The <code>PodResourcesLister</code> kubelet gRPC service lets you monitor in-use devices.
The <code>DynamicResource</code> message provides information that's specific to dynamic
resource allocation, such as the device name and the claim name. For details,
see
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources">Monitoring device plugin resources</a>.</p><h3 id="resourceclaim-device-status">ResourceClaim device status</h3><div class="feature-state-notice feature-beta" title="Feature Gate: DRAResourceClaimDeviceStatus"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>DRA drivers can report driver-specific
<a href="/docs/concepts/overview/working-with-objects/#object-spec-and-status">device status</a>
data for each allocated device in the <code>status.devices</code> field of a ResourceClaim.
For example, the driver might list the IP addresses that are assigned to a
network interface device.</p><p>The accuracy of the information that a driver adds to a ResourceClaim
<code>status.devices</code> field depends on the driver. Evaluate drivers to decide whether
you can rely on this field as the only source of device information.</p><p>If you disable the <code>DRAResourceClaimDeviceStatus</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>, the
<code>status.devices</code> field automatically gets cleared when storing the ResourceClaim.
A ResourceClaim device status is supported when it is possible, from a DRA
driver, to update an existing ResourceClaim where the <code>status.devices</code> field is
set.</p><p>For details about the <code>status.devices</code> field, see the
<a href="#ResourceClaimStatus">ResourceClaim</a> API reference.</p><h3 id="device-health-monitoring">Device Health Monitoring</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: ResourceHealthStatus"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [alpha]</code> (enabled by default: false)</div><p>As an alpha feature, Kubernetes provides a mechanism for monitoring and reporting the health of dynamically allocated infrastructure resources.
For stateful applications running on specialized hardware, it is critical to know when a device has failed or become unhealthy. It is also helpful to find out if the device recovers.</p><p>To enable this functionality, the <code>ResourceHealthStatus</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/resource-health-status/">feature gate</a> must be enabled, and the DRA driver must implement the <code>DRAResourceHealth</code> gRPC service.</p><p>When a DRA driver detects that an allocated device has become unhealthy, it reports this status back to the kubelet. This health information is then exposed directly in the Pod's status. The kubelet populates the <code>allocatedResourcesStatus</code> field in the status of each container, detailing the health of each device assigned to that container.</p><p>This provides crucial visibility for users and controllers to react to hardware failures. For a Pod that is failing, you can inspect this status to determine if the failure was related to an unhealthy device.</p><h2 id="pre-scheduled-pods">Pre-scheduled Pods</h2><p>When you - or another API client - create a Pod with <code>spec.nodeName</code> already set, the scheduler gets bypassed.
If some ResourceClaim needed by that Pod does not exist yet, is not allocated
or not reserved for the Pod, then the kubelet will fail to run the Pod and
re-check periodically because those requirements might still get fulfilled
later.</p><p>Such a situation can also arise when support for dynamic resource allocation
was not enabled in the scheduler at the time when the Pod got scheduled
(version skew, configuration, feature gate, etc.). kube-controller-manager
detects this and tries to make the Pod runnable by reserving the required
ResourceClaims. However, this only works if those were allocated by
the scheduler for some other pod.</p><p>It is better to avoid bypassing the scheduler because a Pod that is assigned to a node
blocks normal resources (RAM, CPU) that then cannot be used for other Pods
while the Pod is stuck. To make a Pod run on a specific node while still going
through the normal scheduling flow, create the Pod with a node selector that
exactly matches the desired node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-with-cats<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/hostname</span>:<span style="color:#bbb"> </span>name-of-the-intended-node<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>You may also be able to mutate the incoming Pod, at admission time, to unset
the <code>.spec.nodeName</code> field and to use a node selector instead.</p><h2 id="beta-features">DRA beta features</h2><p>The following sections describe DRA features that are available in the Beta
<a href="/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">feature stage</a>.
For more information, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set up DRA in the cluster</a>.</p><h3 id="admin-access">Admin access</h3><div class="feature-state-notice feature-beta" title="Feature Gate: DRAAdminAccess"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>You can mark a request in a ResourceClaim or ResourceClaimTemplate as having
privileged features for maintenance and troubleshooting tasks. A request with
admin access grants access to in-use devices and may enable additional
permissions when making the device available in a container:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceClaimTemplate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>large-black-cat-claim-template<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>req-0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">exactly</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">deviceClassName</span>:<span style="color:#bbb"> </span>resource.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">allocationMode</span>:<span style="color:#bbb"> </span>All<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">adminAccess</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>If this feature is disabled, the <code>adminAccess</code> field will be removed
automatically when creating such a ResourceClaim.</p><p>Admin access is a privileged mode and should not be granted to regular users in
multi-tenant clusters. Starting with Kubernetes v1.33, only users authorized to
create ResourceClaim or ResourceClaimTemplate objects in namespaces labeled with
<code>resource.k8s.io/admin-access: "true"</code> (case-sensitive) can use the
<code>adminAccess</code> field. This ensures that non-admin users cannot misuse the
feature. Starting with Kubernetes v1.34, this label has been updated to <code>resource.kubernetes.io/admin-access: "true"</code>.</p><h2 id="alpha-features">DRA alpha features</h2><p>The following sections describe DRA features that are available in the Alpha
<a href="/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">feature stage</a>.
To use any of these features, you must also set up DRA in your clusters by
enabling the DynamicResourceAllocation feature gate and the DRA
<a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank" aria-label="API groups">API groups</a>. For more
information, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set up DRA in the cluster</a>.</p><h3 id="extended-resource">Extended resource allocation by DRA</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRAExtendedResource"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>You can provide an extended resource name for a DeviceClass. The scheduler will then
select the devices matching the class for the extended resource requests. This allows
users to continue using extended resource requests in a pod to request either
extended resources provided by device plugin, or DRA devices. The same extended
resource can be provided either by device plugin, or DRA on one single cluster node.
The same extended resource can be provided by device plugin on some nodes, and
DRA on other nodes in the same cluster.</p><p>In the example below, the DeviceClass is given an extendedResourceName <code>example.com/gpu</code>.
If a pod requested for the extended resource <code>example.com/gpu: 2</code>, it can be scheduled to
a node with two or more devices matching the DeviceClass.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>DeviceClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>gpu.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selectors</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">cel</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">expression</span>:<span style="color:#bbb"> </span>device.driver == 'gpu.example.com' &amp;&amp; device.attributes['gpu.example.com'].type<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>== 'gpu'<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">extendedResourceName</span>:<span style="color:#bbb"> </span>example.com/gpu<span style="color:#bbb">
</span></span></span></code></pre></div><p>In addition, users can use a special extended resource to allocate devices without
having to explicitly create a ResourceClaim. Using the extended resource name
prefix <code>deviceclass.resource.kubernetes.io/</code> and the DeviceClass name. This works
for any DeviceClass, even if it does not specify the an extended resource name.
The resulting ResourceClaim will contain a request for an <code>ExactCount</code> of the
specified number of devices of that DeviceClass.</p><p>Extended resource allocation by DRA is an <em>alpha feature</em> and only enabled when the
<code>DRAExtendedResource</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled in the kube-apiserver, kube-scheduler, and kubelet.</p><h3 id="partitionable-devices">Partitionable devices</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRAPartitionableDevices"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>Devices represented in DRA don't necessarily have to be a single unit connected to a single machine,
but can also be a logical device comprised of multiple devices connected to multiple machines. These
devices might consume overlapping resources of the underlying phyical devices, meaning that when one
logical device is allocated other devices will no longer be available.</p><p>In the ResourceSlice API, this is represented as a list of named CounterSets, each of which
contains a set of named counters. The counters represent the resources available on the physical
device that are used by the logical devices advertised through DRA.</p><p>Logical devices can specify the ConsumesCounters list. Each entry contains a reference to a CounterSet
and a set of named counters with the amounts they will consume. So for a device to be allocatable,
the referenced counter sets must have sufficient quantity for the counters referenced by the device.</p><p>Here is an example of two devices, each consuming 6Gi of memory from the a shared counter with
8Gi of memory. Thus, only one of the devices can be allocated at any point in time. The scheduler
handles this and it is transparent to the consumer as the ResourceClaim API is not affected.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>resourceslice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeName</span>:<span style="color:#bbb"> </span>worker-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">pool</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pool<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resourceSliceCount</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>dra.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">sharedCounters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>gpu-1-counters<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">counters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>8Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>device-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">consumesCounters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">counterSet</span>:<span style="color:#bbb"> </span>gpu-1-counters<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">counters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>6Gi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>device-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">consumesCounters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">counterSet</span>:<span style="color:#bbb"> </span>gpu-1-counters<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">counters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>6Gi<span style="color:#bbb">
</span></span></span></code></pre></div><p>Partitionable devices is an <em>alpha feature</em> and only enabled when the
<code>DRAPartitionableDevices</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled in the kube-apiserver and kube-scheduler.</p><h2 id="consumable-capacity">Consumable capacity</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: DRAConsumableCapacity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>The consumable capacity feature allows the same devices to be consumed by multiple independent ResourceClaims, with the Kubernetes scheduler
managing how much of the device's capacity is used up by each claim. This is analogous to how Pods can share
the resources on a Node; ResourceClaims can share the resources on a Device.</p><p>The device driver can set <code>allowMultipleAllocations</code> field added in <code>.spec.devices</code> of <code>ResourceSlice</code> to allow allocating that device to multiple independent ResourceClaims or to multiple requests within a ResourceClaim.</p><p>Users can set <code>capacity</code> field added in <code>spec.devices.requests</code> of <code>ResourceClaim</code> to specify the device resource requirements for each allocation.</p><p>For the device that allows multiple allocations, the requested capacity is drawn from â€” or consumed from â€” its total capacity, a concept known as <strong>consumable capacity</strong>.
Then, the scheduler ensures that the aggregate consumed capacity across all claims does not exceed the deviceâ€™s overall capacity. Furthermore, driver authors can use the <code>requestPolicy</code> constraints on individual device capacities to control how those capacities are consumed. For example, the driver author can specify that a given capacity is only consumed in increments of 1Gi.</p><p>Here is an example of a network device which allows multiple allocations and contains
a consumable bandwidth capacity.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>resourceslice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeName</span>:<span style="color:#bbb"> </span>worker-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">pool</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pool<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resourceSliceCount</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>dra.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>eth1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">allowMultipleAllocations</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">attributes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">string</span>:<span style="color:#bbb"> </span><span style="color:#b44">"eth1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">bandwidth</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">requestPolicy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">default</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1M"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">validRange</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">min</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1M"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">step</span>:<span style="color:#bbb"> </span><span style="color:#b44">"8"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"10G"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The consumable capacity can be requested as shown in the below example.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceClaimTemplate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>bandwidth-claim-template<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>req-0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">exactly</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">deviceClassName</span>:<span style="color:#bbb"> </span>resource.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">capacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">bandwidth</span>:<span style="color:#bbb"> </span>1G<span style="color:#bbb">
</span></span></span></code></pre></div><p>The allocation result will include the consumed capacity and the identifier of the share.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceClaim<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">allocation</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">results</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">consumedCapacity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">bandwidth</span>:<span style="color:#bbb"> </span>1G<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">device</span>:<span style="color:#bbb"> </span>eth1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">shareID</span>:<span style="color:#bbb"> </span><span style="color:#b44">"a671734a-e8e5-11e4-8fde-42010af09327"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>In this example, a multiply-allocatable device was chosen. However, any <code>resource.example.com</code> device with at least the requested 1G bandwidth could have met the requirement. If a non-multiply-allocatable device were chosen, the allocation would have resulted in the entire device. To force the use of a only multiply-allocatable devices, you can use the CEL criteria <code>device.allowMultipleAllocations == true</code>.</p><h3 id="device-taints-and-tolerations">Device taints and tolerations</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRADeviceTaints"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>Device taints are similar to node taints: a taint has a string key, a string
value, and an effect. The effect is applied to the ResourceClaim which is
using a tainted device and to all Pods referencing that ResourceClaim.
The "NoSchedule" effect prevents scheduling those Pods.
Tainted devices are ignored when trying to allocate a ResourceClaim
because using them would prevent scheduling of Pods.</p><p>The "NoExecute" effect implies "NoSchedule" and in addition causes eviction
of all Pods which have been scheduled already. This eviction is implemented
in the device taint eviction controller in kube-controller-manager by
deleting affected Pods.</p><p>ResourceClaims can tolerate taints. If a taint is tolerated, its effect does
not apply. An empty toleration matches all taints. A toleration can be limited to
certain effects and/or match certain key/value pairs. A toleration can check
that a certain key exists, regardless which value it has, or it can check
for specific values of a key.
For more information on this matching see the
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts">node taint concepts</a>.</p><p>Eviction can be delayed by tolerating a taint for a certain duration.
That delay starts at the time when a taint gets added to a device, which is recorded in a field
of the taint.</p><p>Taints apply as described above also to ResourceClaims allocating "all" devices on a node.
All devices must be untainted or all of their taints must be tolerated.
Allocating a device with admin access (described <a href="#admin-access">above</a>)
is not exempt either. An admin using that mode must explicitly tolerate all taints
to access tainted devices.</p><p>Device taints and tolerations is an <em>alpha feature</em> and only enabled when the
<code>DRADeviceTaints</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled in the kube-apiserver, kube-controller-manager and kube-scheduler.
To use DeviceTaintRules, the <code>resource.k8s.io/v1alpha3</code> API version must be
enabled.</p><p>You can add taints to devices in the following ways, by using the
DeviceTaintRule API kind.</p><h4 id="taints-set-by-the-driver">Taints set by the driver</h4><p>A DRA driver can add taints to the device information that it publishes in ResourceSlices.
Consult the documentation of a DRA driver to learn whether the driver uses taints and what
their keys and values are.</p><h4 id="taints-set-by-an-admin">Taints set by an admin</h4><p>An admin or a control plane component can taint devices without having to tell
the DRA driver to include taints in its device information in ResourceSlices. They do that by
creating DeviceTaintRules. Each DeviceTaintRule adds one taint to devices which
match the device selector. Without such a selector, no devices are tainted. This
makes it harder to accidentally evict all pods using ResourceClaims when leaving out
the selector by mistake.</p><p>Devices can be selected by giving the name of a DeviceClass, driver, pool,
and/or device. The DeviceClass selects all devices that are selected by the
selectors in that DeviceClass. With just the driver name, an admin can taint
all devices managed by that driver, for example while doing some kind of
maintenance of that driver across the entire cluster. Adding a pool name can
limit the taint to a single node, if the driver manages node-local devices.</p><p>Finally, adding the device name can select one specific device. The device name
and pool name can also be used alone, if desired. For example, drivers for node-local
devices are encouraged to use the node name as their pool name. Then tainting with
that pool name automatically taints all devices on a node.</p><p>Drivers might use stable names like "gpu-0" that hide which specific device is
currently assigned to that name. To support tainting a specific hardware
instance, CEL selectors can be used in a DeviceTaintRule to match a vendor-specific
unique ID attribute, if the driver supports one for its hardware.</p><p>The taint applies as long as the DeviceTaintRule exists. It can be modified and
and removed at any time. Here is one example of a DeviceTaintRule for a fictional
DRA driver:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1alpha3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>DeviceTaintRule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># The entire hardware installation for this</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># particular driver is broken.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Evict all pods and don't schedule new ones.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">deviceSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>dra.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">taint</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>dra.example.com/unhealthy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>Broken<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoExecute<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="device-binding-conditions">Device Binding Conditions</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRADeviceBindingConditions"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>Device Binding Conditions allow the Kubernetes scheduler to delay Pod binding until
external resources, such as fabric-attached GPUs or reprogrammable FPGAs, are confirmed
to be ready.</p><p>This waiting behavior is implemented in the
<a href="/docs/concepts/scheduling-eviction/scheduling-framework/#pre-bind">PreBind phase</a>
of the scheduling framework.
During this phase, the scheduler checks whether all required device conditions are
satisfied before proceeding with binding.</p><p>This improves scheduling reliability by avoiding premature binding and enables coordination
with external device controllers.</p><p>To use this feature, device drivers (typically managed by driver owners) must publish the
following fields in the <code>Device</code> section of a <code>ResourceSlice</code>. Cluster administrators
must enable the <code>DRADeviceBindingConditions</code> and <code>DRAResourceClaimDeviceStatus</code> feature
gates for the scheduler to honor these fields.</p><ul><li><code>bindingConditions</code>: A list of condition types that must be set to True in the
status.conditions field of the associated ResourceClaim before the Pod can be bound.
These typically represent readiness signals such as "DeviceAttached" or "DeviceInitialized".</li><li><code>bindingFailureConditions</code>: A list of condition types that, if set to True in
status.conditions field of the associated ResourceClaim, indicate a failure state.
If any of these conditions are True, the scheduler will abort binding and reschedule the Pod.</li><li><code>bindsToNode</code>: if set to <code>true</code>, the scheduler records the selected node name in the
<code>status.allocation.nodeSelector</code> field of the ResourceClaim.
This does not affect the Pod's <code>spec.nodeSelector</code>. Instead, it sets a node selector
inside the ResourceClaim, which external controllers can use to perform node-specific
operations such as device attachment or preparation.</li></ul><p>All condition types listed in bindingConditions and bindingFailureConditions are evaluated
from the <code>status.conditions</code> field of the ResourceClaim.
External controllers are responsible for updating these conditions using standard Kubernetes
condition semantics (<code>type</code>, <code>status</code>, <code>reason</code>, <code>message</code>, <code>lastTransitionTime</code>).</p><p>The scheduler waits up to <strong>600 seconds</strong> for all <code>bindingConditions</code> to become <code>True</code>.
If the timeout is reached or any <code>bindingFailureConditions</code> are <code>True</code>, the scheduler
clears the allocation and reschedules the Pod.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>resource.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ResourceSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>gpu-slice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>dra.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">matchExpressions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>accelerator-type<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:#b44">"high-performance"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">pool</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>gpu-pool<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resourceSliceCount</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">devices</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>gpu-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">attributes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">vendor</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">string</span>:<span style="color:#bbb"> </span><span style="color:#b44">"example"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">model</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">string</span>:<span style="color:#bbb"> </span><span style="color:#b44">"example-gpu"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">bindsToNode</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">bindingConditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- dra.example.com/is-prepared<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">bindingFailureConditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- dra.example.com/preparing-failed<span style="color:#bbb">
</span></span></span></code></pre></div><p>This example ResourceSlice has the following properties:</p><ul><li>The ResourceSlice targets nodes labeled with <code>accelerator-type=high-performance</code>,
so that the scheduler uses only a specific set of eligible nodes.</li><li>The scheduler selects one node from the selected group (for example, <code>node-3</code>) and sets
the <code>status.allocation.nodeSelector</code> field in the ResourceClaim to that node name.</li><li>The <code>dra.example.com/is-prepared</code> binding condition indicates that the device <code>gpu-1</code>
must be prepared (the <code>is-prepared</code> condition has a status of <code>True</code>) before binding.</li><li>If the <code>gpu-1</code> device preparation fails (the <code>preparing-failed</code> condition has a status of <code>True</code>), the scheduler aborts binding.</li><li>The scheduler waits up to 600 seconds for the device to become ready.</li><li>External controllers can use the node selector in the ResourceClaim to perform
node-specific setup on the selected node.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set Up DRA in a Cluster</a></li><li><a href="/docs/tasks/configure-pod-container/assign-resources/allocate-devices-dra/">Allocate devices to workloads using DRA</a></li><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4381-dra-structured-parameters">Dynamic Resource Allocation with Structured Parameters</a>
KEP.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Volume Snapshot Classes</h1><p>This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity
with <a href="/docs/concepts/storage/volume-snapshots/">volume snapshots</a> and
<a href="/docs/concepts/storage/storage-classes/">storage classes</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>Just like StorageClass provides a way for administrators to describe the "classes"
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the "classes" of storage when provisioning a volume snapshot.</p><h2 id="the-volumesnapshotclass-resource">The VolumeSnapshotClass Resource</h2><p>Each VolumeSnapshotClass contains the fields <code>driver</code>, <code>deletionPolicy</code>, and <code>parameters</code>,
which are used when a VolumeSnapshot belonging to the class needs to be
dynamically provisioned.</p><p>The name of a VolumeSnapshotClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating VolumeSnapshotClass objects, and the objects cannot
be updated once they are created.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Installation of the CRDs is the responsibility of the Kubernetes distribution.
Without the required CRDs present, the creation of a VolumeSnapshotClass fails.</div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span></code></pre></div><p>Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots
that don't request any particular class to bind to by adding the
<code>snapshot.storage.kubernetes.io/is-default-class: "true"</code> annotation:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">snapshot.storage.kubernetes.io/is-default-class</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span></code></pre></div><p>If multiple CSI drivers exist, a default VolumeSnapshotClass can be specified
for each of them.</p><h3 id="volumesnapshotclass-dependencies">VolumeSnapshotClass dependencies</h3><p>When you create a VolumeSnapshot without specifying a VolumeSnapshotClass, Kubernetes
automatically selects a default VolumeSnapshotClass that has a CSI driver matching
the CSI driver of the PVCâ€™s StorageClass.</p><p>This behavior allows multiple default VolumeSnapshotClass objects to coexist in a cluster, as long as
each one is associated with a unique CSI driver.</p><p>Always ensure that there is only one default VolumeSnapshotClass for each CSI driver. If
multiple default VolumeSnapshotClass objects are created using the same CSI driver,
a VolumeSnapshot creation will fail because Kubernetes cannot determine which one to use.</p><h3 id="driver">Driver</h3><p>Volume snapshot classes have a driver that determines what CSI volume plugin is
used for provisioning VolumeSnapshots. This field must be specified.</p><h3 id="deletionpolicy">DeletionPolicy</h3><p>Volume snapshot classes have a <a href="/docs/concepts/storage/volume-snapshots/#delete">deletionPolicy</a>.
It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot
object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can
either be <code>Retain</code> or <code>Delete</code>. This field must be specified.</p><p>If the deletionPolicy is <code>Delete</code>, then the underlying storage snapshot will be
deleted along with the VolumeSnapshotContent object. If the deletionPolicy is <code>Retain</code>,
then both the underlying snapshot and VolumeSnapshotContent remain.</p><h2 id="parameters">Parameters</h2><p>Volume snapshot classes have parameters that describe volume snapshots belonging to
the volume snapshot class. Different parameters may be accepted depending on the
<code>driver</code>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Resource Bin Packing</h1><p>In the <a href="/docs/reference/scheduling/config/#scheduling-plugins">scheduling-plugin</a> <code>NodeResourcesFit</code> of kube-scheduler, there are two
scoring strategies that support the bin packing of resources: <code>MostAllocated</code> and <code>RequestedToCapacityRatio</code>.</p><h2 id="enabling-bin-packing-using-mostallocated-strategy">Enabling bin packing using MostAllocated strategy</h2><p>The <code>MostAllocated</code> strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation.
For each resource type, you can set a weight to modify its influence in the node score.</p><p>To set the <code>MostAllocated</code> strategy for the <code>NodeResourcesFit</code> plugin, use a
<a href="/docs/reference/scheduling/config/">scheduler configuration</a> similar to the following:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">profiles</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">pluginConfig</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">scoringStrategy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>memory<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>intel.com/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>intel.com/bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>MostAllocated<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>NodeResourcesFit<span style="color:#bbb">
</span></span></span></code></pre></div><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs"><code>NodeResourcesFitArgs</code></a>.</p><h2 id="enabling-bin-packing-using-requestedtocapacityratio">Enabling bin packing using RequestedToCapacityRatio</h2><p>The <code>RequestedToCapacityRatio</code> strategy allows the users to specify the resources along with weights for
each resource to score nodes based on the request to capacity ratio. This
allows users to bin pack extended resources by using appropriate parameters
to improve the utilization of scarce resources in large clusters. It favors nodes according to a
configured function of the allocated resources. The behavior of the <code>RequestedToCapacityRatio</code> in
the <code>NodeResourcesFit</code> score function can be controlled by the
<a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-ScoringStrategy">scoringStrategy</a> field.
Within the <code>scoringStrategy</code> field, you can configure two parameters: <code>requestedToCapacityRatio</code> and
<code>resources</code>. The <code>shape</code> in the <code>requestedToCapacityRatio</code>
parameter allows the user to tune the function as least requested or most
requested based on <code>utilization</code> and <code>score</code> values. The <code>resources</code> parameter
comprises both the <code>name</code> of the resource to be considered during scoring and
its corresponding <code>weight</code>, which specifies the weight of each resource.</p><p>Below is an example configuration that sets
the bin packing behavior for extended resources <code>intel.com/foo</code> and <code>intel.com/bar</code>
using the <code>requestedToCapacityRatio</code> field.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">profiles</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">pluginConfig</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">scoringStrategy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>intel.com/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>intel.com/bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">requestedToCapacityRatio</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">shape</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">score</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">score</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>RequestedToCapacityRatio<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>NodeResourcesFit<span style="color:#bbb">
</span></span></span></code></pre></div><p>Referencing the <code>KubeSchedulerConfiguration</code> file with the kube-scheduler
flag <code>--config=/path/to/config/file</code> will pass the configuration to the
scheduler.</p><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs"><code>NodeResourcesFitArgs</code></a>.</p><h3 id="tuning-the-score-function">Tuning the score function</h3><p><code>shape</code> is used to specify the behavior of the <code>RequestedToCapacityRatio</code> function.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">shape</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">score</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">score</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The above arguments give the node a <code>score</code> of 0 if <code>utilization</code> is 0% and 10 for
<code>utilization</code> 100%, thus enabling bin packing behavior. To enable least
requested the score value must be reversed as follows.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">shape</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">score</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">score</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span></code></pre></div><p><code>resources</code> is an optional parameter which defaults to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>memory<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>It can be used to add extended resources as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>intel.com/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>memory<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The <code>weight</code> parameter is optional and is set to 1 if not specified. Also, the
<code>weight</code> cannot be set to a negative value.</p><h3 id="node-scoring-for-capacity-allocation">Node scoring for capacity allocation</h3><p>This section is intended for those who want to understand the internal details
of this feature.
Below is an example of how the node score is calculated for a given set of values.</p><p>Requested resources:</p><pre tabindex="0"><code>intel.com/foo : 2
memory: 256MB
cpu: 2
</code></pre><p>Resource weights:</p><pre tabindex="0"><code>intel.com/foo : 5
memory: 1
cpu: 3
</code></pre><p>FunctionShapePoint {{0, 0}, {100, 10}}</p><p>Node 1 spec:</p><pre tabindex="0"><code>Available:
  intel.com/foo: 4
  memory: 1 GB
  cpu: 8

Used:
  intel.com/foo: 1
  memory: 256MB
  cpu: 1
</code></pre><p>Node score:</p><pre tabindex="0"><code>intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4))
               = (100 - 25)
               = 75                       # requested + used = 75% * available
               = rawScoringFunction(75)
               = 7                        # floor(75/10)

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50                       # requested + used = 50% * available
               = rawScoringFunction(50)
               = 5                        # floor(50/10)

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5                     # requested + used = 37.5% * available
               = rawScoringFunction(37.5)
               = 3                        # floor(37.5/10)

NodeScore   =  ((7 * 5) + (5 * 1) + (3 * 3)) / (5 + 1 + 3)
            =  5
</code></pre><p>Node 2 spec:</p><pre tabindex="0"><code>Available:
  intel.com/foo: 8
  memory: 1GB
  cpu: 8
Used:
  intel.com/foo: 2
  memory: 512MB
  cpu: 6
</code></pre><p>Node score:</p><pre tabindex="0"><code>intel.com/foo  = resourceScoringFunction((2+2),8)
               =  (100 - ((8-4)*100/8)
               =  (100 - 50)
               =  50
               =  rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  ((5 * 5) + (7 * 1) + (10 * 3)) / (5 + 1 + 3)
            =  7
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Read more about the <a href="/docs/concepts/scheduling-eviction/scheduling-framework/">scheduling framework</a></li><li>Read more about <a href="/docs/reference/scheduling/config/">scheduler configuration</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Priority and Preemption</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code></div><p><a href="/docs/concepts/workloads/pods/">Pods</a> can have <em>priority</em>. Priority indicates the
importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the
scheduler tries to preempt (evict) lower priority Pods to make scheduling of the
pending Pod possible.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><p>In a cluster where not all users are trusted, a malicious user could create Pods
at the highest possible priorities, causing other Pods to be evicted/not get
scheduled.
An administrator can use ResourceQuota to prevent users from creating pods at
high priorities.</p><p>See <a href="/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default">limit Priority Class consumption by default</a>
for details.</p></div><h2 id="how-to-use-priority-and-preemption">How to use priority and preemption</h2><p>To use priority and preemption:</p><ol><li><p>Add one or more <a href="#priorityclass">PriorityClasses</a>.</p></li><li><p>Create Pods with<a href="#pod-priority"><code>priorityClassName</code></a> set to one of the added
PriorityClasses. Of course you do not need to create the Pods directly;
normally you would add <code>priorityClassName</code> to the Pod template of a
collection object like a Deployment.</p></li></ol><p>Keep reading for more information about these steps.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes already ships with two PriorityClasses:
<code>system-cluster-critical</code> and <code>system-node-critical</code>.
These are common classes and are used to <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">ensure that critical components are always scheduled first</a>.</div><h2 id="priorityclass">PriorityClass</h2><p>A PriorityClass is a non-namespaced object that defines a mapping from a
priority class name to the integer value of the priority. The name is specified
in the <code>name</code> field of the PriorityClass object's metadata. The value is
specified in the required <code>value</code> field. The higher the value, the higher the
priority.
The name of a PriorityClass object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>,
and it cannot be prefixed with <code>system-</code>.</p><p>A PriorityClass object can have any 32-bit integer value smaller than or equal
to 1 billion. This means that the range of values for a PriorityClass object is
from -2147483648 to 1000000000 inclusive. Larger numbers are reserved for
built-in PriorityClasses that represent critical system Pods. A cluster
admin should create one PriorityClass object for each such mapping that they want.</p><p>PriorityClass also has two optional fields: <code>globalDefault</code> and <code>description</code>.
The <code>globalDefault</code> field indicates that the value of this PriorityClass should
be used for Pods without a <code>priorityClassName</code>. Only one PriorityClass with
<code>globalDefault</code> set to true can exist in the system. If there is no
PriorityClass with <code>globalDefault</code> set, the priority of Pods with no
<code>priorityClassName</code> is zero.</p><p>The <code>description</code> field is an arbitrary string. It is meant to tell users of the
cluster when they should use this PriorityClass.</p><h3 id="notes-about-podpriority-and-existing-clusters">Notes about PodPriority and existing clusters</h3><ul><li><p>If you upgrade an existing cluster without this feature, the priority
of your existing Pods is effectively zero.</p></li><li><p>Addition of a PriorityClass with <code>globalDefault</code> set to <code>true</code> does not
change the priorities of existing Pods. The value of such a PriorityClass is
used only for Pods created after the PriorityClass is added.</p></li><li><p>If you delete a PriorityClass, existing Pods that use the name of the
deleted PriorityClass remain unchanged, but you cannot create more Pods that
use the name of the deleted PriorityClass.</p></li></ul><h3 id="example-priorityclass">Example PriorityClass</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>scheduling.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">1000000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">globalDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">"This priority class should be used for XYZ service pods only."</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="non-preempting-priority-class">Non-preempting PriorityClass</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Pods with <code>preemptionPolicy: Never</code> will be placed in the scheduling queue
ahead of lower-priority pods,
but they cannot preempt other pods.
A non-preempting pod waiting to be scheduled will stay in the scheduling queue,
until sufficient resources are free,
and it can be scheduled.
Non-preempting pods,
like other pods,
are subject to scheduler back-off.
This means that if the scheduler tries these pods and they cannot be scheduled,
they will be retried with lower frequency,
allowing other pods with lower priority to be scheduled before them.</p><p>Non-preempting pods may still be preempted by other,
high-priority pods.</p><p><code>preemptionPolicy</code> defaults to <code>PreemptLowerPriority</code>,
which will allow pods of that PriorityClass to preempt lower-priority pods
(as is existing default behavior).
If <code>preemptionPolicy</code> is set to <code>Never</code>,
pods in that PriorityClass will be non-preempting.</p><p>An example use case is for data science workloads.
A user may submit a job that they want to be prioritized above other workloads,
but do not wish to discard existing work by preempting running pods.
The high priority job with <code>preemptionPolicy: Never</code> will be scheduled
ahead of other queued pods,
as soon as sufficient cluster resources "naturally" become free.</p><h3 id="example-non-preempting-priorityclass">Example Non-preempting PriorityClass</h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>scheduling.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>high-priority-nonpreempting<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#666">1000000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">preemptionPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">globalDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">false</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">"This priority class will not cause other pods to be preempted."</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="pod-priority">Pod priority</h2><p>After you have one or more PriorityClasses, you can create Pods that specify one
of those PriorityClass names in their specifications. The priority admission
controller uses the <code>priorityClassName</code> field and populates the integer value of
the priority. If the priority class is not found, the Pod is rejected.</p><p>The following YAML is an example of a Pod configuration that uses the
PriorityClass created in the preceding example. The priority admission
controller checks the specification and resolves the priority of the Pod to
1000000.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">priorityClassName</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="effect-of-pod-priority-on-scheduling-order">Effect of Pod priority on scheduling order</h3><p>When Pod priority is enabled, the scheduler orders pending Pods by
their priority and a pending Pod is placed ahead of other pending Pods
with lower priority in the scheduling queue. As a result, the higher
priority Pod may be scheduled sooner than Pods with lower priority if
its scheduling requirements are met. If such Pod cannot be scheduled, the
scheduler will continue and try to schedule other lower priority Pods.</p><h2 id="preemption">Preemption</h2><p>When Pods are created, they go to a queue and wait to be scheduled. The
scheduler picks a Pod from the queue and tries to schedule it on a Node. If no
Node is found that satisfies all the specified requirements of the Pod,
preemption logic is triggered for the pending Pod. Let's call the pending Pod P.
Preemption logic tries to find a Node where removal of one or more Pods with
lower priority than P would enable P to be scheduled on that Node. If such a
Node is found, one or more lower priority Pods get evicted from the Node. After
the Pods are gone, P can be scheduled on the Node.</p><h3 id="user-exposed-information">User exposed information</h3><p>When Pod P preempts one or more Pods on Node N, <code>nominatedNodeName</code> field of Pod
P's status is set to the name of Node N. This field helps the scheduler track
resources reserved for Pod P and also gives users information about preemptions
in their clusters.</p><p>Please note that Pod P is not necessarily scheduled to the "nominated Node".
The scheduler always tries the "nominated Node" before iterating over any other nodes.
After victim Pods are preempted, they get their graceful termination period. If
another node becomes available while scheduler is waiting for the victim Pods to
terminate, scheduler may use the other node to schedule Pod P. As a result
<code>nominatedNodeName</code> and <code>nodeName</code> of Pod spec are not always the same. Also, if
the scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P
arrives, the scheduler may give Node N to the new higher priority Pod. In such a
case, scheduler clears <code>nominatedNodeName</code> of Pod P. By doing this, scheduler
makes Pod P eligible to preempt Pods on another Node.</p><h3 id="limitations-of-preemption">Limitations of preemption</h3><h4 id="graceful-termination-of-preemption-victims">Graceful termination of preemption victims</h4><p>When Pods are preempted, the victims get their
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">graceful termination period</a>.
They have that much time to finish their work and exit. If they don't, they are
killed. This graceful termination period creates a time gap between the point
that the scheduler preempts Pods and the time when the pending Pod (P) can be
scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other
pending Pods. As victims exit or get terminated, the scheduler tries to schedule
Pods in the pending queue. Therefore, there is usually a time gap between the
point that scheduler preempts victims and the time that Pod P is scheduled. In
order to minimize this gap, one can set graceful termination period of lower
priority Pods to zero or a small number.</p><h4 id="poddisruptionbudget-is-supported-but-not-guaranteed">PodDisruptionBudget is supported, but not guaranteed</h4><p>A <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> (PDB)
allows application owners to limit the number of Pods of a replicated application
that are down simultaneously from voluntary disruptions. Kubernetes supports
PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries
to find victims whose PDB are not violated by preemption, but if no such victims
are found, preemption will still happen, and lower priority Pods will be removed
despite their PDBs being violated.</p><h4 id="inter-pod-affinity-on-lower-priority-pods">Inter-Pod affinity on lower-priority Pods</h4><p>A Node is considered for preemption only when the answer to this question is
yes: "If all the Pods with lower priority than the pending Pod are removed from
the Node, can the pending Pod be scheduled on the Node?"</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Preemption does not necessarily remove all lower-priority
Pods. If the pending Pod can be scheduled by removing fewer than all
lower-priority Pods, then only a portion of the lower-priority Pods are removed.
Even so, the answer to the preceding question must be yes. If the answer is no,
the Node is not considered for preemption.</div><p>If a pending Pod has inter-pod <a class="glossary-tooltip" title="Rules used by the scheduler to determine where to place pods" data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank" aria-label="affinity">affinity</a>
to one or more of the lower-priority Pods on the Node, the inter-Pod affinity
rule cannot be satisfied in the absence of those lower-priority Pods. In this case,
the scheduler does not preempt any Pods on the Node. Instead, it looks for another
Node. The scheduler might find a suitable Node or it might not. There is no
guarantee that the pending Pod can be scheduled.</p><p>Our recommended solution for this problem is to create inter-Pod affinity only
towards equal or higher priority Pods.</p><h4 id="cross-node-preemption">Cross node preemption</h4><p>Suppose a Node N is being considered for preemption so that a pending Pod P can
be scheduled on N. P might become feasible on N only if a Pod on another Node is
preempted. Here's an example:</p><ul><li>Pod P is being considered for Node N.</li><li>Pod Q is running on another Node in the same Zone as Node N.</li><li>Pod P has Zone-wide anti-affinity with Pod Q (<code>topologyKey: topology.kubernetes.io/zone</code>).</li><li>There are no other cases of anti-affinity between Pod P and other Pods in
the Zone.</li><li>In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler
does not perform cross-node preemption. So, Pod P will be deemed
unschedulable on Node N.</li></ul><p>If Pod Q were removed from its Node, the Pod anti-affinity violation would be
gone, and Pod P could possibly be scheduled on Node N.</p><p>We may consider adding cross Node preemption in future versions if there is
enough demand and if we find an algorithm with reasonable performance.</p><h2 id="troubleshooting">Troubleshooting</h2><p>Pod priority and preemption can have unwanted side effects. Here are some
examples of potential problems and ways to deal with them.</p><h3 id="pods-are-preempted-unnecessarily">Pods are preempted unnecessarily</h3><p>Preemption removes existing Pods from a cluster under resource pressure to make
room for higher priority pending Pods. If you give high priorities to
certain Pods by mistake, these unintentionally high priority Pods may cause
preemption in your cluster. Pod priority is specified by setting the
<code>priorityClassName</code> field in the Pod's specification. The integer value for
priority is then resolved and populated to the <code>priority</code> field of <code>podSpec</code>.</p><p>To address the problem, you can change the <code>priorityClassName</code> for those Pods
to use lower priority classes, or leave that field empty. An empty
<code>priorityClassName</code> is resolved to zero by default.</p><p>When a Pod is preempted, there will be events recorded for the preempted Pod.
Preemption should happen only when a cluster does not have enough resources for
a Pod. In such cases, preemption happens only when the priority of the pending
Pod (preemptor) is higher than the victim Pods. Preemption must not happen when
there is no pending Pod, or when the pending Pods have equal or lower priority
than the victims. If preemption happens in such scenarios, please file an issue.</p><h3 id="pods-are-preempted-but-the-preemptor-is-not-scheduled">Pods are preempted, but the preemptor is not scheduled</h3><p>When pods are preempted, they receive their requested graceful termination
period, which is by default 30 seconds. If the victim Pods do not terminate within
this period, they are forcibly terminated. Once all the victims go away, the
preemptor Pod can be scheduled.</p><p>While the preemptor Pod is waiting for the victims to go away, a higher priority
Pod may be created that fits on the same Node. In this case, the scheduler will
schedule the higher priority Pod instead of the preemptor.</p><p>This is expected behavior: the Pod with the higher priority should take the place
of a Pod with a lower priority.</p><h3 id="higher-priority-pods-are-preempted-before-lower-priority-pods">Higher priority Pods are preempted before lower priority pods</h3><p>The scheduler tries to find nodes that can run a pending Pod. If no node is
found, the scheduler tries to remove Pods with lower priority from an arbitrary
node in order to make room for the pending pod.
If a node with low priority Pods is not feasible to run the pending Pod, the scheduler
may choose another node with higher priority Pods (compared to the Pods on the
other node) for preemption. The victims must still have lower priority than the
preemptor Pod.</p><p>When there are multiple nodes available for preemption, the scheduler tries to
choose the node with a set of Pods with lowest priority. However, if such Pods
have PodDisruptionBudget that would be violated if they are preempted then the
scheduler may choose another node with higher priority Pods.</p><p>When multiple nodes exist for preemption and none of the above scenarios apply,
the scheduler chooses a node with the lowest priority.</p><h2 id="interactions-of-pod-priority-and-qos">Interactions between Pod priority and quality of service</h2><p>Pod priority and <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/pod-qos/" target="_blank" aria-label="QoS class">QoS class</a>
are two orthogonal features with few interactions and no default restrictions on
setting the priority of a Pod based on its QoS classes. The scheduler's
preemption logic does not consider QoS when choosing preemption targets.
Preemption considers Pod priority and attempts to choose a set of targets with
the lowest priority. Higher-priority Pods are considered for preemption only if
the removal of the lowest priority Pods is not sufficient to allow the scheduler
to schedule the preemptor Pod, or if the lowest priority Pods are protected by
<code>PodDisruptionBudget</code>.</p><p>The kubelet uses Priority to determine pod order for <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">node-pressure eviction</a>.
You can use the QoS class to estimate the order in which pods are most likely
to get evicted. The kubelet ranks pods for eviction based on the following factors:</p><ol><li>Whether the starved resource usage exceeds requests</li><li>Pod Priority</li><li>Amount of resource usage relative to requests</li></ol><p>See <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction">Pod selection for kubelet eviction</a>
for more details.</p><p>kubelet node-pressure eviction does not evict Pods when their
usage does not exceed their requests. If a Pod with lower priority is not
exceeding its requests, it won't be evicted. Another Pod with higher priority
that exceeds its requests may be evicted.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about using ResourceQuotas in connection with PriorityClasses: <a href="/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default">limit Priority Class consumption by default</a></li><li>Learn about <a href="/docs/concepts/workloads/pods/disruptions/">Pod Disruption</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated Eviction</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Volume Health Monitoring</h1><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [alpha]</code></div><p><a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/#csi" target="_blank" aria-label="CSI">CSI</a> volume health monitoring allows
CSI Drivers to detect abnormal volume conditions from the underlying storage systems
and report them as events on <a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" aria-label="PVCs">PVCs</a>
or <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>.</p><h2 id="volume-health-monitoring">Volume health monitoring</h2><p>Kubernetes <em>volume health monitoring</em> is part of how Kubernetes implements the
Container Storage Interface (CSI). Volume health monitoring feature is implemented
in two components: an External Health Monitor controller, and the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a>.</p><p>If a CSI Driver supports Volume Health Monitoring feature from the controller side,
an event will be reported on the related
<a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" aria-label="PersistentVolumeClaim">PersistentVolumeClaim</a> (PVC)
when an abnormal volume condition is detected on a CSI volume.</p><p>The External Health Monitor <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>
also watches for node failure events. You can enable node failure monitoring by setting
the <code>enable-node-watcher</code> flag to true. When the external health monitor detects a node
failure event, the controller reports an Event will be reported on the PVC to indicate
that pods using this PVC are on a failed node.</p><p>If a CSI Driver supports Volume Health Monitoring feature from the node side,
an Event will be reported on every Pod using the PVC when an abnormal volume
condition is detected on a CSI volume. In addition, Volume Health information
is exposed as Kubelet VolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal
is added. This metric includes two labels: <code>namespace</code> and <code>persistentvolumeclaim</code>.
The count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume
is healthy. For more information, please check
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor#kubelet-metrics-changes">KEP</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You need to enable the <code>CSIVolumeHealth</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
to use this feature from the node side.</div><h2 id="what-s-next">What's next</h2><p>See the <a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI driver documentation</a>
to find out which CSI drivers have implemented this feature.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Role Based Access Control Good Practices</h1><div class="lead">Principles and practices for good RBAC design for cluster operators.</div><p>Kubernetes <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/reference/access-authn-authz/rbac/" target="_blank" aria-label="RBAC">RBAC</a> is a key security control
to ensure that cluster users and workloads have only the access to resources required to
execute their roles. It is important to ensure that, when designing permissions for cluster
users, the cluster administrator understands the areas where privilege escalation could occur,
to reduce the risk of excessive access leading to security incidents.</p><p>The good practices laid out here should be read in conjunction with the general
<a href="/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update">RBAC documentation</a>.</p><h2 id="general-good-practice">General good practice</h2><h3 id="least-privilege">Least privilege</h3><p>Ideally, minimal RBAC rights should be assigned to users and service accounts. Only permissions
explicitly required for their operation should be used. While each cluster will be different,
some general rules that can be applied are :</p><ul><li>Assign permissions at the namespace level where possible. Use RoleBindings as opposed to
ClusterRoleBindings to give users rights only within a specific namespace.</li><li>Avoid providing wildcard permissions when possible, especially to all resources.
As Kubernetes is an extensible system, providing wildcard access gives rights
not just to all object types that currently exist in the cluster, but also to all object types
which are created in the future.</li><li>Administrators should not use <code>cluster-admin</code> accounts except where specifically needed.
Providing a low privileged account with
<a href="/docs/reference/access-authn-authz/authentication/#user-impersonation">impersonation rights</a>
can avoid accidental modification of cluster resources.</li><li>Avoid adding users to the <code>system:masters</code> group. Any user who is a member of this group
bypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot be
revoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster is
using an authorization webhook, membership of this group also bypasses that webhook (requests
from users who are members of that group are never sent to the webhook)</li></ul><h3 id="minimize-distribution-of-privileged-tokens">Minimize distribution of privileged tokens</h3><p>Ideally, pods shouldn't be assigned service accounts that have been granted powerful permissions
(for example, any of the rights listed under <a href="#privilege-escalation-risks">privilege escalation risks</a>).
In cases where a workload requires powerful permissions, consider the following practices:</p><ul><li>Limit the number of nodes running powerful pods. Ensure that any DaemonSets you run
are necessary and are run with least privilege to limit the blast radius of container escapes.</li><li>Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Toleration</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">NodeAffinity</a>, or
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">PodAntiAffinity</a>
to ensure pods don't run alongside untrusted or less-trusted Pods. Pay special attention to
situations where less-trustworthy Pods are not meeting the <strong>Restricted</strong> Pod Security Standard.</li></ul><h3 id="hardening">Hardening</h3><p>Kubernetes defaults to providing access which may not be required in every cluster. Reviewing
the RBAC rights provided by default can provide opportunities for security hardening.
In general, changes should not be made to rights provided to <code>system:</code> accounts some options
to harden cluster rights exist:</p><ul><li>Review bindings for the <code>system:unauthenticated</code> group and remove them where possible, as this gives
access to anyone who can contact the API server at a network level.</li><li>Avoid the default auto-mounting of service account tokens by setting
<code>automountServiceAccountToken: false</code>. For more details, see
<a href="/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server">using default service account token</a>.
Setting this value for a Pod will overwrite the service account setting, workloads
which require service account tokens can still mount them.</li></ul><h3 id="periodic-review">Periodic review</h3><p>It is vital to periodically review the Kubernetes RBAC settings for redundant entries and
possible privilege escalations.
If an attacker is able to create a user account with the same name as a deleted user,
they can automatically inherit all the rights of the deleted user, especially the
rights assigned to that user.</p><h2 id="privilege-escalation-risks">Kubernetes RBAC - privilege escalation risks</h2><p>Within Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account
to escalate their privileges in the cluster or affect systems outside the cluster.</p><p>This section is intended to provide visibility of the areas where cluster operators
should take care, to ensure that they do not inadvertently allow for more access to clusters than intended.</p><h3 id="listing-secrets">Listing secrets</h3><p>It is generally clear that allowing <code>get</code> access on Secrets will allow a user to read their contents.
It is also important to note that <code>list</code> and <code>watch</code> access also effectively allow for users to reveal the Secret contents.
For example, when a List response is returned (for example, via <code>kubectl get secrets -A -o yaml</code>), the response
includes the contents of all Secrets.</p><h3 id="workload-creation">Workload creation</h3><p>Permission to create workloads (either Pods, or
<a href="/docs/concepts/workloads/controllers/">workload resources</a> that manage Pods) in a namespace
implicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, and
PersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as any
<a href="/docs/reference/access-authn-authz/service-accounts-admin/">ServiceAccount</a>, granting permission
to create workloads also implicitly grants the API access levels of any service account in that
namespace.</p><p>Users who can run privileged Pods can use that access to gain node access and potentially to
further elevate their privileges. Where you do not fully trust a user or other principal
with the ability to create suitably secure and isolated Pods, you should enforce either the
<strong>Baseline</strong> or <strong>Restricted</strong> Pod Security Standard.
You can use <a href="/docs/concepts/security/pod-security-admission/">Pod Security admission</a>
or other (third party) mechanisms to implement that enforcement.</p><p>For these reasons, namespaces should be used to separate resources requiring different levels of
trust or tenancy. It is still considered best practice to follow <a href="#least-privilege">least privilege</a>
principles and assign the minimum set of permissions, but boundaries within a namespace should be
considered weak.</p><h3 id="persistent-volume-creation">Persistent volume creation</h3><p>If someone - or some application - is allowed to create arbitrary PersistentVolumes, that access
includes the creation of <code>hostPath</code> volumes, which then means that a Pod would get access
to the underlying host filesystem(s) on the associated node. Granting that ability is a security risk.</p><p>There are many ways a container with unrestricted access to the host filesystem can escalate privileges, including
reading data from other containers, and abusing the credentials of system services, such as Kubelet.</p><p>You should only allow access to create PersistentVolume objects for:</p><ul><li>Users (cluster operators) that need this access for their work, and who you trust.</li><li>The Kubernetes control plane components which creates PersistentVolumes based on PersistentVolumeClaims
that are configured for automatic provisioning.
This is usually setup by the Kubernetes provider or by the operator when installing a CSI driver.</li></ul><p>Where access to persistent storage is required trusted administrators should create
PersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.</p><h3 id="access-to-proxy-subresource-of-nodes">Access to <code>proxy</code> subresource of Nodes</h3><p>Users with access to the proxy sub-resource of node objects have rights to the Kubelet API,
which allows for command execution on every pod on the node(s) to which they have rights.
This access bypasses audit logging and admission control, so care should be taken before
granting rights to this resource.</p><h3 id="escalate-verb">Escalate verb</h3><p>Generally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses.
The exception to this is the <code>escalate</code> verb. As noted in the <a href="/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update">RBAC documentation</a>,
users with this right can effectively escalate their privileges.</p><h3 id="bind-verb">Bind verb</h3><p>Similar to the <code>escalate</code> verb, granting users this right allows for the bypass of Kubernetes
in-built protections against privilege escalation, allowing users to create bindings to
roles with rights they do not already have.</p><h3 id="impersonate-verb">Impersonate verb</h3><p>This verb allows users to impersonate and gain the rights of other users in the cluster.
Care should be taken when granting it, to ensure that excessive permissions cannot be gained
via one of the impersonated accounts.</p><h3 id="csrs-and-certificate-issuing">CSRs and certificate issuing</h3><p>The CSR API allows for users with <code>create</code> rights to CSRs and <code>update</code> rights on <code>certificatesigningrequests/approval</code>
where the signer is <code>kubernetes.io/kube-apiserver-client</code> to create new client certificates
which allow users to authenticate to the cluster. Those client certificates can have arbitrary
names including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.</p><h3 id="token-request">Token request</h3><p>Users with <code>create</code> rights on <code>serviceaccounts/token</code> can create TokenRequests to issue
tokens for existing service accounts.</p><h3 id="control-admission-webhooks">Control admission webhooks</h3><p>Users with control over <code>validatingwebhookconfigurations</code> or <code>mutatingwebhookconfigurations</code>
can control webhooks that can read any object admitted to the cluster, and in the case of
mutating webhooks, also mutate admitted objects.</p><h3 id="namespace-modification">Namespace modification</h3><p>Users who can perform <strong>patch</strong> operations on Namespace objects (through a namespaced RoleBinding to a Role with that access) can modify
labels on that namespace. In clusters where Pod Security Admission is used, this may allow a user to configure the namespace
for a more permissive policy than intended by the administrators.
For clusters where NetworkPolicy is used, users may be set labels that indirectly allow
access to services that an administrator did not intend to allow.</p><h2 id="denial-of-service-risks">Kubernetes RBAC - denial of service risks</h2><h3 id="object-creation-dos">Object creation denial-of-service</h3><p>Users who have rights to create objects in a cluster may be able to create sufficient large
objects to create a denial of service condition either based on the size or number of objects, as discussed in
<a href="https://github.com/kubernetes/kubernetes/issues/107325">etcd used by Kubernetes is vulnerable to OOM attack</a>. This may be
specifically relevant in multi-tenant clusters if semi-trusted or untrusted users
are allowed limited access to a system.</p><p>One option for mitigation of this issue would be to use
<a href="/docs/concepts/policy/resource-quotas/#object-count-quota">resource quotas</a>
to limit the quantity of objects which can be created.</p><h2 id="what-s-next">What's next</h2><ul><li>To learn more about RBAC, see the <a href="/docs/reference/access-authn-authz/rbac/">RBAC documentation</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Good practices for Kubernetes Secrets</h1><div class="lead">Principles and practices for good Secret management for cluster administrators and application developers.</div><p><p>In Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys.</p></p><p>Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
<a href="/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted">encrypted at rest</a>.</p><p>A <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a> can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a> are
designed for non-confidential data.</p><p>The following good practices are intended for both cluster administrators and
application developers. Use these guidelines to improve the security of your
sensitive information in Secret objects, as well as to more effectively manage
your Secrets.</p><h2 id="cluster-administrators">Cluster administrators</h2><p>This section provides good practices that cluster administrators can use to
improve the security of confidential information in the cluster.</p><h3 id="configure-encryption-at-rest">Configure encryption at rest</h3><p>By default, Secret objects are stored unencrypted in <a class="glossary-tooltip" title="Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data." data-toggle="tooltip" data-placement="top" href="/docs/tasks/administer-cluster/configure-upgrade-etcd/" target="_blank" aria-label="etcd">etcd</a>. You should configure encryption of your Secret
data in <code>etcd</code>. For instructions, refer to
<a href="/docs/tasks/administer-cluster/encrypt-data/">Encrypt Secret Data at Rest</a>.</p><h3 id="least-privilege-secrets">Configure least-privilege access to Secrets</h3><p>When planning your access control mechanism, such as Kubernetes
<a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/reference/access-authn-authz/rbac/" target="_blank" aria-label="Role-based Access Control">Role-based Access Control</a> <a href="/docs/reference/access-authn-authz/rbac/">(RBAC)</a>,
consider the following guidelines for access to <code>Secret</code> objects. You should
also follow the other guidelines in
<a href="/docs/concepts/security/rbac-good-practices/">RBAC good practices</a>.</p><ul><li><strong>Components</strong>: Restrict <code>watch</code> or <code>list</code> access to only the most
privileged, system-level components. Only grant <code>get</code> access for Secrets if
the component's normal behavior requires it.</li><li><strong>Humans</strong>: Restrict <code>get</code>, <code>watch</code>, or <code>list</code> access to Secrets. Only allow
cluster administrators to access <code>etcd</code>. This includes read-only access. For
more complex access control, such as restricting access to Secrets with
specific annotations, consider using third-party authorization mechanisms.</li></ul><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Granting <code>list</code> access to Secrets implicitly lets the subject fetch the
contents of the Secrets.</div><p>A user who can create a Pod that uses a Secret can also see the value of that
Secret. Even if cluster policies do not allow a user to read the Secret
directly, the same user could have access to run a Pod that then exposes the
Secret. You can detect or limit the impact caused by Secret data being exposed,
either intentionally or unintentionally, by a user with this access. Some
recommendations include:</p><ul><li>Use short-lived Secrets</li><li>Implement audit rules that alert on specific events, such as concurrent
reading of multiple Secrets by a single user</li></ul><h4 id="restrict-access-for-secrets">Restrict Access for Secrets</h4><p>Use separate namespaces to isolate access to mounted secrets.</p><h3 id="improve-etcd-management-policies">Improve etcd management policies</h3><p>Consider wiping or shredding the durable storage used by <code>etcd</code> once it is
no longer in use.</p><p>If there are multiple <code>etcd</code> instances, configure encrypted SSL/TLS
communication between the instances to protect the Secret data in transit.</p><h3 id="configure-access-to-external-secrets">Configure access to external Secrets</h3><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>You can use third-party Secrets store providers to keep your confidential data
outside your cluster and then configure Pods to access that information.
The <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Kubernetes Secrets Store CSI Driver</a>
is a DaemonSet that lets the kubelet retrieve Secrets from external stores, and
mount the Secrets as a volume into specific Pods that you authorize to access
the data.</p><p>For a list of supported providers, refer to
<a href="https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver">Providers for the Secret Store CSI Driver</a>.</p><h2 id="good-practices-for-using-swap-memory">Good practices for using swap memory</h2><p>For best practices for setting swap memory for Linux nodes, please refer to
<a href="/docs/concepts/cluster-administration/swap-memory-management/#good-practice-for-using-swap-in-a-kubernetes-cluster">swap memory management</a>.</p><h2 id="developers">Developers</h2><p>This section provides good practices for developers to use to improve the
security of confidential data when building and deploying Kubernetes resources.</p><h3 id="restrict-secret-access-to-specific-containers">Restrict Secret access to specific containers</h3><p>If you are defining multiple containers in a Pod, and only one of those
containers needs access to a Secret, define the volume mount or environment
variable configuration so that the other containers do not have access to that
Secret.</p><h3 id="protect-secret-data-after-reading">Protect Secret data after reading</h3><p>Applications still need to protect the value of confidential information after
reading it from an environment variable or volume. For example, your
application must avoid logging the secret data in the clear or transmitting it
to an untrusted party.</p><h3 id="avoid-sharing-secret-manifests">Avoid sharing Secret manifests</h3><p>If you configure a Secret through a
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-manifest" target="_blank" aria-label="manifest">manifest</a>, with the secret
data encoded as base64, sharing this file or checking it in to a source
repository means the secret is available to everyone who can read the manifest.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Base64 encoding is <em>not</em> an encryption method, it provides no additional
confidentiality over plain text.</div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Ingress Controllers</h1><div class="lead">In order for an <a href="/docs/concepts/services-networking/ingress/">Ingress</a> to work in your cluster, there must be an <em>ingress controller</em> running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.</div><p>In order for the Ingress resource to work, the cluster must have an ingress controller running.</p><p>Unlike other types of controllers which run as part of the <code>kube-controller-manager</code> binary, Ingress controllers
are not started automatically with a cluster. Use this page to choose the ingress controller implementation
that best fits your cluster.</p><p>Kubernetes as a project supports and maintains <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme">AWS</a>, <a href="https://git.k8s.io/ingress-gce/README.md#readme">GCE</a>, and
<a href="https://git.k8s.io/ingress-nginx/README.md#readme">nginx</a> ingress controllers.</p><h2 id="additional-controllers">Additional controllers</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><ul><li><a href="https://docs.microsoft.com/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&amp;bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json">AKS Application Gateway Ingress Controller</a> is an ingress controller that configures the <a href="https://docs.microsoft.com/azure/application-gateway/overview">Azure Application Gateway</a>.</li><li><a href="https://www.alibabacloud.com/help/en/mse/user-guide/overview-of-mse-ingress-gateways">Alibaba Cloud MSE Ingress</a> is an ingress controller that configures the <a href="https://www.alibabacloud.com/help/en/mse/product-overview/cloud-native-gateway-overview?spm=a2c63.p38356.0.0.20563003HJK9is">Alibaba Cloud Native Gateway</a>, which is also the commercial version of <a href="https://github.com/alibaba/higress">Higress</a>.</li><li><a href="https://github.com/apache/apisix-ingress-controller">Apache APISIX ingress controller</a> is an <a href="https://github.com/apache/apisix">Apache APISIX</a>-based ingress controller.</li><li><a href="https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes">Avi Kubernetes Operator</a> provides L4-L7 load-balancing using <a href="https://avinetworks.com/">VMware NSX Advanced Load Balancer</a>.</li><li><a href="https://github.com/bfenetworks/ingress-bfe">BFE Ingress Controller</a> is a <a href="https://www.bfe-networks.net">BFE</a>-based ingress controller.</li><li><a href="https://docs.cilium.io/en/stable/network/servicemesh/ingress/">Cilium Ingress Controller</a> is an ingress controller powered by <a href="https://cilium.io/">Cilium</a>.</li><li>The <a href="https://github.com/citrix/citrix-k8s-ingress-controller#readme">Citrix ingress controller</a> works with
Citrix Application Delivery Controller.</li><li><a href="https://projectcontour.io/">Contour</a> is an <a href="https://www.envoyproxy.io/">Envoy</a> based ingress controller.</li><li><a href="https://www.getambassador.io/products/api-gateway">Emissary-Ingress</a> API Gateway is an <a href="https://www.envoyproxy.io">Envoy</a>-based ingress
controller.</li><li><a href="https://getenroute.io/">EnRoute</a> is an <a href="https://www.envoyproxy.io">Envoy</a> based API gateway that can run as an ingress controller.</li><li><a href="https://megaease.com/docs/easegress/04.cloud-native/4.1.kubernetes-ingress-controller/">Easegress IngressController</a> is an <a href="https://megaease.com/easegress/">Easegress</a> based API gateway that can run as an ingress controller.</li><li>F5 BIG-IP <a href="https://clouddocs.f5.com/containers/latest/userguide/kubernetes/">Container Ingress Services for Kubernetes</a>
lets you use an Ingress to configure F5 BIG-IP virtual servers.</li><li><a href="https://docs.fortinet.com/document/fortiadc/7.0.0/fortiadc-ingress-controller/742835/fortiadc-ingress-controller-overview">FortiADC Ingress Controller</a> support the Kubernetes Ingress resources and allows you to manage FortiADC objects from Kubernetes</li><li><a href="https://gloo.solo.io">Gloo</a> is an open-source ingress controller based on <a href="https://www.envoyproxy.io">Envoy</a>,
which offers API gateway functionality.</li><li><a href="https://haproxy-ingress.github.io/">HAProxy Ingress</a> is an ingress controller for
<a href="https://www.haproxy.org/#desc">HAProxy</a>.</li><li><a href="https://github.com/alibaba/higress">Higress</a> is an <a href="https://www.envoyproxy.io">Envoy</a> based API gateway that can run as an ingress controller.</li><li>The <a href="https://github.com/haproxytech/kubernetes-ingress#readme">HAProxy Ingress Controller for Kubernetes</a>
is also an ingress controller for <a href="https://www.haproxy.org/#desc">HAProxy</a>.</li><li><a href="https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/">Istio Ingress</a>
is an <a href="https://istio.io/">Istio</a> based ingress controller.</li><li>The <a href="https://github.com/Kong/kubernetes-ingress-controller#readme">Kong Ingress Controller for Kubernetes</a>
is an ingress controller driving <a href="https://konghq.com/kong/">Kong Gateway</a>.</li><li><a href="https://kusk.kubeshop.io/">Kusk Gateway</a> is an OpenAPI-driven ingress controller based on <a href="https://www.envoyproxy.io">Envoy</a>.</li><li>The <a href="https://www.nginx.com/products/nginx-ingress-controller/">NGINX Ingress Controller for Kubernetes</a>
works with the <a href="https://www.nginx.com/resources/glossary/nginx/">NGINX</a> webserver (as a proxy).</li><li>The <a href="https://github.com/ngrok/kubernetes-ingress-controller">ngrok Kubernetes Ingress Controller</a> is an open source controller for adding secure public access to your K8s services using the <a href="https://ngrok.com">ngrok platform</a>.</li><li>The <a href="https://github.com/oracle/oci-native-ingress-controller#readme">OCI Native Ingress Controller</a> is an Ingress controller for Oracle Cloud Infrastructure which allows you to manage the <a href="https://docs.oracle.com/en-us/iaas/Content/Balance/home.htm">OCI Load Balancer</a>.</li><li><a href="https://gitee.com/njet-rd/open-njet-kic">OpenNJet Ingress Controller</a> is a <a href="https://njet.org.cn/">OpenNJet</a>-based ingress controller.</li><li>The <a href="https://www.pomerium.com/docs/k8s/ingress.html">Pomerium Ingress Controller</a> is based on <a href="https://pomerium.com/">Pomerium</a>, which offers context-aware access policy.</li><li><a href="https://opensource.zalando.com/skipper/kubernetes/ingress-controller/">Skipper</a> HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.</li><li>The <a href="https://doc.traefik.io/traefik/providers/kubernetes-ingress/">Traefik Kubernetes Ingress provider</a> is an
ingress controller for the <a href="https://traefik.io/traefik/">Traefik</a> proxy.</li><li><a href="https://github.com/TykTechnologies/tyk-operator">Tyk Operator</a> extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway &amp; Tyk Cloud control plane.</li><li><a href="https://voyagermesh.com">Voyager</a> is an ingress controller for
<a href="https://www.haproxy.org/#desc">HAProxy</a>.</li><li><a href="https://www.wallarm.com/solutions/waf-for-kubernetes">Wallarm Ingress Controller</a> is an Ingress Controller that provides WAAP (WAF) and API Security capabilities.</li></ul><h2 id="using-multiple-ingress-controllers">Using multiple Ingress controllers</h2><p>You may deploy any number of ingress controllers using <a href="/docs/concepts/services-networking/ingress/#ingress-class">ingress class</a>
within a cluster. Note the <code>.metadata.name</code> of your ingress class resource. When you create an ingress you would need that name to specify the <code>ingressClassName</code> field on your Ingress object (refer to <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec">IngressSpec v1 reference</a>). <code>ingressClassName</code> is a replacement of the older <a href="/docs/concepts/services-networking/ingress/#deprecated-annotation">annotation method</a>.</p><p>If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes <a href="/docs/concepts/services-networking/ingress/#default-ingress-class">applies</a> the cluster's default IngressClass to the Ingress.
You mark an IngressClass as default by setting the <a href="/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class"><code>ingressclass.kubernetes.io/is-default-class</code> annotation</a> on that IngressClass, with the string value <code>"true"</code>.</p><p>Ideally, all ingress controllers should fulfill this specification, but the various ingress
controllers operate slightly differently.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Make sure you review your ingress controller's documentation to understand the caveats of choosing it.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/services-networking/ingress/">Ingress</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Service</h1><div class="lead">Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.</div><p>In Kubernetes, a Service is a method for exposing a network application that is running as one or more
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> in your cluster.</p><p>A key aim of Services in Kubernetes is that you don't need to modify your existing
application to use an unfamiliar service discovery mechanism.
You can run code in Pods, whether this is a code designed for a cloud-native world, or
an older app you've containerized. You use a Service to make that set of Pods available
on the network so that clients can interact with it.</p><p>If you use a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a> to run your app,
that Deployment can create and destroy Pods dynamically. From one moment to the next,
you don't know how many of those Pods are working and healthy; you might not even know
what those healthy Pods are named.
Kubernetes <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> are created and destroyed
to match the desired state of your cluster. Pods are ephemeral resources (you should not
expect that an individual Pod is reliable and durable).</p><p>Each Pod gets its own IP address (Kubernetes expects network plugins to ensure this).
For a given Deployment in your cluster, the set of Pods running in one moment in
time could be different from the set of Pods running that application a moment later.</p><p>This leads to a problem: if some set of Pods (call them "backends") provides
functionality to other Pods (call them "frontends") inside your cluster,
how do the frontends find out and keep track of which IP address to connect
to, so that the frontend can use the backend part of the workload?</p><p>Enter <em>Services</em>.</p><h2 id="services-in-kubernetes">Services in Kubernetes</h2><p>The Service API, part of Kubernetes, is an abstraction to help you expose groups of
Pods over a network. Each Service object defines a logical set of endpoints (usually
these endpoints are Pods) along with a policy about how to make those pods accessible.</p><p>For example, consider a stateless image-processing backend which is running with
3 replicas. Those replicas are fungibleâ€”frontends do not care which backend
they use. While the actual Pods that compose the backend set may change, the
frontend clients should not need to be aware of that, nor should they need to keep
track of the set of backends themselves.</p><p>The Service abstraction enables this decoupling.</p><p>The set of Pods targeted by a Service is usually determined
by a <a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels/" target="_blank" aria-label="selector">selector</a> that you
define.
To learn about other ways to define Service endpoints,
see <a href="#services-without-selectors">Services <em>without</em> selectors</a>.</p><p>If your workload speaks HTTP, you might choose to use an
<a href="/docs/concepts/services-networking/ingress/">Ingress</a> to control how web traffic
reaches that workload.
Ingress is not a Service type, but it acts as the entry point for your
cluster. An Ingress lets you consolidate your routing rules into a single resource, so
that you can expose multiple components of your workload, running separately in your
cluster, behind a single listener.</p><p>The <a href="https://gateway-api.sigs.k8s.io/#what-is-the-gateway-api">Gateway</a> API for Kubernetes
provides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster -
it is a family of extension APIs, implemented using
<a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." data-toggle="tooltip" data-placement="top" href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank" aria-label="CustomResourceDefinitions">CustomResourceDefinitions</a> -
and then use these to configure access to network services that are running in your cluster.</p><h3 id="cloud-native-service-discovery">Cloud-native service discovery</h3><p>If you're able to use Kubernetes APIs for service discovery in your application,
you can query the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="API server">API server</a>
for matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service
whenever the set of Pods in a Service changes.</p><p>For non-native applications, Kubernetes offers ways to place a network port or load
balancer in between your application and the backend Pods.</p><p>Either way, your workload can use these <a href="#discovering-services">service discovery</a>
mechanisms to find the target it wants to connect to.</p><h2 id="defining-a-service">Defining a Service</h2><p>A Service is an <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank" aria-label="object">object</a>
(the same way that a Pod or a ConfigMap is an object). You can create,
view or modify Service definitions using the Kubernetes API. Usually
you use a tool such as <code>kubectl</code> to make those API calls for you.</p><p>For example, suppose you have a set of Pods that each listen on TCP port 9376
and are labelled as <code>app.kubernetes.io/name=MyApp</code>. You can define a Service to
publish that TCP listener:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/simple-service.yaml" download="service/simple-service.yaml"><code>service/simple-service.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-simple-service-yaml&quot;)" title="Copy service/simple-service.yaml to clipboard"/></div><div class="includecode" id="service-simple-service-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Applying this manifest creates a new Service named "my-service" with the default
ClusterIP <a href="#publishing-services-service-types">service type</a>. The Service
targets TCP port 9376 on any Pod with the <code>app.kubernetes.io/name: MyApp</code> label.</p><p>Kubernetes assigns this Service an IP address (the <em>cluster IP</em>),
that is used by the virtual IP address mechanism. For more details on that mechanism,
read <a href="/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a>.</p><p>The controller for that Service continuously scans for Pods that
match its selector, and then makes any necessary updates to the set of
EndpointSlices for the Service.</p><p>The name of a Service object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#rfc-1035-label-names">RFC 1035 label name</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A Service can map <em>any</em> incoming <code>port</code> to a <code>targetPort</code>. By default and
for convenience, the <code>targetPort</code> is set to the same value as the <code>port</code>
field.</div><h3 id="relaxed-naming-requirements-for-service-objects">Relaxed naming requirements for Service objects</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: RelaxedServiceNameValidation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>The <code>RelaxedServiceNameValidation</code> feature gate allows Service object names to start with a digit. When this feature gate is enabled, Service object names must be valid <a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">RFC 1123 label names</a>.</p><h3 id="field-spec-ports">Port definitions</h3><p>Port definitions in Pods have names, and you can reference these names in the
<code>targetPort</code> attribute of a Service. For example, we can bind the <code>targetPort</code>
of the Service to the Pod port in the following way:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>proxy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:stable<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http-web-svc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>proxy<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>name-of-service-port<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span>http-web-svc<span style="color:#bbb">
</span></span></span></code></pre></div><p>This works even if there is a mixture of Pods in the Service using a single
configured name, with the same network protocol available via different
port numbers. This offers a lot of flexibility for deploying and evolving
your Services. For example, you can change the port numbers that Pods expose
in the next version of your backend software, without breaking clients.</p><p>The default protocol for Services is
<a href="/docs/reference/networking/service-protocols/#protocol-tcp">TCP</a>; you can also
use any other <a href="/docs/reference/networking/service-protocols/">supported protocol</a>.</p><p>Because many Services need to expose more than one port, Kubernetes supports
<a href="#multi-port-services">multiple port definitions</a> for a single Service.
Each port definition can have the same <code>protocol</code>, or a different one.</p><h3 id="services-without-selectors">Services without selectors</h3><p>Services most commonly abstract access to Kubernetes Pods thanks to the selector,
but when used with a corresponding set of
<a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/endpoint-slices/" target="_blank" aria-label="EndpointSlices">EndpointSlices</a>
objects and without a selector, the Service can abstract other kinds of backends,
including ones that run outside the cluster.</p><p>For example:</p><ul><li>You want to have an external database cluster in production, but in your
test environment you use your own databases.</li><li>You want to point your Service to a Service in a different
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="Namespace">Namespace</a> or on another cluster.</li><li>You are migrating a workload to Kubernetes. While evaluating the approach,
you run only a portion of your backends in Kubernetes.</li></ul><p>In any of these scenarios you can define a Service <em>without</em> specifying a
selector to match Pods. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Because this Service has no selector, the corresponding EndpointSlice
objects are not created automatically. You can map the Service
to the network address and port where it's running, by adding an EndpointSlice
object manually. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>discovery.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EndpointSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service-1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># by convention, use the name of the Service</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                     </span><span style="color:#080;font-style:italic"># as a prefix for the name of the EndpointSlice</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># You should set the "kubernetes.io/service-name" label.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Set its value to match the name of the Service</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/service-name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">addressType</span>:<span style="color:#bbb"> </span>IPv4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># should match with the name of the service port defined above</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">appProtocol</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">endpoints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">addresses</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"10.4.5.6"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">addresses</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"10.1.2.3"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h4 id="custom-endpointslices">Custom EndpointSlices</h4><p>When you create an <a href="#endpointslices">EndpointSlice</a> object for a Service, you can
use any name for the EndpointSlice. Each EndpointSlice in a namespace must have a
unique name. You link an EndpointSlice to a Service by setting the
<code>kubernetes.io/service-name</code> <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="label">label</a>
on that EndpointSlice.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The endpoint IPs <em>must not</em> be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or
link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).</p><p>The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,
because <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a> doesn't support virtual IPs
as a destination.</p></div><p>For an EndpointSlice that you create yourself, or in your own code,
you should also pick a value to use for the label
<a href="/docs/reference/labels-annotations-taints/#endpointslicekubernetesiomanaged-by"><code>endpointslice.kubernetes.io/managed-by</code></a>.
If you create your own controller code to manage EndpointSlices, consider using a
value similar to <code>"my-domain.example/name-of-controller"</code>. If you are using a third
party tool, use the name of the tool in all-lowercase and change spaces and other
punctuation to dashes (<code>-</code>).
If people are directly using a tool such as <code>kubectl</code> to manage EndpointSlices,
use a name that describes this manual management, such as <code>"staff"</code> or
<code>"cluster-admins"</code>. You should
avoid using the reserved value <code>"controller"</code>, which identifies EndpointSlices
managed by Kubernetes' own control plane.</p><h4 id="service-no-selector-access">Accessing a Service without a selector</h4><p>Accessing a Service without a selector works the same as if it had a selector.
In the <a href="#services-without-selectors">example</a> for a Service without a selector,
traffic is routed to one of the two endpoints defined in
the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The Kubernetes API server does not allow proxying to endpoints that are not mapped to
pods. Actions such as <code>kubectl port-forward service/&lt;service-name&gt; forwardedPort:servicePort</code> where the service has no
selector will fail due to this constraint. This prevents the Kubernetes API server
from being used as a proxy to endpoints the caller may not be authorized to access.</div><p>An <code>ExternalName</code> Service is a special case of Service that does not have
selectors and uses DNS names instead. For more information, see the
<a href="#externalname">ExternalName</a> section.</p><h3 id="endpointslices">EndpointSlices</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p><a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a> are objects that
represent a subset (a <em>slice</em>) of the backing network endpoints for a Service.</p><p>Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents.
If there are so many endpoints for a Service that a threshold is reached, then
Kubernetes adds another empty EndpointSlice and stores new endpoint information
there.
By default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices
all contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice
until an extra endpoint needs to be added.</p><p>See <a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a> for more
information about this API.</p><h3 id="endpoints">Endpoints (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [deprecated]</code></div><p>The EndpointSlice API is the evolution of the older
<a href="/docs/reference/kubernetes-api/service-resources/endpoints-v1/">Endpoints</a>
API. The deprecated Endpoints API has several problems relative to
EndpointSlice:</p><ul><li>It does not support dual-stack clusters.</li><li>It does not contain information needed to support newer features, such as
<a href="/docs/concepts/services-networking/service/#traffic-distribution">trafficDistribution</a>.</li><li>It will truncate the list of endpoints if it is too long to fit in a single object.</li></ul><p>Because of this, it is recommended that all clients use the
EndpointSlice API rather than Endpoints.</p><h4 id="over-capacity-endpoints">Over-capacity endpoints</h4><p>Kubernetes limits the number of endpoints that can fit in a single Endpoints
object. When there are over 1000 backing endpoints for a Service, Kubernetes
truncates the data in the Endpoints object. Because a Service can be linked
with more than one EndpointSlice, the 1000 backing endpoint limit only
affects the legacy Endpoints API.</p><p>In that case, Kubernetes selects at most 1000 possible backend endpoints to store
into the Endpoints object, and sets an
<a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/annotations" target="_blank" aria-label="annotation">annotation</a> on the Endpoints:
<a href="/docs/reference/labels-annotations-taints/#endpoints-kubernetes-io-over-capacity"><code>endpoints.kubernetes.io/over-capacity: truncated</code></a>.
The control plane also removes that annotation if the number of backend Pods drops below 1000.</p><p>Traffic is still sent to backends, but any load balancing mechanism that relies on the
legacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints.</p><p>The same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.</p><h3 id="application-protocol">Application protocol</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>The <code>appProtocol</code> field provides a way to specify an application protocol for
each Service port. This is used as a hint for implementations to offer
richer behavior for protocols that they understand.
The value of this field is mirrored by the corresponding
Endpoints and EndpointSlice objects.</p><p>This field follows standard Kubernetes label syntax. Valid values are one of:</p><ul><li><p><a href="https://www.iana.org/assignments/service-names">IANA standard service names</a>.</p></li><li><p>Implementation-defined prefixed names such as <code>mycompany.com/my-custom-protocol</code>.</p></li><li><p>Kubernetes-defined prefixed names:</p></li></ul><table><thead><tr><th>Protocol</th><th>Description</th></tr></thead><tbody><tr><td><code>kubernetes.io/h2c</code></td><td>HTTP/2 over cleartext as described in <a href="https://www.rfc-editor.org/rfc/rfc7540">RFC 7540</a></td></tr><tr><td><code>kubernetes.io/ws</code></td><td>WebSocket over cleartext as described in <a href="https://www.rfc-editor.org/rfc/rfc6455">RFC 6455</a></td></tr><tr><td><code>kubernetes.io/wss</code></td><td>WebSocket over TLS as described in <a href="https://www.rfc-editor.org/rfc/rfc6455">RFC 6455</a></td></tr></tbody></table><h3 id="multi-port-services">Multi-port Services</h3><p>For some Services, you need to expose more than one port.
Kubernetes lets you configure multiple port definitions on a Service object.
When using multiple ports for a Service, you must give all of your ports names
so that these are unambiguous.
For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>https<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">443</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9377</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>As with Kubernetes <a class="glossary-tooltip" title="A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/names" target="_blank" aria-label="names">names</a> in general, names for ports
must only contain lowercase alphanumeric characters and <code>-</code>. Port names must
also start and end with an alphanumeric character.</p><p>For example, the names <code>123-abc</code> and <code>web</code> are valid, but <code>123_abc</code> and <code>-web</code> are not.</p></div><h2 id="publishing-services-service-types">Service type</h2><p>For some parts of your application (for example, frontends) you may want to expose a
Service onto an external IP address, one that's accessible from outside of your
cluster.</p><p>Kubernetes Service types allow you to specify what kind of Service you want.</p><p>The available <code>type</code> values and their behaviors are:</p><dl><dt><a href="#type-clusterip"><code>ClusterIP</code></a></dt><dd>Exposes the Service on a cluster-internal IP. Choosing this value
makes the Service only reachable from within the cluster. This is the
default that is used if you don't explicitly specify a <code>type</code> for a Service.
You can expose the Service to the public internet using an
<a href="/docs/concepts/services-networking/ingress/">Ingress</a> or a
<a href="https://gateway-api.sigs.k8s.io/">Gateway</a>.</dd><dt><a href="#type-nodeport"><code>NodePort</code></a></dt><dd>Exposes the Service on each Node's IP at a static port (the <code>NodePort</code>).
To make the node port available, Kubernetes sets up a cluster IP address,
the same as if you had requested a Service of <code>type: ClusterIP</code>.</dd><dt><a href="#loadbalancer"><code>LoadBalancer</code></a></dt><dd>Exposes the Service externally using an external load balancer. Kubernetes
does not directly offer a load balancing component; you must provide one, or
you can integrate your Kubernetes cluster with a cloud provider.</dd><dt><a href="#externalname"><code>ExternalName</code></a></dt><dd>Maps the Service to the contents of the <code>externalName</code> field (for example,
to the hostname <code>api.foo.bar.example</code>). The mapping configures your cluster's
DNS server to return a <code>CNAME</code> record with that external hostname value.
No proxying of any kind is set up.</dd></dl><p>The <code>type</code> field in the Service API is designed as nested functionality - each level
adds to the previous. However there is an exception to this nested design. You can
define a <code>LoadBalancer</code> Service by
<a href="/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation">disabling the load balancer <code>NodePort</code> allocation</a>.</p><h3 id="type-clusterip"><code>type: ClusterIP</code></h3><p>This default Service type assigns an IP address from a pool of IP addresses that
your cluster has reserved for that purpose.</p><p>Several of the other types for Service build on the <code>ClusterIP</code> type as a
foundation.</p><p>If you define a Service that has the <code>.spec.clusterIP</code> set to <code>"None"</code> then
Kubernetes does not assign an IP address. See <a href="#headless-services">headless Services</a>
for more information.</p><h4 id="choosing-your-own-ip-address">Choosing your own IP address</h4><p>You can specify your own cluster IP address as part of a <code>Service</code> creation
request. To do this, set the <code>.spec.clusterIP</code> field. For example, if you
already have an existing DNS entry that you wish to reuse, or legacy systems
that are configured for a specific IP address and difficult to re-configure.</p><p>The IP address that you choose must be a valid IPv4 or IPv6 address from within the
<code>service-cluster-ip-range</code> CIDR range that is configured for the API server.
If you try to create a Service with an invalid <code>clusterIP</code> address value, the API
server will return a 422 HTTP status code to indicate that there's a problem.</p><p>Read <a href="/docs/reference/networking/virtual-ips/#avoiding-collisions">avoiding collisions</a>
to learn how Kubernetes helps reduce the risk and impact of two different Services
both trying to use the same IP address.</p><h3 id="type-nodeport"><code>type: NodePort</code></h3><p>If you set the <code>type</code> field to <code>NodePort</code>, the Kubernetes control plane
allocates a port from a range specified by <code>--service-node-port-range</code> flag (default: 30000-32767).
Each node proxies that port (the same port number on every Node) into your Service.
Your Service reports the allocated port in its <code>.spec.ports[*].nodePort</code> field.</p><p>Using a NodePort gives you the freedom to set up your own load balancing solution,
to configure environments that are not fully supported by Kubernetes, or even
to expose one or more nodes' IP addresses directly.</p><p>For a node port Service, Kubernetes additionally allocates a port (TCP, UDP or
SCTP to match the protocol of the Service). Every node in the cluster configures
itself to listen on that assigned port and to forward traffic to one of the ready
endpoints associated with that Service. You'll be able to contact the <code>type: NodePort</code>
Service, from outside the cluster, by connecting to any node using the appropriate
protocol (for example: TCP), and the appropriate port (as assigned to that Service).</p><h4 id="nodeport-custom-port">Choosing your own port</h4><p>If you want a specific port number, you can specify a value in the <code>nodePort</code>
field. The control plane will either allocate you that port or report that
the API transaction failed.
This means that you need to take care of possible port collisions yourself.
You also have to use a valid port number, one that's inside the range configured
for NodePort use.</p><p>Here is an example manifest for a Service of <code>type: NodePort</code> that specifies
a NodePort value (30007, in this example):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>NodePort<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># By default and for convenience, the `targetPort` is set to</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># the same value as the `port` field.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Optional field</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># By default and for convenience, the Kubernetes control plane</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># will allocate a port from a range (default: 30000-32767)</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">nodePort</span>:<span style="color:#bbb"> </span><span style="color:#666">30007</span><span style="color:#bbb">
</span></span></span></code></pre></div><h4 id="avoid-nodeport-collisions">Reserve Nodeport ranges to avoid collisions</h4><p>The policy for assigning ports to NodePort services applies to both the auto-assignment and
the manual assignment scenarios. When a user wants to create a NodePort service that
uses a specific port, the target port may conflict with another port that has already been assigned.</p><p>To avoid this problem, the port range for NodePort services is divided into two bands.
Dynamic port assignment uses the upper band by default, and it may use the lower band once the
upper band has been exhausted. Users can then allocate from the lower band with a lower risk of port collision.</p><h4 id="service-nodeport-custom-listen-address">Custom IP address configuration for <code>type: NodePort</code> Services</h4><p>You can set up nodes in your cluster to use a particular IP address for serving node port
services. You might want to do this if each node is connected to multiple networks (for example:
one network for application traffic, and another network for traffic between nodes and the
control plane).</p><p>If you want to specify particular IP address(es) to proxy the port, you can set the
<code>--nodeport-addresses</code> flag for kube-proxy or the equivalent <code>nodePortAddresses</code>
field of the <a href="/docs/reference/config-api/kube-proxy-config.v1alpha1/">kube-proxy configuration file</a>
to particular IP block(s).</p><p>This flag takes a comma-delimited list of IP blocks (e.g. <code>10.0.0.0/8</code>, <code>192.0.2.0/25</code>)
to specify IP address ranges that kube-proxy should consider as local to this node.</p><p>For example, if you start kube-proxy with the <code>--nodeport-addresses=127.0.0.0/8</code> flag,
kube-proxy only selects the loopback interface for NodePort Services.
The default for <code>--nodeport-addresses</code> is an empty list.
This means that kube-proxy should consider all available network interfaces for NodePort.
(That's also compatible with earlier Kubernetes releases.)<div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This Service is visible as <code>&lt;NodeIP&gt;:spec.ports[*].nodePort</code> and <code>.spec.clusterIP:spec.ports[*].port</code>.
If the <code>--nodeport-addresses</code> flag for kube-proxy or the equivalent field
in the kube-proxy configuration file is set, <code>&lt;NodeIP&gt;</code> would be a filtered
node IP address (or possibly IP addresses).</div></p><h3 id="loadbalancer"><code>type: LoadBalancer</code></h3><p>On cloud providers which support external load balancers, setting the <code>type</code>
field to <code>LoadBalancer</code> provisions a load balancer for your Service.
The actual creation of the load balancer happens asynchronously, and
information about the provisioned balancer is published in the Service's
<code>.status.loadBalancer</code> field.
For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.171.239</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">loadBalancer</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ingress</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">ip</span>:<span style="color:#bbb"> </span><span style="color:#666">192.0.2.127</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Traffic from the external load balancer is directed at the backend Pods. The cloud
provider decides how it is load balanced.</p><p>To implement a Service of <code>type: LoadBalancer</code>, Kubernetes typically starts off
by making the changes that are equivalent to you requesting a Service of
<code>type: NodePort</code>. The cloud-controller-manager component then configures the external
load balancer to forward traffic to that assigned node port.</p><p>You can configure a load balanced Service to
<a href="#load-balancer-nodeport-allocation">omit</a> assigning a node port, provided that the
cloud provider implementation supports this.</p><p>Some cloud providers allow you to specify the <code>loadBalancerIP</code>. In those cases, the load-balancer is created
with the user-specified <code>loadBalancerIP</code>. If the <code>loadBalancerIP</code> field is not specified,
the load balancer is set up with an ephemeral IP address. If you specify a <code>loadBalancerIP</code>
but your cloud provider does not support the feature, the <code>loadbalancerIP</code> field that you
set is ignored.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The<code>.spec.loadBalancerIP</code> field for a Service was deprecated in Kubernetes v1.24.</p><p>This field was under-specified and its meaning varies across implementations.
It also cannot support dual-stack networking. This field may be removed in a future API version.</p><p>If you're integrating with a provider that supports specifying the load balancer IP address(es)
for a Service via a (provider specific) annotation, you should switch to doing that.</p><p>If you are writing code for a load balancer integration with Kubernetes, avoid using this field.
You can integrate with <a href="https://gateway-api.sigs.k8s.io/">Gateway</a> rather than Service, or you
can define your own (provider specific) annotations on the Service that specify the equivalent detail.</p></div><h4 id="node-liveness-impact-on-load-balancer-traffic">Node liveness impact on load balancer traffic</h4><p>Load balancer health checks are critical to modern applications. They are used to
determine which server (virtual machine, or IP address) the load balancer should
dispatch traffic to. The Kubernetes APIs do not define how health checks have to be
implemented for Kubernetes managed load balancers, instead it's the cloud providers
(and the people implementing integration code) who decide on the behavior. Load
balancer health checks are extensively used within the context of supporting the
<code>externalTrafficPolicy</code> field for Services.</p><h4 id="load-balancers-with-mixed-protocol-types">Load balancers with mixed protocol types</h4><div class="feature-state-notice feature-stable" title="Feature Gate: MixedProtocolLBService"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code> (enabled by default: true)</div><p>By default, for LoadBalancer type of Services, when there is more than one port defined, all
ports must have the same protocol, and the protocol must be one which is supported
by the cloud provider.</p><p>The feature gate <code>MixedProtocolLBService</code> (enabled by default for the kube-apiserver as of v1.24) allows the use of
different protocols for LoadBalancer type of Services, when there is more than one port defined.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The set of protocols that can be used for load balanced Services is defined by your
cloud provider; they may impose restrictions beyond what the Kubernetes API enforces.</div><h4 id="load-balancer-nodeport-allocation">Disabling load balancer NodePort allocation</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can optionally disable node port allocation for a Service of <code>type: LoadBalancer</code>, by setting
the field <code>spec.allocateLoadBalancerNodePorts</code> to <code>false</code>. This should only be used for load balancer implementations
that route traffic directly to pods as opposed to using node ports. By default, <code>spec.allocateLoadBalancerNodePorts</code>
is <code>true</code> and type LoadBalancer Services will continue to allocate node ports. If <code>spec.allocateLoadBalancerNodePorts</code>
is set to <code>false</code> on an existing Service with allocated node ports, those node ports will <strong>not</strong> be de-allocated automatically.
You must explicitly remove the <code>nodePorts</code> entry in every Service port to de-allocate those node ports.</p><h4 id="load-balancer-class">Specifying class of load balancer implementation</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>For a Service with <code>type</code> set to <code>LoadBalancer</code>, the <code>.spec.loadBalancerClass</code> field
enables you to use a load balancer implementation other than the cloud provider default.</p><p>By default, <code>.spec.loadBalancerClass</code> is not set and a <code>LoadBalancer</code>
type of Service uses the cloud provider's default load balancer implementation if the
cluster is configured with a cloud provider using the <code>--cloud-provider</code> component
flag.</p><p>If you specify <code>.spec.loadBalancerClass</code>, it is assumed that a load balancer
implementation that matches the specified class is watching for Services.
Any default load balancer implementation (for example, the one provided by
the cloud provider) will ignore Services that have this field set.
<code>spec.loadBalancerClass</code> can be set on a Service of type <code>LoadBalancer</code> only.
Once set, it cannot be changed.
The value of <code>spec.loadBalancerClass</code> must be a label-style identifier,
with an optional prefix such as "<code>internal-vip</code>" or "<code>example.com/internal-vip</code>".
Unprefixed names are reserved for end-users.</p><h4 id="load-balancer-ip-mode">Load balancer IP address mode</h4><div class="feature-state-notice feature-stable" title="Feature Gate: LoadBalancerIPMode"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>For a Service of <code>type: LoadBalancer</code>, a controller can set <code>.status.loadBalancer.ingress.ipMode</code>.
The <code>.status.loadBalancer.ingress.ipMode</code> specifies how the load-balancer IP behaves.
It may be specified only when the <code>.status.loadBalancer.ingress.ip</code> field is also specified.</p><p>There are two possible values for <code>.status.loadBalancer.ingress.ipMode</code>: "VIP" and "Proxy".
The default value is "VIP" meaning that traffic is delivered to the node
with the destination set to the load-balancer's IP and port.
There are two cases when setting this to "Proxy", depending on how the load-balancer
from the cloud provider delivers the traffics:</p><ul><li>If the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port;</li><li>If the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port.</li></ul><p>Service implementations may use this information to adjust traffic routing.</p><h4 id="internal-load-balancer">Internal load balancer</h4><p>In a mixed environment it is sometimes necessary to route traffic from Services inside the same
(virtual) network address block.</p><p>In a split-horizon DNS environment you would need two Services to be able to route both external
and internal traffic to your endpoints.</p><p>To set an internal load balancer, add one of the following annotations to your Service
depending on the cloud service provider you're using:</p><ul class="nav nav-tabs" id="service-tabs" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#service-tabs-0" role="tab" aria-controls="service-tabs-0" aria-selected="true">Default</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-1" role="tab" aria-controls="service-tabs-1">GCP</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-2" role="tab" aria-controls="service-tabs-2">AWS</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-3" role="tab" aria-controls="service-tabs-3">Azure</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-4" role="tab" aria-controls="service-tabs-4">IBM Cloud</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-5" role="tab" aria-controls="service-tabs-5">OpenStack</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-6" role="tab" aria-controls="service-tabs-6">Baidu Cloud</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-7" role="tab" aria-controls="service-tabs-7">Tencent Cloud</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-8" role="tab" aria-controls="service-tabs-8">Alibaba Cloud</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-9" role="tab" aria-controls="service-tabs-9">OCI</a></li></ul><div class="tab-content" id="service-tabs"><div id="service-tabs-0" class="tab-pane show active" role="tabpanel" aria-labelledby="service-tabs-0"><p><p>Select one of the tabs.</p></p></div><div id="service-tabs-1" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-1"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">networking.gke.io/load-balancer-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Internal"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-2" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-2"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.beta.kubernetes.io/aws-load-balancer-scheme</span>:<span style="color:#bbb"> </span><span style="color:#b44">"internal"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-3" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-3"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.beta.kubernetes.io/azure-load-balancer-internal</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-4" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-4"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">"private"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-5" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-5"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.beta.kubernetes.io/openstack-internal-load-balancer</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-6" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-6"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.beta.kubernetes.io/cce-load-balancer-internal-vpc</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-7" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-7"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.kubernetes.io/qcloud-loadbalancer-internal-subnetid</span>:<span style="color:#bbb"> </span>subnet-xxxxx<span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-8" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-8"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">"intranet"</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="service-tabs-9" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-9"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service.beta.kubernetes.io/oci-load-balancer-internal</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div></div><h3 id="externalname"><code>type: ExternalName</code></h3><p>Services of type ExternalName map a Service to a DNS name, not to a typical selector such as
<code>my-service</code> or <code>cassandra</code>. You specify these Services with the <code>spec.externalName</code> parameter.</p><p>This Service definition, for example, maps
the <code>my-service</code> Service in the <code>prod</code> namespace to <code>my.database.example.com</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>prod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>ExternalName<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">externalName</span>:<span style="color:#bbb"> </span>my.database.example.com<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>A Service of <code>type: ExternalName</code> accepts an IPv4 address string,
but treats that string as a DNS name comprised of digits,
not as an IP address (the internet does not however allow such names in DNS).
Services with external names that resemble IPv4
addresses are not resolved by DNS servers.</p><p>If you want to map a Service directly to a specific IP address, consider using
<a href="#headless-services">headless Services</a>.</p></div><p>When looking up the host <code>my-service.prod.svc.cluster.local</code>, the cluster DNS Service
returns a <code>CNAME</code> record with the value <code>my.database.example.com</code>. Accessing
<code>my-service</code> works in the same way as other Services but with the crucial
difference that redirection happens at the DNS level rather than via proxying or
forwarding. Should you later decide to move your database into your cluster, you
can start its Pods, add appropriate selectors or endpoints, and change the
Service's <code>type</code>.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS.
If you use ExternalName then the hostname used by clients inside your cluster is different from
the name that the ExternalName references.</p><p>For protocols that use hostnames this difference may lead to errors or unexpected responses.
HTTP requests will have a <code>Host:</code> header that the origin server does not recognize;
TLS servers will not be able to provide a certificate matching the hostname that the client connected to.</p></div><h2 id="headless-services">Headless Services</h2><p>Sometimes you don't need load-balancing and a single Service IP. In
this case, you can create what are termed <em>headless Services</em>, by explicitly
specifying <code>"None"</code> for the cluster IP address (<code>.spec.clusterIP</code>).</p><p>You can use a headless Service to interface with other service discovery mechanisms,
without being tied to Kubernetes' implementation.</p><p>For headless Services, a cluster IP is not allocated, kube-proxy does not handle
these Services, and there is no load balancing or proxying done by the platform for them.</p><p>A headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't
configure routes and packet forwarding using
<a href="/docs/reference/networking/virtual-ips/">virtual IP addresses and proxies</a>; instead, headless Services report the
endpoint IP addresses of the individual pods via internal DNS records, served through the cluster's
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS service</a>.
To define a headless Service, you make a Service with <code>.spec.type</code> set to ClusterIP (which is also the default for <code>type</code>),
and you additionally set <code>.spec.clusterIP</code> to None.</p><p>The string value None is a special case and is not the same as leaving the <code>.spec.clusterIP</code> field unset.</p><p>How DNS is automatically configured depends on whether the Service has selectors defined:</p><h3 id="with-selectors">With selectors</h3><p>For headless Services that define selectors, the endpoints controller creates
EndpointSlices in the Kubernetes API, and modifies the DNS configuration to return
A or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing the Service.</p><h3 id="without-selectors">Without selectors</h3><p>For headless Services that do not define selectors, the control plane does
not create EndpointSlice objects. However, the DNS system looks for and configures
either:</p><ul><li>DNS CNAME records for <a href="#externalname"><code>type: ExternalName</code></a> Services.</li><li>DNS A / AAAA records for all IP addresses of the Service's ready endpoints,
for all Service types other than <code>ExternalName</code>.<ul><li>For IPv4 endpoints, the DNS system creates A records.</li><li>For IPv6 endpoints, the DNS system creates AAAA records.</li></ul></li></ul><p>When you define a headless Service without a selector, the <code>port</code> must
match the <code>targetPort</code>.</p><h2 id="discovering-services">Discovering services</h2><p>For clients running inside your cluster, Kubernetes supports two primary modes of
finding a Service: environment variables and DNS.</p><h3 id="environment-variables">Environment variables</h3><p>When a Pod is run on a Node, the kubelet adds a set of environment variables
for each active Service. It adds <code>{SVCNAME}_SERVICE_HOST</code> and <code>{SVCNAME}_SERVICE_PORT</code> variables,
where the Service name is upper-cased and dashes are converted to underscores.</p><p>For example, the Service <code>redis-primary</code> which exposes TCP port 6379 and has been
allocated cluster IP address 10.0.0.11, produces the following environment
variables:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_SERVICE_HOST</span><span style="color:#666">=</span>10.0.0.11
</span></span><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_SERVICE_PORT</span><span style="color:#666">=</span><span style="color:#666">6379</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_PORT</span><span style="color:#666">=</span>tcp://10.0.0.11:6379
</span></span><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_PORT_6379_TCP</span><span style="color:#666">=</span>tcp://10.0.0.11:6379
</span></span><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_PORT_6379_TCP_PROTO</span><span style="color:#666">=</span>tcp
</span></span><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_PORT_6379_TCP_PORT</span><span style="color:#666">=</span><span style="color:#666">6379</span>
</span></span><span style="display:flex"><span><span style="color:#b8860b">REDIS_PRIMARY_PORT_6379_TCP_ADDR</span><span style="color:#666">=</span>10.0.0.11
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>When you have a Pod that needs to access a Service, and you are using
the environment variable method to publish the port and cluster IP to the client
Pods, you must create the Service <em>before</em> the client Pods come into existence.
Otherwise, those client Pods won't have their environment variables populated.</p><p>If you only use DNS to discover the cluster IP for a Service, you don't need to
worry about this ordering issue.</p></div><p>Kubernetes also supports and provides variables that are compatible with Docker
Engine's "<em><a href="https://docs.docker.com/network/links/">legacy container links</a></em>" feature.
You can read <a href="https://github.com/kubernetes/kubernetes/blob/dd2d12f6dc0e654c15d5db57a5f9f6ba61192726/pkg/kubelet/envvars/envvars.go#L72"><code>makeLinkVariables</code></a>
to see how this is implemented in Kubernetes.</p><h3 id="dns">DNS</h3><p>You can (and almost always should) set up a DNS service for your Kubernetes
cluster using an <a href="/docs/concepts/cluster-administration/addons/">add-on</a>.</p><p>A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new
Services and creates a set of DNS records for each one. If DNS has been enabled
throughout your cluster then all Pods should automatically be able to resolve
Services by their DNS name.</p><p>For example, if you have a Service called <code>my-service</code> in a Kubernetes
namespace <code>my-ns</code>, the control plane and the DNS Service acting together
create a DNS record for <code>my-service.my-ns</code>. Pods in the <code>my-ns</code> namespace
should be able to find the service by doing a name lookup for <code>my-service</code>
(<code>my-service.my-ns</code> would also work).</p><p>Pods in other namespaces must qualify the name as <code>my-service.my-ns</code>. These names
will resolve to the cluster IP assigned for the Service.</p><p>Kubernetes also supports DNS SRV (Service) records for named ports. If the
<code>my-service.my-ns</code> Service has a port named <code>http</code> with the protocol set to
<code>TCP</code>, you can do a DNS SRV query for <code>_http._tcp.my-service.my-ns</code> to discover
the port number for <code>http</code>, as well as the IP address.</p><p>The Kubernetes DNS server is the only way to access <code>ExternalName</code> Services.
You can find more information about <code>ExternalName</code> resolution in
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a>.</p><a id="shortcomings"><a id="the-gory-details-of-virtual-ips"><a id="proxy-modes"><a id="proxy-mode-userspace"><a id="proxy-mode-iptables"><a id="proxy-mode-ipvs"><a id="ips-and-vips"><h2 id="virtual-ip-addressing-mechanism">Virtual IP addressing mechanism</h2><p>Read <a href="/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a> explains the
mechanism Kubernetes provides to expose a Service with a virtual IP address.</p><h3 id="traffic-policies">Traffic policies</h3><p>You can set the <code>.spec.internalTrafficPolicy</code> and <code>.spec.externalTrafficPolicy</code> fields
to control how Kubernetes routes traffic to healthy (â€œreadyâ€) backends.</p><p>See <a href="/docs/reference/networking/virtual-ips/#traffic-policies">Traffic Policies</a> for more details.</p><h3 id="traffic-distribution">Traffic distribution</h3><div class="feature-state-notice feature-stable" title="Feature Gate: ServiceTrafficDistribution"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>The <code>.spec.trafficDistribution</code> field provides another way to influence traffic
routing within a Kubernetes Service. While traffic policies focus on strict
semantic guarantees, traffic distribution allows you to express <em>preferences</em>
(such as routing to topologically closer endpoints). This can help optimize for
performance, cost, or reliability. In Kubernetes 1.34, the
following field value is supported:</p><dl><dt><code>PreferClose</code></dt><dd>Indicates a preference for routing traffic to endpoints that are in the same
zone as the client.</dd></dl><div class="feature-state-notice feature-beta" title="Feature Gate: PreferSameTrafficDistribution"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>In Kubernetes 1.34, two additional values are
available (unless the <code>PreferSameTrafficDistribution</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> is
disabled):</p><dl><dt><code>PreferSameZone</code></dt><dd>This is an alias for <code>PreferClose</code> that is clearer about the intended semantics.</dd><dt><code>PreferSameNode</code></dt><dd>Indicates a preference for routing traffic to endpoints that are on the same
node as the client.</dd></dl><p>If the field is not set, the implementation will apply its default routing strategy.</p><p>See <a href="/docs/reference/networking/virtual-ips/#traffic-distribution">Traffic
Distribution</a> for
more details</p><h3 id="session-stickiness">Session stickiness</h3><p>If you want to make sure that connections from a particular client are passed to
the same Pod each time, you can configure session affinity based on the client's
IP address. Read <a href="/docs/reference/networking/virtual-ips/#session-affinity">session affinity</a>
to learn more.</p><h2 id="external-ips">External IPs</h2><p>If there are external IPs that route to one or more cluster nodes, Kubernetes Services
can be exposed on those <code>externalIPs</code>. When network traffic arrives into the cluster, with
the external IP (as destination IP) and the port matching that Service, rules and routes
that Kubernetes has configured ensure that the traffic is routed to one of the endpoints
for that Service.</p><p>When you define a Service, you can specify <code>externalIPs</code> for any
<a href="#publishing-services-service-types">service type</a>.
In the example below, the Service named <code>"my-service"</code> can be accessed by clients using TCP,
on <code>"198.51.100.32:80"</code> (calculated from <code>.spec.externalIPs[]</code> and <code>.spec.ports[].port</code>).</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">49152</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">externalIPs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:#666">198.51.100.32</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes does not manage allocation of <code>externalIPs</code>; these are the responsibility
of the cluster administrator.</div><h2 id="api-object">API Object</h2><p>Service is a top-level resource in the Kubernetes REST API. You can find more details
about the <a href="/docs/reference/generated/kubernetes-api/v1.34/#service-v1-core">Service API object</a>.</p><h2 id="what-s-next">What's next</h2><p>Learn more about Services and how they fit into Kubernetes:</p><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a>
tutorial.</li><li>Read about <a href="/docs/concepts/services-networking/ingress/">Ingress</a>, which
exposes HTTP and HTTPS routes from outside the cluster to Services within
your cluster.</li><li>Read about <a href="/docs/concepts/services-networking/gateway/">Gateway</a>, an extension to
Kubernetes that provides more flexibility than Ingress.</li></ul><p>For more context, read the following:</p><ul><li><a href="/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a></li><li><a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a></li><li><a href="/docs/reference/kubernetes-api/service-resources/service-v1/">Service API reference</a></li><li><a href="/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/">EndpointSlice API reference</a></li><li><a href="/docs/reference/kubernetes-api/service-resources/endpoints-v1/">Endpoint API reference (legacy)</a></li></ul></a></a></a></a></a></a></a></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Topology Aware Routing</h1><div class="lead"><em>Topology Aware Routing</em> provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.</div><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Prior to Kubernetes 1.27, this feature was known as <em>Topology Aware Hints</em>.</div><p><em>Topology Aware Routing</em> adjusts routing behavior to prefer keeping traffic in
the zone it originated from. In some cases this can help reduce costs or improve
network performance.</p><h2 id="motivation">Motivation</h2><p>Kubernetes clusters are increasingly deployed in multi-zone environments.
<em>Topology Aware Routing</em> provides a mechanism to help keep traffic within the
zone it originated from. When calculating the endpoints for a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>, the EndpointSlice controller considers
the topology (region and zone) of each endpoint and populates the hints field to
allocate it to a zone. Cluster components such as <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a> can then consume those hints, and use
them to influence how the traffic is routed (favoring topologically closer
endpoints).</p><h2 id="enabling-topology-aware-routing">Enabling Topology Aware Routing</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Prior to Kubernetes 1.27, this behavior was controlled using the
<code>service.kubernetes.io/topology-aware-hints</code> annotation.</div><p>You can enable Topology Aware Routing for a Service by setting the
<code>service.kubernetes.io/topology-mode</code> annotation to <code>Auto</code>. When there are
enough endpoints available in each zone, Topology Hints will be populated on
EndpointSlices to allocate individual endpoints to specific zones, resulting in
traffic being routed closer to where it originated from.</p><h2 id="when-it-works-best">When it works best</h2><p>This feature works best when:</p><h3 id="1-incoming-traffic-is-evenly-distributed">1. Incoming traffic is evenly distributed</h3><p>If a large proportion of traffic is originating from a single zone, that traffic
could overload the subset of endpoints that have been allocated to that zone.
This feature is not recommended when incoming traffic is expected to originate
from a single zone.</p><h3 id="three-or-more-endpoints-per-zone">2. The Service has 3 or more endpoints per zone</h3><p>In a three zone cluster, this means 9 or more endpoints. If there are fewer than
3 endpoints per zone, there is a high (â‰ˆ50%) probability that the EndpointSlice
controller will not be able to allocate endpoints evenly and instead will fall
back to the default cluster-wide routing approach.</p><h2 id="how-it-works">How It Works</h2><p>The "Auto" heuristic attempts to proportionally allocate a number of endpoints
to each zone. Note that this heuristic works best for Services that have a
significant number of endpoints.</p><h3 id="implementation-control-plane">EndpointSlice controller</h3><p>The EndpointSlice controller is responsible for setting hints on EndpointSlices
when this heuristic is enabled. The controller allocates a proportional amount of
endpoints to each zone. This proportion is based on the
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a>
CPU cores for nodes running in that zone. For example, if one zone had 2 CPU
cores and another zone only had 1 CPU core, the controller would allocate twice
as many endpoints to the zone with 2 CPU cores.</p><p>The following example shows what an EndpointSlice looks like when hints have
been populated:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>discovery.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>EndpointSlice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example-hints<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/service-name</span>:<span style="color:#bbb"> </span>example-svc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">addressType</span>:<span style="color:#bbb"> </span>IPv4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">endpoints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">addresses</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"10.1.2.3"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">conditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hostname</span>:<span style="color:#bbb"> </span>pod-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">zone</span>:<span style="color:#bbb"> </span>zone-a<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">hints</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">forZones</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"zone-a"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="implementation-kube-proxy">kube-proxy</h3><p>The kube-proxy component filters the endpoints it routes to based on the hints set by
the EndpointSlice controller. In most cases, this means that the kube-proxy is able
to route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints
from a different zone to ensure more even distribution of endpoints between zones.
This would result in some traffic being routed to other zones.</p><h2 id="safeguards">Safeguards</h2><p>The Kubernetes control plane and the kube-proxy on each node apply some
safeguard rules before using Topology Aware Hints. If these don't check out,
the kube-proxy selects endpoints from anywhere in your cluster, regardless of the
zone.</p><ol><li><p><strong>Insufficient number of endpoints:</strong> If there are less endpoints than zones
in a cluster, the controller will not assign any hints.</p></li><li><p><strong>Impossible to achieve balanced allocation:</strong> In some cases, it will be
impossible to achieve a balanced allocation of endpoints among zones. For
example, if zone-a is twice as large as zone-b, but there are only 2
endpoints, an endpoint allocated to zone-a may receive twice as much traffic
as zone-b. The controller does not assign hints if it can't get this "expected
overload" value below an acceptable threshold for each zone. Importantly this
is not based on real-time feedback. It is still possible for individual
endpoints to become overloaded.</p></li><li><p><strong>One or more Nodes has insufficient information:</strong> If any node does not have
a <code>topology.kubernetes.io/zone</code> label or is not reporting a value for
allocatable CPU, the control plane does not set any topology-aware endpoint
hints and so kube-proxy does not filter endpoints by zone.</p></li><li><p><strong>One or more endpoints does not have a zone hint:</strong> When this happens,
the kube-proxy assumes that a transition from or to Topology Aware Hints is
underway. Filtering endpoints for a Service in this state would be dangerous
so the kube-proxy falls back to using all endpoints.</p></li><li><p><strong>A zone is not represented in hints:</strong> If the kube-proxy is unable to find
at least one endpoint with a hint targeting the zone it is running in, it falls
back to using endpoints from all zones. This is most likely to happen as you add
a new zone into your existing cluster.</p></li></ol><h2 id="constraints">Constraints</h2><ul><li><p>Topology Aware Hints are not used when <code>internalTrafficPolicy</code> is set to <code>Local</code>
on a Service. It is possible to use both features in the same cluster on different
Services, just not on the same Service.</p></li><li><p>This approach will not work well for Services that have a large proportion of
traffic originating from a subset of zones. Instead this assumes that incoming
traffic will be roughly proportional to the capacity of the Nodes in each
zone.</p></li><li><p>The EndpointSlice controller ignores unready nodes as it calculates the
proportions of each zone. This could have unintended consequences if a large
portion of nodes are unready.</p></li><li><p>The EndpointSlice controller ignores nodes with the
<code>node-role.kubernetes.io/control-plane</code> or <code>node-role.kubernetes.io/master</code>
label set. This could be problematic if workloads are also running on those
nodes.</p></li><li><p>The EndpointSlice controller does not take into account <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="tolerations">tolerations</a> when deploying or calculating the
proportions of each zone. If the Pods backing a Service are limited to a
subset of Nodes in the cluster, this will not be taken into account.</p></li><li><p>This may not work well with autoscaling. For example, if a lot of traffic is
originating from a single zone, only the endpoints allocated to that zone will
be handling that traffic. That could result in <a class="glossary-tooltip" title="Object that automatically scales the number of pod replicas based on targeted resource utilization or custom metric targets." data-toggle="tooltip" data-placement="top" href="/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" aria-label="Horizontal Pod Autoscaler">Horizontal Pod Autoscaler</a>
either not picking up on this event, or newly added pods starting in a
different zone.</p></li></ul><h2 id="custom-heuristics">Custom heuristics</h2><p>Kubernetes is deployed in many different ways, there is no single heuristic for
allocating endpoints to zones will work for every use case. A key goal of this
feature is to enable custom heuristics to be developed if the built in heuristic
does not work for your use case. The first steps to enable custom heuristics
were included in the 1.27 release. This is a limited implementation that may not
yet cover some relevant and plausible situations.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li><li>Learn about the
<a href="/docs/concepts/services-networking/service/#traffic-distribution">trafficDistribution</a>
field, which is closely related to the <code>service.kubernetes.io/topology-mode</code>
annotation and provides flexible options for traffic routing within
Kubernetes.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Ingress</h1><div class="lead">Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.</div><p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [stable]</code></div><p>An API object that manages external access to the services in a cluster, typically HTTP.</p><p>Ingress may provide load balancing, SSL termination and name-based virtual hosting.</p></p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Ingress is frozen. New features are being added to the <a href="/docs/concepts/services-networking/gateway/">Gateway API</a>.</div><h2 id="terminology">Terminology</h2><p>For clarity, this guide defines the following terms:</p><ul><li>Node: A worker machine in Kubernetes, part of a cluster.</li><li>Cluster: A set of Nodes that run containerized applications managed by Kubernetes.
For this example, and in most common Kubernetes deployments, nodes in the cluster
are not part of the public internet.</li><li>Edge router: A router that enforces the firewall policy for your cluster. This
could be a gateway managed by a cloud provider or a physical piece of hardware.</li><li>Cluster network: A set of links, logical or physical, that facilitate communication
within a cluster according to the Kubernetes <a href="/docs/concepts/cluster-administration/networking/">networking model</a>.</li><li>Service: A Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> that identifies
a set of Pods using <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="label">label</a> selectors.
Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.</li></ul><h2 id="what-is-ingress">What is Ingress?</h2><p><a href="/docs/reference/generated/kubernetes-api/v1.34/#ingress-v1-networking-k8s-io">Ingress</a>
exposes HTTP and HTTPS routes from outside the cluster to
<a href="/docs/concepts/services-networking/service/" target="_blank">services</a> within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.</p><p>Here is a simple example where an Ingress sends all its traffic to one Service:</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNkstuwyAQRX8F4U0r2VHqPlSRKqt0UamLqlnaWWAYJygYLB59KMm_Fxcix-qmGwbuXA7DwAEzzQETXKutof0Ovb4vaoUQkwKUu6pi3FwXM_QSHGBt0VFFt8DRU2OWSGrKUUMlVQwMmhVLEV1Vcm9-aUksiuXRaO_CEhkv4WjBfAgG1TrGaLa-iaUw6a0DcwGI-WgOsF7zm-pN881fvRx1UDzeiFq7ghb1kgqFWiElyTjnuXVG74FkbdumefEpuNuRu_4rZ1pqQ7L5fL6YQPaPNiFuywcG9_-ihNyUkm6YSONWkjVNM8WUIyaeOJLO3clTB_KhL8NQDmVe-OJjxgZM5FhFiiFTK5zjDkxHBQ9_4zB4a-x20EGNSZhyaKmXrg7f5hSsvufUwTMXThtMWiot5Jh6p9ffimHijIezaSVoeN0uiqcfMJvf7w"><img src="/docs/images/ingress.svg" alt="ingress-diagram"/></a><figcaption><p>Figure. Ingress</p></figcaption></figure><p>An Ingress may be configured to give Services externally-reachable URLs,
load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting.
An <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
is responsible for fulfilling the Ingress, usually with a load balancer, though
it may also configure your edge router or additional frontends to help handle the traffic.</p><p>An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically
uses a service of type <a href="/docs/concepts/services-networking/service/#type-nodeport">Service.Type=NodePort</a> or
<a href="/docs/concepts/services-networking/service/#loadbalancer">Service.Type=LoadBalancer</a>.</p><h2 id="prerequisites">Prerequisites</h2><p>You must have an <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
to satisfy an Ingress. Only creating an Ingress resource has no effect.</p><p>You may need to deploy an Ingress controller such as <a href="https://kubernetes.github.io/ingress-nginx/deploy/">ingress-nginx</a>.
You can choose from a number of <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controllers</a>.</p><p>Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress
controllers operate slightly differently.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.</div><h2 id="the-ingress-resource">The Ingress resource</h2><p>A minimal Ingress resource example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/minimal-ingress.yaml" download="service/networking/minimal-ingress.yaml"><code>service/networking/minimal-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-minimal-ingress-yaml&quot;)" title="Copy service/networking/minimal-ingress.yaml to clipboard"/></div><div class="includecode" id="service-networking-minimal-ingress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>minimal-ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nginx.ingress.kubernetes.io/rewrite-target</span>:<span style="color:#bbb"> </span>/<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ingressClassName</span>:<span style="color:#bbb"> </span>nginx-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/testpath<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>An Ingress needs <code>apiVersion</code>, <code>kind</code>, <code>metadata</code> and <code>spec</code> fields.
The name of an Ingress object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.
For general information about working with config files, see
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">deploying applications</a>,
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">configuring containers</a>,
<a href="/docs/concepts/workloads/management/">managing resources</a>.
Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which
is the <a href="https://github.com/kubernetes/ingress-nginx/blob/main/docs/examples/rewrite/README.md">rewrite-target annotation</a>.
Different <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controllers</a> support different annotations.
Review the documentation for your choice of Ingress controller to learn which annotations are supported.</p><p>The <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec">Ingress spec</a>
has all the information needed to configure a load balancer or proxy server. Most importantly, it
contains a list of rules matched against all incoming requests. Ingress resource only supports rules
for directing HTTP(S) traffic.</p><p>If the <code>ingressClassName</code> is omitted, a <a href="#default-ingress-class">default Ingress class</a>
should be defined.</p><p>There are some ingress controllers, that work without the definition of a
default <code>IngressClass</code>. For example, the Ingress-NGINX controller can be
configured with a <a href="https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#what-is-the-flag-watch-ingress-without-class">flag</a>
<code>--watch-ingress-without-class</code>. It is <a href="https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#i-have-only-one-ingress-controller-in-my-cluster-what-should-i-do">recommended</a> though, to specify the
default <code>IngressClass</code> as shown <a href="#default-ingress-class">below</a>.</p><h3 id="ingress-rules">Ingress rules</h3><p>Each HTTP rule contains the following information:</p><ul><li>An optional host. In this example, no host is specified, so the rule applies to all inbound
HTTP traffic through the IP address specified. If a host is provided (for example,
foo.bar.com), the rules apply to that host.</li><li>A list of paths (for example, <code>/testpath</code>), each of which has an associated
backend defined with a <code>service.name</code> and a <code>service.port.name</code> or
<code>service.port.number</code>. Both the host and path must match the content of an
incoming request before the load balancer directs traffic to the referenced
Service.</li><li>A backend is a combination of Service and port names as described in the
<a href="/docs/concepts/services-networking/service/">Service doc</a> or a <a href="#resource-backend">custom resource backend</a>
by way of a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." data-toggle="tooltip" data-placement="top" href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank" aria-label="CRD">CRD</a>. HTTP (and HTTPS) requests to the
Ingress that match the host and path of the rule are sent to the listed backend.</li></ul><p>A <code>defaultBackend</code> is often configured in an Ingress controller to service any requests that do not
match a path in the spec.</p><h3 id="default-backend">DefaultBackend</h3><p>An Ingress with no rules sends all traffic to a single default backend and <code>.spec.defaultBackend</code>
is the backend that should handle requests in that case.
The <code>defaultBackend</code> is conventionally a configuration option of the
<a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a> and
is not specified in your Ingress resources.
If no <code>.spec.rules</code> are specified, <code>.spec.defaultBackend</code> must be specified.
If <code>defaultBackend</code> is not set, the handling of requests that do not match any of the rules will be up to the
ingress controller (consult the documentation for your ingress controller to find out how it handles this case).</p><p>If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is
routed to your default backend.</p><h3 id="resource-backend">Resource backends</h3><p>A <code>Resource</code> backend is an ObjectRef to another Kubernetes resource within the
same namespace as the Ingress object. A <code>Resource</code> is a mutually exclusive
setting with Service, and will fail validation if both are specified. A common
usage for a <code>Resource</code> backend is to ingress data to an object storage backend
with static assets.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/ingress-resource-backend.yaml" download="service/networking/ingress-resource-backend.yaml"><code>service/networking/ingress-resource-backend.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-ingress-resource-backend-yaml&quot;)" title="Copy service/networking/ingress-resource-backend.yaml to clipboard"/></div><div class="includecode" id="service-networking-ingress-resource-backend-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ingress-resource-backend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">defaultBackend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageBucket<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>static-assets<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/icons<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>ImplementationSpecific<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">resource</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StorageBucket<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>icon-assets<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>After creating the Ingress above, you can view it with the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl describe ingress ingress-resource-backend
</span></span></code></pre></div><pre tabindex="0"><code>Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &lt;none&gt;
Events:       &lt;none&gt;
</code></pre><h3 id="path-types">Path types</h3><p>Each path in an Ingress is required to have a corresponding path type. Paths
that do not include an explicit <code>pathType</code> will fail validation. There are three
supported path types:</p><ul><li><p><code>ImplementationSpecific</code>: With this path type, matching is up to the
IngressClass. Implementations can treat this as a separate <code>pathType</code> or treat
it identically to <code>Prefix</code> or <code>Exact</code> path types.</p></li><li><p><code>Exact</code>: Matches the URL path exactly and with case sensitivity.</p></li><li><p><code>Prefix</code>: Matches based on a URL path prefix split by <code>/</code>. Matching is case
sensitive and done on a path element by element basis. A path element refers
to the list of labels in the path split by the <code>/</code> separator. A request is a
match for path <em>p</em> if every <em>p</em> is an element-wise prefix of <em>p</em> of the
request path.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If the last element of the path is a substring of the last
element in request path, it is not a match (for example: <code>/foo/bar</code>
matches <code>/foo/bar/baz</code>, but does not match <code>/foo/barbaz</code>).</div></li></ul><h3 id="examples">Examples</h3><table><thead><tr><th>Kind</th><th>Path(s)</th><th>Request path(s)</th><th>Matches?</th></tr></thead><tbody><tr><td>Prefix</td><td><code>/</code></td><td>(all paths)</td><td>Yes</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/foo</code></td><td>Yes</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/bar</code></td><td>No</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/foo/</code></td><td>No</td></tr><tr><td>Exact</td><td><code>/foo/</code></td><td><code>/foo</code></td><td>No</td></tr><tr><td>Prefix</td><td><code>/foo</code></td><td><code>/foo</code>, <code>/foo/</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/foo/</code></td><td><code>/foo</code>, <code>/foo/</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/aaa/bb</code></td><td><code>/aaa/bbb</code></td><td>No</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb/</code></td><td><code>/aaa/bbb</code></td><td>Yes, ignores trailing slash</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb/</code></td><td>Yes, matches trailing slash</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb/ccc</code></td><td>Yes, matches subpath</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbbxyz</code></td><td>No, does not match string prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code></td><td><code>/aaa/ccc</code></td><td>Yes, matches <code>/aaa</code> prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td><td><code>/aaa/bbb</code></td><td>Yes, matches <code>/aaa/bbb</code> prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td><td><code>/ccc</code></td><td>Yes, matches <code>/</code> prefix</td></tr><tr><td>Prefix</td><td><code>/aaa</code></td><td><code>/ccc</code></td><td>No, uses default backend</td></tr><tr><td>Mixed</td><td><code>/foo</code> (Prefix), <code>/foo</code> (Exact)</td><td><code>/foo</code></td><td>Yes, prefers Exact</td></tr></tbody></table><h4 id="multiple-matches">Multiple matches</h4><p>In some cases, multiple paths within an Ingress will match a request. In those
cases precedence will be given first to the longest matching path. If two paths
are still equally matched, precedence will be given to paths with an exact path
type over prefix path type.</p><h2 id="hostname-wildcards">Hostname wildcards</h2><p>Hosts can be precise matches (for example â€œ<code>foo.bar.com</code>â€) or a wildcard (for
example â€œ<code>*.foo.com</code>â€). Precise matches require that the HTTP <code>host</code> header
matches the <code>host</code> field. Wildcard matches require the HTTP <code>host</code> header is
equal to the suffix of the wildcard rule.</p><table><thead><tr><th>Host</th><th>Host header</th><th>Match?</th></tr></thead><tbody><tr><td><code>*.foo.com</code></td><td><code>bar.foo.com</code></td><td>Matches based on shared suffix</td></tr><tr><td><code>*.foo.com</code></td><td><code>baz.bar.foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr><tr><td><code>*.foo.com</code></td><td><code>foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr></tbody></table><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/ingress-wildcard-host.yaml" download="service/networking/ingress-wildcard-host.yaml"><code>service/networking/ingress-wildcard-host.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-ingress-wildcard-host-yaml&quot;)" title="Copy service/networking/ingress-wildcard-host.yaml to clipboard"/></div><div class="includecode" id="service-networking-ingress-wildcard-host-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ingress-wildcard-host<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span><span style="color:#b44">"foo.bar.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/bar"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span><span style="color:#b44">"*.foo.com"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/foo"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="ingress-class">Ingress class</h2><p>Ingresses can be implemented by different controllers, often with different
configuration. Each Ingress should specify a class, a reference to an
IngressClass resource that contains additional configuration including the name
of the controller that should implement the class.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/external-lb.yaml" download="service/networking/external-lb.yaml"><code>service/networking/external-lb.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-external-lb-yaml&quot;)" title="Copy service/networking/external-lb.yaml to clipboard"/></div><div class="includecode" id="service-networking-external-lb-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>external-lb<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">controller</span>:<span style="color:#bbb"> </span>example.com/ingress-controller<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>IngressParameters<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>external-lb<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The <code>.spec.parameters</code> field of an IngressClass lets you reference another
resource that provides configuration related to that IngressClass.</p><p>The specific type of parameters to use depends on the ingress controller
that you specify in the <code>.spec.controller</code> field of the IngressClass.</p><h3 id="ingressclass-scope">IngressClass scope</h3><p>Depending on your ingress controller, you may be able to use parameters
that you set cluster-wide, or just for one namespace.</p><ul class="nav nav-tabs" id="tabs-ingressclass-parameter-scope" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabs-ingressclass-parameter-scope-0" role="tab" aria-controls="tabs-ingressclass-parameter-scope-0" aria-selected="true">Cluster</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabs-ingressclass-parameter-scope-1" role="tab" aria-controls="tabs-ingressclass-parameter-scope-1">Namespaced</a></li></ul><div class="tab-content" id="tabs-ingressclass-parameter-scope"><div id="tabs-ingressclass-parameter-scope-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabs-ingressclass-parameter-scope-0"><p><p>The default scope for IngressClass parameters is cluster-wide.</p><p>If you set the <code>.spec.parameters</code> field and don't set
<code>.spec.parameters.scope</code>, or if you set <code>.spec.parameters.scope</code> to
<code>Cluster</code>, then the IngressClass refers to a cluster-scoped resource.
The <code>kind</code> (in combination the <code>apiGroup</code>) of the parameters
refers to a cluster-scoped API (possibly a custom resource), and
the <code>name</code> of the parameters identifies a specific cluster scoped
resource for that API.</p><p>For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>external-lb-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">controller</span>:<span style="color:#bbb"> </span>example.com/ingress-controller<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># The parameters for this IngressClass are specified in a</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ClusterIngressParameter (API group k8s.example.net) named</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># "external-config-1". This definition tells Kubernetes to</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># look for a cluster-scoped parameter resource.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">scope</span>:<span style="color:#bbb"> </span>Cluster<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.net<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterIngressParameter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>external-config-1<span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="tabs-ingressclass-parameter-scope-1" class="tab-pane" role="tabpanel" aria-labelledby="tabs-ingressclass-parameter-scope-1"><p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>If you set the <code>.spec.parameters</code> field and set
<code>.spec.parameters.scope</code> to <code>Namespace</code>, then the IngressClass refers
to a namespaced-scoped resource. You must also set the <code>namespace</code>
field within <code>.spec.parameters</code> to the namespace that contains
the parameters you want to use.</p><p>The <code>kind</code> (in combination the <code>apiGroup</code>) of the parameters
refers to a namespaced API (for example: ConfigMap), and
the <code>name</code> of the parameters identifies a specific resource
in the namespace you specified in <code>namespace</code>.</p><p>Namespace-scoped parameters help the cluster operator delegate control over the
configuration (for example: load balancer settings, API gateway definition)
that is used for a workload. If you used a cluster-scoped parameter then either:</p><ul><li>the cluster operator team needs to approve a different team's changes every
time there's a new configuration change being applied.</li><li>the cluster operator must define specific access controls, such as
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> roles and bindings, that let
the application team make changes to the cluster-scoped parameters resource.</li></ul><p>The IngressClass API itself is always cluster-scoped.</p><p>Here is an example of an IngressClass that refers to parameters that are
namespaced:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>external-lb-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">controller</span>:<span style="color:#bbb"> </span>example.com/ingress-controller<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parameters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># The parameters for this IngressClass are specified in an</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># IngressParameter (API group k8s.example.com) named "external-config",</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># that's in the "external-configuration" namespace.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">scope</span>:<span style="color:#bbb"> </span>Namespace<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>IngressParameter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>external-configuration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>external-config<span style="color:#bbb">
</span></span></span></code></pre></div></p></div></div><h3 id="deprecated-annotation">Deprecated annotation</h3><p>Before the IngressClass resource and <code>ingressClassName</code> field were added in
Kubernetes 1.18, Ingress classes were specified with a
<code>kubernetes.io/ingress.class</code> annotation on the Ingress. This annotation was
never formally defined, but was widely supported by Ingress controllers.</p><p>The newer <code>ingressClassName</code> field on Ingresses is a replacement for that
annotation, but is not a direct equivalent. While the annotation was generally
used to reference the name of the Ingress controller that should implement the
Ingress, the field is a reference to an IngressClass resource that contains
additional Ingress configuration, including the name of the Ingress controller.</p><h3 id="default-ingress-class">Default IngressClass</h3><p>You can mark a particular IngressClass as default for your cluster. Setting the
<code>ingressclass.kubernetes.io/is-default-class</code> annotation to <code>true</code> on an
IngressClass resource will ensure that new Ingresses without an
<code>ingressClassName</code> field specified will be assigned this default IngressClass.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>If you have more than one IngressClass marked as the default for your cluster,
the admission controller prevents creating new Ingress objects that don't have
an <code>ingressClassName</code> specified. You can resolve this by ensuring that at most 1
IngressClass is marked as default in your cluster.</div><p>There are some ingress controllers, that work without the definition of a
default <code>IngressClass</code>. For example, the Ingress-NGINX controller can be
configured with a <a href="https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class">flag</a>
<code>--watch-ingress-without-class</code>. It is <a href="https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do">recommended</a> though, to specify the
default <code>IngressClass</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/default-ingressclass.yaml" download="service/networking/default-ingressclass.yaml"><code>service/networking/default-ingressclass.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-default-ingressclass-yaml&quot;)" title="Copy service/networking/default-ingressclass.yaml to clipboard"/></div><div class="includecode" id="service-networking-default-ingressclass-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>controller<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ingressclass.kubernetes.io/is-default-class</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">controller</span>:<span style="color:#bbb"> </span>k8s.io/ingress-nginx<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="types-of-ingress">Types of Ingress</h2><h3 id="single-service-ingress">Ingress backed by a single Service</h3><p>There are existing Kubernetes concepts that allow you to expose a single Service
(see <a href="#alternatives">alternatives</a>). You can also do this with an Ingress by specifying a
<em>default backend</em> with no rules.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/test-ingress.yaml" download="service/networking/test-ingress.yaml"><code>service/networking/test-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-test-ingress-yaml&quot;)" title="Copy service/networking/test-ingress.yaml to clipboard"/></div><div class="includecode" id="service-networking-test-ingress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test-ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">defaultBackend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>If you create it using <code>kubectl apply -f</code> you should be able to view the state
of the Ingress you added:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get ingress test-ingress
</span></span></code></pre></div><pre tabindex="0"><code>NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
</code></pre><p>Where <code>203.0.113.123</code> is the IP allocated by the Ingress controller to satisfy
this Ingress.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Ingress controllers and load balancers may take a minute or two to allocate an IP address.
Until that time, you often see the address listed as <code>&lt;pending&gt;</code>.</div><h3 id="simple-fanout">Simple fanout</h3><p>A fanout configuration routes traffic from a single IP address to more than one Service,
based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers
down to a minimum. For example, a setup like:</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNUslOwzAQ_RXLvYCUhMQpUFzUUzkgcUBwbHpw4klr4diR7bCo8O8k2FFbFomLPZq3jP00O1xpDpjijWHtFt09zAuFUCUFKHey8vf6NE7QrdoYsDZumGIb4Oi6NAskNeOoZJKpCgxK4oXwrFVgRyi7nCVXWZKRPMlysv5yD6Q4Xryf1Vq_WzDPooJs9egLNDbolKTpT03JzKgh3zWEztJZ0Niu9L-qZGcdmAMfj4cxvWmreba613z9C0B-AMQD-V_AdA-A4j5QZu0SatRKJhSqhZR0wjmPrDP6CeikrutQxy-Cuy2dtq9RpaU2dJKm6fzI5Glmg0VOLio4_5dLjx27hFSC015KJ2VZHtuQvY2fuHcaE43G0MaCREOow_FV5cMxHZ5-oPX75UM5avuXhXuOI9yAaZjg_aLuBl6B3RYaKDDtSw4166QrcKE-emrXcubghgunDaY1kxYizDqnH99UhakzHYykpWD9hjS--fEJoIELqQ"><img src="/docs/images/ingressFanOut.svg" alt="ingress-fanout-diagram"/></a><figcaption><p>Figure. Ingress Fan Out</p></figcaption></figure><p>It would require an Ingress such as:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/simple-fanout-example.yaml" download="service/networking/simple-fanout-example.yaml"><code>service/networking/simple-fanout-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-simple-fanout-example-yaml&quot;)" title="Copy service/networking/simple-fanout-example.yaml to clipboard"/></div><div class="includecode" id="service-networking-simple-fanout-example-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>simple-fanout-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>foo.bar.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">4200</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/bar<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>When you create the Ingress with <code>kubectl apply -f</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe ingress simple-fanout-example
</span></span></code></pre></div><pre tabindex="0"><code>Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
</code></pre><p>The Ingress controller provisions an implementation-specific load balancer
that satisfies the Ingress, as long as the Services (<code>service1</code>, <code>service2</code>) exist.
When it has done so, you can see the address of the load balancer at the
Address field.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Depending on the <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
you are using, you may need to create a default-http-backend
<a href="/docs/concepts/services-networking/service/">Service</a>.</div><h3 id="name-based-virtual-hosting">Name based virtual hosting</h3><p>Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNkl9PwyAUxb8KYS-atM1Kp05m9qSJJj4Y97jugcLtRqTQAPVPdN_dVlq3qUt8gZt7zvkBN7xjbgRgiteW1Rt0_zjLNUJcSdD-ZBn21WmcoDu9tuBcXDHN1iDQVWHnSBkmUMEU0xwsSuK5DK5l745QejFNLtMkJVmSZmT1Re9NcTz_uDXOU1QakxTMJtxUHw7ss-SQLhehQEODTsdH4l20Q-zFyc84-Y67pghv5apxHuweMuj9eS2_NiJdPhix-kMgvwQShOyYMNkJoEUYM3PuGkpUKyY1KqVSdCSEiJy35gnoqCzLvo5fpPAbOqlfI26UsXQ0Ho9nB5CnqesRGTnncPYvSqsdUvqp9KRdlI6KojjEkB0mnLgjDRONhqENBYm6oXbLV5V1y6S7-l42_LowlIN2uFm_twqOcAW2YlK0H_i9c-bYb6CCHNO2FFCyRvkc53rbWptaMA83QnpjMS2ZchBh1nizeNMcU28bGEzXkrV_pArN7Sc0rBTu"><img src="/docs/images/ingressNameBased.svg" alt="ingress-namebase-diagram"/></a><figcaption><p>Figure. Ingress Name Based Virtual hosting</p></figcaption></figure><p>The following Ingress tells the backing load balancer to route requests based on
the <a href="https://tools.ietf.org/html/rfc7230#section-5.4">Host header</a>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/name-virtual-host-ingress.yaml" download="service/networking/name-virtual-host-ingress.yaml"><code>service/networking/name-virtual-host-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-name-virtual-host-ingress-yaml&quot;)" title="Copy service/networking/name-virtual-host-ingress.yaml to clipboard"/></div><div class="includecode" id="service-networking-name-virtual-host-ingress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>name-virtual-host-ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>foo.bar.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>bar.foo.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.</p><p>For example, the following Ingress routes traffic
requested for <code>first.bar.com</code> to <code>service1</code>, <code>second.bar.com</code> to <code>service2</code>,
and any traffic whose request host header doesn't match <code>first.bar.com</code>
and <code>second.bar.com</code> to <code>service3</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml" download="service/networking/name-virtual-host-ingress-no-third-host.yaml"><code>service/networking/name-virtual-host-ingress-no-third-host.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-name-virtual-host-ingress-no-third-host-yaml&quot;)" title="Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard"/></div><div class="includecode" id="service-networking-name-virtual-host-ingress-no-third-host-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>name-virtual-host-ingress-no-third-host<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>first.bar.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>second.bar.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h3 id="tls">TLS</h3><p>You can secure an Ingress by specifying a <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a>
that contains a TLS private key and certificate. The Ingress resource only
supports a single TLS port, 443, and assumes TLS termination at the ingress point
(traffic to the Service and its Pods is in plaintext).
If the TLS configuration section in an Ingress specifies different hosts, they are
multiplexed on the same port according to the hostname specified through the
SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret
must contain keys named <code>tls.crt</code> and <code>tls.key</code> that contain the certificate
and private key to use for TLS. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>testsecret-tls<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tls.crt</span>:<span style="color:#bbb"> </span>base64 encoded cert<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tls.key</span>:<span style="color:#bbb"> </span>base64 encoded key<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>kubernetes.io/tls<span style="color:#bbb">
</span></span></span></code></pre></div><p>Referencing this secret in an Ingress tells the Ingress controller to
secure the channel from the client to the load balancer using TLS. You need to make
sure the TLS secret you created came from a certificate that contains a Common
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for <code>https-example.foo.com</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Keep in mind that TLS will not work on the default rule because the
certificates would have to be issued for all the possible sub-domains. Therefore,
<code>hosts</code> in the <code>tls</code> section need to explicitly match the <code>host</code> in the <code>rules</code>
section.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/tls-example-ingress.yaml" download="service/networking/tls-example-ingress.yaml"><code>service/networking/tls-example-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;service-networking-tls-example-ingress-yaml&quot;)" title="Copy service/networking/tls-example-ingress.yaml to clipboard"/></div><div class="includecode" id="service-networking-tls-example-ingress-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>tls-example-ingress<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tls</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">hosts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- https-example.foo.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">secretName</span>:<span style="color:#bbb"> </span>testsecret-tls<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>https-example.foo.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There is a gap between TLS features supported by various Ingress
controllers. Please refer to documentation on
<a href="https://kubernetes.github.io/ingress-nginx/user-guide/tls/">nginx</a>,
<a href="https://git.k8s.io/ingress-gce/README.md#frontend-https">GCE</a>, or any other
platform specific Ingress controller to understand how TLS works in your environment.</div><h3 id="load-balancing">Load balancing</h3><p>An Ingress controller is bootstrapped with some load balancing policy settings
that it applies to all Ingress, such as the load balancing algorithm, backend
weight scheme, and others. More advanced load balancing concepts
(e.g. persistent sessions, dynamic weights) are not yet exposed through the
Ingress. You can instead get these features through the load balancer used for
a Service.</p><p>It's also worth noting that even though health checks are not exposed directly
through the Ingress, there exist parallel concepts in Kubernetes such as
<a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">readiness probes</a>
that allow you to achieve the same end result. Please review the controller
specific documentation to see how they handle health checks (for example:
<a href="https://git.k8s.io/ingress-nginx/README.md">nginx</a>, or
<a href="https://git.k8s.io/ingress-gce/README.md#health-checks">GCE</a>).</p><h2 id="updating-an-ingress">Updating an Ingress</h2><p>To update an existing Ingress to add a new Host, you can update it by editing the resource:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe ingress <span style="color:#a2f">test</span>
</span></span></code></pre></div><pre tabindex="0"><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit ingress <span style="color:#a2f">test</span>
</span></span></code></pre></div><p>This pops up an editor with the existing configuration in YAML format.
Modify it to include the new Host:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>foo.bar.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">host</span>:<span style="color:#bbb"> </span>bar.baz.com<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">http</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">paths</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">backend</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>..<span style="color:#bbb">
</span></span></span></code></pre></div><p>After you save your changes, kubectl updates the resource in the API server, which tells the
Ingress controller to reconfigure the load balancer.</p><p>Verify this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe ingress <span style="color:#a2f">test</span>
</span></span></code></pre></div><pre tabindex="0"><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
</code></pre><p>You can achieve the same outcome by invoking <code>kubectl replace -f</code> on a modified Ingress YAML file.</p><h2 id="failing-across-availability-zones">Failing across availability zones</h2><p>Techniques for spreading traffic across failure domains differ between cloud providers.
Please check the documentation of the relevant <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a> for details.</p><h2 id="alternatives">Alternatives</h2><p>You can expose a Service in multiple ways that don't directly involve the Ingress resource:</p><ul><li>Use <a href="/docs/concepts/services-networking/service/#loadbalancer">Service.Type=LoadBalancer</a></li><li>Use <a href="/docs/concepts/services-networking/service/#type-nodeport">Service.Type=NodePort</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn about the <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress</a> API</li><li>Learn about <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controllers</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Kubernetes API Server Bypass Risks</h1><div class="lead">Security architecture information relating to the API server and other components</div><p>The Kubernetes API server is the main point of entry to a cluster for external parties
(users and services) interacting with it.</p><p>As part of this role, the API server has several key built-in security controls, such as
audit logging and <a class="glossary-tooltip" title="A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object." data-toggle="tooltip" data-placement="top" href="/docs/reference/access-authn-authz/admission-controllers/" target="_blank" aria-label="admission controllers">admission controllers</a>.
However, there are ways to modify the configuration
or content of the cluster that bypass these controls.</p><p>This page describes the ways in which the security controls built into the
Kubernetes API server can be bypassed, so that cluster operators
and security architects can ensure that these bypasses are appropriately restricted.</p><h2 id="static-pods">Static Pods</h2><p>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> on each node loads and
directly manages any manifests that are stored in a named directory or fetched from
a specific URL as <a href="/docs/tasks/configure-pod-container/static-pod/"><em>static Pods</em></a> in
your cluster. The API server doesn't manage these static Pods. An attacker with write
access to this location could modify the configuration of static pods loaded from that
source, or could introduce new static Pods.</p><p>Static Pods are restricted from accessing other objects in the Kubernetes API. For example,
you can't configure a static Pod to mount a Secret from the cluster. However, these Pods can
take other security sensitive actions, such as using <code>hostPath</code> mounts from the underlying
node.</p><p>By default, the kubelet creates a <a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank" aria-label="mirror pod">mirror pod</a>
so that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalid
namespace name when creating the Pod, it will not be visible in the Kubernetes API and can only
be discovered by tooling that has access to the affected host(s).</p><p>If a static Pod fails admission control, the kubelet won't register the Pod with the
API server. However, the Pod still runs on the node. For more information, refer to
<a href="https://github.com/kubernetes/kubeadm/issues/1541#issuecomment-487331701">kubeadm issue #1541</a>.</p><h3 id="static-pods-mitigations">Mitigations</h3><ul><li>Only <a href="/docs/tasks/configure-pod-container/static-pod/#static-pod-creation">enable the kubelet static Pod manifest functionality</a>
if required by the node.</li><li>If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory
or URL to users who need the access.</li><li>Restrict access to kubelet configuration parameters and files to prevent an attacker setting
a static Pod path or URL.</li><li>Regularly audit and centrally report all access to directories or web storage locations that host
static Pod manifests and kubelet configuration files.</li></ul><h2 id="kubelet-api">The kubelet API</h2><p>The kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster
worker nodes. The API might also be exposed on control plane nodes depending on the Kubernetes
distribution in use. Direct access to the API allows for disclosure of information about
the pods running on a node, the logs from those pods, and execution of commands in
every container running on the node.</p><p>When Kubernetes cluster users have RBAC access to <code>Node</code> object sub-resources, that access
serves as authorization to interact with the kubelet API. The exact access depends on
which sub-resource access has been granted, as detailed in
<a href="/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization">kubelet authorization</a>.</p><p>Direct access to the kubelet API is not subject to admission control and is not logged
by Kubernetes audit logging. An attacker with direct access to this API may be able to
bypass controls that detect or prevent certain actions.</p><p>The kubelet API can be configured to authenticate requests in a number of ways.
By default, the kubelet configuration allows anonymous access. Most Kubernetes providers
change the default to use webhook and certificate authentication. This lets the control plane
ensure that the caller is authorized to access the <code>nodes</code> API resource or sub-resources.
The default anonymous access doesn't make this assertion with the control plane.</p><h3 id="mitigations">Mitigations</h3><ul><li>Restrict access to sub-resources of the <code>nodes</code> API object using mechanisms such as
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>. Only grant this access when required,
such as by monitoring services.</li><li>Restrict access to the kubelet port. Only allow specified and trusted IP address
ranges to access the port.</li><li>Ensure that <a href="/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication">kubelet authentication</a>.
is set to webhook or certificate mode.</li><li>Ensure that the unauthenticated "read-only" Kubelet port is not enabled on the cluster.</li></ul><h2 id="the-etcd-api">The etcd API</h2><p>Kubernetes clusters use etcd as a datastore. The <code>etcd</code> service listens on TCP port 2379.
The only clients that need access are the Kubernetes API server and any backup tooling
that you use. Direct access to this API allows for disclosure or modification of any
data held in the cluster.</p><p>Access to the etcd API is typically managed by client certificate authentication.
Any certificate issued by a certificate authority that etcd trusts allows full access
to the data stored inside etcd.</p><p>Direct access to etcd is not subject to Kubernetes admission control and is not logged
by Kubernetes audit logging. An attacker who has read access to the API server's
etcd client certificate private key (or can create a new trusted client certificate) can gain
cluster admin rights by accessing cluster secrets or modifying access rules. Even without
elevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object
or create new workloads inside the cluster.</p><p>Many Kubernetes providers configure
etcd to use mutual TLS (both client and server verify each other's certificate for authentication).
There is no widely accepted implementation of authorization for the etcd API, although
the feature exists. Since there is no authorization model, any certificate
with client access to etcd can be used to gain full access to etcd. Typically, etcd client certificates
that are only used for health checking can also grant full read and write access.</p><h3 id="etcd-api-mitigations">Mitigations</h3><ul><li>Ensure that the certificate authority trusted by etcd is used only for the purposes of
authentication to that service.</li><li>Control access to the private key for the etcd server certificate, and to the API server's
client certificate and key.</li><li>Consider restricting access to the etcd port at a network level, to only allow access
from specified and trusted IP address ranges.</li></ul><h2 id="runtime-socket">Container runtime socket</h2><p>On each node in a Kubernetes cluster, access to interact with containers is controlled
by the container runtime (or runtimes, if you have configured more than one). Typically,
the container runtime exposes a Unix socket that the kubelet can access. An attacker with
access to this socket can launch new containers or interact with running containers.</p><p>At the cluster level, the impact of this access depends on whether the containers that
run on the compromised node have access to Secrets or other confidential
data that an attacker could use to escalate privileges to other worker nodes or to
control plane components.</p><h3 id="runtime-socket-mitigations">Mitigations</h3><ul><li>Ensure that you tightly control filesystem access to container runtime sockets.
When possible, restrict this access to the <code>root</code> user.</li><li>Isolate the kubelet from other components running on the node, using
mechanisms such as Linux kernel namespaces.</li><li>Ensure that you restrict or forbid the use of <a href="/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code> mounts</a>
that include the container runtime socket, either directly or by mounting a parent
directory. Also <code>hostPath</code> mounts must be set as read-only to mitigate risks
of attackers bypassing directory restrictions.</li><li>Restrict user access to nodes, and especially restrict superuser access to nodes.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Security Admission</h1><div class="lead">An overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>The Kubernetes <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> define
different isolation levels for Pods. These standards let you define how you want to restrict the
behavior of pods in a clear, consistent fashion.</p><p>Kubernetes offers a built-in <em>Pod Security</em> <a class="glossary-tooltip" title="A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object." data-toggle="tooltip" data-placement="top" href="/docs/reference/access-authn-authz/admission-controllers/" target="_blank" aria-label="admission controller">admission controller</a> to enforce the Pod Security Standards. Pod security restrictions
are applied at the <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a> level when pods are
created.</p><h3 id="built-in-pod-security-admission-enforcement">Built-in Pod Security admission enforcement</h3><p>This page is part of the documentation for Kubernetes v1.34.
If you are running a different version of Kubernetes, consult the documentation for that release.</p><h2 id="pod-security-levels">Pod Security levels</h2><p>Pod Security admission places requirements on a Pod's <a href="/docs/tasks/configure-pod-container/security-context/">Security
Context</a> and other related fields according
to the three levels defined by the <a href="/docs/concepts/security/pod-security-standards/">Pod Security
Standards</a>: <code>privileged</code>, <code>baseline</code>, and
<code>restricted</code>. Refer to the <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>
page for an in-depth look at those requirements.</p><h2 id="pod-security-admission-labels-for-namespaces">Pod Security Admission labels for namespaces</h2><p>Once the feature is enabled or the webhook is installed, you can configure namespaces to define the admission
control mode you want to use for pod security in each namespace. Kubernetes defines a set of
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="labels">labels</a> that you can set to define which of the
predefined Pod Security Standard levels you want to use for a namespace. The label you select
defines what action the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a>
takes if a potential violation is detected:</p><table><caption style="display:none">Pod Security Admission modes</caption><thead><tr><th style="text-align:left">Mode</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><strong>enforce</strong></td><td style="text-align:left">Policy violations will cause the pod to be rejected.</td></tr><tr><td style="text-align:left"><strong>audit</strong></td><td style="text-align:left">Policy violations will trigger the addition of an audit annotation to the event recorded in the <a href="/docs/tasks/debug/debug-cluster/audit/">audit log</a>, but are otherwise allowed.</td></tr><tr><td style="text-align:left"><strong>warn</strong></td><td style="text-align:left">Policy violations will trigger a user-facing warning, but are otherwise allowed.</td></tr></tbody></table><p>A namespace can configure any or all modes, or even set a different level for different modes.</p><p>For each mode, there are two labels that determine the policy used:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># The per-mode level label indicates which policy level to apply for the mode.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># MODE must be one of `enforce`, `audit`, or `warn`.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># LEVEL must be one of `privileged`, `baseline`, or `restricted`.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">pod-security.kubernetes.io/&lt;MODE&gt;</span>:<span style="color:#bbb"> </span>&lt;LEVEL&gt;<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># Optional: per-mode version label that can be used to pin the policy to the</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># version that shipped with a given Kubernetes minor version (for example v1.34).</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># MODE must be one of `enforce`, `audit`, or `warn`.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># VERSION must be a valid Kubernetes minor version, or `latest`.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">pod-security.kubernetes.io/&lt;MODE&gt;-version</span>:<span style="color:#bbb"> </span>&lt;VERSION&gt;<span style="color:#bbb">
</span></span></span></code></pre></div><p>Check out <a href="/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels</a> to see example usage.</p><h2 id="workload-resources-and-pod-templates">Workload resources and Pod templates</h2><p>Pods are often created indirectly, by creating a <a href="/docs/concepts/workloads/controllers/">workload
object</a> such as a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a> or <a class="glossary-tooltip" title="A finite or batch task that runs to completion." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/job/" target="_blank" aria-label="Job">Job</a>. The workload object defines a
<em>Pod template</em> and a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> for the
workload resource creates Pods based on that template. To help catch violations early, both the
audit and warning modes are applied to the workload resources. However, enforce mode is <strong>not</strong>
applied to workload resources, only to the resulting pod objects.</p><h2 id="exemptions">Exemptions</h2><p>You can define <em>exemptions</em> from pod security enforcement in order to allow the creation of pods that
would have otherwise been prohibited due to the policy associated with a given namespace.
Exemptions can be statically configured in the
<a href="/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller">Admission Controller configuration</a>.</p><p>Exemptions must be explicitly enumerated. Requests meeting exemption criteria are <em>ignored</em> by the
Admission Controller (all <code>enforce</code>, <code>audit</code> and <code>warn</code> behaviors are skipped). Exemption dimensions include:</p><ul><li><strong>Usernames:</strong> requests from users with an exempt authenticated (or impersonated) username are
ignored.</li><li><strong>RuntimeClassNames:</strong> pods and <a href="#workload-resources-and-pod-templates">workload resources</a> specifying an exempt runtime class name are
ignored.</li><li><strong>Namespaces:</strong> pods and <a href="#workload-resources-and-pod-templates">workload resources</a> in an exempt namespace are ignored.</li></ul><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Most pods are created by a controller in response to a <a href="#workload-resources-and-pod-templates">workload
resource</a>, meaning that exempting an end user will only
exempt them from enforcement when creating pods directly, but not when creating a workload resource.
Controller service accounts (such as <code>system:serviceaccount:kube-system:replicaset-controller</code>)
should generally not be exempted, as doing so would implicitly exempt any user that can create the
corresponding workload resource.</div><p>Updates to the following pod fields are exempt from policy checks, meaning that if a pod update
request only changes these fields, it will not be denied even if the pod is in violation of the
current policy level:</p><ul><li>Any metadata updates <strong>except</strong> changes to the seccomp or AppArmor annotations:<ul><li><code>seccomp.security.alpha.kubernetes.io/pod</code> (deprecated)</li><li><code>container.seccomp.security.alpha.kubernetes.io/*</code> (deprecated)</li><li><code>container.apparmor.security.beta.kubernetes.io/*</code> (deprecated)</li></ul></li><li>Valid updates to <code>.spec.activeDeadlineSeconds</code></li><li>Valid updates to <code>.spec.tolerations</code></li></ul><h2 id="metrics">Metrics</h2><p>Here are the Prometheus metrics exposed by kube-apiserver:</p><ul><li><code>pod_security_errors_total</code>: This metric indicates the number of errors preventing normal evaluation.
Non-fatal errors may result in the latest restricted profile being used for enforcement.</li><li><code>pod_security_evaluations_total</code>: This metric indicates the number of policy evaluations that have occurred,
not counting ignored or exempt requests during exporting.</li><li><code>pod_security_exemptions_total</code>: This metric indicates the number of exempt requests, not counting ignored
or out of scope requests.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a></li><li><a href="/docs/setup/best-practices/enforcing-pod-security-standards/">Enforcing Pod Security Standards</a></li><li><a href="/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">Enforce Pod Security Standards by Configuring the Built-in Admission Controller</a></li><li><a href="/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels</a></li></ul><p>If you are running an older version of Kubernetes and want to upgrade
to a version of Kubernetes that does not include PodSecurityPolicies,
read <a href="/docs/tasks/configure-pod-container/migrate-from-psp/">migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.</p></div>