<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Runtime Class</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>This page describes the RuntimeClass resource and runtime selection mechanism.</p><p>RuntimeClass is a feature for selecting the container runtime configuration. The container runtime
configuration is used to run a Pod's containers.</p><h2 id="motivation">Motivation</h2><p>You can set a different RuntimeClass between different Pods to provide a balance of
performance versus security. For example, if part of your workload deserves a high
level of information security assurance, you might choose to schedule those Pods so
that they run in a container runtime that uses hardware virtualization. You'd then
benefit from the extra isolation of the alternative runtime, at the expense of some
additional overhead.</p><p>You can also use RuntimeClass to run different Pods with the same container runtime
but with different settings.</p><h2 id="setup">Setup</h2><ol><li>Configure the CRI implementation on nodes (runtime dependent)</li><li>Create the corresponding RuntimeClass resources</li></ol><h3 id="1-configure-the-cri-implementation-on-nodes">1. Configure the CRI implementation on nodes</h3><p>The configurations available through RuntimeClass are Container Runtime Interface (CRI)
implementation dependent. See the corresponding documentation (<a href="#cri-configuration">below</a>) for your
CRI implementation for how to configure.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means
that all nodes are configured the same way with respect to container runtimes). To support
heterogeneous node configurations, see <a href="#scheduling">Scheduling</a> below.</div><p>The configurations have a corresponding <code>handler</code> name, referenced by the RuntimeClass. The
handler must be a valid <a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label name</a>.</p><h3 id="2-create-the-corresponding-runtimeclass-resources">2. Create the corresponding RuntimeClass resources</h3><p>The configurations setup in step 1 should each have an associated <code>handler</code> name, which identifies
the configuration. For each handler, create a corresponding RuntimeClass object.</p><p>The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name
(<code>metadata.name</code>) and the handler (<code>handler</code>). The object definition looks like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># RuntimeClass is defined in the node.k8s.io API group</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>node.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>RuntimeClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># The name the RuntimeClass will be referenced by.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># RuntimeClass is a non-namespaced resource.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myclass <span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># The name of the corresponding CRI configuration</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">handler</span>:<span style="color:#bbb"> </span>myconfiguration <span style="color:#bbb">
</span></span></span></code></pre></div><p>The name of a RuntimeClass object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>It is recommended that RuntimeClass write operations (create/update/patch/delete) be
restricted to the cluster administrator. This is typically the default. See
<a href="/docs/reference/access-authn-authz/authorization/">Authorization Overview</a> for more details.</div><h2 id="usage">Usage</h2><p>Once RuntimeClasses are configured for the cluster, you can specify a
<code>runtimeClassName</code> in the Pod spec to use it. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">runtimeClassName</span>:<span style="color:#bbb"> </span>myclass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named
RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the
<code>Failed</code> terminal <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">phase</a>. Look for a
corresponding <a href="/docs/tasks/debug/debug-application/debug-running-pod/">event</a> for an
error message.</p><p>If no <code>runtimeClassName</code> is specified, the default RuntimeHandler will be used, which is equivalent
to the behavior when the RuntimeClass feature is disabled.</p><h3 id="cri-configuration">CRI Configuration</h3><p>For more details on setting up CRI runtimes, see <a href="/docs/setup/production-environment/container-runtimes/">CRI installation</a>.</p><h4 id="hahahugoshortcode1751s3hbhb"><a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" data-toggle="tooltip" data-placement="top" href="https://containerd.io/docs/" target="_blank" aria-label="containerd">containerd</a></h4><p>Runtime handlers are configured through containerd's configuration at
<code>/etc/containerd/config.toml</code>. Valid handlers are configured under the runtimes section:</p><pre tabindex="0"><code>[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.${HANDLER_NAME}]
</code></pre><p>See containerd's <a href="https://github.com/containerd/containerd/blob/main/docs/cri/config.md">config documentation</a>
for more details:</p><h4 id="hahahugoshortcode1751s4hbhb"><a class="glossary-tooltip" title="A lightweight container runtime specifically for Kubernetes" data-toggle="tooltip" data-placement="top" href="https://cri-o.io/#what-is-cri-o" target="_blank" aria-label="CRI-O">CRI-O</a></h4><p>Runtime handlers are configured through CRI-O's configuration at <code>/etc/crio/crio.conf</code>. Valid
handlers are configured under the
<a href="https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table">crio.runtime table</a>:</p><pre tabindex="0"><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = "${PATH_TO_BINARY}"
</code></pre><p>See CRI-O's <a href="https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md">config documentation</a> for more details.</p><h2 id="scheduling">Scheduling</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.16 [beta]</code></div><p>By specifying the <code>scheduling</code> field for a RuntimeClass, you can set constraints to
ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.
If <code>scheduling</code> is not set, this RuntimeClass is assumed to be supported by all nodes.</p><p>To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a
common label which is then selected by the <code>runtimeclass.scheduling.nodeSelector</code> field. The
RuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking
the intersection of the set of nodes selected by each. If there is a conflict, the pod will be
rejected.</p><p>If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you
can add <code>tolerations</code> to the RuntimeClass. As with the <code>nodeSelector</code>, the tolerations are merged
with the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated
by each.</p><p>To learn more about configuring the node selector and tolerations, see
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning Pods to Nodes</a>.</p><h3 id="pod-overhead">Pod Overhead</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can specify <em>overhead</em> resources that are associated with running a Pod. Declaring overhead allows
the cluster (including the scheduler) to account for it when making decisions about Pods and resources.</p><p>Pod overhead is defined in RuntimeClass through the <code>overhead</code> field. Through the use of this field,
you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads
are accounted for in Kubernetes.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md">RuntimeClass Design</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling">RuntimeClass Scheduling Design</a></li><li>Read about the <a href="/docs/concepts/scheduling-eviction/pod-overhead/">Pod Overhead</a> concept</li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead">PodOverhead Feature Design</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Admission Webhook Good Practices</h1><div class="lead">Recommendations for designing and deploying admission webhooks in Kubernetes.</div><p>This page provides good practices and considerations when designing
<em>admission webhooks</em> in Kubernetes. This information is intended for
cluster operators who run admission webhook servers or third-party applications
that modify or validate your API requests.</p><p>Before reading this page, ensure that you're familiar with the following
concepts:</p><ul><li><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission controllers</a></li><li><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">Admission webhooks</a></li></ul><h2 id="why-good-webhook-design-matters">Importance of good webhook design</h2><p>Admission control occurs when any create, update, or delete request
is sent to the Kubernetes API. Admission controllers intercept requests that
match specific criteria that you define. These requests are then sent to
mutating admission webhooks or validating admission webhooks. These webhooks are
often written to ensure that specific fields in object specifications exist or
have specific allowed values.</p><p>Webhooks are a powerful mechanism to extend the Kubernetes API. Badly-designed
webhooks often result in workload disruptions because of how much control
the webhooks have over objects in the cluster. Like other API extension
mechanisms, webhooks are challenging to test at scale for compatibility with
all of your workloads, other webhooks, add-ons, and plugins.</p><p>Additionally, with every release, Kubernetes adds or modifies the API with new
features, feature promotions to beta or stable status, and deprecations. Even
stable Kubernetes APIs are likely to change. For example, the <code>Pod</code> API changed
in v1.29 to add the
<a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a> feature.
While it's rare for a Kubernetes object to enter a broken state because of a new
Kubernetes API, webhooks that worked as expected with earlier versions of an API
might not be able to reconcile more recent changes to that API. This can result
in unexpected behavior after you upgrade your clusters to newer versions.</p><p>This page describes common webhook failure scenarios and how to avoid them by
cautiously and thoughtfully designing and implementing your webhooks.</p><h2 id="identify-admission-webhooks">Identify whether you use admission webhooks</h2><p>Even if you don't run your own admission webhooks, some third-party applications
that you run in your clusters might use mutating or validating admission
webhooks.</p><p>To check whether your cluster has any mutating admission webhooks, run the
following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get mutatingwebhookconfigurations
</span></span></code></pre></div><p>The output lists any mutating admission controllers in the cluster.</p><p>To check whether your cluster has any validating admission webhooks, run the
following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get validatingwebhookconfigurations
</span></span></code></pre></div><p>The output lists any validating admission controllers in the cluster.</p><h2 id="choose-admission-mechanism">Choose an admission control mechanism</h2><p>Kubernetes includes multiple admission control and policy enforcement options.
Knowing when to use a specific option can help you to improve latency and
performance, reduce management overhead, and avoid issues during version
upgrades. The following table describes the mechanisms that let you mutate or
validate resources during admission:</p><table><caption>Mutating and validating admission control in Kubernetes</caption><thead><tr><th>Mechanism</th><th>Description</th><th>Use cases</th></tr></thead><tbody><tr><td><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Mutating admission webhook</a></td><td>Intercept API requests before admission and modify as needed using
custom logic.</td><td><ul><li>Make critical modifications that must happen before resource
admission.</li><li>Make complex modifications that require advanced logic, like calling
external APIs.</li></ul></td></tr><tr><td><a href="/docs/reference/access-authn-authz/mutating-admission-policy/">Mutating admission policy</a></td><td>Intercept API requests before admission and modify as needed using
Common Expression Language (CEL) expressions.</td><td><ul><li>Make critical modifications that must happen before resource
admission.</li><li>Make simple modifications, such as adjusting labels or replica
counts.</li></ul></td></tr><tr><td><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Validating admission webhook</a></td><td>Intercept API requests before admission and validate against complex
policy declarations.</td><td><ul><li>Validate critical configurations before resource admission.</li><li>Enforce complex policy logic before admission.</li></ul></td></tr><tr><td><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Validating admission policy</a></td><td>Intercept API requests before admission and validate against CEL
expressions.</td><td><ul><li>Validate critical configurations before resource admission.</li><li>Enforce policy logic using CEL expressions.</li></ul></td></tr></tbody></table><p>In general, use <em>webhook</em> admission control when you want an extensible way to
declare or configure the logic. Use built-in CEL-based admission control when
you want to declare simpler logic without the overhead of running a webhook
server. The Kubernetes project recommends that you use CEL-based admission
control when possible.</p><h3 id="no-crd-validation-defaulting">Use built-in validation and defaulting for CustomResourceDefinitions</h3><p>If you use
<a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." data-toggle="tooltip" data-placement="top" href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank" aria-label="CustomResourceDefinitions">CustomResourceDefinitions</a>,
don't use admission webhooks to validate values in CustomResource specifications
or to set default values for fields. Kubernetes lets you define validation rules
and default field values when you create CustomResourceDefinitions.</p><p>To learn more, see the following resources:</p><ul><li><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation rules</a></li><li><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting">Defaulting</a></li></ul><h2 id="performance-latency">Performance and latency</h2><p>This section describes recommendations for improving performance and reducing
latency. In summary, these are as follows:</p><ul><li>Consolidate webhooks and limit the number of API calls per webhook.</li><li>Use audit logs to check for webhooks that repeatedly do the same action.</li><li>Use load balancing for webhook availability.</li><li>Set a small timeout value for each webhook.</li><li>Consider cluster availability needs during webhook design.</li></ul><h3 id="design-admission-webhooks-low-latency">Design admission webhooks for low latency</h3><p>Mutating admission webhooks are called in sequence. Depending on the mutating
webhook setup, some webhooks might be called multiple times. Every mutating
webhook call adds latency to the admission process. This is unlike validating
webhooks, which get called in parallel.</p><p>When designing your mutating webhooks, consider your latency requirements and
tolerance. The more mutating webhooks there are in your cluster, the greater the
chance of latency increases.</p><p>Consider the following to reduce latency:</p><ul><li>Consolidate webhooks that perform a similar mutation on different objects.</li><li>Reduce the number of API calls made in the mutating webhook server logic.</li><li>Limit the match conditions of each mutating webhook to reduce how many
webhooks are triggered by a specific API request.</li><li>Consolidate small webhooks into one server and configuration to help with
ordering and organization.</li></ul><h3 id="prevent-loops-competing-controllers">Prevent loops caused by competing controllers</h3><p>Consider any other components that run in your cluster that might conflict with
the mutations that your webhook makes. For example, if your webhook adds a label
that a different controller removes, your webhook gets called again. This leads
to a loop.</p><p>To detect these loops, try the following:</p><ol><li><p>Update your cluster audit policy to log audit events. Use the following
parameters:</p><ul><li><code>level</code>: <code>RequestResponse</code></li><li><code>verbs</code>: <code>["patch"]</code></li><li><code>omitStages</code>: <code>RequestReceived</code></li></ul><p>Set the audit rule to create events for the specific resources that your
webhook mutates.</p></li><li><p>Check your audit events for webhooks being reinvoked multiple times with the
same patch being applied to the same object, or for an object having
a field updated and reverted multiple times.</p></li></ol><h3 id="small-timeout">Set a small timeout value</h3><p>Admission webhooks should evaluate as quickly as possible (typically in
milliseconds), since they add to API request latency. Use a small timeout for
webhooks.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts">Timeouts</a>.</p><h3 id="load-balancer-webhook">Use a load balancer to ensure webhook availability</h3><p>Admission webhooks should leverage some form of load-balancing to provide high
availability and performance benefits. If a webhook is running within the
cluster, you can run multiple webhook backends behind a Service of type
<code>ClusterIP</code>.</p><h3 id="ha-deployment">Use a high-availability deployment model</h3><p>Consider your cluster's availability requirements when designing your webhook.
For example, during node downtime or zonal outages, Kubernetes marks Pods as
<code>NotReady</code> to allow load balancers to reroute traffic to available zones and
nodes. These updates to Pods might trigger your mutating webhooks. Depending on
the number of affected Pods, the mutating webhook server has a risk of timing
out or causing delays in Pod processing. As a result, traffic won't get
rerouted as quickly as you need.</p><p>Consider situations like the preceding example when writing your webhooks.
Exclude operations that are a result of Kubernetes responding to unavoidable
incidents.</p><h2 id="request-filtering">Request filtering</h2><p>This section provides recommendations for filtering which requests trigger
specific webhooks. In summary, these are as follows:</p><ul><li>Limit the webhook scope to avoid system components and read-only requests.</li><li>Limit webhooks to specific namespaces.</li><li>Use match conditions to perform fine-grained request filtering.</li><li>Match all versions of an object.</li></ul><h3 id="webhook-limit-scope">Limit the scope of each webhook</h3><p>Admission webhooks are only called when an API request matches the corresponding
webhook configuration. Limit the scope of each webhook to reduce unnecessary
calls to the webhook server. Consider the following scope limitations:</p><ul><li>Avoid matching objects in the <code>kube-system</code> namespace. If you run your own
Pods in the <code>kube-system</code> namespace, use an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-objectselector"><code>objectSelector</code></a>
to avoid mutating a critical workload.</li><li>Don't mutate node leases, which exist as Lease objects in the
<code>kube-node-lease</code> system namespace. Mutating node leases might result in
failed node upgrades. Only apply validation controls to Lease objects in this
namespace if you're confident that the controls won't put your cluster at
risk.</li><li>Don't mutate TokenReview or SubjectAccessReview objects. These are always
read-only requests. Modifying these objects might break your cluster.</li><li>Limit each webhook to a specific namespace by using a
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector"><code>namespaceSelector</code></a>.</li></ul><h3 id="filter-match-conditions">Filter for specific requests by using match conditions</h3><p>Admission controllers support multiple fields that you can use to match requests
that meet specific criteria. For example, you can use a <code>namespaceSelector</code> to
filter for requests that target a specific namespace.</p><p>For more fine-grained request filtering, use the <code>matchConditions</code> field in your
webhook configuration. This field lets you write multiple CEL expressions that
must evaluate to <code>true</code> for a request to trigger your admission webhook. Using
<code>matchConditions</code> might significantly reduce the number of calls to your webhook
server.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchconditions">Matching requests: <code>matchConditions</code></a>.</p><h3 id="match-all-versions">Match all versions of an API</h3><p>By default, admission webhooks run on any API versions that affect a specified
resource. The <code>matchPolicy</code> field in the webhook configuration controls this
behavior. Specify a value of <code>Equivalent</code> in the <code>matchPolicy</code> field or omit
the field to allow the webhook to run on any API version.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchpolicy">Matching requests: <code>matchPolicy</code></a>.</p><h2 id="mutation-scope-considerations">Mutation scope and field considerations</h2><p>This section provides recommendations for the scope of mutations and any special
considerations for object fields. In summary, these are as follows:</p><ul><li>Patch only the fields that you need to patch.</li><li>Don't overwrite array values.</li><li>Avoid side effects in mutations when possible.</li><li>Avoid self-mutations.</li><li>Fail open and validate the final state.</li><li>Plan for future field updates in later versions.</li><li>Prevent webhooks from self-triggering.</li><li>Don't change immutable objects.</li></ul><h3 id="patch-required-fields">Patch only required fields</h3><p>Admission webhook servers send HTTP responses to indicate what to do with a
specific Kubernetes API request. This response is an AdmissionReview object.
A mutating webhook can add specific fields to mutate before allowing admission
by using the <code>patchType</code> field and the <code>patch</code> field in the response. Ensure
that you only modify the fields that require a change.</p><p>For example, consider a mutating webhook that's configured to ensure that
<code>web-server</code> Deployments have at least three replicas. When a request to
create a Deployment object matches your webhook configuration, the webhook
should only update the value in the <code>spec.replicas</code> field.</p><h3 id="dont-overwrite-arrays">Don't overwrite array values</h3><p>Fields in Kubernetes object specifications might include arrays. Some arrays
contain key:value pairs (like the <code>envVar</code> field in a container specification),
while other arrays are unkeyed (like the <code>readinessGates</code> field in a Pod
specification). The order of values in an array field might matter in some
situations. For example, the order of arguments in the <code>args</code> field of a
container specification might affect the container.</p><p>Consider the following when modifying arrays:</p><ul><li>Whenever possible, use the <code>add</code> JSONPatch operation instead of <code>replace</code> to
avoid accidentally replacing a required value.</li><li>Treat arrays that don't use key:value pairs as sets.</li><li>Ensure that the values in the field that you modify aren't required to be
in a specific order.</li><li>Don't overwrite existing key:value pairs unless absolutely necessary.</li><li>Use caution when modifying label fields. An accidental modification might
cause label selectors to break, resulting in unintended behavior.</li></ul><h3 id="avoid-side-effects">Avoid side effects</h3><p>Ensure that your webhooks operate only on the content of the AdmissionReview
that's sent to them, and do not make out-of-band changes. These additional
changes, called <em>side effects</em>, might cause conflicts during admission if they
aren't reconciled properly. The <code>.webhooks[].sideEffects</code> field should
be set to <code>None</code> if a webhook doesn't have any side effect.</p><p>If side effects are required during the admission evaluation, they must be
suppressed when processing an AdmissionReview object with <code>dryRun</code> set to
<code>true</code>, and the <code>.webhooks[].sideEffects</code> field should be set to <code>NoneOnDryRun</code>.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#side-effects">Side effects</a>.</p><h3 id="avoid-self-mutation">Avoid self-mutations</h3><p>A webhook running inside the cluster might cause deadlocks for its own
deployment if it is configured to intercept resources required to start its own
Pods.</p><p>For example, a mutating admission webhook is configured to admit <strong>create</strong> Pod
requests only if a certain label is set in the Pod (such as <code>env: prod</code>).
The webhook server runs in a Deployment that doesn't set the <code>env</code> label.</p><p>When a node that runs the webhook server Pods becomes unhealthy, the webhook
Deployment tries to reschedule the Pods to another node. However, the existing
webhook server rejects the requests since the <code>env</code> label is unset. As a
result, the migration cannot happen.</p><p>Exclude the namespace where your webhook is running with a
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector"><code>namespaceSelector</code></a>.</p><h3 id="avoid-dependency-loops">Avoid dependency loops</h3><p>Dependency loops can occur in scenarios like the following:</p><ul><li>Two webhooks check each other's Pods. If both webhooks become unavailable
at the same time, neither webhook can start.</li><li>Your webhook intercepts cluster add-on components, such as networking plugins
or storage plugins, that your webhook depends on. If both the webhook and the
dependent add-on become unavailable, neither component can function.</li></ul><p>To avoid these dependency loops, try the following:</p><ul><li>Use
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicies</a>
to avoid introducing dependencies.</li><li>Prevent webhooks from validating or mutating other webhooks. Consider
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector">excluding specific namespaces</a>
from triggering your webhook.</li><li>Prevent your webhooks from acting on dependent add-ons by using an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-objectselector"><code>objectSelector</code></a>.</li></ul><h3 id="fail-open-validate-final-state">Fail open and validate the final state</h3><p>Mutating admission webhooks support the <code>failurePolicy</code> configuration field.
This field indicates whether the API server should admit or reject the request
if the webhook fails. Webhook failures might occur because of timeouts or errors
in the server logic.</p><p>By default, admission webhooks set the <code>failurePolicy</code> field to Fail. The API
server rejects a request if the webhook fails. However, rejecting requests by
default might result in compliant requests being rejected during webhook
downtime.</p><p>Let your mutating webhooks "fail open" by setting the <code>failurePolicy</code> field to
Ignore. Use a validating controller to check the state of requests to ensure
that they comply with your policies.</p><p>This approach has the following benefits:</p><ul><li>Mutating webhook downtime doesn't affect compliant resources from deploying.</li><li>Policy enforcement occurs during validating admission control.</li><li>Mutating webhooks don't interfere with other controllers in the cluster.</li></ul><h3 id="plan-future-field-updates">Plan for future updates to fields</h3><p>In general, design your webhooks under the assumption that Kubernetes APIs might
change in a later version. Don't write a server that takes the stability of an
API for granted. For example, the release of sidecar containers in Kubernetes
added a <code>restartPolicy</code> field to the Pod API.</p><h3 id="prevent-webhook-self-trigger">Prevent your webhook from triggering itself</h3><p>Mutating webhooks that respond to a broad range of API requests might
unintentionally trigger themselves. For example, consider a webhook that
responds to all requests in the cluster. If you configure the webhook to create
Event objects for every mutation, it'll respond to its own Event object
creation requests.</p><p>To avoid this, consider setting a unique label in any resources that your
webhook creates. Exclude this label from your webhook match conditions.</p><h3 id="dont-change-immutable-objects">Don't change immutable objects</h3><p>Some Kubernetes objects in the API server can't change. For example, when you
deploy a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static Pod">static Pod</a>, the
kubelet on the node creates a
<a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank" aria-label="mirror Pod">mirror Pod</a> in the API
server to track the static Pod. However, changes to the mirror Pod don't
propagate to the static Pod.</p><p>Don't attempt to mutate these objects during admission. All mirror Pods have the
<code>kubernetes.io/config.mirror</code> annotation. To exclude mirror Pods while reducing
the security risk of ignoring an annotation, allow static Pods to only run in
specific namespaces.</p><h2 id="ordering-idempotence">Mutating webhook ordering and idempotence</h2><p>This section provides recommendations for webhook order and designing idempotent
webhooks. In summary, these are as follows:</p><ul><li>Don't rely on a specific order of execution.</li><li>Validate mutations before admission.</li><li>Check for mutations being overwritten by other controllers.</li><li>Ensure that the set of mutating webhooks is idempotent, not just the
individual webhooks.</li></ul><h3 id="dont-rely-webhook-order">Don't rely on mutating webhook invocation order</h3><p>Mutating admission webhooks don't run in a consistent order. Various factors
might change when a specific webhook is called. Don't rely on your webhook
running at a specific point in the admission process. Other webhooks could still
mutate your modified object.</p><p>The following recommendations might help to minimize the risk of unintended
changes:</p><ul><li><a href="#validate-mutations">Validate mutations before admission</a></li><li>Use a reinvocation policy to observe changes to an object by other plugins
and re-run the webhook as needed. For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#reinvocation-policy">Reinvocation policy</a>.</li></ul><h3 id="ensure-mutating-webhook-idempotent">Ensure that the mutating webhooks in your cluster are idempotent</h3><p>Every mutating admission webhook should be <em>idempotent</em>. The webhook should be
able to run on an object that it already modified without making additional
changes beyond the original change.</p><p>Additionally, all of the mutating webhooks in your cluster should, as a
collection, be idempotent. After the mutation phase of admission control ends,
every individual mutating webhook should be able to run on an object without
making additional changes to the object.</p><p>Depending on your environment, ensuring idempotence at scale might be
challenging. The following recommendations might help:</p><ul><li>Use validating admission controllers to verify the final state of
critical workloads.</li><li>Test your deployments in a staging cluster to see if any objects get modified
multiple times by the same webhook.</li><li>Ensure that the scope of each mutating webhook is specific and limited.</li></ul><p>The following examples show idempotent mutation logic:</p><ol><li><p>For a <strong>create</strong> Pod request, set the field
<code>.spec.securityContext.runAsNonRoot</code> of the Pod to true.</p></li><li><p>For a <strong>create</strong> Pod request, if the field
<code>.spec.containers[].resources.limits</code> of a container is not set, set default
resource limits.</p></li><li><p>For a <strong>create</strong> Pod request, inject a sidecar container with name
<code>foo-sidecar</code> if no container with the name <code>foo-sidecar</code> already exists.</p></li></ol><p>In these cases, the webhook can be safely reinvoked, or admit an object that
already has the fields set.</p><p>The following examples show non-idempotent mutation logic:</p><ol><li><p>For a <strong>create</strong> Pod request, inject a sidecar container with name
<code>foo-sidecar</code> suffixed with the current timestamp (such as
<code>foo-sidecar-19700101-000000</code>).</p><p>Reinvoking the webhook can result in the same sidecar being injected multiple
times to a Pod, each time with a different container name. Similarly, the
webhook can inject duplicated containers if the sidecar already exists in
a user-provided pod.</p></li><li><p>For a <strong>create</strong>/<strong>update</strong> Pod request, reject if the Pod has label <code>env</code>
set, otherwise add an <code>env: prod</code> label to the Pod.</p><p>Reinvoking the webhook will result in the webhook failing on its own output.</p></li><li><p>For a <strong>create</strong> Pod request, append a sidecar container named <code>foo-sidecar</code>
without checking whether a <code>foo-sidecar</code> container exists.</p><p>Reinvoking the webhook will result in duplicated containers in the Pod, which
makes the request invalid and rejected by the API server.</p></li></ol><h2 id="mutation-testing-validation">Mutation testing and validation</h2><p>This section provides recommendations for testing your mutating webhooks and
validating mutated objects. In summary, these are as follows:</p><ul><li>Test webhooks in staging environments.</li><li>Avoid mutations that violate validations.</li><li>Test minor version upgrades for regressions and conflicts.</li><li>Validate mutated objects before admission.</li></ul><h3 id="test-in-staging-environments">Test webhooks in staging environments</h3><p>Robust testing should be a core part of your release cycle for new or updated
webhooks. If possible, test any changes to your cluster webhooks in a staging
environment that closely resembles your production clusters. At the very least,
consider using a tool like <a href="https://minikube.sigs.k8s.io/docs/">minikube</a> or
<a href="https://kind.sigs.k8s.io/">kind</a> to create a small test cluster for webhook
changes.</p><h3 id="ensure-mutations-dont-violate-validations">Ensure that mutations don't violate validations</h3><p>Your mutating webhooks shouldn't break any of the validations that apply to an
object before admission. For example, consider a mutating webhook that sets the
default CPU request of a Pod to a specific value. If the CPU limit of that Pod
is set to a lower value than the mutated request, the Pod fails admission.</p><p>Test every mutating webhook against the validations that run in your cluster.</p><h3 id="test-minor-version-upgrades">Test minor version upgrades to ensure consistent behavior</h3><p>Before upgrading your production clusters to a new minor version, test your
webhooks and workloads in a staging environment. Compare the results to ensure
that your webhooks continue to function as expected after the upgrade.</p><p>Additionally, use the following resources to stay informed about API changes:</p><ul><li><a href="/releases/">Kubernetes release notes</a></li><li><a href="/blog/">Kubernetes blog</a></li></ul><h3 id="validate-mutations">Validate mutations before admission</h3><p>Mutating webhooks run to completion before any validating webhooks run. There is
no stable order in which mutations are applied to objects. As a result, your
mutations could get overwritten by a mutating webhook that runs at a later time.</p><p>Add a validating admission controller like a ValidatingAdmissionWebhook or a
ValidatingAdmissionPolicy to your cluster to ensure that your mutations
are still present. For example, consider a mutating webhook that inserts the
<code>restartPolicy: Always</code> field to specific init containers to make them run as
sidecar containers. You could run a validating webhook to ensure that those
init containers retained the <code>restartPolicy: Always</code> configuration after all
mutations were completed.</p><p>For details, see the following resources:</p><ul><li><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Validating Admission Policy</a></li><li><a href="/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">ValidatingAdmissionWebhooks</a></li></ul><h2 id="mutating-webhook-deployment">Mutating webhook deployment</h2><p>This section provides recommendations for deploying your mutating admission
webhooks. In summary, these are as follows:</p><ul><li>Gradually roll out the webhook configuration and monitor for issues by
namespace.</li><li>Limit access to edit the webhook configuration resources.</li><li>Limit access to the namespace that runs the webhook server, if the server is
in-cluster.</li></ul><h3 id="install-enable-mutating-webhook">Install and enable a mutating webhook</h3><p>When you're ready to deploy your mutating webhook to a cluster, use the
following order of operations:</p><ol><li>Install the webhook server and start it.</li><li>Set the <code>failurePolicy</code> field in the MutatingWebhookConfiguration manifest
to Ignore. This lets you avoid disruptions caused by misconfigured webhooks.</li><li>Set the <code>namespaceSelector</code> field in the MutatingWebhookConfiguration
manifest to a test namespace.</li><li>Deploy the MutatingWebhookConfiguration to your cluster.</li></ol><p>Monitor the webhook in the test namespace to check for any issues, then roll the
webhook out to other namespaces. If the webhook intercepts an API request that
it wasn't meant to intercept, pause the rollout and adjust the scope of the
webhook configuration.</p><h3 id="limit-edit-access">Limit edit access to mutating webhooks</h3><p>Mutating webhooks are powerful Kubernetes controllers. Use RBAC or another
authorization mechanism to limit access to your webhook configurations and
servers. For RBAC, ensure that the following access is only available to trusted
entities:</p><ul><li>Verbs: <strong>create</strong>, <strong>update</strong>, <strong>patch</strong>, <strong>delete</strong>, <strong>deletecollection</strong></li><li>API group: <code>admissionregistration.k8s.io/v1</code></li><li>API kind: MutatingWebhookConfigurations</li></ul><p>If your mutating webhook server runs in the cluster, limit access to create or
modify any resources in that namespace.</p><h2 id="example-good-implementations">Examples of good implementations</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The following projects are examples of "good" custom webhook server
implementations. You can use them as a starting point when designing your own
webhooks. Don't use these examples as-is; use them as a starting point and
design your webhooks to run well in your specific environment.</p><ul><li><a href="https://github.com/cert-manager/cert-manager/tree/master/internal/webhook"><code>cert-manager</code></a></li><li><a href="https://open-policy-agent.github.io/gatekeeper/website/docs/mutation">Gatekeeper Open Policy Agent (OPA)</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/access-authn-authz/webhook/">Use webhooks for authentication and authorization</a></li><li><a href="/docs/reference/access-authn-authz/mutating-admission-policy/">Learn about MutatingAdmissionPolicies</a></li><li><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Learn about ValidatingAdmissionPolicies</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Logging Architecture</h1><p>Application logs can help you understand what is happening inside your application. The
logs are particularly useful for debugging problems and monitoring cluster activity. Most
modern applications have some kind of logging mechanism. Likewise, container engines
are designed to support logging. The easiest and most adopted logging method for
containerized applications is writing to standard output and standard error streams.</p><p>However, the native functionality provided by a container engine or runtime is usually
not enough for a complete logging solution.</p><p>For example, you may want to access your application's logs if a container crashes,
a pod gets evicted, or a node dies.</p><p>In a cluster, logs should have a separate storage and lifecycle independent of nodes,
pods, or containers. This concept is called
<a href="#cluster-level-logging-architectures">cluster-level logging</a>.</p><p>Cluster-level logging architectures require a separate backend to store, analyze, and
query logs. Kubernetes does not provide a native storage solution for log data. Instead,
there are many logging solutions that integrate with Kubernetes. The following sections
describe how to handle and store logs on nodes.</p><h2 id="basic-logging-in-kubernetes">Pod and container logs</h2><p>Kubernetes captures logs from each container in a running Pod.</p><p>This example uses a manifest for a <code>Pod</code> with a container
that writes text to the standard output stream, once per second.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/counter-pod.yaml" download="debug/counter-pod.yaml"><code>debug/counter-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;debug-counter-pod-yaml&quot;)" title="Copy debug/counter-pod.yaml to clipboard"/></div><div class="includecode" id="debug-counter-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c,<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:#b44">'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done'</span>]<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>To run this pod, use the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>The output is:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">pod/counter created
</span></span></span></code></pre></div><p>To fetch the logs, use the <code>kubectl logs</code> command, as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs counter
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">0: Fri Apr  1 11:42:23 UTC 2022
</span></span></span><span style="display:flex"><span><span style="color:#888">1: Fri Apr  1 11:42:24 UTC 2022
</span></span></span><span style="display:flex"><span><span style="color:#888">2: Fri Apr  1 11:42:25 UTC 2022
</span></span></span></code></pre></div><p>You can use <code>kubectl logs --previous</code> to retrieve logs from a previous instantiation of a container.
If your pod has multiple containers, specify which container's logs you want to access by
appending a container name to the command, with a <code>-c</code> flag, like so:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs counter -c count
</span></span></code></pre></div><h3 id="container-log-streams">Container log streams</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: PodLogsQuerySplitStreams"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>As an alpha feature, the kubelet can split out the logs from the two standard streams produced
by a container: <a href="https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)">standard output</a>
and <a href="https://en.wikipedia.org/wiki/Standard_streams#Standard_error_(stderr)">standard error</a>.
To use this behavior, you must enable the <code>PodLogsQuerySplitStreams</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.
With that feature gate enabled, Kubernetes 1.34 allows access to these
log streams directly via the Pod API. You can fetch a specific stream by specifying the stream name (either <code>Stdout</code> or <code>Stderr</code>),
using the <code>stream</code> query string. You must have access to read the <code>log</code> subresource of that Pod.</p><p>To demonstrate this feature, you can create a Pod that periodically writes text to both the standard output and error stream.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/counter-pod-err.yaml" download="debug/counter-pod-err.yaml"><code>debug/counter-pod-err.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;debug-counter-pod-err-yaml&quot;)" title="Copy debug/counter-pod-err.yaml to clipboard"/></div><div class="includecode" id="debug-counter-pod-err-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>counter-err<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c,<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:#b44">'i=0; while true; do echo "$i: $(date)"; echo "$i: err" &gt;&amp;2 ; i=$((i+1)); sleep 1; done'</span>]<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>To run this pod, use the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod-err.yaml
</span></span></code></pre></div><p>To fetch only the stderr log stream, you can run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get --raw <span style="color:#b44">"/api/v1/namespaces/default/pods/counter-err/log?stream=Stderr"</span>
</span></span></code></pre></div><p>See the <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code> documentation</a>
for more details.</p><h3 id="how-nodes-handle-container-logs">How nodes handle container logs</h3><p><img alt="Node level logging" src="/images/docs/user-guide/logging/logging-node-level.png"/></p><p>A container runtime handles and redirects any output generated to a containerized
application's <code>stdout</code> and <code>stderr</code> streams.
Different container runtimes implement this in different ways; however, the integration
with the kubelet is standardized as the <em>CRI logging format</em>.</p><p>By default, if a container restarts, the kubelet keeps one terminated container with its logs.
If a pod is evicted from the node, all corresponding containers are also evicted, along with their logs.</p><p>The kubelet makes logs available to clients via a special feature of the Kubernetes API.
The usual way to access this is by running <code>kubectl logs</code>.</p><h3 id="log-rotation">Log rotation</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>The kubelet is responsible for rotating container logs and managing the
logging directory structure.
The kubelet sends this information to the container runtime (using CRI),
and the runtime writes the container logs to the given location.</p><p>You can configure two kubelet <a href="/docs/reference/config-api/kubelet-config.v1beta1/">configuration settings</a>,
<code>containerLogMaxSize</code> (default 10Mi) and <code>containerLogMaxFiles</code> (default 5),
using the <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.
These settings let you configure the maximum size for each log file and the maximum number of
files allowed for each container respectively.</p><p>In order to perform an efficient log rotation in clusters where the volume of the logs generated by
the workload is large, kubelet also provides a mechanism to tune how the logs are rotated in
terms of how many concurrent log rotations can be performed and the interval at which the logs are
monitored and rotated as required.
You can configure two kubelet <a href="/docs/reference/config-api/kubelet-config.v1beta1/">configuration settings</a>,
<code>containerLogMaxWorkers</code> and <code>containerLogMonitorInterval</code> using the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><p>When you run <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code></a> as in
the basic logging example, the kubelet on the node handles the request and
reads directly from the log file. The kubelet returns the content of the log file.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Only the contents of the latest log file are available through <code>kubectl logs</code>.</p><p>For example, if a Pod writes 40 MiB of logs and the kubelet rotates logs
after 10 MiB, running <code>kubectl logs</code> returns at most 10MiB of data.</p></div><h2 id="system-component-logs">System component logs</h2><p>There are two types of system components: those that typically run in a container,
and those components directly involved in running containers. For example:</p><ul><li>The kubelet and container runtime do not run in containers. The kubelet runs
your containers (grouped together in <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="pods">pods</a>)</li><li>The Kubernetes scheduler, controller manager, and API server run within pods
(usually <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static Pods">static Pods</a>).
The etcd component runs in the control plane, and most commonly also as a static pod.
If your cluster uses kube-proxy, you typically run this as a <code>DaemonSet</code>.</li></ul><h3 id="log-location-node">Log locations</h3><p>The way that the kubelet and container runtime write logs depends on the operating
system that the node uses:</p><ul class="nav nav-tabs" id="log-location-node-tabs" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#log-location-node-tabs-0" role="tab" aria-controls="log-location-node-tabs-0" aria-selected="true">Linux</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#log-location-node-tabs-1" role="tab" aria-controls="log-location-node-tabs-1">Windows</a></li></ul><div class="tab-content" id="log-location-node-tabs"><div id="log-location-node-tabs-0" class="tab-pane show active" role="tabpanel" aria-labelledby="log-location-node-tabs-0"><p><p>On Linux nodes that use systemd, the kubelet and container runtime write to journald
by default. You use <code>journalctl</code> to read the systemd journal; for example:
<code>journalctl -u kubelet</code>.</p><p>If systemd is not present, the kubelet and container runtime write to <code>.log</code> files in the
<code>/var/log</code> directory. If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>By default, kubelet directs your container runtime to write logs into directories within
<code>/var/log/pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href="/docs/concepts/cluster-administration/system-logs/#klog">System Logs</a>.</p></p></div><div id="log-location-node-tabs-1" class="tab-pane" role="tabpanel" aria-labelledby="log-location-node-tabs-1"><p><p>By default, the kubelet writes logs to files within the directory <code>C:\var\logs</code>
(notice that this is not <code>C:\var\log</code>).</p><p>Although <code>C:\var\log</code> is the Kubernetes default location for these logs, several
cluster deployment tools set up Windows nodes to log to <code>C:\var\log\kubelet</code> instead.</p><p>If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>However, by default, kubelet directs your container runtime to write logs within the
directory <code>C:\var\log\pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href="/docs/concepts/cluster-administration/system-logs/#klog">System Logs</a>.</p></p></div></div><p><br/></p><p>For Kubernetes cluster components that run in pods, these write to files inside
the <code>/var/log</code> directory, bypassing the default logging mechanism (the components
do not write to the systemd journal). You can use Kubernetes' storage mechanisms
to map persistent storage into the container that runs the component.</p><p>Kubelet allows changing the pod logs directory from default <code>/var/log/pods</code>
to a custom path. This adjustment can be made by configuring the <code>podLogsDir</code>
parameter in the kubelet's configuration file.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>It's important to note that the default location <code>/var/log/pods</code> has been in use for
an extended period and certain processes might implicitly assume this path.
Therefore, altering this parameter must be approached with caution and at your own risk.</p><p>Another caveat to keep in mind is that the kubelet supports the location being on the same
disk as <code>/var</code>. Otherwise, if the logs are on a separate filesystem from <code>/var</code>,
then the kubelet will not track that filesystem's usage, potentially leading to issues if
it fills up.</p></div><p>For details about etcd and its logs, view the <a href="https://etcd.io/docs/">etcd documentation</a>.
Again, you can use Kubernetes' storage mechanisms to map persistent storage into
the container that runs the component.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you deploy Kubernetes cluster components (such as the scheduler) to log to
a volume shared from the parent node, you need to consider and ensure that those
logs are rotated. <strong>Kubernetes does not manage that log rotation</strong>.</p><p>Your operating system may automatically implement some log rotation - for example,
if you share the directory <code>/var/log</code> into a static Pod for a component, node-level
log rotation treats a file in that directory the same as a file written by any component
outside Kubernetes.</p><p>Some deploy tools account for that log rotation and automate it; others leave this
as your responsibility.</p></div><h2 id="cluster-level-logging-architectures">Cluster-level logging architectures</h2><p>While Kubernetes does not provide a native solution for cluster-level logging, there are
several common approaches you can consider. Here are some options:</p><ul><li>Use a node-level logging agent that runs on every node.</li><li>Include a dedicated sidecar container for logging in an application pod.</li><li>Push logs directly to a backend from within an application.</li></ul><h3 id="using-a-node-logging-agent">Using a node logging agent</h3><p><img alt="Using a node level logging agent" src="/images/docs/user-guide/logging/logging-with-node-agent.png"/></p><p>You can implement cluster-level logging by including a <em>node-level logging agent</em> on each node.
The logging agent is a dedicated tool that exposes logs or pushes logs to a backend.
Commonly, the logging agent is a container that has access to a directory with log files from all of the
application containers on that node.</p><p>Because the logging agent must run on every node, it is recommended to run the agent
as a <code>DaemonSet</code>.</p><p>Node-level logging creates only one agent per node and doesn't require any changes to the
applications running on the node.</p><p>Containers write to stdout and stderr, but with no agreed format. A node-level agent collects
these logs and forwards them for aggregation.</p><h3 id="sidecar-container-with-logging-agent">Using a sidecar container with the logging agent</h3><p>You can use a sidecar container in one of the following ways:</p><ul><li>The sidecar container streams application logs to its own <code>stdout</code>.</li><li>The sidecar container runs a logging agent, which is configured to pick up logs
from an application container.</li></ul><h4 id="streaming-sidecar-container">Streaming sidecar container</h4><p><img alt="Sidecar container with a streaming container" src="/images/docs/user-guide/logging/logging-with-streaming-sidecar.png"/></p><p>By having your sidecar containers write to their own <code>stdout</code> and <code>stderr</code>
streams, you can take advantage of the kubelet and the logging agent that
already run on each node. The sidecar containers read logs from a file, a socket,
or journald. Each sidecar container prints a log to its own <code>stdout</code> or <code>stderr</code> stream.</p><p>This approach allows you to separate several log streams from different
parts of your application, some of which can lack support
for writing to <code>stdout</code> or <code>stderr</code>. The logic behind redirecting logs
is minimal, so it's not a significant overhead. Additionally, because
<code>stdout</code> and <code>stderr</code> are handled by the kubelet, you can use built-in tools
like <code>kubectl logs</code>.</p><p>For example, a pod runs a single container, and the container
writes to two different log files using two different formats. Here's a
manifest for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod.yaml" download="admin/logging/two-files-counter-pod.yaml"><code>admin/logging/two-files-counter-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-logging-two-files-counter-pod-yaml&quot;)" title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"/></div><div class="includecode" id="admin-logging-two-files-counter-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- &gt;<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      i=0;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      while true;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      do
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        echo "$i: $(date)" &gt;&gt; /var/log/1.log;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        i=$((i+1));
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        sleep 1;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      done</span><span style="color:#bbb">      
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>It is not recommended to write log entries with different formats to the same log
stream, even if you managed to redirect both components to the <code>stdout</code> stream of
the container. Instead, you can create two sidecar containers. Each sidecar
container could tail a particular log file from a shared volume and then redirect
the logs to its own <code>stdout</code> stream.</p><p>Here's a manifest for a pod that has two sidecar containers:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml" download="admin/logging/two-files-counter-pod-streaming-sidecar.yaml"><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-logging-two-files-counter-pod-streaming-sidecar-yaml&quot;)" title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"/></div><div class="includecode" id="admin-logging-two-files-counter-pod-streaming-sidecar-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- &gt;<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      i=0;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      while true;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      do
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        echo "$i: $(date)" &gt;&gt; /var/log/1.log;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        i=$((i+1));
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        sleep 1;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      done</span><span style="color:#bbb">      
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count-log-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count-log-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c, 'tail -n+1 -F /var/log/2.log']<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Now when you run this pod, you can access each log stream separately by
running the following commands:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs counter count-log-1
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">0: Fri Apr  1 11:42:26 UTC 2022
</span></span></span><span style="display:flex"><span><span style="color:#888">1: Fri Apr  1 11:42:27 UTC 2022
</span></span></span><span style="display:flex"><span><span style="color:#888">2: Fri Apr  1 11:42:28 UTC 2022
</span></span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs counter count-log-2
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">Fri Apr  1 11:42:29 UTC 2022 INFO 0
</span></span></span><span style="display:flex"><span><span style="color:#888">Fri Apr  1 11:42:30 UTC 2022 INFO 0
</span></span></span><span style="display:flex"><span><span style="color:#888">Fri Apr  1 11:42:31 UTC 2022 INFO 0
</span></span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span></code></pre></div><p>If you installed a node-level agent in your cluster, that agent picks up those log
streams automatically without any further configuration. If you like, you can configure
the agent to parse log lines depending on the source container.</p><p>Even for Pods that only have low CPU and memory usage (order of a couple of millicores
for cpu and order of several megabytes for memory), writing logs to a file and
then streaming them to <code>stdout</code> can double how much storage you need on the node.
If you have an application that writes to a single file, it's recommended to set
<code>/dev/stdout</code> as the destination rather than implement the streaming sidecar
container approach.</p><p>Sidecar containers can also be used to rotate log files that cannot be rotated by
the application itself. An example of this approach is a small container running
<code>logrotate</code> periodically.
However, it's more straightforward to use <code>stdout</code> and <code>stderr</code> directly, and
leave rotation and retention policies to the kubelet.</p><h4 id="sidecar-container-with-a-logging-agent">Sidecar container with a logging agent</h4><p><img alt="Sidecar container with a logging agent" src="/images/docs/user-guide/logging/logging-with-sidecar-agent.png"/></p><p>If the node-level logging agent is not flexible enough for your situation, you
can create a sidecar container with a separate logging agent that you have
configured specifically to run with your application.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Using a logging agent in a sidecar container can lead
to significant resource consumption. Moreover, you won't be able to access
those logs using <code>kubectl logs</code> because they are not controlled
by the kubelet.</div><p>Here are two example manifests that you can use to implement a sidecar container with a logging agent.
The first manifest contains a <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/"><code>ConfigMap</code></a>
to configure fluentd.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/fluentd-sidecar-config.yaml" download="admin/logging/fluentd-sidecar-config.yaml"><code>admin/logging/fluentd-sidecar-config.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-logging-fluentd-sidecar-config-yaml&quot;)" title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"/></div><div class="includecode" id="admin-logging-fluentd-sidecar-config-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fluentd-config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">fluentd.conf</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    &lt;source&gt;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      type tail
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      format none
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      path /var/log/1.log
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      pos_file /var/log/1.log.pos
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      tag count.format1
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    &lt;/source&gt;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    &lt;source&gt;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      type tail
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      format none
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      path /var/log/2.log
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      pos_file /var/log/2.log.pos
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      tag count.format2
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    &lt;/source&gt;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    &lt;match **&gt;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      type google_cloud
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    &lt;/match&gt;</span><span style="color:#bbb">    
</span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In the sample configurations, you can replace fluentd with any logging agent, reading
from any source inside an application container.</div><p>The second manifest describes a pod that has a sidecar container running fluentd.
The pod mounts a volume where fluentd can pick up its configuration data.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml" download="admin/logging/two-files-counter-pod-agent-sidecar.yaml"><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;admin-logging-two-files-counter-pod-agent-sidecar-yaml&quot;)" title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"/></div><div class="includecode" id="admin-logging-two-files-counter-pod-agent-sidecar-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- &gt;<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      i=0;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      while true;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      do
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        echo "$i: $(date)" &gt;&gt; /var/log/1.log;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        i=$((i+1));
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">        sleep 1;
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">      done</span><span style="color:#bbb">      
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>count-agent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/fluentd-gcp:1.30<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>FLUENTD_ARGS<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span>-c /etc/fluentd-config/fluentd.conf<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/etc/fluentd-config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">configMap</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fluentd-config<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h3 id="exposing-logs-directly-from-the-application">Exposing logs directly from the application</h3><p><img alt="Exposing logs directly from the application" src="/images/docs/user-guide/logging/logging-from-application.png"/></p><p>Cluster-logging that exposes or pushes logs directly from every application is outside the scope
of Kubernetes.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/cluster-administration/system-logs/">Kubernetes system logs</a></li><li>Learn about <a href="/docs/concepts/cluster-administration/system-traces/">Traces For Kubernetes System Components</a></li><li>Learn how to <a href="/docs/tasks/debug/debug-application/determine-reason-pod-failure/#customizing-the-termination-message">customise the termination message</a>
that Kubernetes records when a Pod fails</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Swap memory management</h1><p>Kubernetes can be configured to use swap memory on a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a>,
allowing the kernel to free up physical memory by swapping out pages to backing storage.
This is useful for multiple use-cases.
For example, nodes running workloads that can benefit from using swap,
such as those that have large memory footprints but only access a portion of that memory at any given time.
It also helps prevent Pods from being terminated during memory pressure spikes,
shields nodes from system-level memory spikes that might compromise its stability,
allows for more flexible memory management on the node, and much more.</p><p>To learn about configuring swap in your cluster, read
<a href="/docs/tutorials/cluster-management/provision-swap-memory/">Configuring swap memory on Kubernetes nodes</a>.</p><h2 id="operating-system-support">Operating system support</h2><ul><li>Linux nodes support swap; you need to configure each node to enable it.
By default, the kubelet will <strong>not</strong> start on a Linux node that has swap enabled.</li><li>Windows nodes require swap space.
By default, the kubelet does <strong>not</strong> start on a Windows node that has swap disabled.</li></ul><h2 id="how-does-it-work">How does it work?</h2><p>There are a number of possible ways that one could envision swap use on a node.
If kubelet is already running on a node, it would need to be restarted after swap is provisioned in order to identify it.</p><p>When kubelet starts on a node in which swap is provisioned and available
(with the <code>failSwapOn: false</code> configuration), kubelet will:</p><ul><li>Be able to start on this swap-enabled node.</li><li>Direct the Container Runtime Interface (CRI) implementation, often referred to as the container runtime,
to allocate zero swap memory to Kubernetes workloads by default.</li></ul><p>Swap configuration on a node is exposed to a cluster admin via the
<a href="/docs/reference/config-api/kubelet-config.v1/"><code>memorySwap</code> in the KubeletConfiguration</a>.
As a cluster administrator, you can specify the node's behaviour in the
presence of swap memory by setting <code>memorySwap.swapBehavior</code>.</p><h3 id="swap-behaviors">Swap behaviors</h3><p>You need to pick a <a href="/docs/reference/node/swap-behavior/">swap behavior</a> to
use. Different nodes in your cluster can use different swap behaviors.</p><p>The swap behaviors you can choose for Linux nodes are:</p><dl><dt><code>NoSwap</code> (default)</dt><dd>Workloads running as Pods on this node do not and cannot use swap.</dd><dt><code>LimitedSwap</code></dt><dd>Kubernetes workloads can utilize swap memory.</dd></dl><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>If you choose the NoSwap behavior, and you configure the kubelet to tolerate
swap space (<code>failSwapOn: false</code>), then your workloads don't use any swap.</p><p>However, processes outside of Kubernetes-managed containers, such as systemi
services (and even the kubelet itself!) <strong>can</strong> utilize swap.</p></div><p>You can read <a href="/docs/tutorials/cluster-management/provision-swap-memory/">configuring swap memory on Kubernetes nodes</a> to learn about enabling swap for your cluster.</p><h3 id="container-runtime-integration">Container runtime integration</h3><p>The kubelet uses the container runtime API, and directs the container runtime to
apply specific configuration (for example, in the cgroup v2 case, <code>memory.swap.max</code>) in a manner that will
enable the desired swap configuration for a container. For runtimes that use control groups, or cgroups,
the container runtime is then responsible for writing these settings to the container-level cgroup.</p><h2 id="observability-for-swap-use">Observability for swap use</h2><h3 id="node-and-container-level-metric-statistics">Node and container level metric statistics</h3><p>Kubelet now collects node and container level metric statistics,
which can be accessed at the <code>/metrics/resource</code> (which is used mainly by monitoring
tools like Prometheus) and <code>/stats/summary</code> (which is used mainly by Autoscalers) kubelet HTTP endpoints.
This allows clients who can directly request the kubelet to
monitor swap usage and remaining swap memory when using <code>LimitedSwap</code>.
Additionally, a <code>machine_swap_bytes</code> metric has been added to cadvisor to show
the total physical swap capacity of the machine.
See <a href="/docs/reference/instrumentation/node-metrics/">this page</a> for more info.</p><p>For example, these <code>/metrics/resource</code> are supported:</p><ul><li><code>node_swap_usage_bytes</code>: Current swap usage of the node in bytes.</li><li><code>container_swap_usage_bytes</code>: Current amount of the container swap usage in bytes.</li><li><code>container_swap_limit_bytes</code>: Current amount of the container swap limit in bytes.</li></ul><h3 id="using-kubectl-top-show-swap">Using <code>kubectl top --show-swap</code></h3><p>Querying metrics is valuable, but somewhat cumbersome, as these metrics
are designed to be used by software rather than humans.
In order to consume this data in a more user-friendly way,
the <code>kubectl top</code> command has been extended to support swap metrics, using the <code>--show-swap</code> flag.</p><p>In order to receive information about swap usage on nodes, <code>kubectl top nodes --show-swap</code> can be used:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl top nodes --show-swap
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>NAME    CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   SWAP(bytes)    SWAP(%)       
node1   1m           10%      2Mi             10%         1Mi            0%   
node2   5m           10%      6Mi             10%         2Mi            0%   
node3   3m           10%      4Mi             10%         &lt;unknown&gt;      &lt;unknown&gt;   
</code></pre><p>In order to receive information about swap usage by pods, <code>kubectl top nodes --show-swap</code> can be used:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl top pod -n kube-system --show-swap
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>NAME                                      CPU(cores)   MEMORY(bytes)   SWAP(bytes)
coredns-58d5bc5cdb-5nbk4                  2m           19Mi            0Mi
coredns-58d5bc5cdb-jsh26                  3m           37Mi            0Mi
etcd-node01                               51m          143Mi           5Mi
kube-apiserver-node01                     98m          824Mi           16Mi
kube-controller-manager-node01            20m          135Mi           9Mi
kube-proxy-ffgs2                          1m           24Mi            0Mi
kube-proxy-fhvwx                          1m           39Mi            0Mi
kube-scheduler-node01                     13m          69Mi            0Mi
metrics-server-8598789fdb-d2kcj           5m           26Mi            0Mi   
</code></pre><h3 id="nodes-to-report-swap-capacity-as-part-of-node-status">Nodes to report swap capacity as part of node status</h3><p>A new node status field is now added, <code>node.status.nodeInfo.swap.capacity</code>, to report the swap capacity of a node.</p><p>As an example, the following command can be used to retrieve the swap capacity of the nodes in a cluster:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get nodes -o go-template<span style="color:#666">=</span><span style="color:#b44">'{{range .items}}{{.metadata.name}}: {{if .status.nodeInfo.swap.capacity}}{{.status.nodeInfo.swap.capacity}}{{else}}&lt;unknown&gt;{{end}}{{"\n"}}{{end}}'</span>
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>node1: 21474836480
node2: 42949664768
node3: &lt;unknown&gt;
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>&lt;unknown&gt;</code> value indicates that the <code>.status.nodeInfo.swap.capacity</code> field is not set for that Node.
This probably means that the node does not have swap provisioned, or less likely,
that the kubelet is not able to determine the swap capacity of the node.</div><h3 id="node-feature-discovery">Swap discovery using Node Feature Discovery (NFD)</h3><p><a href="https://github.com/kubernetes-sigs/node-feature-discovery">Node Feature Discovery</a>
is a Kubernetes addon for detecting hardware features and configuration.
It can be utilized to discover which nodes are provisioned with swap.</p><p>As an example, to figure out which nodes are provisioned with swap,
use the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get nodes -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{range .items[?(@.metadata.labels.feature\.node\.kubernetes\.io/memory-swap)]}{.metadata.name}{"\t"}{.metadata.labels.feature\.node\.kubernetes\.io/memory-swap}{"\n"}{end}'</span>
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>k8s-worker1: true
k8s-worker2: true
k8s-worker3: false
</code></pre><p>In this example, swap is provisioned on nodes <code>k8s-worker1</code> and <code>k8s-worker2</code>, but not on <code>k8s-worker3</code>.</p><h2 id="risks-and-caveats">Risks and caveats</h2><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>It is deeply encouraged to encrypt the swap space.
See Memory-backed volumes <a href="#memory-backed-volumes">memory-backed volumes</a> for more info.</div><p>Having swap available on a system reduces predictability.
While swap can enhance performance by making more RAM available, swapping data
back to memory is a heavy operation, sometimes slower by many orders of magnitude,
which can cause unexpected performance regressions.
Furthermore, swap changes a system's behaviour under memory pressure.
Enabling swap increases the risk of noisy neighbors,
where Pods that frequently use their RAM may cause other Pods to swap.
In addition, since swap allows for greater memory usage for workloads in Kubernetes that cannot be predictably accounted for,
and due to unexpected packing configurations,
the scheduler currently does not account for swap memory usage.
This heightens the risk of noisy neighbors.</p><p>The performance of a node with swap memory enabled depends on the underlying physical storage.
When swap memory is in use, performance will be significantly worse in an I/O
operations per second (IOPS) constrained environment, such as a cloud VM with
I/O throttling, when compared to faster storage mediums like solid-state drives
or NVMe.
As swap might cause IO pressure, it is recommended to give a higher IO latency
priority to system critical daemons. See the relevant section in the
<a href="#good-practice-for-using-swap-in-a-kubernetes-cluster">recommended practices</a> section below.</p><h3 id="memory-backed-volumes">Memory-backed volumes</h3><p>On Linux nodes, memory-backed volumes (such as <a href="/docs/concepts/configuration/secret/"><code>secret</code></a>
volume mounts, or <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a> with <code>medium: Memory</code>)
are implemented with a <code>tmpfs</code> filesystem.
The contents of such volumes should remain in memory at all times, hence should
not be swapped to disk.
To ensure the contents of such volumes remain in memory, the <code>noswap</code> tmpfs option
is being used.</p><p>The Linux kernel officially supports the <code>noswap</code> option from version 6.3 (more info
can be found in <a href="/docs/reference/node/kernel-version-requirements/#requirements-other">Linux Kernel Version Requirements</a>).
However, the different distributions often choose to backport this mount option to older
Linux versions as well.</p><p>In order to verify whether the node supports the <code>noswap</code> option, the kubelet will do the following:</p><ul><li>If the kernel's version is above 6.3 then the <code>noswap</code> option will be assumed to be supported.</li><li>Otherwise, kubelet would try to mount a dummy tmpfs with the <code>noswap</code> option at startup.
If kubelet fails with an error indicating of an unknown option, <code>noswap</code> will be assumed
to not be supported, hence will not be used.
A kubelet log entry will be emitted to warn the user about memory-backed volumes might swap to disk.
If kubelet succeeds, the dummy tmpfs will be deleted and the <code>noswap</code> option will be used.<ul><li>If the <code>noswap</code> option is not supported, kubelet will emit a warning log entry,
then continue its execution.</li></ul></li></ul><p>See the <a href="#setting-up-encrypted-swap">section above</a> with an example for setting unencrypted swap.
However, handling encrypted swap is not within the scope of kubelet;
rather, it is a general OS configuration concern and should be addressed at that level.
It is the administrator's responsibility to provision encrypted swap to mitigate this risk.</p><h3 id="evictions">Evictions</h3><p>Configuring memory eviction thresholds for swap-enabled nodes can be tricky.</p><p>With swap being disabled, it is reasonable to configure kubelet's eviction thresholds
to be a bit lower than the node's memory capacity.
The rationale is that we want Kubernetes to start evicting Pods before the node runs out of memory
and invokes the Out Of Memory (OOM) killer, since the OOM killer is not Kubernetes-aware,
therefore does not consider things like QoS, pod priority, or other Kubernetes-specific factors.</p><p>With swap enabled, the situation is more complex.
In Linux, the <code>vm.min_free_kbytes</code> parameter defines the memory threshold for the kernel
to start aggressively reclaiming memory, which includes swapping out pages.
If the kubelet's eviction thresholds are set in a way that eviction would take place
before the kernel starts reclaiming memory, it could lead to workloads never
being able to swap out during node memory pressure.
However, setting the eviction thresholds too high could result in the node running out of memory
and invoking the OOM killer, which is not ideal either.</p><p>To address this, it is recommended to set the kubelet's eviction thresholds
to be slightly lower than the <code>vm.min_free_kbytes</code> value.
This way, the node can start swapping before kubelet would start evicting Pods,
allowing workloads to swap out unused data and preventing evictions from happening.
On the other hand, since it is just slightly lower, kubelet is likely to start evicting Pods
before the node runs out of memory, thus avoiding the OOM killer.</p><p>The value of <code>vm.min_free_kbytes</code> can be determined by running the following command on the node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>cat /proc/sys/vm/min_free_kbytes
</span></span></code></pre></div><h3 id="unutilized-swap-space">Unutilized swap space</h3><p>Under the <code>LimitedSwap</code> behavior, the amount of swap available to a Pod is determined automatically,
based on the proportion of the memory requested relative to the node's total memory
(For more details, see the <a href="#how-is-the-swap-limit-being-determined-with-limitedswap">section below</a>).</p><p>This design means that usually there would be some portion of swap that will remain
restricted for Kubernetes workloads.
For example, since Kubernetes 1.34 does not permit swap use for
Pods in the Guaranteed <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/pod-qos/" target="_blank" aria-label="QoS class">QoS class</a>,
the amount of swap that's proportional to the memory request for Guaranteed pods would
remain unused by Kubernetes workloads.</p><p>This behavior carries some risk in a situation where many pods are not eligible for swapping.
On the other hand, it effectively keeps some system-reserved amount of swap memory that can be used by processes
outside of Kubernetes' scope, such as system daemons and even kubelet itself.</p><h2 id="good-practice-for-using-swap-in-a-kubernetes-cluster">Good practice for using swap in a Kubernetes cluster</h2><h3 id="disable-swap-for-system-critical-daemons">Disable swap for system-critical daemons</h3><p>During the testing phase and based on user feedback, it was observed that the performance
of system-critical daemons and services might degrade.
This implies that system daemons, including the kubelet, could operate slower than usual.
If this issue is encountered, it is advisable to configure the cgroup of the system slice
to prevent swapping (i.e., set <code>memory.swap.max=0</code>).</p><h3 id="protect-system-critical-daemons-for-i-o-latency">Protect system-critical daemons for I/O latency</h3><p>Swap can increase the I/O load on a node.
When memory pressure causes the kernel to rapidly swap pages in and out,
system-critical daemons and services that rely on I/O operations may
experience performance degradation.</p><p>To mitigate this, it is recommended for systemd users to prioritize the system slice in terms of I/O latency.
For non-systemd users,
setting up a dedicated cgroup for system daemons and processes and prioritizing I/O latency in the same way is advised.
This can be achieved by setting <code>io.latency</code> for the system slice,
thereby granting it higher I/O priority.
See <a href="https://www.kernel.org/doc/Documentation/admin-guide/cgroup-v2.rst">cgroup's documentation</a> for more info.</p><h3 id="swap-and-control-plane-nodes">Swap and control plane nodes</h3><p>The Kubernetes project recommends running control plane nodes without any swap space configured.
The control plane primarily hosts Guaranteed QoS Pods, so swap can generally be disabled.
The main concern is that swapping critical services on the control plane could negatively impact performance.</p><h3 id="use-of-a-dedicated-disk-for-swap">Use of a dedicated disk for swap</h3><p>The Kubernetes project recommends using encrypted swap, whenever you run nodes with swap enabled.
If swap resides on a partition or the root filesystem, workloads may interfere
with system processes that need to write to disk.
When they share the same disk, processes can overwhelm swap,
disrupting the I/O of kubelet, container runtime, and systemd, which would impact other workloads.
Since swap space is located on a disk, it is crucial to ensure the disk is fast enough for the intended use cases.
Alternatively, one can configure I/O priorities between different mapped areas of a single backing device.</p><h3 id="swap-aware-scheduling">Swap-aware scheduling</h3><p>Kubernetes 1.34 does not support allocating Pods to nodes in a way that accounts
for swap memory usage. The scheduler typically uses <em>requests</em> for infrastructure resources
to guide Pod placement, and Pods do not request swap space; they just request <code>memory</code>.
This means that the scheduler does not consider swap memory when making scheduling decisions.
While this is something we are actively working on, it is not yet implemented.</p><p>In order for administrators to ensure that Pods are not scheduled on nodes
with swap memory unless they are specifically intended to use it,
Administrators can taint nodes with swap available to protect against this problem.
Taints will ensure that workloads which tolerate swap will not spill onto nodes without swap under load.</p><h3 id="selecting-storage-for-optimal-performance">Selecting storage for optimal performance</h3><p>The storage device designated for swap space is critical to maintaining system responsiveness
during high memory usage.
Rotational hard disk drives (HDDs) are ill-suited for this task as their mechanical nature introduces significant latency,
leading to severe performance degradation and system thrashing.
For modern performance needs, a device such as a Solid State Drive (SSD) is probably the appropriate choice for swap,
as its low-latency electronic access minimizes the slowdown.</p><h2 id="swap-behavior-details">Swap behavior details</h2><h3 id="how-is-the-swap-limit-being-determined-with-limitedswap">How is the swap limit being determined with LimitedSwap?</h3><p>The configuration of swap memory, including its limitations, presents a significant
challenge. Not only is it prone to misconfiguration, but as a system-level property, any
misconfiguration could potentially compromise the entire node rather than just a specific
workload. To mitigate this risk and ensure the health of the node, we have implemented
Swap with automatic configuration of limitations.</p><p>With <code>LimitedSwap</code>, Pods that do not fall under the Burstable QoS classification (i.e.
<code>BestEffort</code>/<code>Guaranteed</code> QoS Pods) are prohibited from utilizing swap memory.
<code>BestEffort</code> QoS Pods exhibit unpredictable memory consumption patterns and lack
information regarding their memory usage, making it difficult to determine a safe
allocation of swap memory.
Conversely, <code>Guaranteed</code> QoS Pods are typically employed for applications that rely on the
precise allocation of resources specified by the workload, with memory being immediately available.
To maintain the aforementioned security and node health guarantees,
these Pods are not permitted to use swap memory when <code>LimitedSwap</code> is in effect.
In addition, high-priority pods are not permitted to use swap in order to ensure the memory
they consume always residents on disk, hence ready to use.</p><p>Prior to detailing the calculation of the swap limit, it is necessary to define the following terms:</p><ul><li><code>nodeTotalMemory</code>: The total amount of physical memory available on the node.</li><li><code>totalPodsSwapAvailable</code>: The total amount of swap memory on the node that is available for use by Pods (some swap memory may be reserved for system use).</li><li><code>containerMemoryRequest</code>: The container's memory request.</li></ul><p>Swap limitation is configured as:<br/>( <code>containerMemoryRequest</code> / <code>nodeTotalMemory</code> ) Ã— <code>totalPodsSwapAvailable</code></p><p>In other words, the amount of swap that a container is able to use is proportionate to its
memory request, the node's total physical memory and the total amount of swap memory on
the node that is available for use by Pods.</p><p>It is important to note that, for containers within Burstable QoS Pods, it is possible to
opt-out of swap usage by specifying memory requests that are equal to memory limits.
Containers configured in this manner will not have access to swap memory.</p><h2 id="what-s-next">What's next</h2><ul><li>To learn about managing swap on Linux nodes, read
<a href="/docs/tutorials/cluster-management/provision-swap-memory/">configuring swap memory on Kubernetes nodes</a>.</li><li>You can check out a <a href="/blog/2025/03/25/swap-linux-improvements/">blog post about Kubernetes and swap</a></li><li>For background information, please see the original KEP, <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2400-node-swap">KEP-2400</a>,
and its <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md">design</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Traces For Kubernetes System Components</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [beta]</code></div><p>System component traces record the latency of and relationships between operations in the cluster.</p><p>Kubernetes components emit traces using the
<a href="https://opentelemetry.io/docs/specs/otlp/">OpenTelemetry Protocol</a>
with the gRPC exporter and can be collected and routed to tracing backends using an
<a href="https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector">OpenTelemetry Collector</a>.</p><h2 id="trace-collection">Trace Collection</h2><p>Kubernetes components have built-in gRPC exporters for OTLP to export traces, either with an OpenTelemetry Collector,
or without an OpenTelemetry Collector.</p><p>For a complete guide to collecting traces and using the collector, see
<a href="https://opentelemetry.io/docs/collector/getting-started/">Getting Started with the OpenTelemetry Collector</a>.
However, there are a few things to note that are specific to Kubernetes components.</p><p>By default, Kubernetes components export traces using the grpc exporter for OTLP on the
<a href="https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry">IANA OpenTelemetry port</a>, 4317.
As an example, if the collector is running as a sidecar to a Kubernetes component,
the following receiver configuration will collect spans and log them to standard output:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">receivers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">otlp</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">protocols</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">grpc</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">exporters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Replace this exporter with the exporter for your backend</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">exporters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">debug</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">verbosity</span>:<span style="color:#bbb"> </span>detailed<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">service</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">pipelines</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">traces</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">receivers</span>:<span style="color:#bbb"> </span>[otlp]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">exporters</span>:<span style="color:#bbb"> </span>[debug]<span style="color:#bbb">
</span></span></span></code></pre></div><p>To directly emit traces to a backend without utilizing a collector,
specify the endpoint field in the Kubernetes tracing configuration file with the desired trace backend address.
This method negates the need for a collector and simplifies the overall structure.</p><p>For trace backend header configuration, including authentication details, environment variables can be used with <code>OTEL_EXPORTER_OTLP_HEADERS</code>,
see <a href="https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/">OTLP Exporter Configuration</a>.</p><p>Additionally, for trace resource attribute configuration such as Kubernetes cluster name, namespace, Pod name, etc.,
environment variables can also be used with <code>OTEL_RESOURCE_ATTRIBUTES</code>, see <a href="https://opentelemetry.io/docs/specs/semconv/resource/k8s/">OTLP Kubernetes Resource</a>.</p><h2 id="component-traces">Component traces</h2><h3 id="kube-apiserver-traces">kube-apiserver traces</h3><p>The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests
to webhooks, etcd, and re-entrant requests. It propagates the
<a href="https://www.w3.org/TR/trace-context/">W3C Trace Context</a> with outgoing requests
but does not make use of the trace context attached to incoming requests,
as the kube-apiserver is often a public endpoint.</p><h4 id="enabling-tracing-in-the-kube-apiserver">Enabling tracing in the kube-apiserver</h4><p>To enable tracing, provide the kube-apiserver with a tracing configuration file
with <code>--tracing-config-file=&lt;path-to-config&gt;</code>. This is an example config that records
spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>TracingConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># default value</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic">#endpoint: localhost:4317</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">samplingRatePerMillion</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>For more information about the <code>TracingConfiguration</code> struct, see
<a href="/docs/reference/config-api/apiserver-config.v1/#apiserver-k8s-io-v1-TracingConfiguration">API server config API (v1)</a>.</p><h3 id="kubelet-traces">kubelet traces</h3><div class="feature-state-notice feature-stable" title="Feature Gate: KubeletTracing"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>The kubelet CRI interface and authenticated http servers are instrumented to generate
trace spans. As with the apiserver, the endpoint and sampling rate are configurable.
Trace context propagation is also configured. A parent span's sampling decision is always respected.
A provided tracing configuration sampling rate will apply to spans without a parent.
Enabled without a configured endpoint, the default OpenTelemetry Collector receiver address of "localhost:4317" is set.</p><h4 id="enabling-tracing-in-the-kubelet">Enabling tracing in the kubelet</h4><p>To enable tracing, apply the <a href="https://github.com/kubernetes/component-base/blob/release-1.27/tracing/api/v1/types.go">tracing configuration</a>.
This is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">tracing</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># default value</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#endpoint: localhost:4317</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">samplingRatePerMillion</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>If the <code>samplingRatePerMillion</code> is set to one million (<code>1000000</code>), then every
span will be sent to the exporter.</p><p>The kubelet in Kubernetes v1.34 collects spans from
the garbage collection, pod synchronization routine as well as every gRPC
method. The kubelet propagates trace context with gRPC requests so that
container runtimes with trace instrumentation, such as CRI-O and containerd,
can associate their exported spans with the trace context from the kubelet.
The resulting traces will have parent-child links between kubelet and
container runtime spans, providing helpful context when debugging node
issues.</p><p>Please note that exporting spans always comes with a small performance overhead
on the networking and CPU side, depending on the overall configuration of the
system. If there is any issue like that in a cluster which is running with
tracing enabled, then mitigate the problem by either reducing the
<code>samplingRatePerMillion</code> or disabling tracing completely by removing the
configuration.</p><h2 id="stability">Stability</h2><p>Tracing instrumentation is still under active development, and may change
in a variety of ways. This includes span names, attached attributes,
instrumented endpoints, etc. Until this feature graduates to stable,
there are no guarantees of backwards compatibility for tracing instrumentation.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="https://opentelemetry.io/docs/collector/getting-started/">Getting Started with the OpenTelemetry Collector</a></li><li>Read about <a href="https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/">OTLP Exporter Configuration</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Good practices for Dynamic Resource Allocation as a Cluster Admin</h1><p>This page describes good practices when configuring a Kubernetes cluster
utilizing Dynamic Resource Allocation (DRA). These instructions are for cluster
administrators.</p><h2 id="separate-permissions-to-dra-related-apis">Separate permissions to DRA related APIs</h2><p>DRA is orchestrated through a number of different APIs. Use authorization tools
(like RBAC, or another solution) to control access to the right APIs depending
on the persona of your user.</p><p>In general, DeviceClasses and ResourceSlices should be restricted to admins and
the DRA drivers. Cluster operators that will be deploying Pods with claims will
need access to ResourceClaim and ResourceClaimTemplate APIs; both of these APIs
are namespace scoped.</p><h2 id="dra-driver-deployment-and-maintenance">DRA driver deployment and maintenance</h2><p>DRA drivers are third-party applications that run on each node of your cluster
to interface with the hardware of that node and Kubernetes' native DRA
components. The installation procedure depends on the driver you choose, but is
likely deployed as a DaemonSet to all or a selection of the nodes (using node
selectors or similar mechanisms) in your cluster.</p><h3 id="use-drivers-with-seamless-upgrade-if-available">Use drivers with seamless upgrade if available</h3><p>DRA drivers implement the <a href="https://pkg.go.dev/k8s.io/dynamic-resource-allocation/kubeletplugin"><code>kubeletplugin</code> package
interface</a>.
Your driver may support <em>seamless upgrades</em> by implementing a property of this
interface that allows two versions of the same DRA driver to coexist for a short
time. This is only available for kubelet versions 1.33 and above and may not be
supported by your driver for heterogeneous clusters with attached nodes running
older versions of Kubernetes - check your driver's documentation to be sure.</p><p>If seamless upgrades are available for your situation, consider using it to
minimize scheduling delays when your driver updates.</p><p>If you cannot use seamless upgrades, during driver downtime for upgrades you may
observe that:</p><ul><li>Pods cannot start unless the claims they depend on were already prepared for
use.</li><li>Cleanup after the last pod which used a claim gets delayed until the driver is
available again. The pod is not marked as terminated. This prevents reusing
the resources used by the pod for other pods.</li><li>Running pods will continue to run.</li></ul><h3 id="confirm-your-dra-driver-exposes-a-liveness-probe-and-utilize-it">Confirm your DRA driver exposes a liveness probe and utilize it</h3><p>Your DRA driver likely implements a gRPC socket for healthchecks as part of DRA
driver good practices. The easiest way to utilize this grpc socket is to
configure it as a liveness probe for the DaemonSet deploying your DRA driver.
Your driver's documentation or deployment tooling may already include this, but
if you are building your configuration separately or not running your DRA driver
as a Kubernetes pod, be sure that your orchestration tooling restarts the DRA
driver on failed healthchecks to this grpc socket. Doing so will minimize any
accidental downtime of the DRA driver and give it more opportunities to self
heal, reducing scheduling delays or troubleshooting time.</p><h3 id="when-draining-a-node-drain-the-dra-driver-as-late-as-possible">When draining a node, drain the DRA driver as late as possible</h3><p>The DRA driver is responsible for unpreparing any devices that were allocated to
Pods, and if the DRA driver is <a class="glossary-tooltip" title="Safely evicts Pods from a Node to prepare for maintenance or removal." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-drain" target="_blank" aria-label="drained">drained</a> before Pods with claims have been deleted, it will not be
able to finalize its cleanup. If you implement custom drain logic for nodes,
consider checking that there are no allocated/reserved ResourceClaim or
ResourceClaimTemplates before terminating the DRA driver itself.</p><h2 id="monitor-and-tune-components-for-higher-load-especially-in-high-scale-environments">Monitor and tune components for higher load, especially in high scale environments</h2><p>Control plane component <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a> and the internal ResourceClaim controller
orchestrated by the component <a class="glossary-tooltip" title="Control Plane component that runs controller processes." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" aria-label="kube-controller-manager">kube-controller-manager</a> do the
heavy lifting during scheduling of Pods with claims based on metadata stored in
the DRA APIs. Compared to non-DRA scheduled Pods, the number of API server
calls, memory, and CPU utilization needed by these components is increased for
Pods using DRA claims. In addition, node local components like the DRA driver
and kubelet utilize DRA APIs to allocated the hardware request at Pod sandbox
creation time. Especially in high scale environments where clusters have many
nodes, and/or deploy many workloads that heavily utilize DRA defined resource
claims, the cluster administrator should configure the relevant components to
anticipate the increased load.</p><p>The effects of mistuned components can have direct or snowballing affects
causing different symptoms during the Pod lifecycle. If the <code>kube-scheduler</code>
component's QPS and burst configurations are too low, the scheduler might
quickly identify a suitable node for a Pod but take longer to bind the Pod to
that node. With DRA, during Pod scheduling, the QPS and Burst parameters in the
client-go configuration within <code>kube-controller-manager</code> are critical.</p><p>The specific values to tune your cluster to depend on a variety of factors like
number of nodes/pods, rate of pod creation, churn, even in non-DRA environments;
see the <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">SIG Scalability README on Kubernetes scalability
thresholds</a>
for more information. In scale tests performed against a DRA enabled cluster
with 100 nodes, involving 720 long-lived pods (90% saturation) and 80 churn pods
(10% churn, 10 times), with a job creation QPS of 10, <code>kube-controller-manager</code>
QPS could be set to as low as 75 and Burst to 150 to meet equivalent metric
targets for non-DRA deployments. At this lower bound, it was observed that the
client side rate limiter was triggered enough to protect the API server from
explosive burst but was high enough that pod startup SLOs were not impacted.
While this is a good starting point, you can get a better idea of how to tune
the different components that have the biggest effect on DRA performance for
your deployment by monitoring the following metrics. For more information on all
the stable metrics in Kubernetes, see the <a href="/docs/reference/generated/metrics/">Kubernetes Metrics
Reference</a>.</p><h3 id="kube-controller-manager-metrics"><code>kube-controller-manager</code> metrics</h3><p>The following metrics look closely at the internal ResourceClaim controller
managed by the <code>kube-controller-manager</code> component.</p><ul><li>Workqueue Add Rate: Monitor <code class="code-inline language-promql"><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">workqueue_adds_total</span>{<span style="color:#a0a000">name</span><span style="color:#666">=</span>"<span style="color:#b44">resource_claim</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb"> </span></code>to gauge how quickly items are added to the ResourceClaim controller.</li><li>Workqueue Depth: Track
<code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#b8860b">workqueue_depth</span>{<span style="color:#a0a000">endpoint</span><span style="color:#666">=</span>"<span style="color:#b44">kube-controller-manager</span>",<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a0a000">name</span><span style="color:#666">=</span>"<span style="color:#b44">resource_claim</span>"}<span style="color:#666">)</span></code> to identify any backlogs in the ResourceClaim
controller.</li><li>Workqueue Work Duration: Observe <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">workqueue_work_duration_seconds_bucket</span>{<span style="color:#a0a000">name</span><span style="color:#666">=</span>"<span style="color:#b44">resource_claim</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code> to understand the speed at which the ResourceClaim controller
processes work.</li></ul><p>If you are experiencing low Workqueue Add Rate, high Workqueue Depth, and/or
high Workqueue Work Duration, this suggests the controller isn't performing
optimally. Consider tuning parameters like QPS, burst, and CPU/memory
configurations.</p><p>If you are experiencing high Workequeue Add Rate, high Workqueue Depth, but
reasonable Workqueue Work Duration, this indicates the controller is processing
work, but concurrency might be insufficient. Concurrency is hardcoded in the
controller, so as a cluster administrator, you can tune for this by reducing the
pod creation QPS, so the add rate to the resource claim workqueue is more
manageable.</p><h3 id="kube-scheduler-metrics"><code>kube-scheduler</code> metrics</h3><p>The following scheduler metrics are high level metrics aggregating performance
across all Pods scheduled, not just those using DRA. It is important to note
that the end-to-end metrics are ultimately influenced by the
<code>kube-controller-manager</code>'s performance in creating ResourceClaims from
ResourceClainTemplates in deployments that heavily use ResourceClainTemplates.</p><ul><li>Scheduler End-to-End Duration: Monitor <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">increase</span><span style="color:#666">(</span><span style="color:#b8860b">scheduler_pod_scheduling_sli_duration_seconds_bucket</span>[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li><li>Scheduler Algorithm Latency: Track <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">increase</span><span style="color:#666">(</span><span style="color:#b8860b">scheduler_scheduling_algorithm_duration_seconds_bucket</span>[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li></ul><h3 id="kubelet-metrics"><code>kubelet</code> metrics</h3><p>When a Pod bound to a node must have a ResourceClaim satisfied, kubelet calls
the <code>NodePrepareResources</code> and <code>NodeUnprepareResources</code> methods of the DRA
driver. You can observe this behavior from the kubelet's point of view with the
following metrics.</p><ul><li>Kubelet NodePrepareResources: Monitor <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">operation_name</span><span style="color:#666">=</span>"<span style="color:#b44">PrepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li><li>Kubelet NodeUnprepareResources: Track <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">operation_name</span><span style="color:#666">=</span>"<span style="color:#b44">UnprepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li></ul><h3 id="dra-kubeletplugin-operations">DRA kubeletplugin operations</h3><p>DRA drivers implement the <a href="https://pkg.go.dev/k8s.io/dynamic-resource-allocation/kubeletplugin"><code>kubeletplugin</code> package
interface</a>
which surfaces its own metric for the underlying gRPC operation
<code>NodePrepareResources</code> and <code>NodeUnprepareResources</code>. You can observe this
behavior from the point of view of the internal kubeletplugin with the following
metrics.</p><ul><li>DRA kubeletplugin gRPC NodePrepareResources operation: Observe <code class="code-inline language-promql"><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_grpc_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">method_name</span><span style="color:#666">=~</span>"<span style="color:#b44">.*NodePrepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li><li>DRA kubeletplugin gRPC NodeUnprepareResources operation: Observe <code class="code-inline language-promql"><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">histogram_quantile</span><span style="color:#666">(</span><span style="color:#666">0.99</span>,<span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">sum</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:700">rate</span><span style="color:#666">(</span><span style="color:#b8860b">dra_grpc_operations_duration_seconds_bucket</span>{<span style="color:#a0a000">method_name</span><span style="color:#666">=~</span>"<span style="color:#b44">.*NodeUnprepareResources</span>"}[<span style="color:#b44">5m</span>]<span style="color:#666">))</span><span style="color:#bbb">
</span><span style="color:#bbb"/><span style="color:#a2f;font-weight:700">by</span><span style="color:#bbb"> </span><span style="color:#666">(</span><span style="color:#b8860b">le</span><span style="color:#666">))</span></code>.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Learn more about
DRA</a></li><li>Read the <a href="/docs/reference/generated/metrics/">Kubernetes Metrics
Reference</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">System Logs</h1><p>System component logs record events happening in cluster, which can be very useful for debugging.
You can configure log verbosity to see more or less detail.
Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing
step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or
scheduler decisions).</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>In contrast to the command line flags described here, the <em>log
output</em> itself does <em>not</em> fall under the Kubernetes API stability guarantees:
individual log entries and their formatting may change from one release
to the next!</div><h2 id="klog">Klog</h2><p>klog is the Kubernetes logging library. <a href="https://github.com/kubernetes/klog">klog</a>
generates log messages for the Kubernetes system components.</p><p>Kubernetes is in the process of simplifying logging in its components.
The following klog command line flags
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">are deprecated</a>
starting with Kubernetes v1.23 and removed in Kubernetes v1.26:</p><ul><li><code>--add-dir-header</code></li><li><code>--alsologtostderr</code></li><li><code>--log-backtrace-at</code></li><li><code>--log-dir</code></li><li><code>--log-file</code></li><li><code>--log-file-max-size</code></li><li><code>--logtostderr</code></li><li><code>--one-output</code></li><li><code>--skip-headers</code></li><li><code>--skip-log-headers</code></li><li><code>--stderrthreshold</code></li></ul><p>Output will always be written to stderr, regardless of the output format. Output redirection is
expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX
shell or a tool like systemd.</p><p>In some cases, for example a distroless container or a Windows system service, those options are
not available. Then the
<a href="https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md"><code>kube-log-runner</code></a>
binary can be used as wrapper around a Kubernetes component to redirect
output. A prebuilt binary is included in several Kubernetes base images under
its traditional name as <code>/go-runner</code> and as <code>kube-log-runner</code> in server and
node release archives.</p><p>This table shows how <code>kube-log-runner</code> invocations correspond to shell redirection:</p><table><thead><tr><th>Usage</th><th>POSIX shell (such as bash)</th><th><code>kube-log-runner &lt;options&gt; &lt;cmd&gt;</code></th></tr></thead><tbody><tr><td>Merge stderr and stdout, write to stdout</td><td><code>2&gt;&amp;1</code></td><td><code>kube-log-runner</code> (default behavior)</td></tr><tr><td>Redirect both into log file</td><td><code>1&gt;&gt;/tmp/log 2&gt;&amp;1</code></td><td><code>kube-log-runner -log-file=/tmp/log</code></td></tr><tr><td>Copy into log file and to stdout</td><td><code>2&gt;&amp;1 | tee -a /tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -also-stdout</code></td></tr><tr><td>Redirect only stdout into log file</td><td><code>&gt;/tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -redirect-stderr=false</code></td></tr></tbody></table><h3 id="klog-output">Klog output</h3><p>An example of the traditional klog native format:</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><p>The message string may contain line breaks:</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 example.go:79] This is a message
which has a line break.
</code></pre><h3 id="structured-logging">Structured Logging</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><p>Migration to structured log messages is an ongoing process. Not all log messages are structured in
this version. When parsing log files, you must also handle unstructured log messages.</p><p>Log formatting and value serialization are subject to change.</p></div><p>Structured logging introduces a uniform structure in log messages allowing for programmatic
extraction of information. You can store and process structured logs with less effort and cost.
The code which generates a log message determines whether it uses the traditional unstructured
klog output or structured logging.</p><p>The default formatting of structured log messages is as text, with a format that is backward
compatible with traditional klog:</p><pre tabindex="0"><code>&lt;klog header&gt; "&lt;message&gt;" &lt;key1&gt;="&lt;value1&gt;" &lt;key2&gt;="&lt;value2&gt;" ...
</code></pre><p>Example:</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 controller_utils.go:116] "Pod status updated" pod="kube-system/kubedns" status="ready"
</code></pre><p>Strings are quoted. Other values are formatted with
<a href="https://pkg.go.dev/fmt#hdr-Printing"><code>%+v</code></a>, which may cause log messages to
continue on the next line <a href="https://github.com/kubernetes/kubernetes/issues/106428">depending on the data</a>.</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 example.go:116] "Example" data="This is text with a line break\nand \"quotation marks\"." someInt=1 someFloat=0.1 someStruct={StringField: First line,
second line.}
</code></pre><h3 id="contextual-logging">Contextual Logging</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code></div><p>Contextual logging builds on top of structured logging. It is primarily about
how developers use logging calls: code based on that concept is more flexible
and supports additional use cases as described in the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging">Contextual Logging
KEP</a>.</p><p>If developers use additional functions like <code>WithValues</code> or <code>WithName</code> in
their components, then log entries contain additional information that gets
passed into functions by their caller.</p><p>For Kubernetes 1.34, this is gated behind the <code>ContextualLogging</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> and is
enabled by default. The infrastructure for this was added in 1.24 without
modifying components. The
<a href="https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go"><code>component-base/logs/example</code></a>
command demonstrates how to use the new logging calls and how a component
behaves that supports contextual logging.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:navy;font-weight:700">$</span> <span style="color:#a2f">cd</span> <span style="color:#b8860b">$GOPATH</span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/
</span></span><span style="display:flex"><span><span style="color:navy;font-weight:700">$</span> go run . --help
</span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span><span style="display:flex"><span><span style="color:#888">      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
</span></span></span><span style="display:flex"><span><span style="color:#888">                                     AllAlpha=true|false (ALPHA - default=false)
</span></span></span><span style="display:flex"><span><span style="color:#888">                                     AllBeta=true|false (BETA - default=false)
</span></span></span><span style="display:flex"><span><span style="color:#888">                                     ContextualLogging=true|false (BETA - default=true)
</span></span></span><span style="display:flex"><span><span style="color:#888"/><span style="color:navy;font-weight:700">$</span> go run . --feature-gates <span style="color:#b8860b">ContextualLogging</span><span style="color:#666">=</span><span style="color:#a2f">true</span>
</span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span><span style="display:flex"><span><span style="color:#888">I0222 15:13:31.645988  197901 example.go:54] "runtime" logger="example.myname" foo="bar" duration="1m0s"
</span></span></span><span style="display:flex"><span><span style="color:#888">I0222 15:13:31.646007  197901 example.go:55] "another runtime" logger="example" foo="bar" duration="1h0m0s" duration="1m0s"
</span></span></span></code></pre></div><p>The <code>logger</code> key and <code>foo="bar"</code> were added by the caller of the function
which logs the <code>runtime</code> message and <code>duration="1m0s"</code> value, without having to
modify that function.</p><p>With contextual logging disable, <code>WithValues</code> and <code>WithName</code> do nothing and log
calls go through the global klog logger. Therefore this additional information
is not in the log output anymore:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:navy;font-weight:700">$</span> go run . --feature-gates <span style="color:#b8860b">ContextualLogging</span><span style="color:#666">=</span><span style="color:#a2f">false</span>
</span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span><span style="display:flex"><span><span style="color:#888">I0222 15:14:40.497333  198174 example.go:54] "runtime" duration="1m0s"
</span></span></span><span style="display:flex"><span><span style="color:#888">I0222 15:14:40.497346  198174 example.go:55] "another runtime" duration="1h0m0s" duration="1m0s"
</span></span></span></code></pre></div><h3 id="json-log-format">JSON log format</h3><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [alpha]</code></div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4><p>JSON output does not support many standard klog flags. For list of unsupported klog flags, see the
<a href="/docs/reference/command-line-tools-reference/">Command line tool reference</a>.</p><p>Not all logs are guaranteed to be written in JSON format (for example, during process start).
If you intend to parse logs, make sure you can handle log lines that are not JSON as well.</p><p>Field names and JSON serialization are subject to change.</p></div><p>The <code>--logging-format=json</code> flag changes the format of logs from klog native format to JSON format.
Example of JSON log format (pretty printed):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"ts"</span>: <span style="color:#666">1580306777.04728</span>,
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"v"</span>: <span style="color:#666">4</span>,
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"msg"</span>: <span style="color:#b44">"Pod status updated"</span>,
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"pod"</span>:{
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"name"</span>: <span style="color:#b44">"nginx-1"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"namespace"</span>: <span style="color:#b44">"default"</span>
</span></span><span style="display:flex"><span>   },
</span></span><span style="display:flex"><span>   <span style="color:green;font-weight:700">"status"</span>: <span style="color:#b44">"ready"</span>
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>Keys with special meaning:</p><ul><li><code>ts</code> - timestamp as Unix time (required, float)</li><li><code>v</code> - verbosity (only for info and not for error messages, int)</li><li><code>err</code> - error string (optional, string)</li><li><code>msg</code> - message (required, string)</li></ul><p>List of components currently supporting JSON format:</p><ul><li><a class="glossary-tooltip" title="Control Plane component that runs controller processes." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" aria-label="kube-controller-manager">kube-controller-manager</a></li><li><a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="kube-apiserver">kube-apiserver</a></li><li><a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a></li><li><a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a></li></ul><h3 id="log-verbosity-level">Log verbosity level</h3><p>The <code>-v</code> flag controls log verbosity. Increasing the value increases the number of logged events.
Decreasing the value decreases the number of logged events. Increasing verbosity settings logs
increasingly less severe events. A verbosity setting of 0 logs only critical events.</p><h3 id="log-location">Log location</h3><p>There are two types of system components: those that run in a container and those
that do not run in a container. For example:</p><ul><li>The Kubernetes scheduler and kube-proxy run in a container.</li><li>The kubelet and <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>
do not run in containers.</li></ul><p>On machines with systemd, the kubelet and container runtime write to journald.
Otherwise, they write to <code>.log</code> files in the <code>/var/log</code> directory.
System components inside containers always write to <code>.log</code> files in the <code>/var/log</code> directory,
bypassing the default logging mechanism.
Similar to the container logs, you should rotate system component logs in the <code>/var/log</code> directory.
In Kubernetes clusters created by the <code>kube-up.sh</code> script, log rotation is configured by the <code>logrotate</code> tool.
The <code>logrotate</code> tool rotates logs daily, or once the log size is greater than 100MB.</p><h2 id="log-query">Log query</h2><div class="feature-state-notice feature-beta" title="Feature Gate: NodeLogQuery"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code> (enabled by default: false)</div><p>To help with debugging issues on nodes, Kubernetes v1.27 introduced a feature that allows viewing logs of services
running on the node. To use the feature, ensure that the <code>NodeLogQuery</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled for that node, and that the
kubelet configuration options <code>enableSystemLogHandler</code> and <code>enableSystemLogQuery</code> are both set to true. On Linux
the assumption is that service logs are available via journald. On Windows the assumption is that service logs are
available in the application log provider. On both operating systems, logs are also available by reading files within
<code>/var/log/</code>.</p><p>Provided you are authorized to interact with node objects, you can try out this feature on all your nodes or
just a subset. Here is an example to retrieve the kubelet service logs from a node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Fetch kubelet logs from a node named node-1.example</span>
</span></span><span style="display:flex"><span>kubectl get --raw <span style="color:#b44">"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet"</span>
</span></span></code></pre></div><p>You can also fetch files, provided that the files are in a directory that the kubelet allows for log
fetches. For example, you can fetch a log from <code>/var/log</code> on a Linux node:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get --raw <span style="color:#b44">"/api/v1/nodes/&lt;insert-node-name-here&gt;/proxy/logs/?query=/&lt;insert-log-file-name-here&gt;"</span>
</span></span></code></pre></div><p>The kubelet uses heuristics to retrieve logs. This helps if you are not aware whether a given system service is
writing logs to the operating system's native logger like journald or to a log file in <code>/var/log/</code>. The heuristics
first checks the native logger and if that is not available attempts to retrieve the first logs from
<code>/var/log/&lt;servicename&gt;</code> or <code>/var/log/&lt;servicename&gt;.log</code> or <code>/var/log/&lt;servicename&gt;/&lt;servicename&gt;.log</code>.</p><p>The complete list of options that can be used are:</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>boot</code></td><td>boot show messages from a specific system boot</td></tr><tr><td><code>pattern</code></td><td>pattern filters log entries by the provided PERL-compatible regular expression</td></tr><tr><td><code>query</code></td><td>query specifies services(s) or files from which to return logs (required)</td></tr><tr><td><code>sinceTime</code></td><td>an <a href="https://www.rfc-editor.org/rfc/rfc3339">RFC3339</a> timestamp from which to show logs (inclusive)</td></tr><tr><td><code>untilTime</code></td><td>an <a href="https://www.rfc-editor.org/rfc/rfc3339">RFC3339</a> timestamp until which to show logs (inclusive)</td></tr><tr><td><code>tailLines</code></td><td>specify how many lines from the end of the log to retrieve; the default is to fetch the whole log</td></tr></tbody></table><p>Example of a more complex query:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># Fetch kubelet logs from a node named node-1.example that have the word "error"</span>
</span></span><span style="display:flex"><span>kubectl get --raw <span style="color:#b44">"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&amp;pattern=error"</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read about the <a href="/docs/concepts/cluster-administration/logging/">Kubernetes Logging Architecture</a></li><li>Read about <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging">Structured Logging</a></li><li>Read about <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging">Contextual Logging</a></li><li>Read about <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">deprecation of klog flags</a></li><li>Read about the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md">Conventions for logging severity</a></li><li>Read about <a href="https://kep.k8s.io/2258">Log Query</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Resource Management for Windows nodes</h1><p>This page outlines the differences in how resources are managed between Linux and Windows.</p><p>On Linux nodes, <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank" aria-label="cgroups">cgroups</a> are used
as a pod boundary for resource control. Containers are created within that boundary
for network, process and file system isolation. The Linux cgroup APIs can be used to
gather CPU, I/O, and memory use statistics.</p><p>In contrast, Windows uses a <a href="https://docs.microsoft.com/windows/win32/procthread/job-objects"><em>job object</em></a> per container with a system namespace filter
to contain all processes in a container and provide logical isolation from the
host.
(Job objects are a Windows process isolation mechanism and are different from
what Kubernetes refers to as a <a class="glossary-tooltip" title="A finite or batch task that runs to completion." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/job/" target="_blank" aria-label="Job">Job</a>).</p><p>There is no way to run a Windows container without the namespace filtering in
place. This means that system privileges cannot be asserted in the context of the
host, and thus privileged containers are not available on Windows.
Containers cannot assume an identity from the host because the Security Account Manager
(SAM) is separate.</p><h2 id="resource-management-memory">Memory management</h2><p>Windows does not have an out-of-memory process killer as Linux does. Windows always
treats all user-mode memory allocations as virtual, and pagefiles are mandatory.</p><p>Windows nodes do not overcommit memory for processes. The
net effect is that Windows won't reach out of memory conditions the same way Linux
does, and processes page to disk instead of being subject to out of memory (OOM)
termination. If memory is over-provisioned and all physical memory is exhausted,
then paging can slow down performance.</p><h2 id="resource-management-cpu">CPU management</h2><p>Windows can limit the amount of CPU time allocated for different processes but cannot
guarantee a minimum amount of CPU time.</p><p>On Windows, the kubelet supports a command-line flag to set the
<a href="https://docs.microsoft.com/windows/win32/procthread/scheduling-priorities">scheduling priority</a> of the
kubelet process: <code>--windows-priorityclass</code>. This flag allows the kubelet process to get
more CPU time slices when compared to other processes running on the Windows host.
More information on the allowable values and their meaning is available at
<a href="https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class">Windows Priority Classes</a>.
To ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to <code>ABOVE_NORMAL_PRIORITY_CLASS</code> or above.</p><h2 id="resource-reservation">Resource reservation</h2><p>To account for memory and CPU used by the operating system, the container runtime, and by
Kubernetes host processes such as the kubelet, you can (and should) reserve
memory and CPU resources with the <code>--kube-reserved</code> and/or <code>--system-reserved</code> kubelet flags.
On Windows these values are only used to calculate the node's
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a> resources.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>As you deploy workloads, set resource memory and CPU limits on containers.
This also subtracts from <code>NodeAllocatable</code> and helps the cluster-wide scheduler in determining which pods to place on which nodes.</p><p>Scheduling pods without limits may over-provision the Windows nodes and in extreme
cases can cause the nodes to become unhealthy.</p></div><p>On Windows, a good practice is to reserve at least 2GiB of memory.</p><p>To determine how much CPU to reserve,
identify the maximum pod density for each node and monitor the CPU usage of
the system services running there, then choose a value that meets your workload needs.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Images</h1><p>A container image represents binary data that encapsulates an application and all its
software dependencies. Container images are executable software bundles that can run
standalone and that make very well-defined assumptions about their runtime environment.</p><p>You typically create a container image of your application and push it to a registry
before referring to it in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>.</p><p>This page provides an outline of the container image concept.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you are looking for the container images for a Kubernetes
release (such as v1.34, the latest minor release),
visit <a href="https://kubernetes.io/releases/download/">Download Kubernetes</a>.</div><h2 id="image-names">Image names</h2><p>Container images are usually given a name such as <code>pause</code>, <code>example/mycontainer</code>, or <code>kube-apiserver</code>.
Images can also include a registry hostname; for example: <code>fictional.registry.example/imagename</code>,
and possibly a port number as well; for example: <code>fictional.registry.example:10443/imagename</code>.</p><p>If you don't specify a registry hostname, Kubernetes assumes that you mean the <a href="https://hub.docker.com/">Docker public registry</a>.
You can change this behavior by setting a default image registry in the
<a href="/docs/setup/production-environment/container-runtimes/">container runtime</a> configuration.</p><p>After the image name part you can add a <em>tag</em> or <em>digest</em> (in the same way you would when using with commands
like <code>docker</code> or <code>podman</code>). Tags let you identify different versions of the same series of images.
Digests are a unique identifier for a specific version of an image. Digests are hashes of the image's content,
and are immutable. Tags can be moved to point to different images, but digests are fixed.</p><p>Image tags consist of lowercase and uppercase letters, digits, underscores (<code>_</code>),
periods (<code>.</code>), and dashes (<code>-</code>). A tag can be up to 128 characters long, and must
conform to the following regex pattern: <code>[a-zA-Z0-9_][a-zA-Z0-9._-]{0,127}</code>.
You can read more about it and find the validation regex in the
<a href="https://github.com/opencontainers/distribution-spec/blob/master/spec.md#workflow-categories">OCI Distribution Specification</a>.
If you don't specify a tag, Kubernetes assumes you mean the tag <code>latest</code>.</p><p>Image digests consists of a hash algorithm (such as <code>sha256</code>) and a hash value. For example:
<code>sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07</code>.
You can find more information about the digest format in the
<a href="https://github.com/opencontainers/image-spec/blob/master/descriptor.md#digests">OCI Image Specification</a>.</p><p>Some image name examples that Kubernetes can use are:</p><ul><li><code>busybox</code> â€” Image name only, no tag or digest. Kubernetes will use the Docker
public registry and latest tag. Equivalent to <code>docker.io/library/busybox:latest</code>.</li><li><code>busybox:1.32.0</code> â€” Image name with tag. Kubernetes will use the Docker
public registry. Equivalent to <code>docker.io/library/busybox:1.32.0</code>.</li><li><code>registry.k8s.io/pause:latest</code> â€” Image name with a custom registry and latest tag.</li><li><code>registry.k8s.io/pause:3.5</code> â€” Image name with a custom registry and non-latest tag.</li><li><code>registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07</code> â€”
Image name with digest.</li><li><code>registry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07</code> â€”
Image name with tag and digest. Only the digest will be used for pulling.</li></ul><h2 id="updating-images">Updating images</h2><p>When you first create a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>,
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a>, Pod, or other
object that includes a PodTemplate, and a pull policy was not explicitly specified,
then by default the pull policy of all containers in that Pod will be set to
<code>IfNotPresent</code>. This policy causes the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> to skip pulling an
image if it already exists.</p><h3 id="image-pull-policy">Image pull policy</h3><p>The <code>imagePullPolicy</code> for a container and the tag of the image both affect <em>when</em> the
<a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> attempts to pull
(download) the specified image.</p><p>Here's a list of the values you can set for <code>imagePullPolicy</code> and the effects
these values have:</p><dl><dt><code>IfNotPresent</code></dt><dd>the image is pulled only if it is not already present locally.</dd><dt><code>Always</code></dt><dd>every time the kubelet launches a container, the kubelet queries the container
image registry to resolve the name to an image
<a href="https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier">digest</a>.
If the kubelet has a container image with that exact digest cached locally, the kubelet uses its
cached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that image
to launch the container.</dd><dt><code>Never</code></dt><dd>the kubelet does not try fetching the image. If the image is somehow already present
locally, the kubelet attempts to start the container; otherwise, startup fails.
See <a href="#pre-pulled-images">pre-pulled images</a> for more details.</dd></dl><p>The caching semantics of the underlying image provider make even
<code>imagePullPolicy: Always</code> efficient, as long as the registry is reliably accessible.
Your container runtime can notice that the image layers already exist on the node
so that they don't need to be downloaded again.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>You should avoid using the <code>:latest</code> tag when deploying containers in production as
it is harder to track which version of the image is running and more difficult to
roll back properly.</p><p>Instead, specify a meaningful tag such as <code>v1.42.0</code> and/or a digest.</p></div><p>To make sure the Pod always uses the same version of a container image, you can specify
the image's digest;
replace <code>&lt;image-name&gt;:&lt;tag&gt;</code> with <code>&lt;image-name&gt;@&lt;digest&gt;</code>
(for example, <code>image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</code>).</p><p>When using image tags, if the image registry were to change the code that the tag on that image
represents, you might end up with a mix of Pods running the old and new code. An image digest
uniquely identifies a specific version of the image, so Kubernetes runs the same code every time
it starts a container with that image name and digest specified. Specifying an image by digest
pins the code that you run so that a change at the registry cannot lead to that mix of versions.</p><p>There are third-party <a href="/docs/reference/access-authn-authz/admission-controllers/">admission controllers</a>
that mutate Pods (and PodTemplates) when they are created, so that the
running workload is defined based on an image digest rather than a tag.
That might be useful if you want to make sure that your entire workload is
running the same code no matter what tag changes happen at the registry.</p><h4 id="imagepullpolicy-defaulting">Default image pull policy</h4><p>When you (or a controller) submit a new Pod to the API server, your cluster sets the
<code>imagePullPolicy</code> field when specific conditions are met:</p><ul><li>if you omit the <code>imagePullPolicy</code> field, and you specify the digest for the
container image, the <code>imagePullPolicy</code> is automatically set to <code>IfNotPresent</code>.</li><li>if you omit the <code>imagePullPolicy</code> field, and the tag for the container image is
<code>:latest</code>, <code>imagePullPolicy</code> is automatically set to <code>Always</code>.</li><li>if you omit the <code>imagePullPolicy</code> field, and you don't specify the tag for the
container image, <code>imagePullPolicy</code> is automatically set to <code>Always</code>.</li><li>if you omit the <code>imagePullPolicy</code> field, and you specify a tag for the container
image that isn't <code>:latest</code>, the <code>imagePullPolicy</code> is automatically set to
<code>IfNotPresent</code>.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>The value of <code>imagePullPolicy</code> of the container is always set when the object is
first <em>created</em>, and is not updated if the image's tag or digest later changes.</p><p>For example, if you create a Deployment with an image whose tag is <em>not</em>
<code>:latest</code>, and later update that Deployment's image to a <code>:latest</code> tag, the
<code>imagePullPolicy</code> field will <em>not</em> change to <code>Always</code>. You must manually change
the pull policy of any object after its initial creation.</p></div><h4 id="required-image-pull">Required image pull</h4><p>If you would like to always force a pull, you can do one of the following:</p><ul><li>Set the <code>imagePullPolicy</code> of the container to <code>Always</code>.</li><li>Omit the <code>imagePullPolicy</code> and use <code>:latest</code> as the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Omit the <code>imagePullPolicy</code> and the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Enable the <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages</a>
admission controller.</li></ul><h3 id="imagepullbackoff">ImagePullBackOff</h3><p>When a kubelet starts creating containers for a Pod using a container runtime,
it might be possible the container is in <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting">Waiting</a>
state because of <code>ImagePullBackOff</code>.</p><p>The status <code>ImagePullBackOff</code> means that a container could not start because Kubernetes
could not pull a container image (for reasons such as invalid image name, or pulling
from a private registry without <code>imagePullSecret</code>). The <code>BackOff</code> part indicates
that Kubernetes will keep trying to pull the image, with an increasing back-off delay.</p><p>Kubernetes raises the delay between each attempt until it reaches a compiled-in limit,
which is 300 seconds (5 minutes).</p><h3 id="image-pull-per-runtime-class">Image pull per runtime class</h3><p><div class="feature-state-notice feature-alpha" title="Feature Gate: RuntimeClassInImageCriApi"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [alpha]</code> (enabled by default: false)</div>Kubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.</p><p>If you enable the <code>RuntimeClassInImageCriApi</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>,
the kubelet references container images by a tuple of image name and runtime handler
rather than just the image name or digest. Your
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>
may adapt its behavior based on the selected runtime handler.
Pulling images based on runtime class is useful for VM-based containers, such as
Windows Hyper-V containers.</p><h2 id="serial-and-parallel-image-pulls">Serial and parallel image pulls</h2><p>By default, the kubelet pulls images serially. In other words, the kubelet sends
only one image pull request to the image service at a time. Other image pull
requests have to wait until the one being processed is complete.</p><p>Nodes make image pull decisions in isolation. Even when you use serialized image
pulls, two different nodes can pull the same image in parallel.</p><p>If you would like to enable parallel image pulls, you can set the field
<code>serializeImagePulls</code> to false in the <a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>.
With <code>serializeImagePulls</code> set to false, image pull requests will be sent to the image service immediately,
and multiple images will be pulled at the same time.</p><p>When enabling parallel image pulls, ensure that the image service of your container
runtime can handle parallel image pulls.</p><p>The kubelet never pulls multiple images in parallel on behalf of one Pod. For example,
if you have a Pod that has an init container and an application container, the image
pulls for the two containers will not be parallelized. However, if you have two
Pods that use different images, and the parallel image pull feature is enabled,
the kubelet will pull the images in parallel on behalf of the two different Pods.</p><h3 id="maximum-parallel-image-pulls">Maximum parallel image pulls</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [beta]</code></div><p>When <code>serializeImagePulls</code> is set to false, the kubelet defaults to no limit on
the maximum number of images being pulled at the same time. If you would like to
limit the number of parallel image pulls, you can set the field <code>maxParallelImagePulls</code>
in the kubelet configuration. With <code>maxParallelImagePulls</code> set to <em>n</em>, only <em>n</em>
images can be pulled at the same time, and any image pull beyond <em>n</em> will have to
wait until at least one ongoing image pull is complete.</p><p>Limiting the number of parallel image pulls prevents image pulling from consuming
too much network bandwidth or disk I/O, when parallel image pulling is enabled.</p><p>You can set <code>maxParallelImagePulls</code> to a positive number that is greater than or
equal to 1. If you set <code>maxParallelImagePulls</code> to be greater than or equal to 2,
you must set <code>serializeImagePulls</code> to false. The kubelet will fail to start
with an invalid <code>maxParallelImagePulls</code> setting.</p><h2 id="multi-architecture-images-with-image-indexes">Multi-architecture images with image indexes</h2><p>As well as providing binary images, a container registry can also serve a
<a href="https://github.com/opencontainers/image-spec/blob/master/image-index.md">container image index</a>.
An image index can point to multiple <a href="https://github.com/opencontainers/image-spec/blob/master/manifest.md">image manifests</a>
for architecture-specific versions of a container. The idea is that you can have
a name for an image (for example: <code>pause</code>, <code>example/mycontainer</code>, <code>kube-apiserver</code>)
and allow different systems to fetch the right binary image for the machine
architecture they are using.</p><p>The Kubernetes project typically creates container images for its releases with
names that include the suffix <code>-$(ARCH)</code>. For backward compatibility, generate
older images with suffixes. For instance, an image named as <code>pause</code> would be a
multi-architecture image containing manifests for all supported architectures,
while <code>pause-amd64</code> would be a backward-compatible version for older configurations,
or for YAML files with hardcoded image names containing suffixes.</p><h2 id="using-a-private-registry">Using a private registry</h2><p>Private registries may require authentication to be able to discover and/or pull
images from them.
Credentials can be provided in several ways:</p><ul><li><p><a href="#specifying-imagepullsecrets-on-a-pod">Specifying <code>imagePullSecrets</code> when you define a Pod</a></p><p>Only Pods which provide their own keys can access the private registry.</p></li><li><p><a href="#configuring-nodes-to-authenticate-to-a-private-registry">Configuring Nodes to Authenticate to a Private Registry</a></p><ul><li>All Pods can read any configured private registries.</li><li>Requires node configuration by cluster administrator.</li></ul></li><li><p>Using a <em>kubelet credential provider</em> plugin to <a href="#kubelet-credential-provider">dynamically fetch credentials for private registries</a></p><p>The kubelet can be configured to use credential provider exec plugin for the
respective private registry.</p></li><li><p><a href="#pre-pulled-images">Pre-pulled Images</a></p><ul><li>All Pods can use any images cached on a node.</li><li>Requires root access to all nodes to set up.</li></ul></li><li><p>Vendor-specific or local extensions</p><p>If you're using a custom node configuration, you (or your cloud provider) can
implement your mechanism for authenticating the node to the container registry.</p></li></ul><p>These options are explained in more detail below.</p><h3 id="specifying-imagepullsecrets-on-a-pod">Specifying <code>imagePullSecrets</code> on a Pod</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This is the recommended approach to run containers based on images
in private registries.</div><p>Kubernetes supports specifying container image registry keys on a Pod.
All <code>imagePullSecrets</code> must be Secrets that exist in the same
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="Namespace">Namespace</a> as the
Pod. These Secrets must be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p><h3 id="configuring-nodes-to-authenticate-to-a-private-registry">Configuring nodes to authenticate to a private registry</h3><p>Specific instructions for setting credentials depends on the container runtime and registry you
chose to use. You should refer to your solution's documentation for the most accurate information.</p><p>For an example of configuring a private container image registry, see the
<a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">Pull an Image from a Private Registry</a>
task. That example uses a private registry in Docker Hub.</p><h3 id="kubelet-credential-provider">Kubelet credential provider for authenticated image pulls</h3><p>You can configure the kubelet to invoke a plugin binary to dynamically fetch
registry credentials for a container image. This is the most robust and versatile
way to fetch credentials for private registries, but also requires kubelet-level
configuration to enable.</p><p>This technique can be especially useful for running <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static Pods">static Pods</a>
that require container images hosted in a private registry.
Using a <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" aria-label="ServiceAccount">ServiceAccount</a> or a
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a> to provide private registry credentials
is not possible in the specification of a static Pod, because it <em>cannot</em>
have references to other API resources in its specification.</p><p>See <a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure a kubelet image credential provider</a> for more details.</p><h3 id="config-json">Interpretation of config.json</h3><p>The interpretation of <code>config.json</code> varies between the original Docker
implementation and the Kubernetes interpretation. In Docker, the <code>auths</code> keys
can only specify root URLs, whereas Kubernetes allows glob URLs as well as
prefix-matched paths. The only limitation is that glob patterns (<code>*</code>) have to
include the dot (<code>.</code>) for each subdomain. The amount of matched subdomains has
to be equal to the amount of glob patterns (<code>*.</code>), for example:</p><ul><li><code>*.kubernetes.io</code> will <em>not</em> match <code>kubernetes.io</code>, but will match
<code>abc.kubernetes.io</code>.</li><li><code>*.*.kubernetes.io</code> will <em>not</em> match <code>abc.kubernetes.io</code>, but will match
<code>abc.def.kubernetes.io</code>.</li><li><code>prefix.*.io</code> will match <code>prefix.kubernetes.io</code>.</li><li><code>*-good.kubernetes.io</code> will match <code>prefix-good.kubernetes.io</code>.</li></ul><p>This means that a <code>config.json</code> like this is valid:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"auths"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"my-registry.example/images"</span>: { <span style="color:green;font-weight:700">"auth"</span>: <span style="color:#b44">"â€¦"</span> },
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"*.my-registry.example/images"</span>: { <span style="color:green;font-weight:700">"auth"</span>: <span style="color:#b44">"â€¦"</span> }
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>Image pull operations pass the credentials to the CRI container runtime for every
valid pattern. For example, the following container image names would match
successfully:</p><ul><li><code>my-registry.example/images</code></li><li><code>my-registry.example/images/my-image</code></li><li><code>my-registry.example/images/another-image</code></li><li><code>sub.my-registry.example/images/my-image</code></li></ul><p>However, these container image names would <em>not</em> match:</p><ul><li><code>a.sub.my-registry.example/images/my-image</code></li><li><code>a.b.sub.my-registry.example/images/my-image</code></li></ul><p>The kubelet performs image pulls sequentially for every found credential. This
means that multiple entries in <code>config.json</code> for different paths are possible, too:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"auths"</span>: {
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"my-registry.example/images"</span>: {
</span></span><span style="display:flex"><span>            <span style="color:green;font-weight:700">"auth"</span>: <span style="color:#b44">"â€¦"</span>
</span></span><span style="display:flex"><span>        },
</span></span><span style="display:flex"><span>        <span style="color:green;font-weight:700">"my-registry.example/images/subpath"</span>: {
</span></span><span style="display:flex"><span>            <span style="color:green;font-weight:700">"auth"</span>: <span style="color:#b44">"â€¦"</span>
</span></span><span style="display:flex"><span>        }
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><p>If now a container specifies an image <code>my-registry.example/images/subpath/my-image</code>
to be pulled, then the kubelet will try to download it using both authentication
sources if one of them fails.</p><h3 id="pre-pulled-images">Pre-pulled images</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This approach is suitable if you can control node configuration. It
will not work reliably if your cloud provider manages nodes and replaces
them automatically.</div><p>By default, the kubelet tries to pull each image from the specified registry.
However, if the <code>imagePullPolicy</code> property of the container is set to <code>IfNotPresent</code> or <code>Never</code>,
then a local image is used (preferentially or exclusively, respectively).</p><p>If you want to rely on pre-pulled images as a substitute for registry authentication,
you must ensure all nodes in the cluster have the same pre-pulled images.</p><p>This can be used to preload certain images for speed or as an alternative to
authenticating to a private registry.</p><p>Similar to the usage of the <a href="#kubelet-credential-provider">kubelet credential provider</a>,
pre-pulled images are also suitable for launching
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static Pods">static Pods</a> that depend
on images hosted in a private registry.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: KubeletEnsureSecretPulledImages"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>Access to pre-pulled images may be authorized according to <a href="#ensureimagepullcredentialverification">image pull credential verification</a>.</p></div><h4 id="ensureimagepullcredentialverification">Ensure image pull credential verification</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: KubeletEnsureSecretPulledImages"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>If the <code>KubeletEnsureSecretPulledImages</code> feature gate is enabled for your cluster,
Kubernetes will validate image credentials for every image that requires credentials
to be pulled, even if that image is already present on the node. This validation
ensures that images in a Pod request which have not been successfully pulled
with the provided credentials must re-pull the images from the registry.
Additionally, image pulls that re-use the same credentials
which previously resulted in a successful image pull will not need to re-pull from
the registry and are instead validated locally without accessing the registry
(provided the image is available locally).
This is controlled by the<code>imagePullCredentialsVerificationPolicy</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-ImagePullCredentialsVerificationPolicy">Kubelet configuration</a>.</p><p>This configuration controls when image pull credentials must be verified if the
image is already present on the node:</p><ul><li><code>NeverVerify</code>: Mimics the behavior of having this feature gate disabled.
If the image is present locally, image pull credentials are not verified.</li><li><code>NeverVerifyPreloadedImages</code>: Images pulled outside the kubelet are not verified,
but all other images will have their credentials verified. This is the default behavior.</li><li><code>NeverVerifyAllowListedImages</code>: Images pulled outside the kubelet and mentioned within the
<code>preloadedImagesVerificationAllowlist</code> specified in the kubelet config are not verified.</li><li><code>AlwaysVerify</code>: All images will have their credentials verified
before they can be used.</li></ul><p>This verification applies to <a href="#pre-pulled-images">pre-pulled images</a>,
images pulled using node-wide secrets, and images pulled using Pod-level secrets.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In the case of credential rotation, the credentials previously used to pull the image
will continue to verify without the need to access the registry. New or rotated credentials
will require the image to be re-pulled from the registry.</div><h4 id="creating-a-secret-with-a-docker-config">Creating a Secret with a Docker config</h4><p>You need to know the username, registry password and client email address for authenticating
to the registry, as well as its hostname.
Run the following command, substituting placeholders with the appropriate values:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret docker-registry &lt;name&gt; <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-server<span style="color:#666">=</span>&lt;docker-registry-server&gt; <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-username<span style="color:#666">=</span>&lt;docker-user&gt; <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-password<span style="color:#666">=</span>&lt;docker-password&gt; <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-email<span style="color:#666">=</span>&lt;docker-email&gt;
</span></span></code></pre></div><p>If you already have a Docker credentials file then, rather than using the above
command, you can import the credentials file as a Kubernetes
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a>.
<a href="/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials">Create a Secret based on existing Docker credentials</a>
explains how to set this up.</p><p>This is particularly useful if you are using multiple private container
registries, as <code>kubectl create secret docker-registry</code> creates a Secret that
only works with a single private registry.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Pods can only reference image pull secrets in their own namespace,
so this process needs to be done one time per namespace.</div><h4 id="referring-to-imagepullsecrets-on-a-pod">Referring to <code>imagePullSecrets</code> on a Pod</h4><p>Now, you can create pods which reference that secret by adding the <code>imagePullSecrets</code>
section to a Pod definition. Each item in the <code>imagePullSecrets</code> array can only
reference one Secret in the same namespace.</p><p>For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>cat <span style="color:#b44">&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span style="display:flex"><span><span style="color:#b44">apiVersion: v1
</span></span></span><span style="display:flex"><span><span style="color:#b44">kind: Pod
</span></span></span><span style="display:flex"><span><span style="color:#b44">metadata:
</span></span></span><span style="display:flex"><span><span style="color:#b44">  name: foo
</span></span></span><span style="display:flex"><span><span style="color:#b44">  namespace: awesomeapps
</span></span></span><span style="display:flex"><span><span style="color:#b44">spec:
</span></span></span><span style="display:flex"><span><span style="color:#b44">  containers:
</span></span></span><span style="display:flex"><span><span style="color:#b44">    - name: foo
</span></span></span><span style="display:flex"><span><span style="color:#b44">      image: janedoe/awesomeapp:v1
</span></span></span><span style="display:flex"><span><span style="color:#b44">  imagePullSecrets:
</span></span></span><span style="display:flex"><span><span style="color:#b44">    - name: myregistrykey
</span></span></span><span style="display:flex"><span><span style="color:#b44">EOF</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>cat <span style="color:#b44">&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span></span></span><span style="display:flex"><span><span style="color:#b44">resources:
</span></span></span><span style="display:flex"><span><span style="color:#b44">- pod.yaml
</span></span></span><span style="display:flex"><span><span style="color:#b44">EOF</span>
</span></span></code></pre></div><p>This needs to be done for each Pod that is using a private registry.</p><p>However, you can automate this process by specifying the <code>imagePullSecrets</code> section
in a <a href="/docs/tasks/configure-pod-container/configure-service-account/">ServiceAccount</a>
resource. See <a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a Service Account</a>
for detailed instructions.</p><p>You can use this in conjunction with a per-node <code>.docker/config.json</code>. The credentials
will be merged.</p><h2 id="use-cases">Use cases</h2><p>There are a number of solutions for configuring private registries. Here are some
common use cases and suggested solutions.</p><ol><li>Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.<ul><li>Use public images from a public registry<ul><li>No configuration required.</li><li>Some cloud providers automatically cache or mirror public images, which improves
availability and reduces the time to pull images.</li></ul></li></ul></li><li>Cluster running some proprietary images which should be hidden to those outside the company, but
visible to all cluster users.<ul><li>Use a hosted private registry<ul><li>Manual configuration may be required on the nodes that need to access to private registry.</li></ul></li><li>Or, run an internal private registry behind your firewall with open read access.<ul><li>No Kubernetes configuration is required.</li></ul></li><li>Use a hosted container image registry service that controls image access<ul><li>It will work better with Node autoscaling than manual node configuration.</li></ul></li><li>Or, on a cluster where changing the node configuration is inconvenient, use <code>imagePullSecrets</code>.</li></ul></li><li>Cluster with proprietary images, a few of which require stricter access control.<ul><li>Ensure <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages admission controller</a>
is active. Otherwise, all Pods potentially have access to all images.</li><li>Move sensitive data into a Secret resource, instead of packaging it in an image.</li></ul></li><li>A multi-tenant cluster where each tenant needs own private registry.<ul><li>Ensure <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages admission controller</a>
is active. Otherwise, all Pods of all tenants potentially have access to all images.</li><li>Run a private registry with authorization required.</li><li>Generate registry credentials for each tenant, store into a Secret, and propagate
the Secret to every tenant namespace.</li><li>The tenant then adds that Secret to <code>imagePullSecrets</code> of each namespace.</li></ul></li></ol><p>If you need access to multiple registries, you can create one Secret per registry.</p><h2 id="legacy-built-in-kubelet-credential-provider">Legacy built-in kubelet credential provider</h2><p>In older versions of Kubernetes, the kubelet had a direct integration with cloud
provider credentials. This provided the ability to dynamically fetch credentials
for image registries.</p><p>There were three built-in implementations of the kubelet credential provider
integration: ACR (Azure Container Registry), ECR (Elastic Container Registry),
and GCR (Google Container Registry).</p><p>Starting with version 1.26 of Kubernetes, the legacy mechanism has been removed,
so you would need to either:</p><ul><li>configure a kubelet image credential provider on each node; or</li><li>specify image pull credentials using <code>imagePullSecrets</code> and at least one Secret.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="https://github.com/opencontainers/image-spec/blob/main/manifest.md">OCI Image Manifest Specification</a>.</li><li>Learn about <a href="/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection">container image garbage collection</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">pulling an Image from a Private Registry</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">API Priority and Fairness</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div><p>Controlling the behavior of the Kubernetes API server in an overload situation
is a key task for cluster administrators. The <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="kube-apiserver">kube-apiserver</a> has some controls available
(i.e. the <code>--max-requests-inflight</code> and <code>--max-mutating-requests-inflight</code>
command-line flags) to limit the amount of outstanding work that will be
accepted, preventing a flood of inbound requests from overloading and
potentially crashing the API server, but these flags are not enough to ensure
that the most important requests get through in a period of high traffic.</p><p>The API Priority and Fairness feature (APF) is an alternative that improves upon
aforementioned max-inflight limitations. APF classifies
and isolates requests in a more fine-grained way. It also introduces
a limited amount of queuing, so that no requests are rejected in cases
of very brief bursts. Requests are dispatched from queues using a
fair queuing technique so that, for example, a poorly-behaved
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> need not
starve others (even at the same priority level).</p><p>This feature is designed to work well with standard controllers, which
use informers and react to failures of API requests with exponential
back-off, and other clients that also work this way.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Some requests classified as "long-running"â€”such as remote
command execution or log tailingâ€”are not subject to the API
Priority and Fairness filter. This is also true for the
<code>--max-requests-inflight</code> flag without the API Priority and Fairness
feature enabled. API Priority and Fairness <em>does</em> apply to <strong>watch</strong>
requests. When API Priority and Fairness is disabled, <strong>watch</strong> requests
are not subject to the <code>--max-requests-inflight</code> limit.</div><h2 id="enabling-disabling-api-priority-and-fairness">Enabling/Disabling API Priority and Fairness</h2><p>The API Priority and Fairness feature is controlled by a command-line flag
and is enabled by default. See
<a href="/docs/reference/command-line-tools-reference/kube-apiserver/#options">Options</a>
for a general explanation of the available kube-apiserver command-line
options and how to enable and disable them. The name of the
command-line option for APF is "--enable-priority-and-fairness". This feature
also involves an <a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank" aria-label="API Group">API Group</a>
with: (a) a stable <code>v1</code> version, introduced in 1.29, and
enabled by default (b) a <code>v1beta3</code> version, enabled by default, and
deprecated in v1.29. You can
disable the API group beta version <code>v1beta3</code> by adding the
following command-line flags to your <code>kube-apiserver</code> invocation:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kube-apiserver <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>--runtime-config<span style="color:#666">=</span>flowcontrol.apiserver.k8s.io/v1beta3<span style="color:#666">=</span><span style="color:#a2f">false</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/> <span style="color:#080;font-style:italic"># â€¦and other flags as usual</span>
</span></span></code></pre></div><p>The command-line flag <code>--enable-priority-and-fairness=false</code> will disable the
API Priority and Fairness feature.</p><h2 id="recursive-server-scenarios">Recursive server scenarios</h2><p>API Priority and Fairness must be used carefully in recursive server
scenarios. These are scenarios in which some server A, while serving
a request, issues a subsidiary request to some server B. Perhaps
server B might even make a further subsidiary call back to server
A. In situations where Priority and Fairness control is applied to
both the original request and some subsidiary ones(s), no matter how
deep in the recursion, there is a danger of priority inversions and/or
deadlocks.</p><p>One example of recursion is when the <code>kube-apiserver</code> issues an
admission webhook call to server B, and while serving that call,
server B makes a further subsidiary request back to the
<code>kube-apiserver</code>. Another example of recursion is when an <code>APIService</code>
object directs the <code>kube-apiserver</code> to delegate requests about a
certain API group to a custom external server B (this is one of the
things called "aggregation").</p><p>When the original request is known to belong to a certain priority
level, and the subsidiary controlled requests are classified to higher
priority levels, this is one possible solution. When the original
requests can belong to any priority level, the subsidiary controlled
requests have to be exempt from Priority and Fairness limitation. One
way to do that is with the objects that configure classification and
handling, discussed below. Another way is to disable Priority and
Fairness on server B entirely, using the techniques discussed above. A
third way, which is the simplest to use when server B is not
<code>kube-apiserver</code>, is to build server B with Priority and Fairness
disabled in the code.</p><h2 id="concepts">Concepts</h2><p>There are several distinct features involved in the API Priority and Fairness
feature. Incoming requests are classified by attributes of the request using
<em>FlowSchemas</em>, and assigned to priority levels. Priority levels add a degree of
isolation by maintaining separate concurrency limits, so that requests assigned
to different priority levels cannot starve each other. Within a priority level,
a fair-queuing algorithm prevents requests from different <em>flows</em> from starving
each other, and allows for requests to be queued to prevent bursty traffic from
causing failed requests when the average load is acceptably low.</p><h3 id="priority-levels">Priority Levels</h3><p>Without APF enabled, overall concurrency in the API server is limited by the
<code>kube-apiserver</code> flags <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. With APF enabled, the concurrency limits
defined by these flags are summed and then the sum is divided up among a
configurable set of <em>priority levels</em>. Each incoming request is assigned to a
single priority level, and each priority level will only dispatch as many
concurrent requests as its particular limit allows.</p><p>The default configuration, for example, includes separate priority levels for
leader-election requests, requests from built-in controllers, and requests from
Pods. This means that an ill-behaved Pod that floods the API server with
requests cannot prevent leader election or actions by the built-in controllers
from succeeding.</p><p>The concurrency limits of the priority levels are periodically
adjusted, allowing under-utilized priority levels to temporarily lend
concurrency to heavily-utilized levels. These limits are based on
nominal limits and bounds on how much concurrency a priority level may
lend and how much it may borrow, all derived from the configuration
objects mentioned below.</p><h3 id="seats-occupied-by-a-request">Seats Occupied by a Request</h3><p>The above description of concurrency management is the baseline story.
Requests have different durations but are counted equally at any given
moment when comparing against a priority level's concurrency limit. In
the baseline story, each request occupies one unit of concurrency. The
word "seat" is used to mean one unit of concurrency, inspired by the
way each passenger on a train or aircraft takes up one of the fixed
supply of seats.</p><p>But some requests take up more than one seat. Some of these are <strong>list</strong>
requests that the server estimates will return a large number of
objects. These have been found to put an exceptionally heavy burden
on the server. For this reason, the server estimates the number of objects
that will be returned and considers the request to take a number of seats
that is proportional to that estimated number.</p><h3 id="execution-time-tweaks-for-watch-requests">Execution time tweaks for watch requests</h3><p>API Priority and Fairness manages <strong>watch</strong> requests, but this involves a
couple more excursions from the baseline behavior. The first concerns
how long a <strong>watch</strong> request is considered to occupy its seat. Depending
on request parameters, the response to a <strong>watch</strong> request may or may not
begin with <strong>create</strong> notifications for all the relevant pre-existing
objects. API Priority and Fairness considers a <strong>watch</strong> request to be
done with its seat once that initial burst of notifications, if any,
is over.</p><p>The normal notifications are sent in a concurrent burst to all
relevant <strong>watch</strong> response streams whenever the server is notified of an
object create/update/delete. To account for this work, API Priority
and Fairness considers every write request to spend some additional
time occupying seats after the actual writing is done. The server
estimates the number of notifications to be sent and adjusts the write
request's number of seats and seat occupancy time to include this
extra work.</p><h3 id="queuing">Queuing</h3><p>Even within a priority level there may be a large number of distinct sources of
traffic. In an overload situation, it is valuable to prevent one stream of
requests from starving others (in particular, in the relatively common case of a
single buggy client flooding the kube-apiserver with requests, that buggy client
would ideally not have much measurable impact on other clients at all). This is
handled by use of a fair-queuing algorithm to process requests that are assigned
the same priority level. Each request is assigned to a <em>flow</em>, identified by the
name of the matching FlowSchema plus a <em>flow distinguisher</em> â€” which
is either the requesting user, the target resource's namespace, or nothing â€” and the
system attempts to give approximately equal weight to requests in different
flows of the same priority level.
To enable distinct handling of distinct instances, controllers that have
many instances should authenticate with distinct usernames</p><p>After classifying a request into a flow, the API Priority and Fairness
feature then may assign the request to a queue. This assignment uses
a technique known as <a class="glossary-tooltip" title="A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-shuffle-sharding" target="_blank" aria-label="shuffle sharding">shuffle sharding</a>, which makes relatively efficient use of
queues to insulate low-intensity flows from high-intensity flows.</p><p>The details of the queuing algorithm are tunable for each priority level, and
allow administrators to trade off memory use, fairness (the property that
independent flows will all make progress when total traffic exceeds capacity),
tolerance for bursty traffic, and the added latency induced by queuing.</p><h3 id="exempt-requests">Exempt requests</h3><p>Some requests are considered sufficiently important that they are not subject to
any of the limitations imposed by this feature. These exemptions prevent an
improperly-configured flow control configuration from totally disabling an API
server.</p><h2 id="resources">Resources</h2><p>The flow control API involves two kinds of resources.
<a href="/docs/reference/generated/kubernetes-api/v1.34/#prioritylevelconfiguration-v1-flowcontrol-apiserver-k8s-io">PriorityLevelConfigurations</a>
define the available priority levels, the share of the available concurrency
budget that each can handle, and allow for fine-tuning queuing behavior.
<a href="/docs/reference/generated/kubernetes-api/v1.34/#flowschema-v1-flowcontrol-apiserver-k8s-io">FlowSchemas</a>
are used to classify individual inbound requests, matching each to a
single PriorityLevelConfiguration.</p><h3 id="prioritylevelconfiguration">PriorityLevelConfiguration</h3><p>A PriorityLevelConfiguration represents a single priority level. Each
PriorityLevelConfiguration has an independent limit on the number of outstanding
requests, and limitations on the number of queued requests.</p><p>The nominal concurrency limit for a PriorityLevelConfiguration is not
specified in an absolute number of seats, but rather in "nominal
concurrency shares." The total concurrency limit for the API Server is
distributed among the existing PriorityLevelConfigurations in
proportion to these shares, to give each level its nominal limit in
terms of seats. This allows a cluster administrator to scale up or
down the total amount of traffic to a server by restarting
<code>kube-apiserver</code> with a different value for <code>--max-requests-inflight</code>
(or <code>--max-mutating-requests-inflight</code>), and all
PriorityLevelConfigurations will see their maximum allowed concurrency
go up (or down) by the same fraction.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>In the versions before <code>v1beta3</code> the relevant
PriorityLevelConfiguration field is named "assured concurrency shares"
rather than "nominal concurrency shares". Also, in Kubernetes release
1.25 and earlier there were no periodic adjustments: the
nominal/assured limits were always applied without adjustment.</div><p>The bounds on how much concurrency a priority level may lend and how
much it may borrow are expressed in the PriorityLevelConfiguration as
percentages of the level's nominal limit. These are resolved to
absolute numbers of seats by multiplying with the nominal limit /
100.0 and rounding. The dynamically adjusted concurrency limit of a
priority level is constrained to lie between (a) a lower bound of its
nominal limit minus its lendable seats and (b) an upper bound of its
nominal limit plus the seats it may borrow. At each adjustment the
dynamic limits are derived by each priority level reclaiming any lent
seats for which demand recently appeared and then jointly fairly
responding to the recent seat demand on the priority levels, within
the bounds just described.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>With the Priority and Fairness feature enabled, the total concurrency limit for
the server is set to the sum of <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. There is no longer any distinction made
between mutating and non-mutating requests; if you want to treat them
separately for a given resource, make separate FlowSchemas that match the
mutating and non-mutating verbs respectively.</div><p>When the volume of inbound requests assigned to a single
PriorityLevelConfiguration is more than its permitted concurrency level, the
<code>type</code> field of its specification determines what will happen to extra requests.
A type of <code>Reject</code> means that excess traffic will immediately be rejected with
an HTTP 429 (Too Many Requests) error. A type of <code>Queue</code> means that requests
above the threshold will be queued, with the shuffle sharding and fair queuing techniques used
to balance progress between request flows.</p><p>The queuing configuration allows tuning the fair queuing algorithm for a
priority level. Details of the algorithm can be read in the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness">enhancement proposal</a>, but in short:</p><ul><li><p>Increasing <code>queues</code> reduces the rate of collisions between different flows, at
the cost of increased memory usage. A value of 1 here effectively disables the
fair-queuing logic, but still allows requests to be queued.</p></li><li><p>Increasing <code>queueLengthLimit</code> allows larger bursts of traffic to be
sustained without dropping any requests, at the cost of increased
latency and memory usage.</p></li><li><p>Changing <code>handSize</code> allows you to adjust the probability of collisions between
different flows and the overall concurrency available to a single flow in an
overload situation.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A larger <code>handSize</code> makes it less likely for two individual flows to collide
(and therefore for one to be able to starve the other), but more likely that
a small number of flows can dominate the apiserver. A larger <code>handSize</code> also
potentially increases the amount of latency that a single high-traffic flow
can cause. The maximum number of queued requests possible from a
single flow is <code>handSize * queueLengthLimit</code>.</div></li></ul><p>Following is a table showing an interesting collection of shuffle
sharding configurations, showing for each the probability that a
given mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for
an illustrative collection of numbers of elephants. See
<a href="https://play.golang.org/p/Gi0PLgVHiUg">https://play.golang.org/p/Gi0PLgVHiUg</a> , which computes this table.</p><table><caption style="display:none">Example Shuffle Sharding Configurations</caption><thead><tr><th>HandSize</th><th>Queues</th><th>1 elephant</th><th>4 elephants</th><th>16 elephants</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id="flowschema">FlowSchema</h3><p>A FlowSchema matches some inbound requests and assigns them to a
priority level. Every inbound request is tested against FlowSchemas,
starting with those with the numerically lowest <code>matchingPrecedence</code> and
working upward. The first match wins.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Only the first matching FlowSchema for a given request matters. If multiple
FlowSchemas match a single inbound request, it will be assigned based on the one
with the highest <code>matchingPrecedence</code>. If multiple FlowSchemas with equal
<code>matchingPrecedence</code> match the same request, the one with lexicographically
smaller <code>name</code> will win, but it's better not to rely on this, and instead to
ensure that no two FlowSchemas have the same <code>matchingPrecedence</code>.</div><p>A FlowSchema matches a given request if at least one of its <code>rules</code>
matches. A rule matches if at least one of its <code>subjects</code> <em>and</em> at least
one of its <code>resourceRules</code> or <code>nonResourceRules</code> (depending on whether the
incoming request is for a resource or non-resource URL) match the request.</p><p>For the <code>name</code> field in subjects, and the <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>,
<code>namespaces</code>, and <code>nonResourceURLs</code> fields of resource and non-resource rules,
the wildcard <code>*</code> may be specified to match all values for the given field,
effectively removing it from consideration.</p><p>A FlowSchema's <code>distinguisherMethod.type</code> determines how requests matching that
schema will be separated into flows. It may be <code>ByUser</code>, in which one requesting
user will not be able to starve other users of capacity; <code>ByNamespace</code>, in which
requests for resources in one namespace will not be able to starve requests for
resources in other namespaces of capacity; or blank (or <code>distinguisherMethod</code> may be
omitted entirely), in which all requests matched by this FlowSchema will be
considered part of a single flow. The correct choice for a given FlowSchema
depends on the resource and your particular environment.</p><h2 id="defaults">Defaults</h2><p>Each kube-apiserver maintains two sorts of APF configuration objects:
mandatory and suggested.</p><h3 id="mandatory-configuration-objects">Mandatory Configuration Objects</h3><p>The four mandatory configuration objects reflect fixed built-in
guardrail behavior. This is behavior that the servers have before
those objects exist, and when those objects exist their specs reflect
this behavior. The four mandatory objects are as follows.</p><ul><li><p>The mandatory <code>exempt</code> priority level is used for requests that are
not subject to flow control at all: they will always be dispatched
immediately. The mandatory <code>exempt</code> FlowSchema classifies all
requests from the <code>system:masters</code> group into this priority
level. You may define other FlowSchemas that direct other requests
to this priority level, if appropriate.</p></li><li><p>The mandatory <code>catch-all</code> priority level is used in combination with
the mandatory <code>catch-all</code> FlowSchema to make sure that every request
gets some kind of classification. Typically you should not rely on
this catch-all configuration, and should create your own catch-all
FlowSchema and PriorityLevelConfiguration (or use the suggested
<code>global-default</code> priority level that is installed by default) as
appropriate. Because it is not expected to be used normally, the
mandatory <code>catch-all</code> priority level has a very small concurrency
share and does not queue requests.</p></li></ul><h3 id="suggested-configuration-objects">Suggested Configuration Objects</h3><p>The suggested FlowSchemas and PriorityLevelConfigurations constitute a
reasonable default configuration. You can modify these and/or create
additional configuration objects if you want. If your cluster is
likely to experience heavy load then you should consider what
configuration will work best.</p><p>The suggested configuration groups requests into six priority levels:</p><ul><li><p>The <code>node-high</code> priority level is for health updates from nodes.</p></li><li><p>The <code>system</code> priority level is for non-health requests from the
<code>system:nodes</code> group, i.e. Kubelets, which must be able to contact
the API server in order for workloads to be able to schedule on
them.</p></li><li><p>The <code>leader-election</code> priority level is for leader election requests from
built-in controllers (in particular, requests for <code>endpoints</code>, <code>configmaps</code>,
or <code>leases</code> coming from the <code>system:kube-controller-manager</code> or
<code>system:kube-scheduler</code> users and service accounts in the <code>kube-system</code>
namespace). These are important to isolate from other traffic because failures
in leader election cause their controllers to fail and restart, which in turn
causes more expensive traffic as the new controllers sync their informers.</p></li><li><p>The <code>workload-high</code> priority level is for other requests from built-in
controllers.</p></li><li><p>The <code>workload-low</code> priority level is for requests from any other service
account, which will typically include all requests from controllers running in
Pods.</p></li><li><p>The <code>global-default</code> priority level handles all other traffic, e.g.
interactive <code>kubectl</code> commands run by nonprivileged users.</p></li></ul><p>The suggested FlowSchemas serve to steer requests into the above
priority levels, and are not enumerated here.</p><h3 id="maintenance-of-the-mandatory-and-suggested-configuration-objects">Maintenance of the Mandatory and Suggested Configuration Objects</h3><p>Each <code>kube-apiserver</code> independently maintains the mandatory and
suggested configuration objects, using initial and periodic behavior.
Thus, in a situation with a mixture of servers of different versions
there may be thrashing as long as different servers have different
opinions of the proper content of these objects.</p><p>Each <code>kube-apiserver</code> makes an initial maintenance pass over the
mandatory and suggested configuration objects, and after that does
periodic maintenance (once per minute) of those objects.</p><p>For the mandatory configuration objects, maintenance consists of
ensuring that the object exists and, if it does, has the proper spec.
The server refuses to allow a creation or update with a spec that is
inconsistent with the server's guardrail behavior.</p><p>Maintenance of suggested configuration objects is designed to allow
their specs to be overridden. Deletion, on the other hand, is not
respected: maintenance will restore the object. If you do not want a
suggested configuration object then you need to keep it around but set
its spec to have minimal consequences. Maintenance of suggested
objects is also designed to support automatic migration when a new
version of the <code>kube-apiserver</code> is rolled out, albeit potentially with
thrashing while there is a mixed population of servers.</p><p>Maintenance of a suggested configuration object consists of creating
it --- with the server's suggested spec --- if the object does not
exist. OTOH, if the object already exists, maintenance behavior
depends on whether the <code>kube-apiservers</code> or the users control the
object. In the former case, the server ensures that the object's spec
is what the server suggests; in the latter case, the spec is left
alone.</p><p>The question of who controls the object is answered by first looking
for an annotation with key <code>apf.kubernetes.io/autoupdate-spec</code>. If
there is such an annotation and its value is <code>true</code> then the
kube-apiservers control the object. If there is such an annotation
and its value is <code>false</code> then the users control the object. If
neither of those conditions holds then the <code>metadata.generation</code> of the
object is consulted. If that is 1 then the kube-apiservers control
the object. Otherwise the users control the object. These rules were
introduced in release 1.22 and their consideration of
<code>metadata.generation</code> is for the sake of migration from the simpler
earlier behavior. Users who wish to control a suggested configuration
object should set its <code>apf.kubernetes.io/autoupdate-spec</code> annotation
to <code>false</code>.</p><p>Maintenance of a mandatory or suggested configuration object also
includes ensuring that it has an <code>apf.kubernetes.io/autoupdate-spec</code>
annotation that accurately reflects whether the kube-apiservers
control the object.</p><p>Maintenance also includes deleting objects that are neither mandatory
nor suggested but are annotated
<code>apf.kubernetes.io/autoupdate-spec=true</code>.</p><h2 id="health-check-concurrency-exemption">Health check concurrency exemption</h2><p>The suggested configuration gives no special treatment to the health
check requests on kube-apiservers from their local kubelets --- which
tend to use the secured port but supply no credentials. With the
suggested config, these requests get assigned to the <code>global-default</code>
FlowSchema and the corresponding <code>global-default</code> priority level,
where other traffic can crowd them out.</p><p>If you add the following additional FlowSchema, this exempts those
requests from rate limiting.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Making this change also allows any hostile party to then send
health-check requests that match this FlowSchema, at any volume they
like. If you have a web traffic filter or similar external security
mechanism to protect your cluster's API server from general internet
traffic, you can configure rules to block any health check requests
that originate from outside your cluster.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priority-and-fairness/health-for-strangers.yaml" download="priority-and-fairness/health-for-strangers.yaml"><code>priority-and-fairness/health-for-strangers.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;priority-and-fairness-health-for-strangers-yaml&quot;)" title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard"/></div><div class="includecode" id="priority-and-fairness-health-for-strangers-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>flowcontrol.apiserver.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>FlowSchema<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>health-for-strangers<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">matchingPrecedence</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">priorityLevelConfiguration</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>exempt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">nonResourceRules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">nonResourceURLs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:#b44">"/healthz"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:#b44">"/livez"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:#b44">"/readyz"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:#b44">"*"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">subjects</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Group<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">group</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"system:unauthenticated"</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="observability">Observability</h2><h3 id="metrics">Metrics</h3><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In versions of Kubernetes before v1.20, the labels <code>flow_schema</code> and
<code>priority_level</code> were inconsistently named <code>flowSchema</code> and <code>priorityLevel</code>,
respectively. If you're running Kubernetes versions v1.19 and earlier, you
should refer to the documentation for your version.</div><p>When you enable the API Priority and Fairness feature, the kube-apiserver
exports additional metrics. Monitoring these can help you determine whether your
configuration is inappropriately throttling important traffic, or find
poorly-behaved workloads that may be harming system health.</p><h4 id="maturity-level-beta">Maturity level BETA</h4><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> is a counter vector
(cumulative since server start) of requests that were rejected,
broken down by the labels <code>flow_schema</code> (indicating the one that
matched the request), <code>priority_level</code> (indicating the one to which
the request was assigned), and <code>reason</code>. The <code>reason</code> label will be
one of the following values:</p><ul><li><code>queue-full</code>, indicating that too many requests were already
queued.</li><li><code>concurrency-limit</code>, indicating that the
PriorityLevelConfiguration is configured to reject rather than
queue excess requests.</li><li><code>time-out</code>, indicating that the request was still in the queue
when its queuing time limit expired.</li><li><code>cancelled</code>, indicating that the request is not purge locked
and has been ejected from the queue.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> is a counter
vector (cumulative since server start) of requests that began
executing, broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> is a gauge vector
holding the instantaneous number of queued (not executing) requests,
broken down by <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> is a gauge vector
holding the instantaneous number of executing (not waiting in a
queue) requests, broken down by <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_seats</code> is a gauge vector
holding the instantaneous number of occupied seats, broken down by
<code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> is a histogram
vector of how long requests spent queued, broken down by the labels
<code>flow_schema</code>, <code>priority_level</code>, and <code>execute</code>. The <code>execute</code> label
indicates whether the request has started executing.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Since each FlowSchema always assigns requests to a single
PriorityLevelConfiguration, you can add the histograms for all the
FlowSchemas for one priority level to get the effective histogram for
requests assigned to that priority level.</div></li><li><p><code>apiserver_flowcontrol_nominal_limit_seats</code> is a gauge vector
holding each priority level's nominal concurrency limit, computed
from the API server's total concurrency limit and the priority
level's configured nominal concurrency shares.</p></li></ul><h4 id="maturity-level-alpha">Maturity level ALPHA</h4><ul><li><p><code>apiserver_current_inqueue_requests</code> is a gauge vector of recent
high water marks of the number of queued requests, grouped by a
label named <code>request_kind</code> whose value is <code>mutating</code> or <code>readOnly</code>.
These high water marks describe the largest number seen in the one
second window most recently completed. These complement the older
<code>apiserver_current_inflight_requests</code> gauge vector that holds the
last window's high water mark of number of requests actively being
served.</p></li><li><p><code>apiserver_current_inqueue_seats</code> is a gauge vector of the sum over
queued requests of the largest number of seats each will occupy,
grouped by labels named <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_current_requests</code> is a
histogram vector of observations, made at the end of every
nanosecond, of the number of requests broken down by the labels
<code>phase</code> (which takes on the values <code>waiting</code> and <code>executing</code>) and
<code>request_kind</code> (which takes on the values <code>mutating</code> and
<code>readOnly</code>). Each observed value is a ratio, between 0 and 1, of
the number of requests divided by the corresponding limit on the
number of requests (queue volume limit for waiting and concurrency
limit for executing).</p></li><li><p><code>apiserver_flowcontrol_request_concurrency_in_use</code> is a gauge vector
holding the instantaneous number of occupied seats, broken down by
<code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_utilization</code> is a
histogram vector of observations, made at the end of each
nanosecond, of the number of requests broken down by the labels
<code>phase</code> (which takes on the values <code>waiting</code> and <code>executing</code>) and
<code>priority_level</code>. Each observed value is a ratio, between 0 and 1,
of a number of requests divided by the corresponding limit on the
number of requests (queue volume limit for waiting and concurrency
limit for executing).</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_utilization</code> is a
histogram vector of observations, made at the end of each
nanosecond, of the utilization of a priority level's concurrency
limit, broken down by <code>priority_level</code>. This utilization is the
fraction (number of seats occupied) / (concurrency limit). This
metric considers all stages of execution (both normal and the extra
delay at the end of a write to cover for the corresponding
notification work) of all requests except WATCHes; for those it
considers only the initial stage that delivers notifications of
pre-existing objects. Each histogram in the vector is also labeled
with <code>phase: executing</code> (there is no seat limit for the waiting
phase).</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> is a
histogram vector of queue lengths for the queues, broken down by
<code>priority_level</code> and <code>flow_schema</code>, as sampled by the enqueued requests.
Each request that gets queued contributes one sample to its histogram,
reporting the length of the queue immediately after the request was added.
Note that this produces different statistics than an unbiased survey would.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>An outlier value in a histogram here means it is likely that a single flow
(i.e., requests by one user or for one namespace, depending on
configuration) is flooding the API server, and being throttled. By contrast,
if one priority level's histogram shows that all queues for that priority
level are longer than those for other priority levels, it may be appropriate
to increase that PriorityLevelConfiguration's concurrency shares.</div></li><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> is the same as
<code>apiserver_flowcontrol_nominal_limit_seats</code>. Before the
introduction of concurrency borrowing between priority levels,
this was always equal to <code>apiserver_flowcontrol_current_limit_seats</code>
(which did not exist as a distinct metric).</p></li><li><p><code>apiserver_flowcontrol_lower_limit_seats</code> is a gauge vector holding
the lower bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_upper_limit_seats</code> is a gauge vector holding
the upper bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_demand_seats</code> is a histogram vector counting
observations, at the end of every nanosecond, of each priority
level's ratio of (seat demand) / (nominal concurrency limit).
A priority level's seat demand is the sum, over both queued requests
and those in the initial phase of execution, of the maximum of the
number of seats occupied in the request's initial and final
execution phases.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_high_watermark</code> is a gauge vector
holding, for each priority level, the maximum seat demand seen
during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_average</code> is a gauge vector
holding, for each priority level, the time-weighted average seat
demand seen during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_stdev</code> is a gauge vector
holding, for each priority level, the time-weighted population
standard deviation of seat demand seen during the last concurrency
borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_smoothed</code> is a gauge vector
holding, for each priority level, the smoothed enveloped seat demand
determined at the last concurrency adjustment.</p></li><li><p><code>apiserver_flowcontrol_target_seats</code> is a gauge vector holding, for
each priority level, the concurrency target going into the borrowing
allocation problem.</p></li><li><p><code>apiserver_flowcontrol_seat_fair_frac</code> is a gauge holding the fair
allocation fraction determined in the last borrowing adjustment.</p></li><li><p><code>apiserver_flowcontrol_current_limit_seats</code> is a gauge vector
holding, for each priority level, the dynamic concurrency limit
derived in the last adjustment.</p></li><li><p><code>apiserver_flowcontrol_request_execution_seconds</code> is a histogram
vector of how long requests took to actually execute, broken down by
<code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_watch_count_samples</code> is a histogram vector of
the number of active WATCH requests relevant to a given write,
broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_work_estimated_seats</code> is a histogram vector
of the number of estimated seats (maximum of initial and final stage
of execution) associated with requests, broken down by <code>flow_schema</code>
and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_dispatch_no_accommodation_total</code> is a
counter vector of the number of events that in principle could have led
to a request being dispatched but did not, due to lack of available
concurrency, broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_epoch_advance_total</code> is a counter vector of
the number of attempts to jump a priority level's progress meter
backward to avoid numeric overflow, grouped by <code>priority_level</code> and
<code>success</code>.</p></li></ul><h2 id="good-practices-for-using-api-priority-and-fairness">Good practices for using API Priority and Fairness</h2><p>When a given priority level exceeds its permitted concurrency, requests can
experience increased latency or be dropped with an HTTP 429 (Too Many Requests)
error. To prevent these side effects of APF, you can modify your workload or
tweak your APF settings to ensure there are sufficient seats available to serve
your requests.</p><p>To detect whether requests are being rejected due to APF, check the following
metrics:</p><ul><li>apiserver_flowcontrol_rejected_requests_total: the total number of requests
rejected per FlowSchema and PriorityLevelConfiguration.</li><li>apiserver_flowcontrol_current_inqueue_requests: the current number of requests
queued per FlowSchema and PriorityLevelConfiguration.</li><li>apiserver_flowcontrol_request_wait_duration_seconds: the latency added to
requests waiting in queues.</li><li>apiserver_flowcontrol_priority_level_seat_utilization: the seat utilization
per PriorityLevelConfiguration.</li></ul><h3 id="good-practice-workload-modifications">Workload modifications</h3><p>To prevent requests from queuing and adding latency or being dropped due to APF,
you can optimize your requests by:</p><ul><li>Reducing the rate at which requests are executed. A fewer number of requests
over a fixed period will result in a fewer number of seats being needed at a
given time.</li><li>Avoid issuing a large number of expensive requests concurrently. Requests can
be optimized to use fewer seats or have lower latency so that these requests
hold those seats for a shorter duration. List requests can occupy more than 1
seat depending on the number of objects fetched during the request. Restricting
the number of objects retrieved in a list request, for example by using
pagination, will use less total seats over a shorter period. Furthermore,
replacing list requests with watch requests will require lower total concurrency
shares as watch requests only occupy 1 seat during its initial burst of
notifications. If using streaming lists in versions 1.27 and later, watch
requests will occupy the same number of seats as a list request for its initial
burst of notifications because the entire state of the collection has to be
streamed. Note that in both cases, a watch request will not hold any seats after
this initial phase.</li></ul><p>Keep in mind that queuing or rejected requests from APF could be induced by
either an increase in the number of requests or an increase in latency for
existing requests. For example, if requests that normally take 1s to execute
start taking 60s, it is possible that APF will start rejecting requests because
requests are occupying seats for a longer duration than normal due to this
increase in latency. If APF starts rejecting requests across multiple priority
levels without a significant change in workload, it is possible there is an
underlying issue with control plane performance rather than the workload or APF
settings.</p><h3 id="good-practice-apf-settings">Priority and fairness settings</h3><p>You can also modify the default FlowSchema and PriorityLevelConfiguration
objects or create new objects of these types to better accommodate your
workload.</p><p>APF settings can be modified to:</p><ul><li>Give more seats to high priority requests.</li><li>Isolate non-essential or expensive requests that would starve a concurrency
level if it was shared with other flows.</li></ul><h4 id="give-more-seats-to-high-priority-requests">Give more seats to high priority requests</h4><ol><li>If possible, the number of seats available across all priority levels for a
particular <code>kube-apiserver</code> can be increased by increasing the values for the
<code>max-requests-inflight</code> and <code>max-mutating-requests-inflight</code> flags. Alternatively,
horizontally scaling the number of <code>kube-apiserver</code> instances will increase the
total concurrency per priority level across the cluster assuming there is
sufficient load balancing of requests.</li><li>You can create a new FlowSchema which references a PriorityLevelConfiguration
with a larger concurrency level. This new PriorityLevelConfiguration could be an
existing level or a new level with its own set of nominal concurrency shares.
For example, a new FlowSchema could be introduced to change the
PriorityLevelConfiguration for your requests from global-default to workload-low
to increase the number of seats available to your user. Creating a new
PriorityLevelConfiguration will reduce the number of seats designated for
existing levels. Recall that editing a default FlowSchema or
PriorityLevelConfiguration will require setting the
<code>apf.kubernetes.io/autoupdate-spec</code> annotation to false.</li><li>You can also increase the NominalConcurrencyShares for the
PriorityLevelConfiguration which is serving your high priority requests.
Alternatively, for versions 1.26 and later, you can increase the LendablePercent
for competing priority levels so that the given priority level has a higher pool
of seats it can borrow.</li></ol><h4 id="isolate-non-essential-requests-from-starving-other-flows">Isolate non-essential requests from starving other flows</h4><p>For request isolation, you can create a FlowSchema whose subject matches the
user making these requests or create a FlowSchema that matches what the request
is (corresponding to the resourceRules). Next, you can map this FlowSchema to a
PriorityLevelConfiguration with a low share of seats.</p><p>For example, suppose list event requests from Pods running in the default namespace
are using 10 seats each and execute for 1 minute. To prevent these expensive
requests from impacting requests from other Pods using the existing service-accounts
FlowSchema, you can apply the following FlowSchema to isolate these list calls
from other requests.</p><p>Example FlowSchema object to isolate list event requests:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priority-and-fairness/list-events-default-service-account.yaml" download="priority-and-fairness/list-events-default-service-account.yaml"><code>priority-and-fairness/list-events-default-service-account.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;priority-and-fairness-list-events-default-service-account-yaml&quot;)" title="Copy priority-and-fairness/list-events-default-service-account.yaml to clipboard"/></div><div class="includecode" id="priority-and-fairness-list-events-default-service-account-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>flowcontrol.apiserver.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>FlowSchema<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>list-events-default-service-account<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">distinguisherMethod</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>ByUser<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">matchingPrecedence</span>:<span style="color:#bbb"> </span><span style="color:#666">8000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">priorityLevelConfiguration</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>catch-all<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">resourceRules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">apiGroups</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:#b44">'*'</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">namespaces</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- events<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- list<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">subjects</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">serviceAccount</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default</span></span></code></pre></div></div></div><ul><li>This FlowSchema captures all list event calls made by the default service
account in the default namespace. The matching precedence 8000 is lower than the
value of 9000 used by the existing service-accounts FlowSchema so these list
event calls will match list-events-default-service-account rather than
service-accounts.</li><li>The catch-all PriorityLevelConfiguration is used to isolate these requests.
The catch-all priority level has a very small concurrency share and does not
queue requests.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>You can visit flow control <a href="/docs/reference/debug-cluster/flow-control/">reference doc</a> to learn more about troubleshooting.</li><li>For background information on design details for API priority and fairness, see
the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness">enhancement proposal</a>.</li><li>You can make suggestions and feature requests via <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery</a>
or the feature's <a href="https://kubernetes.slack.com/messages/api-priority-and-fairness">slack channel</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Container Environment</h1><p>This page describes the resources available to Containers in the Container environment.</p><h2 id="container-environment">Container environment</h2><p>The Kubernetes Container environment provides several important resources to Containers:</p><ul><li>A filesystem, which is a combination of an <a href="/docs/concepts/containers/images/">image</a> and one or more <a href="/docs/concepts/storage/volumes/">volumes</a>.</li><li>Information about the Container itself.</li><li>Information about other objects in the cluster.</li></ul><h3 id="container-information">Container information</h3><p>The <em>hostname</em> of a Container is the name of the Pod in which the Container is running.
It is available through the <code>hostname</code> command or the
<a href="https://man7.org/linux/man-pages/man2/gethostname.2.html"><code>gethostname</code></a>
function call in libc.</p><p>The Pod name and namespace are available as environment variables through the
<a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">downward API</a>.</p><p>User defined environment variables from the Pod definition are also available to the Container,
as are any environment variables specified statically in the container image.</p><h3 id="cluster-information">Cluster information</h3><p>A list of all services that were running when a Container was created is available to that Container as environment variables.
This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</p><p>For a service named <em>foo</em> that maps to a Container named <em>bar</em>,
the following variables are defined:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">FOO_SERVICE_HOST</span><span style="color:#666">=</span>&lt;the host the service is running on&gt;
</span></span><span style="display:flex"><span><span style="color:#b8860b">FOO_SERVICE_PORT</span><span style="color:#666">=</span>&lt;the port the service is running on&gt;
</span></span></code></pre></div><p>Services have dedicated IP addresses and are available to the Container via DNS,
if <a href="https://releases.k8s.io/v1.34.0/cluster/addons/dns/">DNS addon</a> is enabled.Â </p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/containers/container-lifecycle-hooks/">Container lifecycle hooks</a>.</li><li>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to Container lifecycle events</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Observability</h1><div class="lead">Understand how to gain end-to-end visibility of a Kubernetes cluster through the collection of metrics, logs, and traces.</div><p>In Kubernetes, observability is the process of collecting and analyzing metrics, logs, and tracesâ€”often referred to as the three pillars of observabilityâ€”in order to obtain a better understanding of the internal state, performance, and health of the cluster.</p><p>Kubernetes control plane components, as well as many add-ons, generate and emit these signals. By aggregating and correlating them, you can gain a unified picture of the control plane, add-ons, and applications across the cluster.</p><p>Figure 1 outlines how cluster components emit the three primary signal types.</p><figure><div class="mermaid">flowchart LR
A[Cluster components] --&gt; M[Metrics pipeline]
A --&gt; L[Log pipeline]
A --&gt; T[Trace pipeline]
M --&gt; S[(Storage and analysis)]
L --&gt; S
T --&gt; S
S --&gt; O[Operators and automation]</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 1. High-level signals emitted by cluster components and their consumers.</em></p><h2 id="metrics">Metrics</h2><p>Kubernetes components emit metrics in <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a> from their <code>/metrics</code> endpoints, including:</p><ul><li>kube-controller-manager</li><li>kube-proxy</li><li>kube-apiserver</li><li>kube-scheduler</li><li>kubelet</li></ul><p>The kubelet also exposes metrics at <code>/metrics/cadvisor</code>, <code>/metrics/resource</code>, and <code>/metrics/probes</code>, and add-ons such as <a href="/docs/concepts/cluster-administration/kube-state-metrics/">kube-state-metrics</a> enrich those control plane signals with Kubernetes object status.</p><p>A typical Kubernetes metrics pipeline periodically scrapes these endpoints and stores the samples in a time series database (for example with Prometheus).</p><p>See the <a href="/docs/concepts/cluster-administration/system-metrics/">system metrics guide</a> for details and configuration options.</p><p>Figure 2 outlines a common Kubernetes metrics pipeline.</p><figure><div class="mermaid">flowchart LR
C[Cluster components] --&gt; P[Prometheus scraper]
P --&gt; TS[(Time series storage)]
TS --&gt; D[Dashboards and alerts]
TS --&gt; A[Automated actions]</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 2. Components of a typical Kubernetes metrics pipeline.</em></p><p>For multi-cluster or multi-cloud visibility, distributed time series databases (for example Thanos or Cortex) can complement Prometheus.</p><p>See <a href="#metrics-tools">Common observability tools - metrics tools</a> for metrics scrapers and time series databases.</p><h4 id="see-also">See Also</h4><ul><li><a href="/docs/concepts/cluster-administration/system-metrics/">System metrics for Kubernetes components</a></li><li><a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">Resource usage monitoring with metrics-server</a></li><li><a href="/docs/concepts/cluster-administration/kube-state-metrics/">kube-state-metrics concept</a></li><li><a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/">Resource metrics pipeline overview</a></li></ul><h2 id="logs">Logs</h2><p>Logs provide a chronological record of events inside applications, Kubernetes system components, and security-related activities such as audit logging.</p><p>Container runtimes capture a containerized applicationâ€™s output from standard output (<code>stdout</code>) and standard error (<code>stderr</code>) streams. While runtimes implement this differently, the integration with the kubelet is standardized through the <em>CRI logging format</em>, and the kubelet makes these logs available through <code>kubectl logs</code>.</p><p><img alt="Node-level logging" src="/images/docs/user-guide/logging/logging-node-level.png"/></p><p><em>Figure 3a. Node-level logging architecture.</em></p><p>System component logs capture events from the cluster and are often useful for debugging and troubleshooting. These components are classified in two different ways: those that run in a container and those that do not. For example, the <code>kube-scheduler</code> and <code>kube-proxy</code> usually run in containers, whereas the <code>kubelet</code> and the container runtime run directly on the host.</p><ul><li>On machines with <code>systemd</code>, the kubelet and container runtime write to journald. Otherwise, they write to <code>.log</code> files in the <code>/var/log</code> directory.</li><li>System components that run inside containers always write to <code>.log</code> files in <code>/var/log</code>, bypassing the default container logging mechanism.</li></ul><p>System component and container logs stored under <code>/var/log</code> require log rotation to prevent uncontrolled growth. Some cluster provisioning scripts install log rotation by default; verify your environment and adjust as needed. See the <a href="/docs/concepts/cluster-administration/system-logs/">system logs reference</a> for details on locations, formats, and configuration options.</p><p>Most clusters run a node-level logging agent (for example, Fluent Bit or Fluentd) that tails these files and forwards entries to a central log store. The <a href="/docs/concepts/cluster-administration/logging/">logging architecture guidance</a> explains how to design such pipelines, apply retention, and log flows to backends.</p><p>Figure 3 outlines a common log aggregation pipeline.</p><figure><div class="mermaid">flowchart LR
subgraph Sources
A[Application stdout / stderr]
B[Control plane logs]
C[Audit records]
end
A --&gt; N[Node log agent]
B --&gt; N
C --&gt; N
N --&gt; L[Central log store]
L --&gt; Q[Dashboards, alerting, SIEM]</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 3. Components of a typical Kubernetes logs pipeline.</em></p><p>See <a href="#logging-tools">Common observability tools - logging tools</a> for logging agents and central log stores.</p><h4 id="see-also-1">See Also</h4><ul><li><a href="/docs/concepts/cluster-administration/logging/">Logging architecture</a></li><li><a href="/docs/concepts/cluster-administration/system-logs/">System logs</a></li><li><a href="/docs/tasks/debug/logging/">Logging tasks and tutorials</a></li><li><a href="/docs/tasks/debug/debug-cluster/audit/">Configure audit logging</a></li></ul><h2 id="traces">Traces</h2><p>Traces capture how requests moves across Kubernetes components and applications, linking latency, timing and relationships between operations.By collecting traces, you can visualize end-to-end request flow, diagnose performance issues, and identify bottlenecks or unexpected interactions in the control plane, add-ons, or applications.</p><p>Kubernetes 1.34 can export spans over the <a href="/docs/concepts/cluster-administration/system-traces/">OpenTelemetry Protocol</a> (OTLP), either directly via built-in gRPC exporters or by forwarding them through an OpenTelemetry Collector.</p><p>The OpenTelemetry Collector receives spans from components and applications, processes them (for example by applying sampling or redaction), and forwards them to a tracing backend for storage and analysis.</p><p>Figure 4 outlines a typical distributed tracing pipeline.</p><figure><div class="mermaid">flowchart LR
subgraph Sources
A[Control plane spans]
B[Application spans]
end
A --&gt; X[OTLP exporter]
B --&gt; X
X --&gt; COL[OpenTelemetry Collector]
COL --&gt; TS[(Tracing backend)]
TS --&gt; V[Visualization and analysis]</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 4. Components of a typical Kubernetes traces pipeline.</em></p><p>See <a href="#tracing-tools">Common observability tools - tracing tools</a> for tracing collectors and backends.</p><h4 id="see-also-2">See Also</h4><ul><li><a href="/docs/concepts/cluster-administration/system-traces/">System traces for Kubernetes components</a></li><li><a href="https://opentelemetry.io/docs/collector/getting-started/">OpenTelemetry Collector getting started guide</a></li><li><a href="/docs/tasks/debug/monitoring/">Monitoring and tracing tasks</a></li></ul><h2 id="common-observability-tools">Common observability tools</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Note: This section links to third-party projects that provide observability capabilities required by Kubernetes.
The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a
project to this list, read the <a href="/docs/contribute/style/content-guide/">content guide</a> before submitting a change.</p><h3 id="metrics-tools">Metrics tools</h3><ul><li><a href="https://cortexmetrics.io/">Cortex</a> offers horizontally scalable, long-term Prometheus storage.</li><li><a href="https://grafana.com/oss/mimir/">Grafana Mimir</a> is a Grafana Labs project that provides multi-tenant, horizontally scalable Prometheus-compatible storage.</li><li><a href="https://prometheus.io/">Prometheus</a> is the monitoring system that scrapes and stores metrics from Kubernetes components.</li><li><a href="https://thanos.io/">Thanos</a> extends Prometheus with global querying, downsampling, and object storage support.</li></ul><h3 id="logging-tools">Logging tools</h3><ul><li><a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a> delivers distributed log indexing and search.</li><li><a href="https://fluentbit.io/">Fluent Bit</a> collects and forwards container and node logs with a low resource footprint.</li><li><a href="https://www.fluentd.org/">Fluentd</a> routes and transforms logs to multiple destinations.</li><li><a href="https://grafana.com/oss/loki/">Grafana Loki</a> stores logs in a Prometheus-inspired, label-based format.</li><li><a href="https://opensearch.org/">OpenSearch</a> provides open source log indexing and search compatible with Elasticsearch APIs.</li></ul><h3 id="tracing-tools">Tracing tools</h3><ul><li><a href="https://grafana.com/oss/tempo/">Grafana Tempo</a> offers scalable, low-cost distributed tracing storage.</li><li><a href="https://www.jaegertracing.io/">Jaeger</a> captures and visualizes distributed traces for microservices.</li><li><a href="https://opentelemetry.io/docs/collector/">OpenTelemetry Collector</a> receives, processes, and exports telemetry data including traces.</li><li><a href="https://zipkin.io/">Zipkin</a> provides distributed tracing collection and visualization.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">collect resource usage metrics with metrics-server</a></li><li>Explore <a href="/docs/tasks/debug/logging/">logging tasks and tutorials</a></li><li>Follow the <a href="/docs/tasks/debug/monitoring/">monitoring and tracing task guides</a></li><li>Review the <a href="/docs/concepts/cluster-administration/system-metrics/">system metrics guide</a> for component endpoints and stability</li><li>Review the <a href="#common-observability-tools">common observability tools</a> section for vetted third-party options</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Configuration Best Practices</h1><p>This document highlights and consolidates configuration best practices that are introduced
throughout the user guide, Getting Started documentation, and examples.</p><p>This is a living document. If you think of something that is not on this list but might be useful
to others, please don't hesitate to file an issue or submit a PR.</p><h2 id="general-configuration-tips">General Configuration Tips</h2><ul><li><p>When defining configurations, specify the latest stable API version.</p></li><li><p>Configuration files should be stored in version control before being pushed to the cluster. This
allows you to quickly roll back a configuration change if necessary. It also aids cluster
re-creation and restoration.</p></li><li><p>Write your configuration files using YAML rather than JSON. Though these formats can be used
interchangeably in almost all scenarios, YAML tends to be more user-friendly.</p></li><li><p>Group related objects into a single file whenever it makes sense. One file is often easier to
manage than several. See the
<a href="https://github.com/kubernetes/examples/tree/master/web/guestbook/all-in-one/guestbook-all-in-one.yaml">guestbook-all-in-one.yaml</a>
file as an example of this syntax.</p></li><li><p>Note also that many <code>kubectl</code> commands can be called on a directory. For example, you can call
<code>kubectl apply</code> on a directory of config files.</p></li><li><p>Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.</p></li><li><p>Put object descriptions in annotations, to allow better introspection.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>There is a breaking change introduced in the <a href="https://yaml.org/spec/1.2.0/#id2602744">YAML 1.2</a>
boolean values specification with respect to <a href="https://yaml.org/spec/1.1/#id864510">YAML 1.1</a>.
This is a known <a href="https://github.com/kubernetes/kubernetes/issues/34146">issue</a> in Kubernetes.
YAML 1.2 only recognizes <strong>true</strong> and <strong>false</strong> as valid booleans, while YAML 1.1 also accepts
<strong>yes</strong>, <strong>no</strong>, <strong>on</strong>, and <strong>off</strong> as booleans. However, Kubernetes uses YAML
<a href="https://github.com/kubernetes/kubernetes/issues/34146#issuecomment-252692024">parsers</a> that are
mostly compatible with YAML 1.1, which means that using <strong>yes</strong> or <strong>no</strong> instead of <strong>true</strong> or
<strong>false</strong> in a YAML manifest may cause unexpected errors or behaviors. To avoid this issue, it is
recommended to always use <strong>true</strong> or <strong>false</strong> for boolean values in YAML manifests, and to quote
any strings that may be confused with booleans, such as <strong>"yes"</strong> or <strong>"no"</strong>.</p><p>Besides booleans, there are additional specifications changes between YAML versions. Please refer
to the <a href="https://spec.yaml.io/main/spec/1.2.2/ext/changes">YAML Specification Changes</a> documentation
for a comprehensive list.</p></div><h2 id="naked-pods-vs-replicasets-deployments-and-jobs">"Naked" Pods versus ReplicaSets, Deployments, and Jobs</h2><ul><li><p>Don't use naked Pods (that is, Pods not bound to a <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> or
<a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>) if you can avoid it. Naked Pods
will not be rescheduled in the event of a node failure.</p><p>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is
always available, and specifies a strategy to replace Pods (such as
<a href="/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">RollingUpdate</a>), is
almost always preferable to creating Pods directly, except for some explicit
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>restartPolicy: Never</code></a> scenarios.
A <a href="/docs/concepts/workloads/controllers/job/">Job</a> may also be appropriate.</p></li></ul><h2 id="services">Services</h2><ul><li><p>Create a <a href="/docs/concepts/services-networking/service/">Service</a> before its corresponding backend
workloads (Deployments or ReplicaSets), and before any workloads that need to access it.
When Kubernetes starts a container, it provides environment variables pointing to all the Services
which were running when the container was started. For example, if a Service named <code>foo</code> exists,
all containers will get the following variables in their initial environment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">FOO_SERVICE_HOST</span><span style="color:#666">=</span>&lt;the host the Service is running on&gt;
</span></span><span style="display:flex"><span><span style="color:#b8860b">FOO_SERVICE_PORT</span><span style="color:#666">=</span>&lt;the port the Service is running on&gt;
</span></span></code></pre></div><p><em>This does imply an ordering requirement</em> - any <code>Service</code> that a <code>Pod</code> wants to access must be
created before the <code>Pod</code> itself, or else the environment variables will not be populated.
DNS does not have this restriction.</p></li><li><p>An optional (though strongly recommended) <a href="/docs/concepts/cluster-administration/addons/">cluster add-on</a>
is a DNS server. The DNS server watches the Kubernetes API for new <code>Services</code> and creates a set
of DNS records for each. If DNS has been enabled throughout the cluster then all <code>Pods</code> should be
able to do name resolution of <code>Services</code> automatically.</p></li><li><p>Don't specify a <code>hostPort</code> for a Pod unless it is absolutely necessary. When you bind a Pod to a
<code>hostPort</code>, it limits the number of places the Pod can be scheduled, because each &lt;<code>hostIP</code>,
<code>hostPort</code>, <code>protocol</code>&gt; combination must be unique. If you don't specify the <code>hostIP</code> and
<code>protocol</code> explicitly, Kubernetes will use <code>0.0.0.0</code> as the default <code>hostIP</code> and <code>TCP</code> as the
default <code>protocol</code>.</p><p>If you only need access to the port for debugging purposes, you can use the
<a href="/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls">apiserver proxy</a>
or <a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/"><code>kubectl port-forward</code></a>.</p><p>If you explicitly need to expose a Pod's port on the node, consider using a
<a href="/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> Service before resorting to
<code>hostPort</code>.</p></li><li><p>Avoid using <code>hostNetwork</code>, for the same reasons as <code>hostPort</code>.</p></li><li><p>Use <a href="/docs/concepts/services-networking/service/#headless-services">headless Services</a>
(which have a <code>ClusterIP</code> of <code>None</code>) for service discovery when you don't need <code>kube-proxy</code>
load balancing.</p></li></ul><h2 id="using-labels">Using Labels</h2><ul><li><p>Define and use <a href="/docs/concepts/overview/working-with-objects/labels/">labels</a> that identify
<strong>semantic attributes</strong> of your application or Deployment, such as <code>{ app.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 }</code>. You can use these labels to select the
appropriate Pods for other resources; for example, a Service that selects all <code>tier: frontend</code>
Pods, or all <code>phase: test</code> components of <code>app.kubernetes.io/name: MyApp</code>.
See the <a href="https://github.com/kubernetes/examples/tree/master/web/guestbook/">guestbook</a> app
for examples of this approach.</p><p>A Service can be made to span multiple Deployments by omitting release-specific labels from its
selector. When you need to update a running service without downtime, use a
<a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>.</p><p>A desired state of an object is described by a Deployment, and if changes to that spec are
<em>applied</em>, the deployment controller changes the actual state to the desired state at a controlled
rate.</p></li><li><p>Use the <a href="/docs/concepts/overview/working-with-objects/common-labels/">Kubernetes common labels</a>
for common use cases. These standardized labels enrich the metadata in a way that allows tools,
including <code>kubectl</code> and <a href="/docs/tasks/access-application-cluster/web-ui-dashboard/">dashboard</a>, to
work in an interoperable way.</p></li><li><p>You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and
Services match to Pods using selector labels, removing the relevant labels from a Pod will stop
it from being considered by a controller or from being served traffic by a Service. If you remove
the labels of an existing Pod, its controller will create a new Pod to take its place. This is a
useful way to debug a previously "live" Pod in a "quarantine" environment. To interactively remove
or add labels, use <a href="/docs/reference/generated/kubectl/kubectl-commands#label"><code>kubectl label</code></a>.</p></li></ul><h2 id="using-kubectl">Using kubectl</h2><ul><li><p>Use <code>kubectl apply -f &lt;directory&gt;</code>. This looks for Kubernetes configuration in all <code>.yaml</code>,
<code>.yml</code>, and <code>.json</code> files in <code>&lt;directory&gt;</code> and passes it to <code>apply</code>.</p></li><li><p>Use label selectors for <code>get</code> and <code>delete</code> operations instead of specific object names. See the
sections on <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selectors</a>
and <a href="/docs/concepts/overview/working-with-objects/labels/#using-labels-effectively">using labels effectively</a>.</p></li><li><p>Use <code>kubectl create deployment</code> and <code>kubectl expose</code> to quickly create single-container
Deployments and Services.
See <a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a>
for an example.</p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Installing Addons</h1><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>â€ˆThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Add-ons extend the functionality of Kubernetes.</p><p>This page lists some of the available add-ons and links to their respective
installation instructions. The list does not try to be exhaustive.</p><h2 id="networking-and-network-policy">Networking and Network Policy</h2><ul><li><a href="https://www.github.com/noironetworks/aci-containers">ACI</a> provides integrated
container networking and network security with Cisco ACI.</li><li><a href="https://antrea.io/">Antrea</a> operates at Layer 3/4 to provide networking and
security services for Kubernetes, leveraging Open vSwitch as the networking
data plane. Antrea is a <a href="https://www.cncf.io/projects/antrea/">CNCF project at the Sandbox level</a>.</li><li><a href="https://www.tigera.io/project-calico/">Calico</a> is a networking and network
policy provider. Calico supports a flexible set of networking options so you
can choose the most efficient option for your situation, including non-overlay
and overlay networks, with or without BGP. Calico uses the same engine to
enforce network policy for hosts, pods, and (if using Istio &amp; Envoy)
applications at the service mesh layer.</li><li><a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel">Canal</a>
unites Flannel and Calico, providing networking and network policy.</li><li><a href="https://github.com/cilium/cilium">Cilium</a> is a networking, observability,
and security solution with an eBPF-based data plane. Cilium provides a
simple flat Layer 3 network with the ability to span multiple clusters
in either a native routing or overlay/encapsulation mode, and can enforce
network policies on L3-L7 using an identity-based security model that is
decoupled from network addressing. Cilium can act as a replacement for
kube-proxy; it also offers additional, opt-in observability and security features.
Cilium is a <a href="https://www.cncf.io/projects/cilium/">CNCF project at the Graduated level</a>.</li><li><a href="https://github.com/cni-genie/CNI-Genie">CNI-Genie</a> enables Kubernetes to seamlessly
connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave.
CNI-Genie is a <a href="https://www.cncf.io/projects/cni-genie/">CNCF project at the Sandbox level</a>.</li><li><a href="https://contivpp.io/">Contiv</a> provides configurable networking (native L3 using BGP,
overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich
policy framework. Contiv project is fully <a href="https://github.com/contiv">open sourced</a>.
The <a href="https://github.com/contiv/install">installer</a> provides both kubeadm and
non-kubeadm based installation options.</li><li><a href="https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/">Contrail</a>,
based on <a href="https://tungsten.io">Tungsten Fabric</a>, is an open source, multi-cloud
network virtualization and policy management platform. Contrail and Tungsten
Fabric are integrated with orchestration systems such as Kubernetes, OpenShift,
OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods
and bare metal workloads.</li><li><a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">Flannel</a> is
an overlay network provider that can be used with Kubernetes.</li><li><a href="/docs/concepts/services-networking/gateway/">Gateway API</a> is an open source project managed by
the <a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network</a> community and
provides an expressive, extensible, and role-oriented API for modeling service networking.</li><li><a href="https://github.com/ZTE/Knitter/">Knitter</a> is a plugin to support multiple network
interfaces in a Kubernetes pod.</li><li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</a> is a Multi plugin for
multiple network support in Kubernetes to support all CNI plugins
(e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and
VPP based workloads in Kubernetes.</li><li><a href="https://github.com/ovn-org/ovn-kubernetes/">OVN-Kubernetes</a> is a networking
provider for Kubernetes based on <a href="https://github.com/ovn-org/ovn/">OVN (Open Virtual Network)</a>,
a virtual networking implementation that came out of the Open vSwitch (OVS) project.
OVN-Kubernetes provides an overlay based networking implementation for Kubernetes,
including an OVS based implementation of load balancing and network policy.</li><li><a href="https://github.com/akraino-edge-stack/icn-nodus">Nodus</a> is an OVN based CNI
controller plugin to provide cloud native based Service function chaining(SFC).</li><li><a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html">NSX-T</a> Container Plug-in (NCP)
provides integration between VMware NSX-T and container orchestrators such as
Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS
platforms such as Pivotal Container Service (PKS) and OpenShift.</li><li><a href="https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst">Nuage</a>
is an SDN platform that provides policy-based networking between Kubernetes
Pods and non-Kubernetes environments with visibility and security monitoring.</li><li><a href="https://github.com/romana">Romana</a> is a Layer 3 networking solution for pod
networks that also supports the <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> API.</li><li><a href="https://github.com/spidernet-io/spiderpool">Spiderpool</a> is an underlay and RDMA
networking solution for Kubernetes. Spiderpool is supported on bare metal, virtual machines,
and public cloud environments.</li><li><a href="https://github.com/AliyunContainerService/terway/">Terway</a> is a suite of CNI plugins
based on AlibabaCloud's VPC and ECS network products. It provides native VPC networking
and network policies in AlibabaCloud environments.</li><li><a href="https://github.com/rajch/weave#using-weave-on-kubernetes">Weave Net</a>
provides networking and network policy, will carry on working on both sides
of a network partition, and does not require an external database.</li></ul><h2 id="service-discovery">Service Discovery</h2><ul><li><a href="https://coredns.io">CoreDNS</a> is a flexible, extensible DNS server which can
be <a href="https://github.com/coredns/helm">installed</a>
as the in-cluster DNS for pods.</li></ul><h2 id="visualization-amp-control">Visualization &amp; Control</h2><ul><li><a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard">Dashboard</a>
is a dashboard web interface for Kubernetes.</li></ul><h2 id="infrastructure">Infrastructure</h2><ul><li><a href="https://kubevirt.io/user-guide/#/installation/installation">KubeVirt</a> is an add-on
to run virtual machines on Kubernetes. Usually run on bare-metal clusters.</li><li>The
<a href="https://github.com/kubernetes/node-problem-detector">node problem detector</a>
runs on Linux nodes and reports system issues as either
<a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">Events</a> or
<a href="/docs/concepts/architecture/nodes/#condition">Node conditions</a>.</li></ul><h2 id="instrumentation">Instrumentation</h2><ul><li><a href="/docs/concepts/cluster-administration/kube-state-metrics/">kube-state-metrics</a></li></ul><h2 id="legacy-add-ons">Legacy Add-ons</h2><p>There are several other add-ons documented in the deprecated
<a href="https://git.k8s.io/kubernetes/cluster/addons">cluster/addons</a> directory.</p><p>Well-maintained ones should be linked to here. PRs welcome!</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Coordinated Leader Election</h1><div class="feature-state-notice feature-beta" title="Feature Gate: CoordinatedLeaderElection"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><p>Kubernetes 1.34 includes a beta feature that allows <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> components to
deterministically select a leader via <em>coordinated leader election</em>.
This is useful to satisfy Kubernetes version skew constraints during cluster upgrades.
Currently, the only builtin selection strategy is <code>OldestEmulationVersion</code>,
preferring the leader with the lowest emulation version, followed by binary
version, followed by creation timestamp.</p><h2 id="enabling-coordinated-leader-election">Enabling coordinated leader election</h2><p>Ensure that <code>CoordinatedLeaderElection</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> is enabled
when you start the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="API Server">API Server</a>: and that the <code>coordination.k8s.io/v1beta1</code> API group is
enabled.</p><p>This can be done by setting flags <code>--feature-gates="CoordinatedLeaderElection=true"</code> and
<code>--runtime-config="coordination.k8s.io/v1beta1=true"</code>.</p><h2 id="component-configuration">Component configuration</h2><p>Provided that you have enabled the <code>CoordinatedLeaderElection</code> feature gate <em>and</em><br/>have the <code>coordination.k8s.io/v1beta1</code> API group enabled, compatible control plane<br/>components automatically use the LeaseCandidate and Lease APIs to elect a leader<br/>as needed.</p><p>For Kubernetes 1.34, two control plane components<br/>(kube-controller-manager and kube-scheduler) automatically use coordinated<br/>leader election when the feature gate and API group are enabled.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Certificates</h1><p>To learn how to generate certificates for your cluster, see <a href="/docs/tasks/administer-cluster/certificates/">Certificates</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Metrics For Kubernetes System Components</h1><p>System component metrics can give a better look into what is happening inside them. Metrics are
particularly useful for building dashboards and alerts.</p><p>Kubernetes components emit metrics in <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a>.
This format is structured plain text, designed so that people and machines can both read it.</p><h2 id="metrics-in-kubernetes">Metrics in Kubernetes</h2><p>In most cases metrics are available on <code>/metrics</code> endpoint of the HTTP server. For components that
don't expose endpoint by default, it can be enabled using <code>--bind-address</code> flag.</p><p>Examples of those components:</p><ul><li><a class="glossary-tooltip" title="Control Plane component that runs controller processes." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" aria-label="kube-controller-manager">kube-controller-manager</a></li><li><a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" aria-label="kube-proxy">kube-proxy</a></li><li><a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="kube-apiserver">kube-apiserver</a></li><li><a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a></li><li><a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a></li></ul><p>In a production environment you may want to configure <a href="https://prometheus.io/">Prometheus Server</a>
or some other metrics scraper to periodically gather these metrics and make them available in some
kind of time series database.</p><p>Note that <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> also exposes metrics in
<code>/metrics/cadvisor</code>, <code>/metrics/resource</code> and <code>/metrics/probes</code> endpoints. Those metrics do not
have the same lifecycle.</p><p>If your cluster uses <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/reference/access-authn-authz/rbac/" target="_blank" aria-label="RBAC">RBAC</a>, reading metrics requires
authorization via a user, group or ServiceAccount with a ClusterRole that allows accessing
<code>/metrics</code>. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>prometheus<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">nonResourceURLs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"/metrics"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">verbs</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- get<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="metric-lifecycle">Metric lifecycle</h2><p>Alpha metric â†’ Beta metric â†’ Stable metric â†’ Deprecated metric â†’ Hidden metric â†’ Deleted metric</p><p>Alpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.</p><p>Beta metrics observe a looser API contract than its stable counterparts. No labels can be removed from beta metrics during their lifetime, however, labels can be added while the metric is in the beta stage.</p><p>Stable metrics are guaranteed to not change. This means:</p><ul><li>A stable metric without a deprecated signature will not be deleted or renamed</li><li>A stable metric's type will not be modified</li></ul><p>Deprecated metrics are slated for deletion, but are still available for use.
These metrics include an annotation about the version in which they became deprecated.</p><p>For example:</p><ul><li><p>Before deprecation</p><pre tabindex="0"><code># HELP some_counter this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li><li><p>After deprecation</p><pre tabindex="0"><code># HELP some_counter (Deprecated since 1.15.0) this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li></ul><p>Hidden metrics are no longer published for scraping, but are still available for use.
A deprecated metric becomes a hidden metric after a period of time, based on its stability level:</p><ul><li><strong>STABLE</strong> metrics become hidden after a minimum of 3 releases or 9 months, whichever is longer.</li><li><strong>BETA</strong> metrics become hidden after a minimum of 1 release or 4 months, whichever is longer.</li><li><strong>ALPHA</strong> metrics can be hidden or removed in the same release in which they are deprecated.</li></ul><p>To use a hidden metric, you must enable it. For more details, refer to the <a href="#show-hidden-metrics">Show hidden metrics</a> section.</p><p>Deleted metrics are no longer published and cannot be used.</p><h2 id="show-hidden-metrics">Show hidden metrics</h2><p>As described above, admins can enable hidden metrics through a command-line flag on a specific
binary. This intends to be used as an escape hatch for admins if they missed the migration of the
metrics deprecated in the last release.</p><p>The flag <code>show-hidden-metrics-for-version</code> takes a version for which you want to show metrics
deprecated in that release. The version is expressed as x.y, where x is the major version, y is
the minor version. The patch version is not needed even though a metrics can be deprecated in a
patch release, the reason for that is the metrics deprecation policy runs against the minor release.</p><p>The flag can only take the previous minor version as its value. If you want to show all metrics hidden in the previous release, you can set the <code>show-hidden-metrics-for-version</code> flag to the previous version. Using a version that is too old is not allowed because it violates the metrics deprecation policy.</p><p>For example, let's assume metric <code>A</code> is deprecated in <code>1.29</code>. The version in which metric <code>A</code> becomes hidden depends on its stability level:</p><ul><li>If metric <code>A</code> is <strong>ALPHA</strong>, it could be hidden in <code>1.29</code>.</li><li>If metric <code>A</code> is <strong>BETA</strong>, it will be hidden in <code>1.30</code> at the earliest. If you are upgrading to <code>1.30</code> and still need <code>A</code>, you must use the command-line flag <code>--show-hidden-metrics-for-version=1.29</code>.</li><li>If metric <code>A</code> is <strong>STABLE</strong>, it will be hidden in <code>1.32</code> at the earliest. If you are upgrading to <code>1.32</code> and still need <code>A</code>, you must use the command-line flag <code>--show-hidden-metrics-for-version=1.31</code>.</li></ul><h2 id="component-metrics">Component metrics</h2><h3 id="kube-controller-manager-metrics">kube-controller-manager metrics</h3><p>Controller manager metrics provide important insight into the performance and health of the
controller manager. These metrics include common Go language runtime metrics such as go_routine
count and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE,
OpenStack) API latencies that can be used to gauge the health of a cluster.</p><p>Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations
for GCE, AWS, Vsphere and OpenStack.
These metrics can be used to monitor health of persistent volume operations.</p><p>For example, for GCE these metrics are called:</p><pre tabindex="0"><code>cloudprovider_gce_api_request_duration_seconds { request = "instance_list"}
cloudprovider_gce_api_request_duration_seconds { request = "disk_insert"}
cloudprovider_gce_api_request_duration_seconds { request = "disk_delete"}
cloudprovider_gce_api_request_duration_seconds { request = "attach_disk"}
cloudprovider_gce_api_request_duration_seconds { request = "detach_disk"}
cloudprovider_gce_api_request_duration_seconds { request = "list_disk"}
</code></pre><h3 id="kube-scheduler-metrics">kube-scheduler metrics</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [beta]</code></div><p>The scheduler exposes optional metrics that reports the requested resources and the desired limits
of all running pods. These metrics can be used to build capacity planning dashboards, assess
current or historical scheduling limits, quickly identify workloads that cannot schedule due to
lack of resources, and compare actual usage to the pod's request.</p><p>The kube-scheduler identifies the resource <a href="/docs/concepts/configuration/manage-resources-containers/">requests and limits</a>
configured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a
metrics timeseries. The time series is labelled by:</p><ul><li>namespace</li><li>pod name</li><li>the node where the pod is scheduled or an empty string if not yet scheduled</li><li>priority</li><li>the assigned scheduler for that pod</li><li>the name of the resource (for example, <code>cpu</code>)</li><li>the unit of the resource if known (for example, <code>cores</code>)</li></ul><p>Once a pod reaches completion (has a <code>restartPolicy</code> of <code>Never</code> or <code>OnFailure</code> and is in the
<code>Succeeded</code> or <code>Failed</code> pod phase, or has been deleted and all containers have a terminated state)
the series is no longer reported since the scheduler is now free to schedule other pods to run.
The two metrics are called <code>kube_pod_resource_request</code> and <code>kube_pod_resource_limit</code>.</p><p>The metrics are exposed at the HTTP endpoint <code>/metrics/resources</code>. They require
authorization for the <code>/metrics/resources</code> endpoint, usually granted by a
ClusterRole with the <code>get</code> verb for the <code>/metrics/resources</code> non-resource URL.</p><p>On Kubernetes 1.21 you must use the <code>--show-hidden-metrics-for-version=1.20</code>
flag to expose these alpha stability metrics.</p><h3 id="kubelet-pressure-stall-information-psi-metrics">kubelet Pressure Stall Information (PSI) metrics</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code></div><p>As a beta feature, Kubernetes lets you configure kubelet to collect Linux kernel
<a href="https://docs.kernel.org/accounting/psi.html">Pressure Stall Information</a>
(PSI) for CPU, memory and I/O usage.
The information is collected at node, pod and container level.
The metrics are exposed at the <code>/metrics/cadvisor</code> endpoint with the following names:</p><pre tabindex="0"><code>container_pressure_cpu_stalled_seconds_total
container_pressure_cpu_waiting_seconds_total
container_pressure_memory_stalled_seconds_total
container_pressure_memory_waiting_seconds_total
container_pressure_io_stalled_seconds_total
container_pressure_io_waiting_seconds_total
</code></pre><p>This feature is enabled by default, by setting the <code>KubeletPSI</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>. The information is also exposed in the
<a href="/docs/reference/instrumentation/node-metrics/#psi">Summary API</a>.</p><p>You can learn how to interpret the PSI metrics in <a href="/docs/reference/instrumentation/understand-psi-metrics/">Understand PSI Metrics</a>.</p><h4 id="requirements">Requirements</h4><p>Pressure Stall Information requires:</p><ul><li><a href="/docs/reference/node/kernel-version-requirements/#requirements-psi">Linux kernel versions 4.20 or later</a>.</li><li><a href="/docs/concepts/architecture/cgroups/">cgroup v2</a></li></ul><h2 id="disabling-metrics">Disabling metrics</h2><p>You can explicitly turn off metrics via command line flag <code>--disabled-metrics</code>. This may be
desired if, for example, a metric is causing a performance problem. The input is a list of
disabled metrics (i.e. <code>--disabled-metrics=metric1,metric2</code>).</p><h2 id="metric-cardinality-enforcement">Metric cardinality enforcement</h2><p>Metrics with unbounded dimensions could cause memory issues in the components they instrument. To
limit resource use, you can use the <code>--allow-metric-labels</code> command line option to dynamically
configure an allow-list of label values for a metric.</p><p>In alpha stage, the flag can only take in a series of mappings as metric label allow-list.
Each mapping is of the format <code>&lt;metric_name&gt;,&lt;label_name&gt;=&lt;allowed_labels&gt;</code> where
<code>&lt;allowed_labels&gt;</code> is a comma-separated list of acceptable label names.</p><p>The overall format looks like:</p><pre tabindex="0"><code>--allow-metric-labels &lt;metric_name&gt;,&lt;label_name&gt;='&lt;allow_value1&gt;, &lt;allow_value2&gt;...', &lt;metric_name2&gt;,&lt;label_name&gt;='&lt;allow_value1&gt;, &lt;allow_value2&gt;...', ...
</code></pre><p>Here is an example:</p><pre tabindex="0"><code class="language-none" data-lang="none">--allow-metric-labels number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'
</code></pre><p>In addition to specifying this from the CLI, this can also be done within a configuration file. You
can specify the path to that configuration file using the <code>--allow-metric-labels-manifest</code> command
line argument to a component. Here's an example of the contents of that configuration file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">"metric1,label2": </span><span style="color:#b44">"v1,v2,v3"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">"metric2,label1": </span><span style="color:#b44">"v1,v2,v3"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Additionally, the <code>cardinality_enforcement_unexpected_categorizations_total</code> meta-metric records the
count of unexpected categorizations during cardinality enforcement, that is, whenever a label value
is encountered that is not allowed with respect to the allow-list constraints.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about the <a href="https://github.com/prometheus/docs/blob/main/docs/instrumenting/exposition_formats.md#text-based-format">Prometheus text format</a>
for metrics</li><li>See the list of <a href="https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml">stable Kubernetes metrics</a></li><li>Read about the <a href="/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior">Kubernetes deprecation policy</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Secrets</h1><p>A Secret is an object that contains a small amount of sensitive data such as
a password, a token, or a key. Such information might otherwise be put in a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a> specification or in a
<a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-image" target="_blank" aria-label="container image">container image</a>. Using a
Secret means that you don't need to include confidential data in your
application code.</p><p>Because Secrets can be created independently of the Pods that use them, there
is less risk of the Secret (and its data) being exposed during the workflow of
creating, viewing, and editing Pods. Kubernetes, and applications that run in
your cluster, can also take additional precautions with Secrets, such as avoiding
writing sensitive data to nonvolatile storage.</p><p>Secrets are similar to <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/configmap/" target="_blank" aria-label="ConfigMaps">ConfigMaps</a>
but are specifically intended to hold confidential data.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store
(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.
Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read
any Secret in that namespace; this includes indirect access such as the ability to create a
Deployment.</p><p>In order to safely use Secrets, take at least the following steps:</p><ol><li><a href="/docs/tasks/administer-cluster/encrypt-data/">Enable Encryption at Rest</a> for Secrets.</li><li><a href="/docs/reference/access-authn-authz/authorization/">Enable or configure RBAC rules</a> with
least-privilege access to Secrets.</li><li>Restrict Secret access to specific containers.</li><li><a href="https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver">Consider using external Secret store providers</a>.</li></ol><p>For more guidelines to manage and improve the security of your Secrets, refer to
<a href="/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a>.</p></div><p>See <a href="#information-security-for-secrets">Information security for Secrets</a> for more details.</p><h2 id="uses-for-secrets">Uses for Secrets</h2><p>You can use Secrets for purposes such as the following:</p><ul><li><a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data">Set environment variables for a container</a>.</li><li><a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#provide-prod-test-creds">Provide credentials such as SSH keys or passwords to Pods</a>.</li><li><a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">Allow the kubelet to pull container images from private registries</a>.</li></ul><p>The Kubernetes control plane also uses Secrets; for example,
<a href="#bootstrap-token-secrets">bootstrap token Secrets</a> are a mechanism to
help automate node registration.</p><h3 id="use-case-dotfiles-in-a-secret-volume">Use case: dotfiles in a secret volume</h3><p>You can make your data "hidden" by defining a key that begins with a dot.
This key represents a dotfile or "hidden" file. For example, when the following Secret
is mounted into a volume, <code>secret-volume</code>, the volume will contain a single file,
called <code>.secret-file</code>, and the <code>dotfile-test-container</code> will have this file
present at the path <code>/etc/secret-volume/.secret-file</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Files beginning with dot characters are hidden from the output of <code>ls -l</code>;
you must use <code>ls -la</code> to see them when listing directory contents.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/dotfile-secret.yaml" download="secret/dotfile-secret.yaml"><code>secret/dotfile-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-dotfile-secret-yaml&quot;)" title="Copy secret/dotfile-secret.yaml to clipboard"/></div><div class="includecode" id="secret-dotfile-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dotfile-secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">.secret-file</span>:<span style="color:#bbb"> </span>dmFsdWUtMg0KDQo=<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-dotfiles-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">secretName</span>:<span style="color:#bbb"> </span>dotfile-secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>dotfile-test-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/busybox<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- ls<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:#b44">"-l"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:#b44">"/etc/secret-volume"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/etc/secret-volume"</span></span></span></code></pre></div></div></div><h3 id="use-case-secret-visible-to-one-container-in-a-pod">Use case: Secret visible to one container in a Pod</h3><p>Consider a program that needs to handle HTTP requests, do some complex business
logic, and then sign some messages with an HMAC. Because it has complex
application logic, there might be an unnoticed remote file reading exploit in
the server, which could expose the private key to an attacker.</p><p>This could be divided into two processes in two containers: a frontend container
which handles user interaction and business logic, but which cannot see the
private key; and a signer container that can see the private key, and responds
to simple signing requests from the frontend (for example, over localhost networking).</p><p>With this partitioned approach, an attacker now has to trick the application
server into doing something rather arbitrary, which may be harder than getting
it to read a file.</p><h3 id="alternatives-to-secrets">Alternatives to Secrets</h3><p>Rather than using a Secret to protect confidential data, you can pick from alternatives.</p><p>Here are some of your options:</p><ul><li>If your cloud-native component needs to authenticate to another application that you
know is running within the same Kubernetes cluster, you can use a
<a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">ServiceAccount</a>
and its tokens to identify your client.</li><li>There are third-party tools that you can run, either within or outside your cluster,
that manage sensitive data. For example, a service that Pods access over HTTPS,
that reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount
token).</li><li>For authentication, you can implement a custom signer for X.509 certificates, and use
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/">CertificateSigningRequests</a>
to let that custom signer issue certificates to Pods that need them.</li><li>You can use a <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugin</a>
to expose node-local encryption hardware to a specific Pod. For example, you can schedule
trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.</li></ul><p>You can also combine two or more of those options, including the option to use Secret objects themselves.</p><p>For example: implement (or deploy) an <a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/operator/" target="_blank" aria-label="operator">operator</a>
that fetches short-lived session tokens from an external service, and then creates Secrets based
on those short-lived session tokens. Pods running in your cluster can make use of the session tokens,
and operator ensures they are valid. This separation means that you can run Pods that are unaware of
the exact mechanisms for issuing and refreshing those session tokens.</p><h2 id="secret-types">Types of Secret</h2><p>When creating a Secret, you can specify its type using the <code>type</code> field of
the <a href="/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">Secret</a>
resource, or certain equivalent <code>kubectl</code> command line flags (if available).
The Secret type is used to facilitate programmatic handling of the Secret data.</p><p>Kubernetes provides several built-in types for some common usage scenarios.
These types vary in terms of the validations performed and the constraints
Kubernetes imposes on them.</p><table><thead><tr><th>Built-in Type</th><th>Usage</th></tr></thead><tbody><tr><td><code>Opaque</code></td><td>arbitrary user-defined data</td></tr><tr><td><code>kubernetes.io/service-account-token</code></td><td>ServiceAccount token</td></tr><tr><td><code>kubernetes.io/dockercfg</code></td><td>serialized <code>~/.dockercfg</code> file</td></tr><tr><td><code>kubernetes.io/dockerconfigjson</code></td><td>serialized <code>~/.docker/config.json</code> file</td></tr><tr><td><code>kubernetes.io/basic-auth</code></td><td>credentials for basic authentication</td></tr><tr><td><code>kubernetes.io/ssh-auth</code></td><td>credentials for SSH authentication</td></tr><tr><td><code>kubernetes.io/tls</code></td><td>data for a TLS client or server</td></tr><tr><td><code>bootstrap.kubernetes.io/token</code></td><td>bootstrap token data</td></tr></tbody></table><p>You can define and use your own Secret type by assigning a non-empty string as the
<code>type</code> value for a Secret object (an empty string is treated as an <code>Opaque</code> type).</p><p>Kubernetes doesn't impose any constraints on the type name. However, if you
are using one of the built-in types, you must meet all the requirements defined
for that type.</p><p>If you are defining a type of Secret that's for public use, follow the convention
and structure the Secret type to have your domain name before the name, separated
by a <code>/</code>. For example: <code>cloud-hosting.example.net/cloud-api-credentials</code>.</p><h3 id="opaque-secrets">Opaque Secrets</h3><p><code>Opaque</code> is the default Secret type if you don't explicitly specify a type in
a Secret manifest. When you create a Secret using <code>kubectl</code>, you must use the
<code>generic</code> subcommand to indicate an <code>Opaque</code> Secret type. For example, the
following command creates an empty Secret of type <code>Opaque</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret generic empty-secret
</span></span><span style="display:flex"><span>kubectl get secret empty-secret
</span></span></code></pre></div><p>The output looks like:</p><pre tabindex="0"><code>NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
</code></pre><p>The <code>DATA</code> column shows the number of data items stored in the Secret.
In this case, <code>0</code> means you have created an empty Secret.</p><h3 id="serviceaccount-token-secrets">ServiceAccount token Secrets</h3><p>A <code>kubernetes.io/service-account-token</code> type of Secret is used to store a
token credential that identifies a
<a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" aria-label="ServiceAccount">ServiceAccount</a>. This
is a legacy mechanism that provides long-lived ServiceAccount credentials to
Pods.</p><p>In Kubernetes v1.22 and later, the recommended approach is to obtain a
short-lived, automatically rotating ServiceAccount token by using the
<a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/"><code>TokenRequest</code></a>
API instead. You can get these short-lived tokens using the following methods:</p><ul><li>Call the <code>TokenRequest</code> API either directly or by using an API client like
<code>kubectl</code>. For example, you can use the
<a href="/docs/reference/generated/kubectl/kubectl-commands#-em-token-em-"><code>kubectl create token</code></a>
command.</li><li>Request a mounted token in a
<a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">projected volume</a>
in your Pod manifest. Kubernetes creates the token and mounts it in the Pod.
The token is automatically invalidated when the Pod that it's mounted in is
deleted. For details, see
<a href="/docs/tasks/configure-pod-container/configure-service-account/#launch-a-pod-using-service-account-token-projection">Launch a Pod using service account token projection</a>.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You should only create a ServiceAccount token Secret
if you can't use the <code>TokenRequest</code> API to obtain a token,
and the security exposure of persisting a non-expiring token credential
in a readable API object is acceptable to you. For instructions, see
<a href="/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount">Manually create a long-lived API token for a ServiceAccount</a>.</div><p>When using this Secret type, you need to ensure that the
<code>kubernetes.io/service-account.name</code> annotation is set to an existing
ServiceAccount name. If you are creating both the ServiceAccount and
the Secret objects, you should create the ServiceAccount object first.</p><p>After the Secret is created, a Kubernetes <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>
fills in some other fields such as the <code>kubernetes.io/service-account.uid</code> annotation, and the
<code>token</code> key in the <code>data</code> field, which is populated with an authentication token.</p><p>The following example configuration declares a ServiceAccount token Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/serviceaccount-token-secret.yaml" download="secret/serviceaccount-token-secret.yaml"><code>secret/serviceaccount-token-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-serviceaccount-token-secret-yaml&quot;)" title="Copy secret/serviceaccount-token-secret.yaml to clipboard"/></div><div class="includecode" id="secret-serviceaccount-token-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-sa-sample<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/service-account.name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"sa-name"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>kubernetes.io/service-account-token<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">extra</span>:<span style="color:#bbb"> </span>YmFyCg==</span></span></code></pre></div></div></div><p>After creating the Secret, wait for Kubernetes to populate the <code>token</code> key in the <code>data</code> field.</p><p>See the <a href="/docs/concepts/security/service-accounts/">ServiceAccount</a>
documentation for more information on how ServiceAccounts work.
You can also check the <code>automountServiceAccountToken</code> field and the
<code>serviceAccountName</code> field of the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#pod-v1-core"><code>Pod</code></a>
for information on referencing ServiceAccount credentials from within Pods.</p><h3 id="docker-config-secrets">Docker config Secrets</h3><p>If you are creating a Secret to store credentials for accessing a container image registry,
you must use one of the following <code>type</code> values for that Secret:</p><ul><li><code>kubernetes.io/dockercfg</code>: store a serialized <code>~/.dockercfg</code> which is the
legacy format for configuring Docker command line. The Secret
<code>data</code> field contains a <code>.dockercfg</code> key whose value is the content of a
base64 encoded <code>~/.dockercfg</code> file.</li><li><code>kubernetes.io/dockerconfigjson</code>: store a serialized JSON that follows the
same format rules as the <code>~/.docker/config.json</code> file, which is a new format
for <code>~/.dockercfg</code>. The Secret <code>data</code> field must contain a
<code>.dockerconfigjson</code> key for which the value is the content of a base64
encoded <code>~/.docker/config.json</code> file.</li></ul><p>Below is an example for a <code>kubernetes.io/dockercfg</code> type of Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/dockercfg-secret.yaml" download="secret/dockercfg-secret.yaml"><code>secret/dockercfg-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-dockercfg-secret-yaml&quot;)" title="Copy secret/dockercfg-secret.yaml to clipboard"/></div><div class="includecode" id="secret-dockercfg-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-dockercfg<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>kubernetes.io/dockercfg<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">.dockercfg</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    eyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=</span><span style="color:#bbb">    </span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you do not want to perform the base64 encoding, you can choose to use the
<code>stringData</code> field instead.</div><p>When you create Docker config Secrets using a manifest, the API
server checks whether the expected key exists in the <code>data</code> field, and
it verifies if the value provided can be parsed as a valid JSON. The API
server doesn't validate if the JSON actually is a Docker config file.</p><p>You can also use <code>kubectl</code> to create a Secret for accessing a container
registry, such as when you don't have a Docker configuration file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret docker-registry secret-tiger-docker <span>\</span>
</span></span><span style="display:flex"><span>  --docker-email<span style="color:#666">=</span>tiger@acme.example <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-username<span style="color:#666">=</span>tiger <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-password<span style="color:#666">=</span>pass1234 <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --docker-server<span style="color:#666">=</span>my-registry.example:5000
</span></span></code></pre></div><p>This command creates a Secret of type <code>kubernetes.io/dockerconfigjson</code>.</p><p>Retrieve the <code>.data.dockerconfigjson</code> field from that new Secret and decode the
data:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get secret secret-tiger-docker -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.data.*}'</span> | base64 -d
</span></span></code></pre></div><p>The output is equivalent to the following JSON document (which is also a valid
Docker configuration file):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"auths"</span>: {
</span></span><span style="display:flex"><span>    <span style="color:green;font-weight:700">"my-registry.example:5000"</span>: {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"username"</span>: <span style="color:#b44">"tiger"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"password"</span>: <span style="color:#b44">"pass1234"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"email"</span>: <span style="color:#b44">"tiger@acme.example"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"auth"</span>: <span style="color:#b44">"dGlnZXI6cGFzczEyMzQ="</span>
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  }
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>The <code>auth</code> value there is base64 encoded; it is obscured but not secret.
Anyone who can read that Secret can learn the registry access bearer token.</p><p>It is suggested to use <a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">credential providers</a> to dynamically and securely provide pull secrets on-demand.</p></div><h3 id="basic-authentication-secret">Basic authentication Secret</h3><p>The <code>kubernetes.io/basic-auth</code> type is provided for storing credentials needed
for basic authentication. When using this Secret type, the <code>data</code> field of the
Secret must contain one of the following two keys:</p><ul><li><code>username</code>: the user name for authentication</li><li><code>password</code>: the password or token for authentication</li></ul><p>Both values for the above two keys are base64 encoded strings. You can
alternatively provide the clear text content using the <code>stringData</code> field in the
Secret manifest.</p><p>The following manifest is an example of a basic authentication Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/basicauth-secret.yaml" download="secret/basicauth-secret.yaml"><code>secret/basicauth-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-basicauth-secret-yaml&quot;)" title="Copy secret/basicauth-secret.yaml to clipboard"/></div><div class="includecode" id="secret-basicauth-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-basic-auth<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>kubernetes.io/basic-auth<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">stringData</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">username</span>:<span style="color:#bbb"> </span>admin<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># required field for kubernetes.io/basic-auth</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">password</span>:<span style="color:#bbb"> </span>t0p-Secret<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># required field for kubernetes.io/basic-auth</span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><p>The basic authentication Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for basic authentication.
However, using the defined and public Secret type (<code>kubernetes.io/basic-auth</code>) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.</p><h3 id="ssh-authentication-secrets">SSH authentication Secrets</h3><p>The builtin type <code>kubernetes.io/ssh-auth</code> is provided for storing data used in
SSH authentication. When using this Secret type, you will have to specify a
<code>ssh-privatekey</code> key-value pair in the <code>data</code> (or <code>stringData</code>) field
as the SSH credential to use.</p><p>The following manifest is an example of a Secret used for SSH public/private
key authentication:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/ssh-auth-secret.yaml" download="secret/ssh-auth-secret.yaml"><code>secret/ssh-auth-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-ssh-auth-secret-yaml&quot;)" title="Copy secret/ssh-auth-secret.yaml to clipboard"/></div><div class="includecode" id="secret-ssh-auth-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-ssh-auth<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>kubernetes.io/ssh-auth<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># the data is abbreviated in this example</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ssh-privatekey</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    UG91cmluZzYlRW1vdGljb24lU2N1YmE=</span><span style="color:#bbb">    </span></span></span></code></pre></div></div></div><p>The SSH authentication Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for SSH authentication.
However, using the defined and public Secret type (<code>kubernetes.io/ssh-auth</code>) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.
The Kubernetes API verifies that the required keys are set for a Secret of this type.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>SSH private keys do not establish trusted communication between an SSH client and
host server on their own. A secondary means of establishing trust is needed to
mitigate "man in the middle" attacks, such as a <code>known_hosts</code> file added to a ConfigMap.</div><h3 id="tls-secrets">TLS Secrets</h3><p>The <code>kubernetes.io/tls</code> Secret type is for storing
a certificate and its associated key that are typically used for TLS.</p><p>One common use for TLS Secrets is to configure encryption in transit for
an <a href="/docs/concepts/services-networking/ingress/">Ingress</a>, but you can also use it
with other resources or directly in your workload.
When using this type of Secret, the <code>tls.key</code> and the <code>tls.crt</code> key must be provided
in the <code>data</code> (or <code>stringData</code>) field of the Secret configuration, although the API
server doesn't actually validate the values for each key.</p><p>As an alternative to using <code>stringData</code>, you can use the <code>data</code> field to provide
the base64 encoded certificate and private key. For details, see
<a href="#restriction-names-data">Constraints on Secret names and data</a>.</p><p>The following YAML contains an example config for a TLS Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/tls-auth-secret.yaml" download="secret/tls-auth-secret.yaml"><code>secret/tls-auth-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-tls-auth-secret-yaml&quot;)" title="Copy secret/tls-auth-secret.yaml to clipboard"/></div><div class="includecode" id="secret-tls-auth-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>secret-tls<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>kubernetes.io/tls<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># values are base64 encoded, which obscures them but does NOT provide</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># any useful level of confidentiality</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Replace the following values with your own base64-encoded certificate and key.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tls.crt</span>:<span style="color:#bbb"> </span><span style="color:#b44">"REPLACE_WITH_BASE64_CERT"</span><span style="color:#bbb"> 
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tls.key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"REPLACE_WITH_BASE64_KEY"</span></span></span></code></pre></div></div></div><p>The TLS Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for TLS authentication.
However, using the defined and public Secret type (<code>kubernetes.io/tls</code>)
helps ensure the consistency of Secret format in your project. The API server
verifies if the required keys are set for a Secret of this type.</p><p>To create a TLS Secret using <code>kubectl</code>, use the <code>tls</code> subcommand:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create secret tls my-tls-secret <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --cert<span style="color:#666">=</span>path/to/cert/file <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  --key<span style="color:#666">=</span>path/to/key/file
</span></span></code></pre></div><p>The public/private key pair must exist before hand. The public key certificate for <code>--cert</code> must be .PEM encoded
and must match the given private key for <code>--key</code>.</p><h3 id="bootstrap-token-secrets">Bootstrap token Secrets</h3><p>The <code>bootstrap.kubernetes.io/token</code> Secret type is for
tokens used during the node bootstrap process. It stores tokens used to sign
well-known ConfigMaps.</p><p>A bootstrap token Secret is usually created in the <code>kube-system</code> namespace and
named in the form <code>bootstrap-token-&lt;token-id&gt;</code> where <code>&lt;token-id&gt;</code> is a 6 character
string of the token ID.</p><p>As a Kubernetes manifest, a bootstrap token Secret might look like the
following:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/bootstrap-token-secret-base64.yaml" download="secret/bootstrap-token-secret-base64.yaml"><code>secret/bootstrap-token-secret-base64.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-bootstrap-token-secret-base64-yaml&quot;)" title="Copy secret/bootstrap-token-secret-base64.yaml to clipboard"/></div><div class="includecode" id="secret-bootstrap-token-secret-base64-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>bootstrap-token-5emitj<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>bootstrap.kubernetes.io/token<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">auth-extra-groups</span>:<span style="color:#bbb"> </span>c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">expiration</span>:<span style="color:#bbb"> </span>MjAyMC0wOS0xM1QwNDozOToxMFo=<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">token-id</span>:<span style="color:#bbb"> </span>NWVtaXRq<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">token-secret</span>:<span style="color:#bbb"> </span>a3E0Z2lodnN6emduMXAwcg==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">usage-bootstrap-authentication</span>:<span style="color:#bbb"> </span>dHJ1ZQ==<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">usage-bootstrap-signing</span>:<span style="color:#bbb"> </span>dHJ1ZQ==</span></span></code></pre></div></div></div><p>A bootstrap token Secret has the following keys specified under <code>data</code>:</p><ul><li><code>token-id</code>: A random 6 character string as the token identifier. Required.</li><li><code>token-secret</code>: A random 16 character string as the actual token Secret. Required.</li><li><code>description</code>: A human-readable string that describes what the token is
used for. Optional.</li><li><code>expiration</code>: An absolute UTC time using <a href="https://datatracker.ietf.org/doc/html/rfc3339">RFC3339</a> specifying when the token
should be expired. Optional.</li><li><code>usage-bootstrap-&lt;usage&gt;</code>: A boolean flag indicating additional usage for
the bootstrap token.</li><li><code>auth-extra-groups</code>: A comma-separated list of group names that will be
authenticated as in addition to the <code>system:bootstrappers</code> group.</li></ul><p>You can alternatively provide the values in the <code>stringData</code> field of the Secret
without base64 encoding them:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/bootstrap-token-secret-literal.yaml" download="secret/bootstrap-token-secret-literal.yaml"><code>secret/bootstrap-token-secret-literal.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-bootstrap-token-secret-literal-yaml&quot;)" title="Copy secret/bootstrap-token-secret-literal.yaml to clipboard"/></div><div class="includecode" id="secret-bootstrap-token-secret-literal-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Note how the Secret is named</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>bootstrap-token-5emitj<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># A bootstrap token Secret usually resides in the kube-system namespace</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>bootstrap.kubernetes.io/token<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">stringData</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">auth-extra-groups</span>:<span style="color:#bbb"> </span><span style="color:#b44">"system:bootstrappers:kubeadm:default-node-token"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">expiration</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2020-09-13T04:39:10Z"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># This token ID is used in the name</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">token-id</span>:<span style="color:#bbb"> </span><span style="color:#b44">"5emitj"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">token-secret</span>:<span style="color:#bbb"> </span><span style="color:#b44">"kq4gihvszzgn1p0r"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># This token can be used for authentication</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">usage-bootstrap-authentication</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># and it can be used for signing</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">usage-bootstrap-signing</span>:<span style="color:#bbb"> </span><span style="color:#b44">"true"</span></span></span></code></pre></div></div></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><h2 id="working-with-secrets">Working with Secrets</h2><h3 id="creating-a-secret">Creating a Secret</h3><p>There are several options to create a Secret:</p><ul><li><a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">Use <code>kubectl</code></a></li><li><a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">Use a configuration file</a></li><li><a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">Use the Kustomize tool</a></li></ul><h4 id="restriction-names-data">Constraints on Secret names and data</h4><p>The name of a Secret object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>You can specify the <code>data</code> and/or the <code>stringData</code> field when creating a
configuration file for a Secret. The <code>data</code> and the <code>stringData</code> fields are optional.
The values for all keys in the <code>data</code> field have to be base64-encoded strings.
If the conversion to base64 string is not desirable, you can choose to specify
the <code>stringData</code> field instead, which accepts arbitrary strings as values.</p><p>The keys of <code>data</code> and <code>stringData</code> must consist of alphanumeric characters,
<code>-</code>, <code>_</code> or <code>.</code>. All key-value pairs in the <code>stringData</code> field are internally
merged into the <code>data</code> field. If a key appears in both the <code>data</code> and the
<code>stringData</code> field, the value specified in the <code>stringData</code> field takes
precedence.</p><h4 id="restriction-data-size">Size limit</h4><p>Individual Secrets are limited to 1MiB in size. This is to discourage creation
of very large Secrets that could exhaust the API server and kubelet memory.
However, creation of many smaller Secrets could also exhaust memory. You can
use a <a href="/docs/concepts/policy/resource-quotas/">resource quota</a> to limit the
number of Secrets (or other resources) in a namespace.</p><h3 id="editing-a-secret">Editing a Secret</h3><p>You can edit an existing Secret unless it is <a href="#secret-immutable">immutable</a>. To
edit a Secret, use one of the following methods:</p><ul><li><a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/#edit-secret">Use <code>kubectl</code></a></li><li><a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/#edit-secret">Use a configuration file</a></li></ul><p>You can also edit the data in a Secret using the <a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/#edit-secret">Kustomize tool</a>. However, this
method creates a new <code>Secret</code> object with the edited data.</p><p>Depending on how you created the Secret, as well as how the Secret is used in
your Pods, updates to existing <code>Secret</code> objects are propagated automatically to
Pods that use the data. For more information, refer to <a href="#using-secrets-as-files-from-a-pod">Using Secrets as files from a Pod</a> section.</p><h3 id="using-a-secret">Using a Secret</h3><p>Secrets can be mounted as data volumes or exposed as
<a class="glossary-tooltip" title="Container environment variables are name=value pairs that provide useful information into containers running in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/container-environment/" target="_blank" aria-label="environment variables">environment variables</a>
to be used by a container in a Pod. Secrets can also be used by other parts of the
system, without being directly exposed to the Pod. For example, Secrets can hold
credentials that other parts of the system should use to interact with external
systems on your behalf.</p><p>Secret volume sources are validated to ensure that the specified object
reference actually points to an object of type Secret. Therefore, a Secret
needs to be created before any Pods that depend on it.</p><p>If the Secret cannot be fetched (perhaps because it does not exist, or
due to a temporary lack of connection to the API server) the kubelet
periodically retries running that Pod. The kubelet also reports an Event
for that Pod, including details of the problem fetching the Secret.</p><h4 id="restriction-secret-must-exist">Optional Secrets</h4><p>When you reference a Secret in a Pod, you can mark the Secret as <em>optional</em>,
such as in the following example. If an optional Secret doesn't exist,
Kubernetes ignores it.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/optional-secret.yaml" download="secret/optional-secret.yaml"><code>secret/optional-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;secret-optional-secret-yaml&quot;)" title="Copy secret/optional-secret.yaml to clipboard"/></div><div class="includecode" id="secret-optional-secret-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/etc/foo"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">secret</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">secretName</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">optional</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span></span></span></code></pre></div></div></div><p>By default, Secrets are required. None of a Pod's containers will start until
all non-optional Secrets are available.</p><p>If a Pod references a specific key in a non-optional Secret and that Secret
does exist, but is missing the named key, the Pod fails during startup.</p><h3 id="using-secrets-as-files-from-a-pod">Using Secrets as files from a Pod</h3><p>If you want to access data from a Secret in a Pod, one way to do that is to
have Kubernetes make the value of that Secret be available as a file inside
the filesystem of one or more of the Pod's containers.</p><p>For instructions, refer to
<a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-pod-that-has-access-to-the-secret-data-through-a-volume">Create a Pod that has access to the secret data through a Volume</a>.</p><p>When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks
this and updates the data in the volume, using an eventually-consistent approach.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A container using a Secret as a
<a href="/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount does not receive
automated Secret updates.</div><p>The kubelet keeps a cache of the current keys and values for the Secrets that are used in
volumes for pods on that node.
You can configure the way that the kubelet detects changes from the cached values. The
<code>configMapAndSecretChangeDetectionStrategy</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a> controls
which strategy the kubelet uses. The default strategy is <code>Watch</code>.</p><p>Updates to Secrets can be either propagated by an API watch mechanism (the default), based on
a cache with a defined time-to-live, or polled from the cluster API server on each kubelet
synchronisation loop.</p><p>As a result, the total delay from the moment when the Secret is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(following the same order listed in the previous paragraph, these are:
watch propagation delay, the configured cache TTL, or zero for direct polling).</p><h3 id="using-secrets-as-environment-variables">Using Secrets as environment variables</h3><p>To use a Secret in an <a class="glossary-tooltip" title="Container environment variables are name=value pairs that provide useful information into containers running in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/container-environment/" target="_blank" aria-label="environment variable">environment variable</a>
in a Pod:</p><ol><li>For each container in your Pod specification, add an environment variable
for each Secret key that you want to use to the
<code>env[].valueFrom.secretKeyRef</code> field.</li><li>Modify your image and/or command line so that the program looks for values
in the specified environment variables.</li></ol><p>For instructions, refer to
<a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data">Define container environment variables using Secret data</a>.</p><p>It's important to note that the range of characters allowed for environment variable
names in pods is <a href="/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config">restricted</a>.
If any keys do not meet the rules, those keys are not made available to your container, though
the Pod is allowed to start.</p><h3 id="using-imagepullsecrets">Container image pull Secrets</h3><p>If you want to fetch container images from a private repository, you need a way for
the kubelet on each node to authenticate to that repository. You can configure
<em>image pull Secrets</em> to make this possible. These Secrets are configured at the Pod
level.</p><h4 id="using-imagepullsecrets-1">Using imagePullSecrets</h4><p>The <code>imagePullSecrets</code> field is a list of references to Secrets in the same namespace.
You can use an <code>imagePullSecrets</code> to pass a Secret that contains a Docker (or other) image registry
password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See the <a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec API</a>
for more information about the <code>imagePullSecrets</code> field.</p><h5 id="manually-specifying-an-imagepullsecret">Manually specifying an imagePullSecret</h5><p>You can learn how to specify <code>imagePullSecrets</code> from the
<a href="/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">container images</a>
documentation.</p><h5 id="arranging-for-imagepullsecrets-to-be-automatically-attached">Arranging for imagePullSecrets to be automatically attached</h5><p>You can manually create <code>imagePullSecrets</code>, and reference these from a ServiceAccount. Any Pods
created with that ServiceAccount or created with that ServiceAccount by default, will get their
<code>imagePullSecrets</code> field set to that of the service account.
See <a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a service account</a>
for a detailed explanation of that process.</p><h3 id="restriction-static-pod">Using Secrets with static Pods</h3><p>You cannot use ConfigMaps or Secrets with <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static Pods">static Pods</a>.</p><h2 id="secret-immutable">Immutable Secrets</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes lets you mark specific Secrets (and ConfigMaps) as <em>immutable</em>.
Preventing changes to the data of an existing Secret has the following benefits:</p><ul><li>protects you from accidental (or unwanted) updates that could cause applications outages</li><li>(for clusters that extensively use Secrets - at least tens of thousands of unique Secret
to Pod mounts), switching to immutable Secrets improves the performance of your cluster
by significantly reducing load on kube-apiserver. The kubelet does not need to maintain
a [watch] on any Secrets that are marked as immutable.</li></ul><h3 id="secret-immutable-create">Marking a Secret as immutable</h3><p>You can create an immutable Secret by setting the <code>immutable</code> field to <code>true</code>. For example,</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">immutable</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>You can also update any existing mutable Secret to make it immutable.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Once a Secret or ConfigMap is marked as immutable, it is <em>not</em> possible to revert this change
nor to mutate the contents of the <code>data</code> field. You can only delete and recreate the Secret.
Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate
these pods.</div><h2 id="information-security-for-secrets">Information security for Secrets</h2><p>Although ConfigMap and Secret work similarly, Kubernetes applies some additional
protection for Secret objects.</p><p>Secrets often hold values that span a spectrum of importance, many of which can
cause escalations within Kubernetes (e.g. service account tokens) and to
external systems. Even if an individual app can reason about the power of the
Secrets it expects to interact with, other apps within the same namespace can
render those assumptions invalid.</p><p>A Secret is only sent to a node if a Pod on that node requires it.
For mounting Secrets into Pods, the kubelet stores a copy of the data into a <code>tmpfs</code>
so that the confidential data is not written to durable storage.
Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copy
of the confidential data from the Secret.</p><p>There may be several containers in a Pod. By default, containers you define
only have access to the default ServiceAccount and its related Secret.
You must explicitly define environment variables or map a volume into a
container in order to provide access to any other Secret.</p><p>There may be Secrets for several Pods on the same node. However, only the
Secrets that a Pod requests are potentially visible within its containers.
Therefore, one Pod does not have access to the Secrets of another Pod.</p><h3 id="configure-least-privilege-access-to-secrets">Configure least-privilege access to Secrets</h3><p>To enhance the security measures around Secrets, use separate namespaces to isolate access to mounted secrets.</p><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Any containers that run with <code>privileged: true</code> on a node can access all
Secrets used on that node.</div><h2 id="what-s-next">What's next</h2><ul><li>For guidelines to manage and improve the security of your Secrets, refer to
<a href="/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a>.</li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">manage Secrets using <code>kubectl</code></a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">manage Secrets using config file</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">manage Secrets using kustomize</a></li><li>Read the <a href="/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">API reference</a> for <code>Secret</code></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Resource Management for Pods and Containers</h1><p>When you specify a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>, you can optionally specify how much of each resource a
<a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/" target="_blank" aria-label="container">container</a> needs. The most common resources to specify are CPU and memory
(RAM); there are others.</p><p>When you specify the resource <em>request</em> for containers in a Pod, the
<a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a> uses this information to decide which node to place the Pod on.
When you specify a resource <em>limit</em> for a container, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> enforces those
limits so that the running container is not allowed to use more of that resource
than the limit you set. The kubelet also reserves at least the <em>request</em> amount of
that system resource specifically for that container to use.</p><h2 id="requests-and-limits">Requests and limits</h2><p>If the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than its <code>request</code> for that resource specifies.</p><p>For example, if you set a <code>memory</code> request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.</p><p>Limits are a different story. Both <code>cpu</code> and <code>memory</code> limits are applied by the kubelet (and
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>),
and are ultimately enforced by the kernel. On Linux nodes, the Linux kernel
enforces limits with
<a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank" aria-label="cgroups">cgroups</a>.
The behavior of <code>cpu</code> and <code>memory</code> limit enforcement is slightly different.</p><p><code>cpu</code> limits are enforced by CPU throttling. When a container approaches
its <code>cpu</code> limit, the kernel will restrict access to the CPU corresponding to the
container's limit. Thus, a <code>cpu</code> limit is a hard limit the kernel enforces.
Containers may not use more CPU than is specified in their <code>cpu</code> limit.</p><p><code>memory</code> limits are enforced by the kernel with out of memory (OOM) kills. When
a container uses more than its <code>memory</code> limit, the kernel may terminate it. However,
terminations only happen when the kernel detects memory pressure. Thus, a
container that over allocates memory may not be immediately killed. This means
<code>memory</code> limits are enforced reactively. A container may use more memory than
its <code>memory</code> limit, but if it does, it may get killed.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There is an alpha feature <code>MemoryQoS</code> which attempts to add more preemptive
limit enforcement for memory (as opposed to reactive enforcement by the OOM
killer). However, this effort is
<a href="https://github.com/kubernetes/enhancements/tree/a47155b340/keps/sig-node/2570-memory-qos#latest-update-stalled">stalled</a>
due to a potential livelock situation a memory hungry can cause.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you specify a limit for a resource, but do not specify any request, and no admission-time
mechanism has applied a default request for that resource, then Kubernetes copies the limit
you specified and uses it as the requested value for the resource.</div><h2 id="resource-types">Resource types</h2><p><em>CPU</em> and <em>memory</em> are each a <em>resource type</em>. A resource type has a base unit.
CPU represents compute processing and is specified in units of <a href="#meaning-of-cpu">Kubernetes CPUs</a>.
Memory is specified in units of bytes.
For Linux workloads, you can specify <em>huge page</em> resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.</p><p>For example, on a system where the default page size is 4KiB, you could specify a limit,
<code>hugepages-2Mi: 80Mi</code>. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You cannot overcommit <code>hugepages-*</code> resources.
This is different from the <code>memory</code> and <code>cpu</code> resources.</div><p>CPU and memory are collectively referred to as <em>compute resources</em>, or <em>resources</em>. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
<a href="/docs/concepts/overview/kubernetes-api/">API resources</a>. API resources, such as Pods and
<a href="/docs/concepts/services-networking/service/">Services</a> are objects that can be read and modified
through the Kubernetes API server.</p><h2 id="resource-requests-and-limits-of-pod-and-container">Resource requests and limits of Pod and container</h2><p>For each container, you can specify resource limits and requests,
including the following:</p><ul><li><code>spec.containers[].resources.limits.cpu</code></li><li><code>spec.containers[].resources.limits.memory</code></li><li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code></li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt;</code></li></ul><p>Although you can only specify requests and limits for individual containers,
it is also useful to think about the overall resource requests and limits for
a Pod.
For a particular resource, a <em>Pod resource request/limit</em> is the sum of the
resource requests/limits of that type for each container in the Pod.</p><h2 id="pod-level-resource-specification">Pod-level resource specification</h2><div class="feature-state-notice feature-beta" title="Feature Gate: PodLevelResources"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>Provided your cluster has the <code>PodLevelResources</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> enabled,
you can specify resource requests and limits at
the Pod level. At the Pod level, Kubernetes 1.34
only supports resource requests or limits for specific resource types: <code>cpu</code> and /
or <code>memory</code> and / or <code>hugepages</code>. With this feature, Kubernetes allows you to declare an overall resource
budget for the Pod, which is especially helpful when dealing with a large number of
containers where it can be difficult to accurately gauge individual resource needs.
Additionally, it enables containers within a Pod to share idle resources with each
other, improving resource utilization.</p><p>For a Pod, you can specify resource limits and requests for CPU and memory by including the following:</p><ul><li><code>spec.resources.limits.cpu</code></li><li><code>spec.resources.limits.memory</code></li><li><code>spec.resources.limits.hugepages-&lt;size&gt;</code></li><li><code>spec.resources.requests.cpu</code></li><li><code>spec.resources.requests.memory</code></li><li><code>spec.resources.requests.hugepages-&lt;size&gt;</code></li></ul><h2 id="resource-units-in-kubernetes">Resource units in Kubernetes</h2><h3 id="meaning-of-cpu">CPU resource units</h3><p>Limits and requests for CPU resources are measured in <em>cpu</em> units.
In Kubernetes, 1 CPU unit is equivalent to <strong>1 physical CPU core</strong>,
or <strong>1 virtual core</strong>, depending on whether the node is a physical host
or a virtual machine running inside a physical machine.</p><p>Fractional requests are allowed. When you define a container with
<code>spec.containers[].resources.requests.cpu</code> set to <code>0.5</code>, you are requesting half
as much CPU time compared to if you asked for <code>1.0</code> CPU.
For CPU resource units, the <a href="/docs/reference/kubernetes-api/common-definitions/quantity/">quantity</a> expression <code>0.1</code> is equivalent to the
expression <code>100m</code>, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing.</p><p>CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,
<code>500m</code> CPU represents the roughly same amount of computing power whether that container
runs on a single-core, dual-core, or 48-core machine.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>Kubernetes doesn't allow you to specify CPU resources with a precision finer than
<code>1m</code> or <code>0.001</code> CPU. To avoid accidentally using an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form
instead of the decimal form when using less than 1 CPU unit.</p><p>For example, you have a Pod that uses <code>5m</code> or <code>0.005</code> CPU and would like to decrease
its CPU resources. By using the decimal form, it's harder to spot that <code>0.0005</code> CPU
is an invalid value, while by using the milliCPU form, it's easier to spot that
<code>0.5m</code> is an invalid value.</p></div><h3 id="meaning-of-memory">Memory resource units</h3><p>Limits and requests for <code>memory</code> are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these
<a href="/docs/reference/kubernetes-api/common-definitions/quantity/">quantity</a> suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>128974848, 129e6, 129M,  128974848000m, 123Mi
</span></span></code></pre></div><p>Pay attention to the case of the suffixes. If you request <code>400m</code> of memory, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>)
or 400 megabytes (<code>400M</code>).</p><h2 id="example-1">Container resources example</h2><p>The following Pod has two containers. Both containers are defined with a request for
0.25 CPU
and 64MiB (2<sup>26</sup> bytes) of memory. Each container has a limit of 0.5
CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128
MiB of memory, and a limit of 1 CPU and 256MiB of memory.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>images.my-company.example/app:v4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"64Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"250m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"128Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"500m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>log-aggregator<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>images.my-company.example/log-aggregator:v6<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"64Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"250m"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"128Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"500m"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="example-2">Pod resources example</h2><div class="feature-state-notice feature-beta" title="Feature Gate: PodLevelResources"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>This feature can be enabled by setting the <code>PodLevelResources</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.
The following Pod has an explicit request of 1 CPU and 100 MiB of memory, and an
explicit limit of 1 CPU and 200 MiB of memory. The <code>pod-resources-demo-ctr-1</code>
container has explicit requests and limits set. However, the
<code>pod-resources-demo-ctr-2</code> container will simply share the resources available
within the Pod resource boundaries, as it does not have explicit requests and limits
set.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/pod-level-resources.yaml" download="pods/resource/pod-level-resources.yaml"><code>pods/resource/pod-level-resources.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-resource-pod-level-resources-yaml&quot;)" title="Copy pods/resource/pod-level-resources.yaml to clipboard"/></div><div class="includecode" id="pods-resource-pod-level-resources-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-resources-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>pod-resources-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"200Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-resources-demo-ctr-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0.5"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">"0.5"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">"50Mi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod-resources-demo-ctr-2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>fedora<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- sleep<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- inf <span style="color:#bbb">
</span></span></span></code></pre></div></div></div><h2 id="how-pods-with-resource-requests-are-scheduled">How Pods with resource requests are scheduled</h2><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
containers is less than the capacity of the node.
Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.</p><h2 id="how-pods-with-resource-limits-are-run">How Kubernetes applies resource requests and limits</h2><p>When the kubelet starts a container as part of a Pod, the kubelet passes that container's
requests and limits for memory and CPU to the container runtime.</p><p>On Linux, the container runtime typically configures
kernel <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank" aria-label="cgroups">cgroups</a> that apply and enforce the
limits you defined.</p><ul><li>The CPU limit defines a hard ceiling on how much CPU time the container can use.
During each scheduling interval (time slice), the Linux kernel checks to see if this
limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.</li><li>The CPU request typically defines a weighting. If several different containers (cgroups)
want to run on a contended system, workloads with larger CPU requests are allocated more
CPU time than workloads with small requests.</li><li>The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses
cgroups v2, the container runtime might use the memory request as a hint to set
<code>memory.min</code> and <code>memory.low</code>.</li><li>The memory limit defines a memory limit for that cgroup. If the container tries to
allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates
and, typically, intervenes by stopping one of the processes in the container that tried
to allocate memory. If that process is the container's PID 1, and the container is marked
as restartable, Kubernetes restarts the container.</li><li>The memory limit for the Pod or container can also apply to pages in memory backed
volumes, such as an <code>emptyDir</code>. The kubelet tracks <code>tmpfs</code> emptyDir volumes as container
memory use, rather than as local ephemeral storage.ã€€When using memory backed <code>emptyDir</code>,
be sure to check the notes <a href="#memory-backed-emptydir">below</a>.</li></ul><p>If a container exceeds its memory request and the node that it runs on becomes short of
memory overall, it is likely that the Pod the container belongs to will be
<a class="glossary-tooltip" title="Process of terminating one or more Pods on Nodes" data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/" target="_blank" aria-label="evicted">evicted</a>.</p><p>A container might or might not be allowed to exceed its CPU limit for extended periods of time.
However, container runtimes don't terminate Pods or containers for excessive CPU usage.</p><p>To determine whether a container cannot be scheduled or is being killed due to resource limits,
see the <a href="#troubleshooting">Troubleshooting</a> section.</p><h3 id="monitoring-compute-memory-resource-usage">Monitoring compute &amp; memory resource usage</h3><p>The kubelet reports the resource usage of a Pod as part of the Pod
<a href="/docs/concepts/overview/working-with-objects/#object-spec-and-status"><code>status</code></a>.</p><p>If optional <a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">tools for monitoring</a>
are available in your cluster, then Pod resource usage can be retrieved either
from the <a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api">Metrics API</a>
directly or from your monitoring tools.</p><h3 id="memory-backed-emptydir">Considerations for memory backed <code>emptyDir</code> volumes</h3><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>If you do not specify a <code>sizeLimit</code> for an <code>emptyDir</code> volume, that volume may
consume up to that pod's memory limit (<code>Pod.spec.containers[].resources.limits.memory</code>).
If you do not set a memory limit, the pod has no upper bound on memory consumption,
and can consume all available memory on the node. Kubernetes schedules pods based
on resource requests (<code>Pod.spec.containers[].resources.requests</code>) and will not
consider memory usage above the request when deciding if another pod can fit on
a given node. This can result in a denial of service and cause the OS to do
out-of-memory (OOM) handling. It is possible to create any number of <code>emptyDir</code>s
that could potentially consume all available memory on the node, making OOM
more likely.</div><p>From the perspective of memory management, there are some similarities between
when a process uses memory as a work area and when using memory-backed
<code>emptyDir</code>. But when using memory as a volume, like memory-backed <code>emptyDir</code>,
there are additional points below that you should be careful of:</p><ul><li>Files stored on a memory-backed volume are almost entirely managed by the
user application. Unlike when used as a work area for a process, you can not
rely on things like language-level garbage collection.</li><li>The purpose of writing files to a volume is to save data or pass it between
applications. Neither Kubernetes nor the OS may automatically delete files
from a volume, so memory used by those files can not be reclaimed when the
system or the pod are under memory pressure.</li><li>A memory-backed <code>emptyDir</code> is useful because of its performance, but memory
is generally much smaller in size and much higher in cost than other storage
media, such as disks or SSDs. Using large amounts of memory for <code>emptyDir</code>
volumes may affect the normal operation of your pod or of the whole node,
so should be used carefully.</li></ul><p>If you are administering a cluster or namespace, you can also set
<a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a> that limits memory use;
you may also want to define a <a href="/docs/concepts/policy/limit-range/">LimitRange</a>
for additional enforcement.
If you specify a <code>spec.containers[].resources.limits.memory</code> for each Pod,
then the maximum size of an <code>emptyDir</code> volume will be the pod's memory limit.</p><p>As an alternative, a cluster administrator can enforce size limits for
<code>emptyDir</code> volumes in new Pods using a policy mechanism such as
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidationAdmissionPolicy</a>.</p><h2 id="local-ephemeral-storage">Local ephemeral storage</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.</p><p>Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a>
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volumes">volumes</a> into containers.</p><p>The kubelet also uses this kind of storage to hold
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>,
container images, and the writable layers of running containers.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>If a node fails, the data in its ephemeral storage can be lost.
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>To make the resource quota work on ephemeral-storage, two things need to be done:</p><ul><li>An admin sets the resource quota for ephemeral-storage in a namespace.</li><li>A user needs to specify limits for the ephemeral-storage resource in the Pod spec.</li></ul><p>If the user doesn't specify the ephemeral-storage resource limit in the Pod spec,
the resource quota is not enforced on ephemeral-storage.</p></div><p>Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.</p><h3 id="configurations-for-local-ephemeral-storage">Configurations for local ephemeral storage</h3><p>Kubernetes supports two ways to configure local ephemeral storage on a node:<ul class="nav nav-tabs" id="local-storage-configurations" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#local-storage-configurations-0" role="tab" aria-controls="local-storage-configurations-0" aria-selected="true">Single filesystem</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#local-storage-configurations-1" role="tab" aria-controls="local-storage-configurations-1">Two filesystems</a></li></ul><div class="tab-content" id="local-storage-configurations"><div id="local-storage-configurations-0" class="tab-pane show active" role="tabpanel" aria-labelledby="local-storage-configurations-0"><p><p>In this configuration, you place all different kinds of ephemeral local data
(<code>emptyDir</code> volumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.</p><p>The kubelet also writes
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>
and treats these similarly to ephemeral local storage.</p><p>The kubelet writes logs to files inside its configured log directory (<code>/var/log</code>
by default); and has a base directory for other locally stored data
(<code>/var/lib/kubelet</code> by default).</p><p>Typically, both <code>/var/lib/kubelet</code> and <code>/var/log</code> are on the system root filesystem,
and the kubelet is designed with that layout in mind.</p><p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p></p></div><div id="local-storage-configurations-1" class="tab-pane" role="tabpanel" aria-labelledby="local-storage-configurations-1"><p><p>You have a filesystem on the node that you're using for ephemeral data that
comes from running Pods: logs, and <code>emptyDir</code> volumes. You can use this filesystem
for other data (for example: system logs not related to Kubernetes); it can even
be the root filesystem.</p><p>The kubelet also writes
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>
into the first filesystem, and treats these similarly to ephemeral local storage.</p><p>You also use a separate filesystem, backed by a different logical storage device.
In this configuration, the directory where you tell the kubelet to place
container image layers and writeable layers is on this second filesystem.</p><p>The first filesystem does not hold any image layers or writeable layers.</p><p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p></p></div></div></p><p>The kubelet can measure how much local storage it is using. It does this provided
that you have set up the node using one of the supported configurations for local
ephemeral storage.</p><p>If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather
than as local ephemeral storage.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to <code>/var/lib/kubelet</code> or <code>/var/lib/containers</code> will not report ephemeral storage correctly.</div><h3 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage</h3><p>You can specify <code>ephemeral-storage</code> for managing local ephemeral storage. Each
container of a Pod can specify either or both of the following:</p><ul><li><code>spec.containers[].resources.limits.ephemeral-storage</code></li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><p>Limits and requests for <code>ephemeral-storage</code> are measured in byte quantities.
You can express storage as a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following quantities all represent roughly the same value:</p><ul><li><code>128974848</code></li><li><code>129e6</code></li><li><code>129M</code></li><li><code>123Mi</code></li></ul><p>Pay attention to the case of the suffixes. If you request <code>400m</code> of ephemeral-storage, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>)
or 400 megabytes (<code>400M</code>).</p><p>In the following example, the Pod has two containers. Each container has a request of
2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral
storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and
a limit of 8GiB of local ephemeral storage. 500Mi of that limit could be
consumed by the <code>emptyDir</code> volume.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>images.my-company.example/app:v4<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">"4Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/tmp"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>log-aggregator<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>images.my-company.example/log-aggregator:v6<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">"4Gi"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/tmp"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">sizeLimit</span>:<span style="color:#bbb"> </span>500Mi<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="how-pods-with-ephemeral-storage-requests-are-scheduled">How Pods with ephemeral-storage requests are scheduled</h3><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.
For more information, see
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Node Allocatable</a>.</p><p>The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.</p><h3 id="resource-emphemeralstorage-consumption">Ephemeral storage consumption management</h3><p>If the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:</p><ul><li><code>emptyDir</code> volumes, except <em>tmpfs</em> <code>emptyDir</code> volumes</li><li>directories holding node-level logs</li><li>writeable container layers</li></ul><p>If a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.</p><p>For container-level isolation, if a container's writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.</p><p>For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod's <code>emptyDir</code>
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4><p>If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.</p><p>However, if the filesystem space for writeable container layers, node-level logs,
or <code>emptyDir</code> volumes falls low, the node
<a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="taints">taints</a> itself as short on local storage
and this taint triggers eviction for any Pods that don't specifically tolerate the taint.</p><p>See the supported <a href="#configurations-for-local-ephemeral-storage">configurations</a>
for ephemeral local storage.</p></div><p>The kubelet supports different ways to measure Pod storage use:</p><ul class="nav nav-tabs" id="resource-emphemeralstorage-measurement" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#resource-emphemeralstorage-measurement-0" role="tab" aria-controls="resource-emphemeralstorage-measurement-0" aria-selected="true">Periodic scanning</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#resource-emphemeralstorage-measurement-1" role="tab" aria-controls="resource-emphemeralstorage-measurement-1">Filesystem project quota</a></li></ul><div class="tab-content" id="resource-emphemeralstorage-measurement"><div id="resource-emphemeralstorage-measurement-0" class="tab-pane show active" role="tabpanel" aria-labelledby="resource-emphemeralstorage-measurement-0"><p><p>The kubelet performs regular, scheduled checks that scan each
<code>emptyDir</code> volume, container log directory, and writeable container layer.</p><p>The scan measures how much space is used.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>In this mode, the kubelet does not track open file descriptors
for deleted files.</p><p>If you (or a container) create a file inside an <code>emptyDir</code> volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.</p></div></p></div><div id="resource-emphemeralstorage-measurement-1" class="tab-pane" role="tabpanel" aria-labelledby="resource-emphemeralstorage-measurement-1"><p><div class="feature-state-notice feature-beta" title="Feature Gate: LocalStorageCapacityIsolationFSQuotaMonitoring"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [beta]</code> (enabled by default: false)</div><p>Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing the <code>emptyDir</code> volumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Project quotas let you monitor storage use; they do not enforce limits.</div><p>Kubernetes uses project IDs starting from <code>1048576</code>. The IDs in use are
registered in <code>/etc/projects</code> and <code>/etc/projid</code>. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in <code>/etc/projects</code> and <code>/etc/projid</code> so that
Kubernetes does not use them.</p><p>Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.</p><p>To use quotas to track a pod's resource usage, the pod must be in
a user namespace. Within user namespaces, the kernel restricts changes
to projectIDs on the filesystem, ensuring the reliability of storage
metrics calculated by quotas.</p><p>If you want to use project quotas, you should:</p><ul><li><p>Enable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
using the <code>featureGates</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>.</p></li><li><p>Ensure the <code>UserNamespacesSupport</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled, and that the kernel, CRI implementation and OCI runtime support user namespaces.</p></li><li><p>Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># For ext4, with /dev/block-device not mounted</span>
</span></span><span style="display:flex"><span>sudo tune2fs -O project -Q prjquota /dev/block-device
</span></span></code></pre></div></li><li><p>Ensure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is named <code>prjquota</code>.</p></li></ul><p>If you don't want to use project quotas, you should:</p><ul><li>Disable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
using the <code>featureGates</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>.</li></ul></p></div></div><h2 id="extended-resources">Extended resources</h2><p>Extended resources are fully-qualified resource names outside the
<code>kubernetes.io</code> domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.</p><p>There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.</p><h3 id="managing-extended-resources">Managing extended resources</h3><h4 id="node-level-extended-resources">Node-level extended resources</h4><p>Node-level extended resources are tied to nodes.</p><h5 id="device-plugin-managed-resources">Device plugin managed resources</h5><p>See <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device
Plugin</a>
for how to advertise device plugin managed resources on each node.</p><h5 id="other-resources">Other resources</h5><p>To advertise a new node-level extended resource, the cluster operator can
submit a <code>PATCH</code> HTTP request to the API server to specify the available
quantity in the <code>status.capacity</code> for a node in the cluster. After this
operation, the node's <code>status.capacity</code> will include a new resource. The
<code>status.allocatable</code> field is updated automatically with the new resource
asynchronously by the kubelet.</p><p>Because the scheduler uses the node's <code>status.allocatable</code> value when
evaluating Pod fitness, the scheduler only takes account of the new value after
that asynchronous update. There may be a short delay between patching the
node capacity with a new resource and the time when the first Pod that requests
the resource can be scheduled on that node.</p><p><strong>Example:</strong></p><p>Here is an example showing how to use <code>curl</code> to form an HTTP request that
advertises five "example.com/foo" resources on node <code>k8s-node-1</code> whose master
is <code>k8s-master</code>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl --header <span style="color:#b44">"Content-Type: application/json-patch+json"</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>--request PATCH <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>--data <span style="color:#b44">'[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In the preceding request, <code>~1</code> is the encoding for the character <code>/</code>
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901, section 3</a>.</div><h4 id="cluster-level-extended-resources">Cluster-level extended resources</h4><p>Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.</p><p>You can specify the extended resources that are handled by scheduler extenders
in <a href="/docs/reference/config-api/kube-scheduler-config.v1/">scheduler configuration</a></p><p><strong>Example:</strong></p><p>The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.</p><ul><li>The scheduler sends a Pod to the scheduler extender only if the Pod requests
"example.com/foo".</li><li>The <code>ignoredByScheduler</code> field specifies that the scheduler does not check
the "example.com/foo" resource in its <code>PodFitsResources</code> predicate.</li></ul><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="display:flex"><span>{
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"kind"</span>: <span style="color:#b44">"Policy"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"apiVersion"</span>: <span style="color:#b44">"v1"</span>,
</span></span><span style="display:flex"><span>  <span style="color:green;font-weight:700">"extenders"</span>: [
</span></span><span style="display:flex"><span>    {
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"urlPrefix"</span>:<span style="color:#b44">"&lt;extender-endpoint&gt;"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"bindVerb"</span>: <span style="color:#b44">"bind"</span>,
</span></span><span style="display:flex"><span>      <span style="color:green;font-weight:700">"managedResources"</span>: [
</span></span><span style="display:flex"><span>        {
</span></span><span style="display:flex"><span>          <span style="color:green;font-weight:700">"name"</span>: <span style="color:#b44">"example.com/foo"</span>,
</span></span><span style="display:flex"><span>          <span style="color:green;font-weight:700">"ignoredByScheduler"</span>: <span style="color:#a2f;font-weight:700">true</span>
</span></span><span style="display:flex"><span>        }
</span></span><span style="display:flex"><span>      ]
</span></span><span style="display:flex"><span>    }
</span></span><span style="display:flex"><span>  ]
</span></span><span style="display:flex"><span>}
</span></span></code></pre></div><h4 id="extended-resources-allocation-by-dra">Extended resources allocation by DRA</h4><p>Extended resources allocation by DRA allows cluster administrators to specify an <code>extendedResourceName</code>
in DeviceClass, then the devices matching the DeviceClass can be requested from a pod's extended
resource requests. Read more about
<a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a>.</p><h3 id="consuming-extended-resources">Consuming extended resources</h3><p>Users can consume extended resources in Pod specs like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.</p><p>The API server restricts quantities of extended resources to whole numbers.
Examples of <em>valid</em> quantities are <code>3</code>, <code>3000m</code> and <code>3Ki</code>. Examples of
<em>invalid</em> quantities are <code>0.5</code> and <code>1500m</code> (because <code>1500m</code> would result in <code>1.5</code>).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than <code>kubernetes.io</code> which is reserved.</div><p>To consume an extended resource in a Pod, include the resource name as a key
in the <code>spec.containers[].resources.limits</code> map in the container spec.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.</div><p>A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the <code>PENDING</code> state
as long as the resource request cannot be satisfied.</p><p><strong>Example:</strong></p><p>The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>myimage<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/foo</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">example.com/foo</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="pid-limiting">PID limiting</h2><p>Process ID (PID) limits allow for the configuration of a kubelet
to limit the number of PIDs that a given Pod can consume. See
<a href="/docs/concepts/policy/pid-limiting/">PID Limiting</a> for information.</p><h2 id="troubleshooting">Troubleshooting</h2><h3 id="my-pods-are-pending-with-event-message-failedscheduling">My Pods are pending with event message <code>FailedScheduling</code></h3><p>If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An
<a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">Event</a> is produced
each time the scheduler fails to find a place for the Pod. You can use <code>kubectl</code>
to view the events for a Pod; for example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe pod frontend | grep -A <span style="color:#666">9999999999</span> Events
</span></span></code></pre></div><pre tabindex="0"><code>Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
</code></pre><p>In the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on any node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:</p><ul><li>Add more nodes to the cluster.</li><li>Terminate unneeded Pods to make room for pending Pods.</li><li>Check that the Pod is not larger than all the nodes. For example, if all the
nodes have a capacity of <code>cpu: 1</code>, then a Pod with a request of <code>cpu: 1.1</code> will
never be scheduled.</li><li>Check for node taints. If most of your nodes are tainted, and the new Pod does
not tolerate that taint, the scheduler only considers placements onto the
remaining nodes that don't have that taint.</li></ul><p>You can check node capacities and amounts allocated with the
<code>kubectl describe nodes</code> command. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe nodes e2e-test-node-pool-4lw4
</span></span></code></pre></div><pre tabindex="0"><code>Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre><p>In the preceding output, you can see that if a Pod requests more than 1.120 CPUs
or more than 6.23Gi of memory, that Pod will not fit on the node.</p><p>By looking at the â€œPodsâ€ section, you can see which Pods are taking up space on
the node.</p><p>The amount of resources available to Pods is less than the node capacity because
system daemons use a portion of the available resources. Within the Kubernetes API,
each Node has a <code>.status.allocatable</code> field
(see <a href="/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus">NodeStatus</a>
for details).</p><p>The <code>.status.allocatable</code> field describes the amount of resources that are available
to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).
For more information on node allocatable resources in Kubernetes, see
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a>.</p><p>You can configure <a href="/docs/concepts/policy/resource-quotas/">resource quotas</a>
to limit the total amount of resources that a namespace can consume.
Kubernetes enforces quotas for objects in particular namespace when there is a
ResourceQuota in that namespace.
For example, if you assign specific namespaces to different teams, you
can add ResourceQuotas into those namespaces. Setting resource quotas helps to
prevent one team from using so much of any resource that this over-use affects other teams.</p><p>You should also consider what access you grant to that namespace:
<strong>full</strong> write access to a namespace allows someone with that access to remove any
resource, including a configured ResourceQuota.</p><h3 id="my-container-is-terminated">My container is terminated</h3><p>Your container might get terminated because it is resource-starved. To check
whether a container is being killed because it is hitting a resource limit, call
<code>kubectl describe pod</code> on the Pod of interest:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe pod simmemleak-hra99
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image "saadali/simmemleak:latest" already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
</code></pre><p>In the preceding example, the <code>Restart Count: 5</code> indicates that the <code>simmemleak</code>
container in the Pod was terminated and restarted five times (so far).
The <code>OOMKilled</code> reason shows that the container tried to use more memory than its limit.</p><p>Your next step might be to check the application code for a memory leak. If you
find that the application is behaving how you expect, consider setting a higher
memory limit (and possibly request) for that container.</p><h2 id="what-s-next">What's next</h2><ul><li>Get hands-on experience <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">assigning Memory resources to containers and Pods</a>.</li><li>Get hands-on experience <a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">assigning CPU resources to containers and Pods</a>.</li><li>Read how the API reference defines a <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">container</a>
and its <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources">resource requirements</a></li><li>Read about <a href="https://www.linux.org/docs/man8/xfs_quota.html">project quotas</a> in XFS</li><li>Read more about the <a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler configuration reference (v1)</a></li><li>Read more about <a href="/docs/concepts/workloads/pods/pod-qos/">Quality of Service classes for Pods</a></li><li>Read more about <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Cluster Networking</h1><p>Networking is a central part of Kubernetes, but it can be challenging to
understand exactly how it is expected to work. There are 4 distinct networking
problems to address:</p><ol><li>Highly-coupled container-to-container communications: this is solved by
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> and <code>localhost</code> communications.</li><li>Pod-to-Pod communications: this is the primary focus of this document.</li><li>Pod-to-Service communications: this is covered by <a href="/docs/concepts/services-networking/service/">Services</a>.</li><li>External-to-Service communications: this is also covered by Services.</li></ol><p>Kubernetes is all about sharing machines among applications. Typically,
sharing machines requires ensuring that two applications do not try to use the
same ports. Coordinating ports across multiple developers is very difficult to
do at scale and exposes users to cluster-level issues outside of their control.</p><p>Dynamic port allocation brings a lot of complications to the system - every
application has to take ports as flags, the API servers have to know how to
insert dynamic port numbers into configuration blocks, services have to know
how to find each other, etc. Rather than deal with this, Kubernetes takes a
different approach.</p><p>To learn about the Kubernetes networking model, see <a href="/docs/concepts/services-networking/">here</a>.</p><h2 id="kubernetes-ip-address-ranges">Kubernetes IP address ranges</h2><p>Kubernetes clusters require to allocate non-overlapping IP addresses for Pods, Services and Nodes,
from a range of available addresses configured in the following components:</p><ul><li>The network plugin is configured to assign IP addresses to Pods.</li><li>The kube-apiserver is configured to assign IP addresses to Services.</li><li>The kubelet or the cloud-controller-manager is configured to assign IP addresses to Nodes.</li></ul><figure class="diagram-medium"><img src="/docs/images/kubernetes-cluster-network.svg" alt="A figure illustrating the different network ranges in a kubernetes cluster"/></figure><h2 id="cluster-network-ipfamilies">Cluster networking types</h2><p>Kubernetes clusters, attending to the IP families configured, can be categorized into:</p><ul><li>IPv4 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv4 addresses.</li><li>IPv6 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv6 addresses.</li><li>IPv4/IPv6 or IPv6/IPv4 <a href="/docs/concepts/services-networking/dual-stack/">dual-stack</a>:<ul><li>The network plugin is configured to assign IPv4 and IPv6 addresses.</li><li>The kube-apiserver is configured to assign IPv4 and IPv6 addresses.</li><li>The kubelet or cloud-controller-manager is configured to assign IPv4 and IPv6 address.</li><li>All components must agree on the configured primary IP family.</li></ul></li></ul><p>Kubernetes clusters only consider the IP families present on the Pods, Services and Nodes objects,
independently of the existing IPs of the represented objects. Per example, a server or a pod can have multiple
IP addresses on its interfaces, but only the IP addresses in <code>node.status.addresses</code> or <code>pod.status.ips</code> are
considered for implementing the Kubernetes network model and defining the type of the cluster.</p><h2 id="how-to-implement-the-kubernetes-network-model">How to implement the Kubernetes network model</h2><p>The network model is implemented by the container runtime on each node. The most common container
runtimes use <a href="https://github.com/containernetworking/cni">Container Network Interface</a> (CNI)
plugins to manage their network and security capabilities. Many different CNI plugins exist from
many different vendors. Some of these provide only basic features of adding and removing network
interfaces, while others provide more sophisticated solutions, such as integration with other
container orchestration systems, running multiple CNI plugins, advanced IPAM features etc.</p><p>See <a href="/docs/concepts/cluster-administration/addons/#networking-and-network-policy">this page</a>
for a non-exhaustive list of networking addons supported by Kubernetes.</p><h2 id="what-s-next">What's next</h2><p>The early design of the networking model and its rationale are described in more detail in the
<a href="https://git.k8s.io/design-proposals-archive/network/networking.md">networking design document</a>.
For future plans and some on-going efforts that aim to improve Kubernetes networking, please
refer to the SIG-Network
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network">KEPs</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">ConfigMaps</h1><p><p>A ConfigMap is an API object used to store non-confidential data in key-value pairs.
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volume">volume</a>.</p></p><p>A ConfigMap allows you to decouple environment-specific configuration from your <a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-image" target="_blank" aria-label="container images">container images</a>, so that your applications are easily portable.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>ConfigMap does not provide secrecy or encryption.
If the data you want to store are confidential, use a
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a> rather than a ConfigMap,
or use additional (third party) tools to keep your data private.</div><h2 id="motivation">Motivation</h2><p>Use a ConfigMap for setting configuration data separately from application code.</p><p>For example, imagine that you are developing an application that you can run on your
own computer (for development) and in the cloud (to handle real traffic).
You write the code to look in an environment variable named <code>DATABASE_HOST</code>.
Locally, you set that variable to <code>localhost</code>. In the cloud, you set it to
refer to a Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>
that exposes the database component to your cluster.
This lets you fetch a container image running in the cloud and
debug the exact same code locally if needed.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A ConfigMap is not designed to hold large chunks of data. The data stored in a
ConfigMap cannot exceed 1 MiB. If you need to store settings that are
larger than this limit, you may want to consider mounting a volume or use a
separate database or file service.</div><h2 id="configmap-object">ConfigMap object</h2><p>A ConfigMap is an <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank" aria-label="API object">API object</a>
that lets you store configuration for other objects to use. Unlike most
Kubernetes objects that have a <code>spec</code>, a ConfigMap has <code>data</code> and <code>binaryData</code>
fields. These fields accept key-value pairs as their values. Both the <code>data</code>
field and the <code>binaryData</code> are optional. The <code>data</code> field is designed to
contain UTF-8 strings while the <code>binaryData</code> field is designed to
contain binary data as base64-encoded strings.</p><p>The name of a ConfigMap must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>Each key under the <code>data</code> or the <code>binaryData</code> field must consist of
alphanumeric characters, <code>-</code>, <code>_</code> or <code>.</code>. The keys stored in <code>data</code> must not
overlap with the keys in the <code>binaryData</code> field.</p><p>Starting from v1.19, you can add an <code>immutable</code> field to a ConfigMap
definition to create an <a href="#configmap-immutable">immutable ConfigMap</a>.</p><h2 id="configmaps-and-pods">ConfigMaps and Pods</h2><p>You can write a Pod <code>spec</code> that refers to a ConfigMap and configures the container(s)
in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in
the same <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>spec</code> of a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/static-pod/" target="_blank" aria-label="static Pod">static Pod</a> cannot refer to a ConfigMap
or any other API objects.</div><p>Here's an example ConfigMap that has some keys with single values,
and other keys where the value looks like a fragment of a configuration
format.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>game-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># property-like keys; each key maps to a simple value</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">player_initial_lives</span>:<span style="color:#bbb"> </span><span style="color:#b44">"3"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ui_properties_file_name</span>:<span style="color:#bbb"> </span><span style="color:#b44">"user-interface.properties"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># file-like keys</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">game.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    enemy.types=aliens,monsters
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    player.maximum-lives=5</span><span style="color:#bbb">    
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">user-interface.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    color.good=purple
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    color.bad=yellow
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">    allow.textmode=true</span><span style="color:#bbb">    
</span></span></span></code></pre></div><p>There are four different ways that you can use a ConfigMap to configure
a container inside a Pod:</p><ol><li>Inside a container command and args</li><li>Environment variables for a container</li><li>Add a file in read-only volume, for the application to read</li><li>Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap</li></ol><p>These different methods lend themselves to different ways of modeling
the data being consumed.
For the first three methods, the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> uses the data from
the ConfigMap when it launches container(s) for a Pod.</p><p>The fourth method means you have to write code to read the ConfigMap and its data.
However, because you're using the Kubernetes API directly, your application can
subscribe to get updates whenever the ConfigMap changes, and react
when that happens. By accessing the Kubernetes API directly, this
technique also lets you access a ConfigMap in a different namespace.</p><p>Here's an example Pod that uses values from <code>game-demo</code> to configure a Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/configure-pod.yaml" download="configmap/configure-pod.yaml"><code>configmap/configure-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;configmap-configure-pod-yaml&quot;)" title="Copy configmap/configure-pod.yaml to clipboard"/></div><div class="includecode" id="configmap-configure-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>configmap-demo-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>alpine<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"sleep"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"3600"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># Define the environment variable</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>PLAYER_INITIAL_LIVES<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Notice that the case is different here</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                                     </span><span style="color:#080;font-style:italic"># from the key name in the ConfigMap.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">valueFrom</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">configMapKeyRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>game-demo          <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># The ConfigMap this value comes from.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>player_initial_lives<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># The key to fetch.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>UI_PROPERTIES_FILE_NAME<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">valueFrom</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">configMapKeyRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>game-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>ui_properties_file_name<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/config"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># You set volumes at the Pod level, then mount them into containers inside that Pod</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">configMap</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Provide the name of the ConfigMap you want to mount.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>game-demo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># An array of keys from the ConfigMap to create as files</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">items</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"game.properties"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"game.properties"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"user-interface.properties"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">"user-interface.properties"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span></span></span></code></pre></div></div></div><p>A ConfigMap doesn't differentiate between single line property values and
multi-line file-like values.
What matters is how Pods and other objects consume those values.</p><p>For this example, defining a volume and mounting it inside the <code>demo</code>
container as <code>/config</code> creates two files,
<code>/config/game.properties</code> and <code>/config/user-interface.properties</code>,
even though there are four keys in the ConfigMap. This is because the Pod
definition specifies an <code>items</code> array in the <code>volumes</code> section.
If you omit the <code>items</code> array entirely, every key in the ConfigMap becomes
a file with the same name as the key, and you get 4 files.</p><h2 id="using-configmaps">Using ConfigMaps</h2><p>ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other
parts of the system, without being directly exposed to the Pod. For example,
ConfigMaps can hold data that other parts of the system should use for configuration.</p><p>The most common way to use ConfigMaps is to configure settings for
containers running in a Pod in the same namespace. You can also use a
ConfigMap separately.</p><p>For example, you
might encounter <a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/cluster-administration/addons/" target="_blank" aria-label="addons">addons</a>
or <a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/operator/" target="_blank" aria-label="operators">operators</a> that
adjust their behavior based on a ConfigMap.</p><h3 id="using-configmaps-as-files-from-a-pod">Using ConfigMaps as files from a Pod</h3><p>To consume a ConfigMap in a volume in a Pod:</p><ol><li>Create a ConfigMap or use an existing one. Multiple Pods can reference the
same ConfigMap.</li><li>Modify your Pod definition to add a volume under <code>.spec.volumes[]</code>. Name
the volume anything, and have a <code>.spec.volumes[].configMap.name</code> field set
to reference your ConfigMap object.</li><li>Add a <code>.spec.containers[].volumeMounts[]</code> to each container that needs the
ConfigMap. Specify <code>.spec.containers[].volumeMounts[].readOnly = true</code> and
<code>.spec.containers[].volumeMounts[].mountPath</code> to an unused directory name
where you would like the ConfigMap to appear.</li><li>Modify your image or command line so that the program looks for files in
that directory. Each key in the ConfigMap <code>data</code> map becomes the filename
under <code>mountPath</code>.</li></ol><p>This is an example of a Pod that mounts a ConfigMap in a volume:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">"/etc/foo"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">configMap</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span></span></span></code></pre></div><p>Each ConfigMap you want to use needs to be referred to in <code>.spec.volumes</code>.</p><p>If there are multiple containers in the Pod, then each container needs its
own <code>volumeMounts</code> block, but only one <code>.spec.volumes</code> is needed per ConfigMap.</p><h4 id="mounted-configmaps-are-updated-automatically">Mounted ConfigMaps are updated automatically</h4><p>When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.
The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.
However, the kubelet uses its local cache for getting the current value of the ConfigMap.
The type of the cache is configurable using the <code>configMapAndSecretChangeDetectionStrategy</code> field in
the <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration struct</a>.
A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting
all requests directly to the API server.
As a result, the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(it equals to watch propagation delay, ttl of cache, or zero correspondingly).</p><p>ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A container using a ConfigMap as a <a href="/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive ConfigMap updates.</div><h3 id="using-configmaps-as-environment-variables">Using Configmaps as environment variables</h3><p>To use a Configmap in an <a class="glossary-tooltip" title="Container environment variables are name=value pairs that provide useful information into containers running in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/container-environment/" target="_blank" aria-label="environment variable">environment variable</a>
in a Pod:</p><ol><li>For each container in your Pod specification, add an environment variable
for each Configmap key that you want to use to the
<code>env[].valueFrom.configMapKeyRef</code> field.</li><li>Modify your image and/or command line so that the program looks for values
in the specified environment variables.</li></ol><p>This is an example of defining a ConfigMap as a pod environment variable:</p><p>The following ConfigMap (myconfigmap.yaml) stores two properties: username and access_level:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">username</span>:<span style="color:#bbb"> </span>k8s-admin<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">access_level</span>:<span style="color:#bbb"> </span><span style="color:#b44">"1"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The following command will create the ConfigMap object:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f myconfigmap.yaml
</span></span></code></pre></div><p>The following Pod consumes the content of the ConfigMap as environment variables:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/env-configmap.yaml" download="configmap/env-configmap.yaml"><code>configmap/env-configmap.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;configmap-env-configmap-yaml&quot;)" title="Copy configmap/env-configmap.yaml to clipboard"/></div><div class="includecode" id="configmap-env-configmap-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>env-configmap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"/bin/sh"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-c"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"printenv"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">envFrom</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">configMapRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>The <code>envFrom</code> field instructs Kubernetes to create environment variables from the sources nested within it.
The inner <code>configMapRef</code> refers to a ConfigMap by its name and selects all its key-value pairs.
Add the Pod to your cluster, then retrieve its logs to see the output from the printenv command.
This should confirm that the two key-value pairs from the ConfigMap have been set as environment variables:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f env-configmap.yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs pod/ env-configmap
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-console" data-lang="console"><span style="display:flex"><span><span style="color:#888">...
</span></span></span><span style="display:flex"><span><span style="color:#888">username: "k8s-admin"
</span></span></span><span style="display:flex"><span><span style="color:#888">access_level: "1"
</span></span></span><span style="display:flex"><span><span style="color:#888">...
</span></span></span></code></pre></div><p>Sometimes a Pod won't require access to all the values in a ConfigMap.
For example, you could have another Pod which only uses the username value from the ConfigMap.
For this use case, you can use the <code>env.valueFrom</code> syntax instead, which lets you select individual keys in
a ConfigMap. The name of the environment variable can also be different from the key within the ConfigMap.
For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>env-configmap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>envars-test-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">env</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>CONFIGMAP_USERNAME<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">valueFrom</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">configMapKeyRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span></span></span></code></pre></div><p>In the Pod created from this manifest, you will see that the environment variable
<code>CONFIGMAP_USERNAME</code> is set to the value of the <code>username</code> value from the ConfigMap.
Other keys from the ConfigMap data are not copied into the environment.</p><p>It's important to note that the range of characters allowed for environment
variable names in pods is <a href="/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config">restricted</a>.
If any keys do not meet the rules, those keys are not made available to your container, though
the Pod is allowed to start.</p><h2 id="configmap-immutable">Immutable ConfigMaps</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>The Kubernetes feature <em>Immutable Secrets and ConfigMaps</em> provides an option to set
individual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps
(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their
data has the following advantages:</p><ul><li>protects you from accidental (or unwanted) updates that could cause applications outages</li><li>improves performance of your cluster by significantly reducing load on kube-apiserver, by
closing watches for ConfigMaps marked as immutable.</li></ul><p>You can create an immutable ConfigMap by setting the <code>immutable</code> field to <code>true</code>.
For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">data</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">immutable</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Once a ConfigMap is marked as immutable, it is <em>not</em> possible to revert this change
nor to mutate the contents of the <code>data</code> or the <code>binaryData</code> field. You can
only delete and recreate the ConfigMap. Because existing Pods maintain a mount point
to the deleted ConfigMap, it is recommended to recreate these pods.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/configuration/secret/">Secrets</a>.</li><li>Read <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap</a>.</li><li>Read about <a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">changing a ConfigMap (or any other Kubernetes object)</a></li><li>Read <a href="https://12factor.net/">The Twelve-Factor App</a> to understand the motivation for
separating code from configuration.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Container Lifecycle Hooks</h1><p>This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.</p><h2 id="overview">Overview</h2><p>Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.</p><h2 id="container-hooks">Container hooks</h2><p>There are two hooks that are exposed to Containers:</p><p><code>PostStart</code></p><p>This hook is executed immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.</p><p><code>PreStop</code></p><p>This hook is called immediately before a container is terminated due to an API request or management
event such as a liveness/startup probe failure, preemption, resource contention and others. A call
to the <code>PreStop</code> hook fails if the container is already in a terminated or completed state and the
hook must complete before the TERM signal to stop the container can be sent. The Pod's termination
grace period countdown begins before the <code>PreStop</code> hook is executed, so regardless of the outcome of
the handler, the container will eventually terminate within the Pod's termination grace period. No
parameters are passed to the handler.</p><p>A more detailed description of the termination behavior can be found in
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">Termination of Pods</a>.</p><p><code>StopSignal</code></p><p>The StopSignal lifecycle can be used to define a stop signal which would be sent to the container when it is
stopped. If you set this, it overrides any <code>STOPSIGNAL</code> instruction defined within the container image.</p><p>A more detailed description of termination behaviour with custom stop signals can be found in
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-stop-signals">Stop Signals</a>.</p><h3 id="hook-handler-implementations">Hook handler implementations</h3><p>Containers can access a hook by implementing and registering a handler for that hook.
There are three types of hook handlers that can be implemented for Containers:</p><ul><li>Exec - Executes a specific command, such as <code>pre-stop.sh</code>, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.</li><li>HTTP - Executes an HTTP request against a specific endpoint on the Container.</li><li>Sleep - Pauses the container for a specified duration.</li></ul><h3 id="hook-handler-execution">Hook handler execution</h3><p>When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler according to the hook action,
<code>httpGet</code>, <code>tcpSocket</code> (<a href="/docs/reference/generated/kubernetes-api/v1.31/#lifecyclehandler-v1-core">deprecated</a>)
and <code>sleep</code> are executed by the kubelet process, and <code>exec</code> is executed in the container.</p><p>The <code>PostStart</code> hook handler call is initiated when a container is created,
meaning the container ENTRYPOINT and the <code>PostStart</code> hook are triggered simultaneously.
However, if the <code>PostStart</code> hook takes too long to execute or if it hangs,
it can prevent the container from transitioning to a <code>running</code> state.</p><p><code>PreStop</code> hooks are not executed asynchronously from the signal to stop the Container; the hook must
complete its execution before the TERM signal can be sent. If a <code>PreStop</code> hook hangs during
execution, the Pod's phase will be <code>Terminating</code> and remain there until the Pod is killed after its
<code>terminationGracePeriodSeconds</code> expires. This grace period applies to the total time it takes for
both the <code>PreStop</code> hook to execute and for the Container to stop normally. If, for example,
<code>terminationGracePeriodSeconds</code> is 60, and the hook takes 55 seconds to complete, and the Container
takes 10 seconds to stop normally after receiving the signal, then the Container will be killed
before it can stop normally, since <code>terminationGracePeriodSeconds</code> is less than the total time
(55+10) it takes for these two things to happen.</p><p>If either a <code>PostStart</code> or <code>PreStop</code> hook fails,
it kills the Container.</p><p>Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.</p><h3 id="hook-delivery-guarantees">Hook delivery guarantees</h3><p>Hook delivery is intended to be <em>at least once</em>,
which means that a hook may be called multiple times for any given event,
such as for <code>PostStart</code> or <code>PreStop</code>.
It is up to the hook implementation to handle this correctly.</p><p>Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.</p><h3 id="debugging-hook-handlers">Debugging Hook handlers</h3><p>The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For <code>PostStart</code>, this is the <code>FailedPostStartHook</code> event,
and for <code>PreStop</code>, this is the <code>FailedPreStopHook</code> event.
To generate a failed <code>FailedPostStartHook</code> event yourself, modify the
<a href="https://k8s.io/examples/pods/lifecycle-events.yaml">lifecycle-events.yaml</a>
file to change the postStart command to "badcommand" and apply it.
Here is some example output of the resulting events you see from running <code>kubectl describe pod lifecycle-demo</code>:</p><pre tabindex="0"><code>Events:
  Type     Reason               Age              From               Message
  ----     ------               ----             ----               -------
  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...
  Normal   Pulled               6s               kubelet            Successfully pulled image "nginx" in 229.604315ms
  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image "nginx"
  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container
  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container
  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container "lifecycle-demo-container" in Pod "lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)" failed - error: command 'badcommand' exited with 126: , message: "OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"badcommand\": executable file not found in $PATH: unknown\r\n"
  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook
  Normal   Pulled               4s               kubelet            Successfully pulled image "nginx" in 215.66395ms
  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the <a href="/docs/concepts/containers/container-environment/">Container environment</a>.</li><li>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to Container lifecycle events</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Containers</h1><div class="lead">Technology for packaging an application along with its runtime dependencies.</div><p>This page will discuss containers and container images, as well as their use in operations and solution development.</p><p>The word <em>container</em> is an overloaded term. Whenever you use the word, check whether your audience uses the same definition.</p><p>Each container that you run is repeatable; the standardization from having
dependencies included means that you get the same behavior wherever you
run it.</p><p>Containers decouple applications from the underlying host infrastructure.
This makes deployment easier in different cloud or OS environments.</p><p>Each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a> in a Kubernetes
cluster runs the containers that form the
<a href="/docs/concepts/workloads/pods/">Pods</a> assigned to that node.
Containers in a Pod are co-located and co-scheduled to run on the same node.</p><h2 id="container-images">Container images</h2><p>A <a href="/docs/concepts/containers/images/">container image</a> is a ready-to-run
software package containing everything needed to run an application:
the code and any runtime it requires, application and system libraries,
and default values for any essential settings.</p><p>Containers are intended to be stateless and
<a href="https://glossary.cncf.io/immutable-infrastructure/">immutable</a>:
you should not change
the code of a container that is already running. If you have a containerized
application and want to make changes, the correct process is to build a new
image that includes the change, then recreate the container to start from the
updated image.</p><h2 id="container-runtimes">Container runtimes</h2><p>A fundamental component that empowers Kubernetes to run containers effectively.
It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.</p><p>Kubernetes supports container runtimes such as
<a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" data-toggle="tooltip" data-placement="top" href="https://containerd.io/docs/" target="_blank" aria-label="containerd">containerd</a>, <a class="glossary-tooltip" title="A lightweight container runtime specifically for Kubernetes" data-toggle="tooltip" data-placement="top" href="https://cri-o.io/#what-is-cri-o" target="_blank" aria-label="CRI-O">CRI-O</a>,
and any other implementation of the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Kubernetes CRI (Container Runtime
Interface)</a>.</p><p>Usually, you can allow your cluster to pick the default container runtime
for a Pod. If you need to use more than one container runtime in your cluster,
you can specify the <a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a>
for a Pod to make sure that Kubernetes runs those containers using a
particular container runtime.</p><p>You can also use RuntimeClass to run different Pods with the same container
runtime but with different settings.</p><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/concepts/containers/container-environment/">Container Environment</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/containers/container-lifecycle-hooks/">Container Lifecycle Hooks</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/containers/cri/">Container Runtime Interface (CRI)</a></h5><p/></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Metrics for Kubernetes Object States</h1><div class="lead">kube-state-metrics, an add-on agent to generate and expose cluster-level metrics.</div><p>The state of Kubernetes objects in the Kubernetes API can be exposed as metrics.
An add-on agent called <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a> can connect to the Kubernetes API server and expose a HTTP endpoint with metrics generated from the state of individual objects in the cluster.
It exposes various information about the state of objects like labels and annotations, startup and termination times, status or the phase the object currently is in.
For example, containers running in pods create a <code>kube_pod_container_info</code> metric.
This includes the name of the container, the name of the pod it is part of, the <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a> the pod is running in, the name of the container image, the ID of the image, the image name from the spec of the container, the ID of the running container and the ID of the pod as labels.</p><div class="alert alert-secondary callout third-party-content" role="alert">ðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>An external component that is able and capable to scrape the endpoint of kube-state-metrics (for example via Prometheus) can now be used to enable the following use cases.</p><h2 id="example-kube-state-metrics-query-1">Example: using metrics from kube-state-metrics to query the cluster state</h2><p>Metric series generated by kube-state-metrics are helpful to gather further insights into the cluster, as they can be used for querying.</p><p>If you use Prometheus or another tool that uses the same query language, the following PromQL query returns the number of pods that are not ready:</p><pre tabindex="0"><code>count(kube_pod_status_ready{condition="false"}) by (namespace, pod)
</code></pre><h2 id="example-kube-state-metrics-alert-1">Example: alerting based on from kube-state-metrics</h2><p>Metrics generated from kube-state-metrics also allow for alerting on issues in the cluster.</p><p>If you use Prometheus or a similar tool that uses the same alert rule language, the following alert will fire if there are pods that have been in a <code>Terminating</code> state for more than 5 minutes:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">groups</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>Pod state<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">alert</span>:<span style="color:#bbb"> </span>PodsBlockedInTerminatingState<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">expr</span>:<span style="color:#bbb"> </span>count(kube_pod_deletion_timestamp) by (namespace, pod) * count(kube_pod_status_reason{reason="NodeLost"} == 0) by (namespace, pod) &gt; 0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">for</span>:<span style="color:#bbb"> </span>5m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">severity</span>:<span style="color:#bbb"> </span>page<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">annotations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">summary</span>:<span style="color:#bbb"> </span>Pod {{$labels.namespace}}/{{$labels.pod}} blocked in Terminating state.<span style="color:#bbb">
</span></span></span></code></pre></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Node Shutdowns</h1><p>In a Kubernetes cluster, a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a>
can be shut down in a planned graceful way or unexpectedly because of reasons such
as a power outage or something else external. A node shutdown could lead to workload
failure if the node is not drained before the shutdown. A node shutdown can be
either <strong>graceful</strong> or <strong>non-graceful</strong>.</p><h2 id="graceful-node-shutdown">Graceful node shutdown</h2><p>The kubelet attempts to detect node system shutdown and terminates pods running on the node.</p><p>Kubelet ensures that pods follow the normal
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">pod termination process</a>
during the node shutdown. During node shutdown, the kubelet does not accept new
Pods (even if those Pods are already bound to the node).</p><h3 id="enabling-graceful-node-shutdown">Enabling graceful node shutdown</h3><ul class="nav nav-tabs" id="graceful-shutdown-os" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#graceful-shutdown-os-0" role="tab" aria-controls="graceful-shutdown-os-0" aria-selected="true">Linux</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#graceful-shutdown-os-1" role="tab" aria-controls="graceful-shutdown-os-1">Windows</a></li></ul><div class="tab-content" id="graceful-shutdown-os"><div id="graceful-shutdown-os-0" class="tab-pane show active" role="tabpanel" aria-labelledby="graceful-shutdown-os-0"><p><div class="feature-state-notice feature-beta" title="Feature Gate: GracefulNodeShutdown"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [beta]</code> (enabled by default: true)</div><p>On Linux, the graceful node shutdown feature is controlled with the <code>GracefulNodeShutdown</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> which is
enabled by default in 1.21.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The graceful node shutdown feature depends on systemd since it takes advantage of
<a href="https://www.freedesktop.org/wiki/Software/systemd/inhibit/">systemd inhibitor locks</a> to
delay the node shutdown with a given duration.</div></p></div><div id="graceful-shutdown-os-1" class="tab-pane" role="tabpanel" aria-labelledby="graceful-shutdown-os-1"><p><div class="feature-state-notice feature-beta" title="Feature Gate: WindowsGracefulNodeShutdown"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>On Windows, the graceful node shutdown feature is controlled with the <code>WindowsGracefulNodeShutdown</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
which is introduced in 1.32 as an alpha feature. In Kubernetes 1.34 the feature is Beta
and is enabled by default.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The Windows graceful node shutdown feature depends on kubelet running as a Windows service,
it will then have a registered <a href="https://learn.microsoft.com/en-us/windows/win32/services/service-control-handler-function">service control handler</a>
to delay the preshutdown event with a given duration.</div><p>Windows graceful node shutdown can not be cancelled.</p><p>If kubelet is not running as a Windows service, it will not be able to set and monitor
the <a href="https://learn.microsoft.com/en-us/windows/win32/api/winsvc/ns-winsvc-service_preshutdown_info">Preshutdown</a> event,
the node will have to go through the <a href="#non-graceful-node-shutdown">Non-Graceful Node Shutdown</a> procedure mentioned above.</p><p>In the case where the Windows graceful node shutdown feature is enabled, but the kubelet is not
running as a Windows service, the kubelet will continue running instead of failing. However,
it will log an error indicating that it needs to be run as a Windows service.</p></p></div></div><h3 id="configuring-graceful-node-shutdown">Configuring graceful node shutdown</h3><p>Note that by default, both configuration options described below,
<code>shutdownGracePeriod</code> and <code>shutdownGracePeriodCriticalPods</code>, are set to zero,
thus not activating the graceful node shutdown functionality.
To activate the feature, both options should be configured appropriately and
set to non-zero values.</p><p>Once the kubelet is notified of a node shutdown, it sets a <code>NotReady</code> condition on
the Node, with the <code>reason</code> set to <code>"node is shutting down"</code>. The kube-scheduler honors this condition
and does not schedule any Pods onto the affected node; other third-party schedulers are
expected to follow the same logic. This means that new Pods won't be scheduled onto that node
and therefore none will start.</p><p>The kubelet <strong>also</strong> rejects Pods during the <code>PodAdmission</code> phase if an ongoing
node shutdown has been detected, so that even Pods with a
<a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="toleration">toleration</a> for
<code>node.kubernetes.io/not-ready:NoSchedule</code> do not start there.</p><p>When kubelet is setting that condition on its Node via the API,
the kubelet also begins terminating any Pods that are running locally.</p><p>During a graceful shutdown, kubelet terminates pods in two phases:</p><ol><li>Terminate regular pods running on the node.</li><li>Terminate <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>
running on the node.</li></ol><p>The graceful node shutdown feature is configured with two
<a href="/docs/tasks/administer-cluster/kubelet-config-file/"><code>KubeletConfiguration</code></a> options:</p><ul><li><p><code>shutdownGracePeriod</code>:</p><p>Specifies the total duration that the node should delay the shutdown by. This is the total
grace period for pod termination for both regular and
<a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>.</p></li><li><p><code>shutdownGracePeriodCriticalPods</code>:</p><p>Specifies the duration used to terminate
<a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>
during a node shutdown. This value should be less than <code>shutdownGracePeriod</code>.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There are cases when Node termination was cancelled by the system (or perhaps manually
by an administrator). In either of those situations the Node will return to the <code>Ready</code> state.
However, Pods which already started the process of termination will not be restored by kubelet
and will need to be re-scheduled.</div><p>For example, if <code>shutdownGracePeriod=30s</code>, and
<code>shutdownGracePeriodCriticalPods=10s</code>, kubelet will delay the node shutdown by
30 seconds. During the shutdown, the first 20 (30-10) seconds would be reserved
for gracefully terminating normal pods, and the last 10 seconds would be
reserved for terminating <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>When pods were evicted during the graceful node shutdown, they are marked as shutdown.
Running <code>kubectl get pods</code> shows the status of the evicted pods as <code>Terminated</code>.
And <code>kubectl describe pod</code> indicates that the pod was evicted because of node shutdown:</p><pre tabindex="0"><code>Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
</code></pre></div><h3 id="pod-priority-graceful-node-shutdown">Pod Priority based graceful node shutdown</h3><div class="feature-state-notice feature-beta" title="Feature Gate: GracefulNodeShutdownBasedOnPodPriority"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [beta]</code> (enabled by default: true)</div><p>To provide more flexibility during graceful node shutdown around the ordering
of pods during shutdown, graceful node shutdown honors the PriorityClass for
Pods, provided that you enabled this feature in your cluster. The feature
allows cluster administrators to explicitly define the ordering of pods
during graceful node shutdown based on
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">priority classes</a>.</p><p>The <a href="#graceful-node-shutdown">Graceful Node Shutdown</a> feature, as described
above, shuts down pods in two phases, non-critical pods, followed by critical
pods. If additional flexibility is needed to explicitly define the ordering of
pods during shutdown in a more granular way, pod priority based graceful
shutdown can be used.</p><p>When graceful node shutdown honors pod priorities, this makes it possible to do
graceful node shutdown in multiple phases, each phase shutting down a
particular priority class of pods. The kubelet can be configured with the exact
phases and shutdown time per phase.</p><p>Assuming the following custom pod
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">priority classes</a>
in a cluster,</p><table><thead><tr><th>Pod priority class name</th><th>Pod priority class value</th></tr></thead><tbody><tr><td><code>custom-class-a</code></td><td>100000</td></tr><tr><td><code>custom-class-b</code></td><td>10000</td></tr><tr><td><code>custom-class-c</code></td><td>1000</td></tr><tr><td><code>regular/unset</code></td><td>0</td></tr></tbody></table><p>Within the <a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>
the settings for <code>shutdownGracePeriodByPodPriority</code> could look like:</p><table><thead><tr><th>Pod priority class value</th><th>Shutdown period</th></tr></thead><tbody><tr><td>100000</td><td>10 seconds</td></tr><tr><td>10000</td><td>180 seconds</td></tr><tr><td>1000</td><td>120 seconds</td></tr><tr><td>0</td><td>60 seconds</td></tr></tbody></table><p>The corresponding kubelet config YAML configuration would be:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">shutdownGracePeriodByPodPriority</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">100000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">10000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">180</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">120</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The above table implies that any pod with <code>priority</code> value &gt;= 100000 will get
just 10 seconds to shut down, any pod with value &gt;= 10000 and &lt; 100000 will get 180
seconds to shut down, any pod with value &gt;= 1000 and &lt; 10000 will get 120 seconds to shut down.
Finally, all other pods will get 60 seconds to shut down.</p><p>One doesn't have to specify values corresponding to all of the classes. For
example, you could instead use these settings:</p><table><thead><tr><th>Pod priority class value</th><th>Shutdown period</th></tr></thead><tbody><tr><td>100000</td><td>300 seconds</td></tr><tr><td>1000</td><td>120 seconds</td></tr><tr><td>0</td><td>60 seconds</td></tr></tbody></table><p>In the above case, the pods with <code>custom-class-b</code> will go into the same bucket
as <code>custom-class-c</code> for shutdown.</p><p>If there are no pods in a particular range, then the kubelet does not wait
for pods in that priority range. Instead, the kubelet immediately skips to the
next priority class value range.</p><p>If this feature is enabled and no configuration is provided, then no ordering
action will be taken.</p><p>Using this feature requires enabling the <code>GracefulNodeShutdownBasedOnPodPriority</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>,
and setting <code>ShutdownGracePeriodByPodPriority</code> in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet config</a>
to the desired configuration containing the pod priority class values and
their respective shutdown periods.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The ability to take Pod priority into account during graceful node shutdown was introduced
as an Alpha feature in Kubernetes v1.23. In Kubernetes 1.34
the feature is Beta and is enabled by default.</div><p>Metrics <code>graceful_shutdown_start_time_seconds</code> and <code>graceful_shutdown_end_time_seconds</code>
are emitted under the kubelet subsystem to monitor node shutdowns.</p><h2 id="non-graceful-node-shutdown">Non-graceful node shutdown handling</h2><div class="feature-state-notice feature-stable" title="Feature Gate: NodeOutOfServiceVolumeDetach"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code> (enabled by default: true)</div><p>A node shutdown action may not be detected by kubelet's Node Shutdown Manager,
either because the command does not trigger the inhibitor locks mechanism used by
kubelet or because of a user error, i.e., the ShutdownGracePeriod and
ShutdownGracePeriodCriticalPods are not configured properly. Please refer to above
section <a href="#graceful-node-shutdown">Graceful Node Shutdown</a> for more details.</p><p>When a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods
that are part of a <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a>
will be stuck in terminating status on the shutdown node and cannot move to a new running node.
This is because kubelet on the shutdown node is not available to delete the pods so
the StatefulSet cannot create a new pod with the same name. If there are volumes used by the pods,
the VolumeAttachments will not be deleted from the original shutdown node so the volumes
used by these pods cannot be attached to a new running node. As a result, the
application running on the StatefulSet cannot function properly. If the original
shutdown node comes up, the pods will be deleted by kubelet and new pods will be
created on a different running node. If the original shutdown node does not come up,
these pods will be stuck in terminating status on the shutdown node forever.</p><p>To mitigate the above situation, a user can manually add the taint <code>node.kubernetes.io/out-of-service</code>
with either <code>NoExecute</code> or <code>NoSchedule</code> effect to a Node marking it out-of-service.
If a Node is marked out-of-service with this taint, the pods on the node will be forcefully deleted
if there are no matching tolerations on it and volume detach operations for the pods terminating on
the node will happen immediately. This allows the Pods on the out-of-service node to recover quickly
on a different node.</p><p>During a non-graceful shutdown, Pods are terminated in the two phases:</p><ol><li>Force delete the Pods that do not have matching <code>out-of-service</code> tolerations.</li><li>Immediately perform detach volume operation for such pods.</li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li>Before adding the taint <code>node.kubernetes.io/out-of-service</code>, it should be verified
that the node is already in shutdown or power off state (not in the middle of restarting).</li><li>The user is required to manually remove the out-of-service taint after the pods are
moved to a new node and the user has checked that the shutdown node has been
recovered since the user was the one who originally added the taint.</li></ul></div><h3 id="storage-force-detach-on-timeout">Forced storage detach on timeout</h3><p>In any situation where a pod deletion has not succeeded for 6 minutes, kubernetes will
force detach volumes being unmounted if the node is unhealthy at that instant. Any
workload still running on the node that uses a force-detached volume will cause a
violation of the
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerunpublishvolume">CSI specification</a>,
which states that <code>ControllerUnpublishVolume</code> "<strong>must</strong> be called after all
<code>NodeUnstageVolume</code> and <code>NodeUnpublishVolume</code> on the volume are called and succeed".
In such circumstances, volumes on the node in question might encounter data corruption.</p><p>The forced storage detach behaviour is optional; users might opt to use the "Non-graceful
node shutdown" feature instead.</p><p>Force storage detach on timeout can be disabled by setting the <code>disable-force-detach-on-timeout</code>
config field in <code>kube-controller-manager</code>. Disabling the force detach on timeout feature means
that a volume that is hosted on a node that is unhealthy for more than 6 minutes will not have
its associated
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/volume-attachment-v1/">VolumeAttachment</a>
deleted.</p><p>After this setting has been applied, unhealthy pods still attached to volumes must be recovered
via the <a href="#non-graceful-node-shutdown">Non-Graceful Node Shutdown</a> procedure mentioned above.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li>Caution must be taken while using the <a href="#non-graceful-node-shutdown">Non-Graceful Node Shutdown</a> procedure.</li><li>Deviation from the steps documented above can result in data corruption.</li></ul></div><h2 id="what-s-next">What's next</h2><p>Learn more about the following:</p><ul><li>Blog: <a href="/blog/2023/08/16/kubernetes-1-28-non-graceful-node-shutdown-ga/">Non-Graceful Node Shutdown</a>.</li><li>Cluster Architecture: <a href="/docs/concepts/architecture/nodes/">Nodes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Liveness, Readiness, and Startup Probes</h1><p>Kubernetes has various types of probes:</p><ul><li><a href="#liveness-probe">Liveness probe</a></li><li><a href="#readiness-probe">Readiness probe</a></li><li><a href="#startup-probe">Startup probe</a></li></ul><h2 id="liveness-probe">Liveness probe</h2><p>Liveness probes determine when to restart a container. For example, liveness probes could catch a deadlock when an application is running but unable to make progress.</p><p>If a container fails its liveness probe repeatedly, the kubelet restarts the container.</p><p>Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe, you can either define <code>initialDelaySeconds</code> or use a
<a href="#startup-probe">startup probe</a>.</p><h2 id="readiness-probe">Readiness probe</h2><p>Readiness probes determine when a container is ready to accept traffic. This is useful when waiting for an application to perform time-consuming initial tasks that depend on its backing services; for example: establishing network connections, loading files, and warming caches. Readiness probes can also be useful later in the containerâ€™s lifecycle, for example, when recovering from temporary faults or overloads.</p><p>If the readiness probe returns a failed state, Kubernetes removes the pod from all matching service endpoints.</p><p>Readiness probes run on the container during its whole lifecycle.</p><h2 id="startup-probe">Startup probe</h2><p>A startup probe verifies whether the application within a container is started. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.</p><p>If such a probe is configured, it disables liveness and readiness checks until it succeeds.</p><p>This type of probe is only executed at startup, unlike liveness and readiness probes, which are run periodically.</p><ul><li>Read more about the <a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness, Readiness and Startup Probes</a>.</li></ul></div>
<hr>
<div class="td-content"><h1>Configuration</h1><div class="lead">Resources that Kubernetes provides for configuring Pods.</div><div class="section-index"><hr class="panel-line"/><div class="entry"><h5><a href="/docs/concepts/configuration/overview/">Configuration Best Practices</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/configuration/configmap/">ConfigMaps</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/configuration/secret/">Secrets</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/configuration/liveness-readiness-startup-probes/">Liveness, Readiness, and Startup Probes</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/configuration/manage-resources-containers/">Resource Management for Pods and Containers</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></h5><p/></div><div class="entry"><h5><a href="/docs/concepts/configuration/windows-resource-management/">Resource Management for Windows nodes</a></h5><p/></div></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Container Runtime Interface (CRI)</h1><p>The CRI is a plugin interface which enables the kubelet to use a wide variety of
container runtimes, without having a need to recompile the cluster components.</p><p>You need a working
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a> on
each Node in your cluster, so that the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> can launch
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> and their containers.</p><p><p>The Container Runtime Interface (CRI) is the main protocol for the communication between the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> and Container Runtime.</p></p><p>The Kubernetes Container Runtime Interface (CRI) defines the main
<a href="https://grpc.io">gRPC</a> protocol for the communication between the
<a href="/docs/concepts/architecture/#node-components">node components</a>
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> and
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>.</p><h2 id="api">The API</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>The kubelet acts as a client when connecting to the container runtime via gRPC.
The runtime and image service endpoints have to be available in the container
runtime, which can be configured separately within the kubelet by using the
<code>--container-runtime-endpoint</code>
<a href="/docs/reference/command-line-tools-reference/kubelet/">command line flag</a>.</p><p>For Kubernetes v1.26 and later, the kubelet requires that the container runtime
supports the <code>v1</code> CRI API. If a container runtime does not support the <code>v1</code> API,
the kubelet will not register the node.</p><h2 id="upgrading">Upgrading</h2><p>When upgrading the Kubernetes version on a node, the kubelet restarts. If the
container runtime does not support the <code>v1</code> CRI API, the kubelet will fail to
register and report an error. If a gRPC re-dial is required because the container
runtime has been upgraded, the runtime must support the <code>v1</code> CRI API for the
connection to succeed. This might require a restart of the kubelet after the
container runtime is correctly configured.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the CRI <a href="https://github.com/kubernetes/cri-api/blob/v0.33.1/pkg/apis/runtime/v1/api.proto">protocol definition</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Node Autoscaling</h1><div class="lead">Automatically provision and consolidate the Nodes in your cluster to adapt to demand and optimize cost.</div><p>In order to run workloads in your cluster, you need
<a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Nodes">Nodes</a>. Nodes in your cluster can be <em>autoscaled</em> -
dynamically <a href="#provisioning"><em>provisioned</em></a>, or <a href="#consolidation"><em>consolidated</em></a> to provide needed
capacity while optimizing cost. Autoscaling is performed by Node <a href="#autoscalers"><em>autoscalers</em></a>.</p><h2 id="provisioning">Node provisioning</h2><p>If there are Pods in a cluster that can't be scheduled on existing Nodes, new Nodes can be
automatically added to the clusterâ€”<em>provisioned</em>â€”to accommodate the Pods. This is
especially useful if the number of Pods changes over time, for example as a result of
<a href="#horizontal-workload-autoscaling">combining horizontal workload with Node autoscaling</a>.</p><p>Autoscalers provision the Nodes by creating and deleting cloud provider resources backing them. Most
commonly, the resources backing the Nodes are Virtual Machines.</p><p>The main goal of provisioning is to make all Pods schedulable. This goal is not always attainable
because of various limitations, including reaching configured provisioning limits, provisioning
configuration not being compatible with a particular set of pods, or the lack of cloud provider
capacity. While provisioning, Node autoscalers often try to achieve additional goals (for example
minimizing the cost of the provisioned Nodes or balancing the number of Nodes between failure
domains).</p><p>There are two main inputs to a Node autoscaler when determining Nodes to
provisionâ€”<a href="#provisioning-pod-constraints">Pod scheduling constraints</a>,
and <a href="#provisioning-node-constraints">Node constraints imposed by autoscaler configuration</a>.</p><p>Autoscaler configuration may also include other Node provisioning triggers (for example the number
of Nodes falling below a configured minimum limit).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Provisioning was formerly known as <em>scale-up</em> in Cluster Autoscaler.</div><h3 id="provisioning-pod-constraints">Pod scheduling constraints</h3><p>Pods can express <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">scheduling constraints</a> to
impose limitations on the kind of Nodes they can be scheduled on. Node autoscalers take these
constraints into account to ensure that the pending Pods can be scheduled on the provisioned Nodes.</p><p>The most common kind of scheduling constraints are the resource requests specified by Pod
containers. Autoscalers will make sure that the provisioned Nodes have enough resources to satisfy
the requests. However, they don't directly take into account the real resource usage of the Pods
after they start running. In order to autoscale Nodes based on actual workload resource usage, you
can combine <a href="#horizontal-workload-autoscaling">horizontal workload autoscaling</a> with Node
autoscaling.</p><p>Other common Pod scheduling constraints include
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">Node affinity</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">inter-Pod affinity</a>,
or a requirement for a particular <a href="/docs/concepts/storage/volumes/">storage volume</a>.</p><h3 id="provisioning-node-constraints">Node constraints imposed by autoscaler configuration</h3><p>The specifics of the provisioned Nodes (for example the amount of resources, the presence of a given
label) depend on autoscaler configuration. Autoscalers can either choose them from a pre-defined set
of Node configurations, or use <a href="#autoprovisioning">auto-provisioning</a>.</p><h3 id="autoprovisioning">Auto-provisioning</h3><p>Node auto-provisioning is a mode of provisioning in which a user doesn't have to fully configure the
specifics of the Nodes that can be provisioned. Instead, the autoscaler dynamically chooses the Node
configuration based on the pending Pods it's reacting to, as well as pre-configured constraints (for
example, the minimum amount of resources or the need for a given label).</p><h2 id="consolidation">Node consolidation</h2><p>The main consideration when running a cluster is ensuring that all schedulable pods are running,
whilst keeping the cost of the cluster as low as possible. To achieve this, the Pods' resource
requests should utilize as much of the Nodes' resources as possible. From this perspective, the
overall Node utilization in a cluster can be used as a proxy for how cost-effective the cluster is.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Correctly setting the resource requests of your Pods is as important to the overall
cost-effectiveness of a cluster as optimizing Node utilization.
Combining Node autoscaling with <a href="#vertical-workload-autoscaling">vertical workload autoscaling</a> can
help you achieve this.</div><p>Nodes in your cluster can be automatically <em>consolidated</em> in order to improve the overall Node
utilization, and in turn the cost-effectiveness of the cluster. Consolidation happens through
removing a set of underutilized Nodes from the cluster. Optionally, a different set of Nodes can
be <a href="#provisioning">provisioned</a> to replace them.</p><p>Consolidation, like provisioning, only considers Pod resource requests and not real resource usage
when making decisions.</p><p>For the purpose of consolidation, a Node is considered <em>empty</em> if it only has DaemonSet and static
Pods running on it. Removing empty Nodes during consolidation is more straightforward than non-empty
ones, and autoscalers often have optimizations designed specifically for consolidating empty Nodes.</p><p>Removing non-empty Nodes during consolidation is disruptiveâ€”the Pods running on them are
terminated, and possibly have to be recreated (for example by a Deployment). However, all such
recreated Pods should be able to schedule on existing Nodes in the cluster, or the replacement Nodes
provisioned as part of consolidation. <strong>No Pods should normally become pending as a result of
consolidation.</strong></p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Autoscalers predict how a recreated Pod will likely be scheduled after a Node is provisioned or
consolidated, but they don't control the actual scheduling. Because of this, some Pods might
become pending as a result of consolidation - if for example a completely new Pod appears while
consolidation is being performed.</div><p>Autoscaler configuration may also enable triggering consolidation by other conditions (for example,
the time elapsed since a Node was created), in order to optimize different properties (for example,
the maximum lifespan of Nodes in a cluster).</p><p>The details of how consolidation is performed depend on the configuration of a given autoscaler.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Consolidation was formerly known as <em>scale-down</em> in Cluster Autoscaler.</div><h2 id="autoscalers">Autoscalers</h2><p>The functionalities described in previous sections are provided by Node <em>autoscalers</em>. In addition
to the Kubernetes API, autoscalers also need to interact with cloud provider APIs to provision and
consolidate Nodes. This means that they need to be explicitly integrated with each supported cloud
provider. The performance and feature set of a given autoscaler can differ between cloud provider
integrations.</p><figure><div class="mermaid">graph TD
na[Node autoscaler]
k8s[Kubernetes]
cp[Cloud Provider]
k8s --&gt; |get Pods/Nodes|na
na --&gt; |drain Nodes|k8s
na --&gt; |create/remove resources backing Nodes|cp
cp --&gt; |get resources backing Nodes|na
classDef white_on_blue fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef blue_on_white fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class na blue_on_white;
class k8s,cp white_on_blue;</div></figure><noscript><div class="alert alert-secondary callout" role="alert"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h3 id="autoscaler-implementations">Autoscaler implementations</h3><p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Cluster Autoscaler</a>
and <a href="https://github.com/kubernetes-sigs/karpenter">Karpenter</a> are the two Node autoscalers currently
sponsored by <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG Autoscaling</a>.</p><p>From the perspective of a cluster user, both autoscalers should provide a similar Node autoscaling
experience. Both will provision new Nodes for unschedulable Pods, and both will consolidate the
Nodes that are no longer optimally utilized.</p><p>Different autoscalers may also provide features outside the Node autoscaling scope described on this
page, and those additional features may differ between them.</p><p>Consult the sections below, and the linked documentation for the individual autoscalers to decide
which autoscaler fits your use case better.</p><h4 id="cluster-autoscaler">Cluster Autoscaler</h4><p>Cluster Autoscaler adds or removes Nodes to pre-configured <em>Node groups</em>. Node groups generally map
to some sort of cloud provider resource group (most commonly a Virtual Machine group). A single
instance of Cluster Autoscaler can simultaneously manage multiple Node groups. When provisioning,
Cluster Autoscaler will add Nodes to the group that best fits the requests of pending Pods. When
consolidating, Cluster Autoscaler always selects specific Nodes to remove, as opposed to just
resizing the underlying cloud provider resource group.</p><p>Additional context:</p><ul><li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md">Documentation overview</a></li><li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md#faqdocumentation">Cloud provider integrations</a></li><li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">Cluster Autoscaler FAQ</a></li><li><a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling#contact">Contact</a></li></ul><h4 id="karpenter">Karpenter</h4><p>Karpenter auto-provisions Nodes based on <a href="https://karpenter.sh/docs/concepts/nodepools/">NodePool</a>
configurations provided by the cluster operator. Karpenter handles all aspects of node lifecycle,
not just autoscaling. This includes automatically refreshing Nodes once they reach a certain
lifetime, and auto-upgrading Nodes when new worker Node images are released. It works directly with
individual cloud provider resources (most commonly individual Virtual Machines), and doesn't rely on
cloud provider resource groups.</p><p>Additional context:</p><ul><li><a href="https://karpenter.sh/">Documentation</a></li><li><a href="https://github.com/kubernetes-sigs/karpenter?tab=readme-ov-file#karpenter-implementations">Cloud provider integrations</a></li><li><a href="https://karpenter.sh/docs/faq/">Karpenter FAQ</a></li><li><a href="https://github.com/kubernetes-sigs/karpenter#community-discussion-contribution-and-support">Contact</a></li></ul><h4 id="implementation-comparison">Implementation comparison</h4><p>Main differences between Cluster Autoscaler and Karpenter:</p><ul><li>Cluster Autoscaler provides features related to just Node autoscaling. Karpenter has a wider
scope, and also provides features intended for managing Node lifecycle altogether (for example,
utilizing disruption to auto-recreate Nodes once they reach a certain lifetime, or auto-upgrade
them to new versions).</li><li>Cluster Autoscaler doesn't support auto-provisioning, the Node groups it can provision from have
to be pre-configured. Karpenter supports auto-provisioning, so the user only has to configure a
set of constraints for the provisioned Nodes, instead of fully configuring homogenous groups.</li><li>Cluster Autoscaler provides cloud provider integrations directly, which means that they're a part
of the Kubernetes project. For Karpenter, the Kubernetes project publishes Karpenter as a library
that cloud providers can integrate with to build a Node autoscaler.</li><li>Cluster Autoscaler provides integrations with numerous cloud providers, including smaller and less
popular providers. There are fewer cloud providers that integrate with Karpenter, including
<a href="https://github.com/aws/karpenter-provider-aws">AWS</a>, and
<a href="https://github.com/Azure/karpenter-provider-azure">Azure</a>.</li></ul><h2 id="combine-workload-and-node-autoscaling">Combine workload and Node autoscaling</h2><h3 id="horizontal-workload-autoscaling">Horizontal workload autoscaling</h3><p>Node autoscaling usually works in response to Podsâ€”it provisions new Nodes to accommodate
unschedulable Pods, and then consolidates the Nodes once they're no longer needed.</p><p><a href="/docs/concepts/workloads/autoscaling/#scaling-workloads-horizontally">Horizontal workload autoscaling</a>
automatically scales the number of workload replicas to maintain a desired average resource
utilization across the replicas. In other words, it automatically creates new Pods in response to
application load, and then removes the Pods once the load decreases.</p><p>You can use Node autoscaling together with horizontal workload autoscaling to autoscale the Nodes in
your cluster based on the average real resource utilization of your Pods.</p><p>If the application load increases, the average utilization of its Pods should also increase,
prompting workload autoscaling to create new Pods. Node autoscaling should then provision new Nodes
to accommodate the new Pods.</p><p>Once the application load decreases, workload autoscaling should remove unnecessary Pods. Node
autoscaling should, in turn, consolidate the Nodes that are no longer needed.</p><p>If configured correctly, this pattern ensures that your application always has the Node capacity to
handle load spikes if needed, but you don't have to pay for the capacity when it's not needed.</p><h3 id="vertical-workload-autoscaling">Vertical workload autoscaling</h3><p>When using Node autoscaling, it's important to set Pod resource requests correctly. If the requests
of a given Pod are too low, provisioning a new Node for it might not help the Pod actually run.
If the requests of a given Pod are too high, it might incorrectly prevent consolidating its Node.</p><p><a href="/docs/concepts/workloads/autoscaling/#scaling-workloads-vertically">Vertical workload autoscaling</a>
automatically adjusts the resource requests of your Pods based on their historical resource usage.</p><p>You can use Node autoscaling together with vertical workload autoscaling in order to adjust the
resource requests of your Pods while preserving Node autoscaling capabilities in your cluster.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>When using Node autoscaling, it's not recommended to set up vertical workload autoscaling for
DaemonSet Pods. Autoscalers have to predict what DaemonSet Pods on a new Node will look like in
order to predict available Node resources. Vertical workload autoscaling might make these
predictions unreliable, leading to incorrect scaling decisions.</div><h2 id="related-components">Related components</h2><p>This section describes components providing functionality related to Node autoscaling.</p><h3 id="descheduler">Descheduler</h3><p>The <a href="https://github.com/kubernetes-sigs/descheduler">descheduler</a> is a component providing Node
consolidation functionality based on custom policies, as well as other features related to
optimizing Nodes and Pods (for example deleting frequently restarting Pods).</p><h3 id="workload-autoscalers-based-on-cluster-size">Workload autoscalers based on cluster size</h3><p><a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">Cluster Proportional Autoscaler</a>
and <a href="https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler">Cluster Proportional Vertical
Autoscaler</a> provide
horizontal, and vertical workload autoscaling based on the number of Nodes in the cluster. You can
read more in
<a href="/docs/concepts/workloads/autoscaling/#autoscaling-based-on-cluster-size">autoscaling based on cluster size</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/workloads/autoscaling/">workload-level autoscaling</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Compatibility Version For Kubernetes Control Plane Components</h1><p>Since release v1.32, we introduced configurable version compatibility and emulation options to Kubernetes control plane components to make upgrades safer by providing more control and increasing the granularity of steps available to cluster administrators.</p><h2 id="emulated-version">Emulated Version</h2><p>The emulation option is set by the <code>--emulated-version</code> flag of control plane components. It allows the component to emulate the behavior (APIs, features, ...) of an earlier version of Kubernetes.</p><p>When used, the capabilities available will match the emulated version:</p><ul><li>Any capabilities present in the binary version that were introduced after the emulation version will be unavailable.</li><li>Any capabilities removed after the emulation version will be available.</li></ul><p>This enables a binary from a particular Kubernetes release to emulate the behavior of a previous version with sufficient fidelity that interoperability with other system components can be defined in terms of the emulated version.</p><p>The <code>--emulated-version</code> must be &lt;= <code>binaryVersion</code>. See the help message of the <code>--emulated-version</code> flag for supported range of emulated versions.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Organizing Cluster Access Using kubeconfig Files</h1><p>Use kubeconfig files to organize information about clusters, users, namespaces, and
authentication mechanisms. The <code>kubectl</code> command-line tool uses kubeconfig files to
find the information it needs to choose a cluster and communicate with the API server
of a cluster.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A file that is used to configure access to clusters is called
a <em>kubeconfig file</em>. This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.</div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.</div><p>By default, <code>kubectl</code> looks for a file named <code>config</code> in the <code>$HOME/.kube</code> directory.
You can specify other kubeconfig files by setting the <code>KUBECONFIG</code> environment
variable or by setting the
<a href="/docs/reference/generated/kubectl/kubectl/"><code>--kubeconfig</code></a> flag.</p><p>For step-by-step instructions on creating and specifying kubeconfig files, see
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a>.</p><h2 id="supporting-multiple-clusters-users-and-authentication-mechanisms">Supporting multiple clusters, users, and authentication mechanisms</h2><p>Suppose you have several clusters, and your users and components authenticate
in a variety of ways. For example:</p><ul><li>A running kubelet might authenticate using certificates.</li><li>A user might authenticate using tokens.</li><li>Administrators might have sets of certificates that they provide to individual users.</li></ul><p>With kubeconfig files, you can organize your clusters, users, and namespaces.
You can also define contexts to quickly and easily switch between
clusters and namespaces.</p><h2 id="context">Context</h2><p>A <em>context</em> element in a kubeconfig file is used to group access parameters
under a convenient name. Each context has three parameters: cluster, namespace, and user.
By default, the <code>kubectl</code> command-line tool uses parameters from
the <em>current context</em> to communicate with the cluster.</p><p>To choose the current context:</p><pre tabindex="0"><code>kubectl config use-context
</code></pre><h2 id="the-kubeconfig-environment-variable">The KUBECONFIG environment variable</h2><p>The <code>KUBECONFIG</code> environment variable holds a list of kubeconfig files.
For Linux and Mac, the list is colon-delimited. For Windows, the list
is semicolon-delimited. The <code>KUBECONFIG</code> environment variable is not
required. If the <code>KUBECONFIG</code> environment variable doesn't exist,
<code>kubectl</code> uses the default kubeconfig file, <code>$HOME/.kube/config</code>.</p><p>If the <code>KUBECONFIG</code> environment variable does exist, <code>kubectl</code> uses
an effective configuration that is the result of merging the files
listed in the <code>KUBECONFIG</code> environment variable.</p><h2 id="merging-kubeconfig-files">Merging kubeconfig files</h2><p>To see your configuration, enter this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl config view
</span></span></code></pre></div><p>As described previously, the output might be from a single kubeconfig file,
or it might be the result of merging several kubeconfig files.</p><p>Here are the rules that <code>kubectl</code> uses when it merges kubeconfig files:</p><ol><li><p>If the <code>--kubeconfig</code> flag is set, use only the specified file. Do not merge.
Only one instance of this flag is allowed.</p><p>Otherwise, if the <code>KUBECONFIG</code> environment variable is set, use it as a
list of files that should be merged.
Merge the files listed in the <code>KUBECONFIG</code> environment variable
according to these rules:</p><ul><li>Ignore empty filenames.</li><li>Produce errors for files with content that cannot be deserialized.</li><li>The first file to set a particular value or map key wins.</li><li>Never change the value or map key.
Example: Preserve the context of the first file to set <code>current-context</code>.
Example: If two files specify a <code>red-user</code>, use only values from the first file's <code>red-user</code>.
Even if the second file has non-conflicting entries under <code>red-user</code>, discard them.</li></ul><p>For an example of setting the <code>KUBECONFIG</code> environment variable, see
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable">Setting the KUBECONFIG environment variable</a>.</p><p>Otherwise, use the default kubeconfig file, <code>$HOME/.kube/config</code>, with no merging.</p></li><li><p>Determine the context to use based on the first hit in this chain:</p><ol><li>Use the <code>--context</code> command-line flag if it exists.</li><li>Use the <code>current-context</code> from the merged kubeconfig files.</li></ol><p>An empty context is allowed at this point.</p></li><li><p>Determine the cluster and user. At this point, there might or might not be a context.
Determine the cluster and user based on the first hit in this chain,
which is run twice: once for user and once for cluster:</p><ol><li>Use a command-line flag if it exists: <code>--user</code> or <code>--cluster</code>.</li><li>If the context is non-empty, take the user or cluster from the context.</li></ol><p>The user and cluster can be empty at this point.</p></li><li><p>Determine the actual cluster information to use. At this point, there might or
might not be cluster information.
Build each piece of the cluster information based on this chain; the first hit wins:</p><ol><li>Use command line flags if they exist: <code>--server</code>, <code>--certificate-authority</code>, <code>--insecure-skip-tls-verify</code>.</li><li>If any cluster information attributes exist from the merged kubeconfig files, use them.</li><li>If there is no server location, fail.</li></ol></li><li><p>Determine the actual user information to use. Build user information using the same
rules as cluster information, except allow only one authentication
technique per user:</p><ol><li>Use command line flags if they exist: <code>--client-certificate</code>, <code>--client-key</code>, <code>--username</code>, <code>--password</code>, <code>--token</code>.</li><li>Use the <code>user</code> fields from the merged kubeconfig files.</li><li>If there are two conflicting techniques, fail.</li></ol></li><li><p>For any information still missing, use default values and potentially
prompt for authentication information.</p></li></ol><h2 id="file-references">File references</h2><p>File and path references in a kubeconfig file are relative to the location of the kubeconfig file.
File references on the command line are relative to the current working directory.
In <code>$HOME/.kube/config</code>, relative paths are stored relatively, and absolute paths
are stored absolutely.</p><h2 id="proxy">Proxy</h2><p>You can configure <code>kubectl</code> to use a proxy per cluster using <code>proxy-url</code> in your kubeconfig file, like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">clusters</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">cluster</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">proxy-url</span>:<span style="color:#bbb"> </span>http://proxy.example.org:3128<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">server</span>:<span style="color:#bbb"> </span>https://k8s.example.org/k8s/clusters/c-xxyyzz<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">users</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">contexts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/>- <span style="color:green;font-weight:700">context</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands#config"><code>kubectl config</code></a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Proxies in Kubernetes</h1><p>This page explains proxies used with Kubernetes.</p><h2 id="proxies">Proxies</h2><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li><p>The <a href="/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api">kubectl proxy</a>:</p><ul><li>runs on a user's desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li><p>The <a href="/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services">apiserver proxy</a>:</p><ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li><p>The <a href="/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>:</p><ul><li>runs on each node</li><li>proxies UDP, TCP and SCTP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is only used to reach services</li></ul></li><li><p>A Proxy/Load-balancer in front of apiserver(s):</p><ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li><p>Cloud Load Balancers on external services:</p><ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <code>LoadBalancer</code></li><li>usually supports UDP/TCP only</li><li>SCTP support is up to the load balancer implementation of the cloud provider</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin
will typically ensure that the latter types are set up correctly.</p><h2 id="requesting-redirects">Requesting redirects</h2><p>Proxies have replaced redirect capabilities. Redirects have been deprecated.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Cluster Administration</h1><div class="lead">Lower-level detail relevant to creating or administering a Kubernetes cluster.</div><p>The cluster administration overview is for anyone creating or administering a Kubernetes cluster.
It assumes some familiarity with core Kubernetes <a href="/docs/concepts/">concepts</a>.</p><h2 id="planning-a-cluster">Planning a cluster</h2><p>See the guides in <a href="/docs/setup/">Setup</a> for examples of how to plan, set up, and configure
Kubernetes clusters. The solutions listed in this article are called <em>distros</em>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Not all distros are actively maintained. Choose distros which have been tested with a recent
version of Kubernetes.</div><p>Before choosing a guide, here are some considerations:</p><ul><li>Do you want to try out Kubernetes on your computer, or do you want to build a high-availability,
multi-node cluster? Choose distros best suited for your needs.</li><li>Will you be using <strong>a hosted Kubernetes cluster</strong>, such as
<a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a>, or <strong>hosting your own cluster</strong>?</li><li>Will your cluster be <strong>on-premises</strong>, or <strong>in the cloud (IaaS)</strong>? Kubernetes does not directly
support hybrid clusters. Instead, you can set up multiple clusters.</li><li><strong>If you are configuring Kubernetes on-premises</strong>, consider which
<a href="/docs/concepts/cluster-administration/networking/">networking model</a> fits best.</li><li>Will you be running Kubernetes on <strong>"bare metal" hardware</strong> or on <strong>virtual machines (VMs)</strong>?</li><li>Do you <strong>want to run a cluster</strong>, or do you expect to do <strong>active development of Kubernetes project code</strong>?
If the latter, choose an actively-developed distro. Some distros only use binary releases, but
offer a greater variety of choices.</li><li>Familiarize yourself with the <a href="/docs/concepts/overview/components/">components</a> needed to run a cluster.</li></ul><h2 id="managing-a-cluster">Managing a cluster</h2><ul><li><p>Learn how to <a href="/docs/concepts/architecture/nodes/">manage nodes</a>.</p><ul><li>Read about <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a>.</li></ul></li><li><p>Learn how to set up and manage the <a href="/docs/concepts/policy/resource-quotas/">resource quota</a> for shared clusters.</p></li></ul><h2 id="securing-a-cluster">Securing a cluster</h2><ul><li><p><a href="/docs/tasks/administer-cluster/certificates/">Generate Certificates</a> describes the steps to
generate certificates using different tool chains.</p></li><li><p><a href="/docs/concepts/containers/container-environment/">Kubernetes Container Environment</a> describes
the environment for Kubelet managed containers on a Kubernetes node.</p></li><li><p><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a> describes
how Kubernetes implements access control for its own API.</p></li><li><p><a href="/docs/reference/access-authn-authz/authentication/">Authenticating</a> explains authentication in
Kubernetes, including the various authentication options.</p></li><li><p><a href="/docs/reference/access-authn-authz/authorization/">Authorization</a> is separate from
authentication, and controls how HTTP calls are handled.</p></li><li><p><a href="/docs/reference/access-authn-authz/admission-controllers/">Using Admission Controllers</a>
explains plug-ins which intercepts requests to the Kubernetes API server after authentication
and authorization.</p></li><li><p><a href="/docs/concepts/cluster-administration/admission-webhooks-good-practices/">Admission Webhook Good Practices</a>
provides good practices and considerations when designing mutating admission
webhooks and validating admission webhooks.</p></li><li><p><a href="/docs/tasks/administer-cluster/sysctl-cluster/">Using Sysctls in a Kubernetes Cluster</a>
describes to an administrator how to use the <code>sysctl</code> command-line tool to set kernel parameters
.</p></li><li><p><a href="/docs/tasks/debug/debug-cluster/audit/">Auditing</a> describes how to interact with Kubernetes'
audit logs.</p></li></ul><h3 id="securing-the-kubelet">Securing the kubelet</h3><ul><li><a href="/docs/concepts/architecture/control-plane-node-communication/">Control Plane-Node communication</a></li><li><a href="/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/">TLS bootstrapping</a></li><li><a href="/docs/reference/access-authn-authz/kubelet-authn-authz/">Kubelet authentication/authorization</a></li></ul><h2 id="optional-cluster-services">Optional Cluster Services</h2><ul><li><p><a href="/docs/concepts/services-networking/dns-pod-service/">DNS Integration</a> describes how to resolve
a DNS name directly to a Kubernetes service.</p></li><li><p><a href="/docs/concepts/cluster-administration/logging/">Logging and Monitoring Cluster Activity</a>
explains how logging in Kubernetes works and how to implement it.</p></li></ul><div class="section-index"/></div>