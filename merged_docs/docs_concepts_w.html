<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Init Containers</h1><p>This page provides an overview of init containers: specialized containers that run
before app containers in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>.
Init containers can contain utilities or setup scripts not present in an app image.</p><p>You can specify init containers in the Pod specification alongside the <code>containers</code>
array (which describes app containers).</p><p>In Kubernetes, a <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a> is a container that
starts before the main application container and <em>continues to run</em>. This document is about init containers:
containers that run to completion during Pod initialization.</p><h2 id="understanding-init-containers">Understanding init containers</h2><p>A <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a> can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.</p><p>Init containers are exactly like regular containers, except:</p><ul><li>Init containers always run to completion.</li><li>Each init container must complete successfully before the next one starts.</li></ul><p>If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
However, if the Pod has a <code>restartPolicy</code> of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.</p><p>To specify an init container for a Pod, add the <code>initContainers</code> field into
the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec">Pod specification</a>,
as an array of <code>container</code> items (similar to the app <code>containers</code> field and its contents).
See <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">Container</a> in the
API reference for more details.</p><p>The status of the init containers is returned in <code>.status.initContainerStatuses</code>
field as an array of the container statuses (similar to the <code>.status.containerStatuses</code>
field).</p><h3 id="differences-from-regular-containers">Differences from regular containers</h3><p>Init containers support all the fields and features of app containers,
including resource limits, <a href="/docs/concepts/storage/volumes/">volumes</a>, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in <a href="#resource-sharing-within-containers">Resource sharing within containers</a>.</p><p>Regular init containers (in other words: excluding sidecar containers) do not support the
<code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or <code>startupProbe</code> fields. Init containers
must run to completion before the Pod can be ready; sidecar containers continue running
during a Pod's lifetime, and <em>do</em> support some probes. See <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a>
for further details about sidecar containers.</p><p>If you specify multiple init containers for a Pod, kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, kubelet initializes
the application containers for the Pod and runs them as usual.</p><h3 id="differences-from-sidecar-containers">Differences from sidecar containers</h3><p>Init containers run and complete their tasks before the main application container starts.
Unlike <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>,
init containers are not continuously running alongside the main containers.</p><p>Init containers run to completion sequentially, and the main container does not start
until all the init containers have successfully completed.</p><p>init containers do not support <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or
<code>startupProbe</code> whereas sidecar containers support all these <a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">probes</a> to control their lifecycle.</p><p>Init containers share the same resources (CPU, memory, network) with the main application
containers but do not interact directly with them. They can, however, use shared volumes
for data exchange.</p><h2 id="using-init-containers">Using init containers</h2><p>Because init containers have separate images from app containers, they
have some advantages for start-up related code:</p><ul><li>Init containers can contain utilities or custom code for setup that are not present in an app
image. For example, there is no need to make an image <code>FROM</code> another image just to use a tool like
<code>sed</code>, <code>awk</code>, <code>python</code>, or <code>dig</code> during setup.</li><li>The application image builder and deployer roles can work independently without
the need to jointly build a single app image.</li><li>Init containers can run with a different view of the filesystem than app containers in the
same Pod. Consequently, they can be given access to
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secrets">Secrets</a> that app containers cannot access.</li><li>Because init containers run to completion before any app containers start, init containers offer
a mechanism to block or delay app container startup until a set of preconditions are met. Once
preconditions are met, all of the app containers in a Pod can start in parallel.</li><li>Init containers can securely run utilities or custom code that would otherwise make an app
container image less secure. By keeping unnecessary tools separate you can limit the attack
surface of your app container image.</li></ul><h3 id="examples">Examples</h3><p>Here are some ideas for how to use init containers:</p><ul><li><p>Wait for a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a> to
be created, using a shell one-line command like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f;font-weight:700">for</span> i in <span style="color:#666">{</span>1..100<span style="color:#666">}</span>; <span style="color:#a2f;font-weight:700">do</span> sleep 1; <span style="color:#a2f;font-weight:700">if</span> nslookup myservice; <span style="color:#a2f;font-weight:700">then</span> <span style="color:#a2f">exit</span> 0; <span style="color:#a2f;font-weight:700">fi</span>; <span style="color:#a2f;font-weight:700">done</span>; <span style="color:#a2f">exit</span> <span style="color:#666">1</span>
</span></span></code></pre></div></li><li><p>Register this Pod with a remote server from the downward API with a command like:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>curl -X POST http://<span style="color:#b8860b">$MANAGEMENT_SERVICE_HOST</span>:<span style="color:#b8860b">$MANAGEMENT_SERVICE_PORT</span>/register -d <span style="color:#b44">'instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)'</span>
</span></span></code></pre></div></li><li><p>Wait for some time before starting the app container with a command like</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>sleep <span style="color:#666">60</span>
</span></span></code></pre></div></li><li><p>Clone a Git repository into a <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="Volume">Volume</a></p></li><li><p>Place values into a configuration file and run a template tool to dynamically
generate a configuration file for the main app container. For example,
place the <code>POD_IP</code> value in a configuration and generate the main app
configuration file using Jinja.</p></li></ul><h4 id="init-containers-in-use">Init containers in use</h4><p>This example defines a simple Pod that has two init containers.
The first waits for <code>myservice</code>, and the second waits for <code>mydb</code>. Once both
init containers complete, the Pod runs the app container from its <code>spec</code> section.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myapp-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myapp-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo The app is running! &amp;&amp; sleep 3600'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">initContainers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>init-myservice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>init-mydb<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"</span>]<span style="color:#bbb">
</span></span></span></code></pre></div><p>You can start this Pod by running:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>pod/myapp-pod created
</code></pre><p>And check on its status with:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><p>or for more details:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app.kubernetes.io/name=MyApp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image "busybox"
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image "busybox"
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice
</code></pre><p>To see logs for the init containers in this Pod, run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs myapp-pod -c init-myservice <span style="color:#080;font-style:italic"># Inspect the first init container</span>
</span></span><span style="display:flex"><span>kubectl logs myapp-pod -c init-mydb      <span style="color:#080;font-style:italic"># Inspect the second init container</span>
</span></span></code></pre></div><p>At this point, those init containers will be waiting to discover <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Services">Services</a> named
<code>mydb</code> and <code>myservice</code>.</p><p>Here's a configuration you can use to make those Services appear:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myservice<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>mydb<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9377</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>To create the <code>mydb</code> and <code>myservice</code> services:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f services.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>service/myservice created
service/mydb created
</code></pre><p>You'll then see that those init containers complete, and that the <code>myapp-pod</code>
Pod moves into the Running state:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><p>This simple example should provide some inspiration for you to create your own
init containers. <a href="#what-s-next">What's next</a> contains a link to a more detailed example.</p><h2 id="detailed-behavior">Detailed behavior</h2><p>During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod's init containers in the order
they appear in the Pod's spec.</p><p>Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod <code>restartPolicy</code>. However,
if the Pod <code>restartPolicy</code> is set to Always, the init containers use
<code>restartPolicy</code> OnFailure.</p><p>A Pod cannot be <code>Ready</code> until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the <code>Pending</code> state but should have a condition <code>Initialized</code> set to false.</p><p>If the Pod <a href="#pod-restart-reasons">restarts</a>, or is restarted, all init containers
must execute again.</p><p>Changes to the init container spec are limited to the container image field.
Directly altering the <code>image</code> field of an init container does <em>not</em> restart the
Pod or trigger its recreation. If the Pod has yet to start, that change may
have an effect on how the Pod boots up.</p><p>For a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>
you can typically change any field for an init container; the impact of making
that change depends on where the pod template is used.</p><p>Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes into any <code>emptyDir</code> volume
should be prepared for the possibility that an output file already exists.</p><p>Init containers have all of the fields of an app container. However, Kubernetes
prohibits <code>readinessProbe</code> from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.</p><p>Use <code>activeDeadlineSeconds</code> on the Pod to prevent init containers from failing forever.
The active deadline includes init containers.
However it is recommended to use <code>activeDeadlineSeconds</code> only if teams deploy their application
as a Job, because <code>activeDeadlineSeconds</code> has an effect even after initContainer finished.
The Pod which is already running correctly would be killed by <code>activeDeadlineSeconds</code> if you set.</p><p>The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.</p><h3 id="resource-sharing-within-containers">Resource sharing within containers</h3><p>Given the order of execution for init, sidecar and app containers, the following rules
for resource usage apply:</p><ul><li>The highest of any particular resource request or limit defined on all init
containers is the <em>effective init request/limit</em>. If any resource has no
resource limit specified this is considered as the highest limit.</li><li>The Pod's <em>effective request/limit</em> for a resource is the higher of:<ul><li>the sum of all app containers request/limit for a resource</li><li>the effective init request/limit for a resource</li></ul></li><li>Scheduling is done based on effective requests/limits, which means
init containers can reserve resources for initialization that are not used
during the life of the Pod.</li><li>The QoS (quality of service) tier of the Pod's <em>effective QoS tier</em> is the
QoS tier for init containers and app containers alike.</li></ul><p>Quota and limits are applied based on the effective Pod request and
limit.</p><h3 id="cgroups">Init containers and Linux cgroups</h3><p>On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod
request and limit, the same as the scheduler.</p><h3 id="pod-restart-reasons">Pod restart reasons</h3><p>A Pod can restart, causing re-execution of init containers, for the following
reasons:</p><ul><li>The Pod infrastructure container is restarted. This is uncommon and would
have to be done by someone with root access to nodes.</li><li>All containers in a Pod are terminated while <code>restartPolicy</code> is set to Always,
forcing a restart, and the init container completion record has been lost due
to <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/garbage-collection/" target="_blank" aria-label="garbage collection">garbage collection</a>.</li></ul><p>The Pod will not be restarted when the init container image is changed, or the
init container completion record has been lost due to garbage collection. This
applies for Kubernetes v1.20 and later. If you are using an earlier version of
Kubernetes, consult the documentation for the version you are using.</p><h2 id="what-s-next">What's next</h2><p>Learn more about the following:</p><ul><li><a href="/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">Creating a Pod that has an init container</a>.</li><li><a href="/docs/tasks/debug/debug-application/debug-init-containers/">Debug init containers</a>.</li><li>Overview of <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> and <a href="/docs/reference/kubectl/">kubectl</a>.</li><li><a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">Types of probes</a>: liveness, readiness, startup probe.</li><li><a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Deployments</h1><div class="lead">A Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.</div><p>A <em>Deployment</em> provides declarative updates for <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> and
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank" aria-label="ReplicaSets">ReplicaSets</a>.</p><p>You describe a <em>desired state</em> in a Deployment, and the Deployment <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="Controller">Controller</a> changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.</div><h2 id="use-case">Use Case</h2><p>The following are typical use cases for Deployments:</p><ul><li><a href="#creating-a-deployment">Create a Deployment to rollout a ReplicaSet</a>. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li><li><a href="#updating-a-deployment">Declare the new state of the Pods</a> by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created, and the Deployment gradually scales it up while scaling down the old ReplicaSet, ensuring Pods are replaced at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li><li><a href="#rolling-back-a-deployment">Rollback to an earlier Deployment revision</a> if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li><li><a href="#scaling-a-deployment">Scale up the Deployment to facilitate more load</a>.</li><li><a href="#pausing-and-resuming-a-deployment">Pause the rollout of a Deployment</a> to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li><li><a href="#deployment-status">Use the status of the Deployment</a> as an indicator that a rollout has stuck.</li><li><a href="#clean-up-policy">Clean up older ReplicaSets</a> that you don't need anymore.</li></ul><h2 id="creating-a-deployment">Creating a Deployment</h2><p>The following is an example of a Deployment. It creates a ReplicaSet to bring up three <code>nginx</code> Pods:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/nginx-deployment.yaml" download="controllers/nginx-deployment.yaml"><code>controllers/nginx-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-nginx-deployment-yaml&quot;)" title="Copy controllers/nginx-deployment.yaml to clipboard"/></div><div class="includecode" id="controllers-nginx-deployment-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In this example:</p><ul><li><p>A Deployment named <code>nginx-deployment</code> is created, indicated by the
<code>.metadata.name</code> field. This name will become the basis for the ReplicaSets
and Pods which are created later. See <a href="#writing-a-deployment-spec">Writing a Deployment Spec</a>
for more details.</p></li><li><p>The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the <code>.spec.replicas</code> field.</p></li><li><p>The <code>.spec.selector</code> field defines how the created ReplicaSet finds which Pods to manage.
In this case, you select a label that is defined in the Pod template (<code>app: nginx</code>).
However, more sophisticated selection rules are possible,
as long as the Pod template itself satisfies the rule.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>.spec.selector.matchLabels</code> field is a map of {key,value} pairs.
A single {key,value} in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>,
whose <code>key</code> field is "key", the <code>operator</code> is "In", and the <code>values</code> array contains only "value".
All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, must be satisfied in order to match.</div></li><li><p>The <code>.spec.template</code> field contains the following sub-fields:</p><ul><li>The Pods are labeled <code>app: nginx</code>using the <code>.metadata.labels</code> field.</li><li>The Pod template's specification, or <code>.spec</code> field, indicates that
the Pods run one container, <code>nginx</code>, which runs the <code>nginx</code>
<a href="https://hub.docker.com/">Docker Hub</a> image at version 1.14.2.</li><li>Create one container and name it <code>nginx</code> using the <code>.spec.containers[0].name</code> field.</li></ul></li></ul><p>Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:</p><ol><li><p>Create the Deployment by running the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div></li><li><p>Run <code>kubectl get deployments</code> to check if the Deployment was created.</p><p>If the Deployment is still being created, the output is similar to the following:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
</code></pre><p>When you inspect the Deployments in your cluster, the following fields are displayed:</p><ul><li><code>NAME</code> lists the names of the Deployments in the namespace.</li><li><code>READY</code> displays how many replicas of the application are available to your users. It follows the pattern ready/desired.</li><li><code>UP-TO-DATE</code> displays the number of replicas that have been updated to achieve the desired state.</li><li><code>AVAILABLE</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice how the number of desired replicas is 3 according to <code>.spec.replicas</code> field.</p></li><li><p>To see the Deployment rollout status, run <code>kubectl rollout status deployment/nginx-deployment</code>.</p><p>The output is similar to:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment "nginx-deployment" successfully rolled out
</code></pre></li><li><p>Run the <code>kubectl get deployments</code> again a few seconds later.
The output is similar to this:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
</code></pre><p>Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.</p></li><li><p>To see the ReplicaSet (<code>rs</code>) created by the Deployment, run <code>kubectl get rs</code>. The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
</code></pre><p>ReplicaSet output shows the following fields:</p><ul><li><code>NAME</code> lists the names of the ReplicaSets in the namespace.</li><li><code>DESIRED</code> displays the desired number of <em>replicas</em> of the application, which you define when you create the Deployment. This is the <em>desired state</em>.</li><li><code>CURRENT</code> displays how many replicas are currently running.</li><li><code>READY</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice that the name of the ReplicaSet is always formatted as
<code>[DEPLOYMENT-NAME]-[HASH]</code>. This name will become the basis for the Pods
which are created.</p><p>The <code>HASH</code> string is the same as the <code>pod-template-hash</code> label on the ReplicaSet.</p></li><li><p>To see the labels automatically generated for each Pod, run <code>kubectl get pods --show-labels</code>.
The output is similar to:</p><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897
</code></pre><p>The created ReplicaSet ensures that there are three <code>nginx</code> Pods.</p></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, <code>app: nginx</code>).</p><p>Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.</p></div><h3 id="pod-template-hash-label">Pod-template-hash label</h3><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Do not change this label.</div><p>The <code>pod-template-hash</code> label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.</p><p>This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the <code>PodTemplate</code> of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.</p><h2 id="updating-a-deployment">Updating a Deployment</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, <code>.spec.template</code>)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</div><p>Follow the steps given below to update your Deployment:</p><ol><li><p>Let's update the nginx Pods to use the <code>nginx:1.16.1</code> image instead of the <code>nginx:1.14.2</code> image.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">set</span> image deployment.v1.apps/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</span></span></code></pre></div><p>or use the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</span></span></code></pre></div><p>where <code>deployment/nginx-deployment</code> indicates the Deployment,
<code>nginx</code> indicates the Container the update will take place and
<code>nginx:1.16.1</code> indicates the new image and its tag.</p><p>The output is similar to:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre><p>Alternatively, you can <code>edit</code> the Deployment and change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code> to <code>nginx:1.16.1</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment edited
</code></pre></li><li><p>To see the rollout status, run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre><p>or</p><pre tabindex="0"><code>deployment "nginx-deployment" successfully rolled out
</code></pre></li></ol><p>Get more details on your updated Deployment:</p><ul><li><p>After the rollout succeeds, you can view the Deployment by running <code>kubectl get deployments</code>.
The output is similar to this:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           36s
</code></pre></li><li><p>Run <code>kubectl get rs</code> to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre></li><li><p>Running <code>get pods</code> should now show only the new Pods:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><p>Next time you want to update these Pods, you only need to update the Deployment's Pod template again.</p><p>Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).</p><p>Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).</p><p>For example, if you look at the above Deployment closely, you will see that it first creates a new Pod,
then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of
new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of
a Deployment with 4 replicas, the number of Pods would be between 3 and 5.</p></li><li><p>Get details of your Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe deployments
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &lt;none&gt;
      Mounts:       &lt;none&gt;
    Volumes:        &lt;none&gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &lt;none&gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><p>Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet
to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.
It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.
Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes doesn't count terminating Pods when calculating the number of <code>availableReplicas</code>, which must be between
<code>replicas - maxUnavailable</code> and <code>replicas + maxSurge</code>. As a result, you might notice that there are more Pods than
expected during a rollout, and that the total resources consumed by the Deployment is more than <code>replicas + maxSurge</code>
until the <code>terminationGracePeriodSeconds</code> of the terminating Pods expires.</div><h3 id="rollover-aka-multiple-updates-in-flight">Rollover (aka multiple updates in-flight)</h3><p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> is scaled down. Eventually, the new
ReplicaSet is scaled to <code>.spec.replicas</code> and all old ReplicaSets is scaled to 0.</p><p>If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
-- it will add it to its list of old ReplicaSets and start scaling it down.</p><p>For example, suppose you create a Deployment to create 5 replicas of <code>nginx:1.14.2</code>,
but then update the Deployment to create 5 replicas of <code>nginx:1.16.1</code>, when only 3
replicas of <code>nginx:1.14.2</code> had been created. In that case, the Deployment immediately starts
killing the 3 <code>nginx:1.14.2</code> Pods that it had created, and starts creating
<code>nginx:1.16.1</code> Pods. It does not wait for the 5 replicas of <code>nginx:1.14.2</code> to be created
before changing course.</p><h3 id="label-selector-updates">Label selector updates</h3><p>It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In API version <code>apps/v1</code>, a Deployment's label selector is immutable after it gets created.</div><ul><li>Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.</li><li>Selector updates changes the existing value in a selector key -- result in the same behavior as additions.</li><li>Selector removals removes an existing key from the Deployment selector -- do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.</li></ul><h2 id="rolling-back-a-deployment">Rolling Back a Deployment</h2><p>Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A Deployment's revision is created when a Deployment's rollout is triggered. This means that the
new revision is created if and only if the Deployment's Pod template (<code>.spec.template</code>) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment's Pod template part is
rolled back.</div><ul><li><p>Suppose that you made a typo while updating the Deployment, by putting the image name as <code>nginx:1.161</code> instead of <code>nginx:1.16.1</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.161
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The rollout gets stuck. You can verify it by checking the rollout status:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre></li><li><p>Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
<a href="#deployment-status">read more here</a>.</p></li><li><p>You see that the number of old replicas (adding the replica count from
<code>nginx-deployment-1564180365</code> and <code>nginx-deployment-2035384211</code>) is 3, and the number of
new replicas (from <code>nginx-deployment-3066724191</code>) is 1.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
</code></pre></li><li><p>Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</code></pre><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (<code>maxUnavailable</code> specifically) that you have specified. Kubernetes by default sets the value to 25%.</div></li><li><p>Get the description of the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.161
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><p>To fix this, you need to rollback to a previous revision of Deployment that is stable.</p></li></ul><h3 id="checking-rollout-history-of-a-deployment">Checking Rollout History of a Deployment</h3><p>Follow the steps given below to check the rollout history:</p><ol><li><p>First, check the revisions of this Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployments "nginx-deployment"
REVISION    CHANGE-CAUSE
1           &lt;none&gt;
2           &lt;none&gt;
3           &lt;none&gt;
</code></pre><p><code>CHANGE-CAUSE</code> is copied from the Deployment annotation <code>kubernetes.io/change-cause</code> to its revisions upon creation. You can specify the<code>CHANGE-CAUSE</code> message by:</p><ul><li>Annotating the Deployment with <code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"</code></li><li>Manually editing the manifest of the resource.</li><li>Using tooling that sets the annotation automatically.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>In older versions of Kubernetes, you could use the <code>--record</code> flag with kubectl commands to automatically populate the <code>CHANGE-CAUSE</code> field. This flag is deprecated and will be removed in a future release.</div></li><li><p>To see the details of each revision, run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment --revision<span style="color:#666">=</span><span style="color:#666">2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployments "nginx-deployment" revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre></li></ol><h3 id="rolling-back-to-a-previous-revision">Rolling Back to a Previous Revision</h3><p>Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.</p><ol><li><p>Now you've decided to undo the current rollout and rollback to the previous revision:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout undo deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>Alternatively, you can rollback to a specific revision by specifying it with <code>--to-revision</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout undo deployment/nginx-deployment --to-revision<span style="color:#666">=</span><span style="color:#666">2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>For more details about rollout related commands, read <a href="/docs/reference/generated/kubectl/kubectl-commands#rollout"><code>kubectl rollout</code></a>.</p><p>The Deployment is now rolled back to a previous stable revision. As you can see, a <code>DeploymentRollback</code> event
for rolling back to revision 2 is generated from Deployment controller.</p></li><li><p>Check if the rollback was successful and the Deployment is running as expected, run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
</code></pre></li><li><p>Get the description of the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment "nginx-deployment" to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre></li></ol><h2 id="scaling-a-deployment">Scaling a Deployment</h2><p>You can scale a Deployment by using the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale deployment/nginx-deployment --replicas<span style="color:#666">=</span><span style="color:#666">10</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment scaled
</code></pre><p>Assuming <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">horizontal Pod autoscaling</a> is enabled
in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl autoscale deployment/nginx-deployment --min<span style="color:#666">=</span><span style="color:#666">10</span> --max<span style="color:#666">=</span><span style="color:#666">15</span> --cpu-percent<span style="color:#666">=</span><span style="color:#666">80</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment scaled
</code></pre><h3 id="proportional-scaling">Proportional scaling</h3><p>RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called <em>proportional scaling</em>.</p><p>For example, you are running a Deployment with 10 replicas, <a href="#max-surge">maxSurge</a>=3, and <a href="#max-unavailable">maxUnavailable</a>=2.</p><ul><li><p>Ensure that the 10 replicas in your Deployment are running.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre></li><li><p>You update to a new image which happens to be unresolvable from inside the cluster.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:sometag
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the
<code>maxUnavailable</code> requirement that you mentioned above. Check out the rollout status:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre></li><li><p>Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.</p></li></ul><p>In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
</code></pre><p>The rollout status confirms how the replicas were added to each ReplicaSet.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre><h2 id="pausing-and-resuming-a-deployment">Pausing and Resuming a rollout of a Deployment</h2><p>When you update a Deployment, or plan to, you can pause rollouts
for that Deployment before you trigger one or more updates. When
you're ready to apply those changes, you resume rollouts for the
Deployment. This approach allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.</p><ul><li><p>For example, with a Deployment that was created:</p><p>Get the Deployment details:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
</code></pre><p>Get the rollout status:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre></li><li><p>Pause by running the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout pause deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment paused
</code></pre></li><li><p>Then update the image of the Deployment:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>Notice that no new rollout started:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployments "nginx"
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre></li><li><p>Get the rollout status to verify that the existing ReplicaSet has not changed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre></li><li><p>You can make as many updates as you wish, for example, update the resources that will be used:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl <span style="color:#a2f">set</span> resources deployment/nginx-deployment -c<span style="color:#666">=</span>nginx --limits<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>200m,memory<span style="color:#666">=</span>512Mi
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><p>The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to
the Deployment will not have any effect as long as the Deployment rollout is paused.</p></li><li><p>Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout resume deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment resumed
</code></pre></li><li><p><a class="glossary-tooltip" title="A verb that is used to track changes to an object in Kubernetes as a stream." data-toggle="tooltip" data-placement="top" href="/docs/reference/using-api/api-concepts/#api-verbs" target="_blank" aria-label="Watch">Watch</a> the status of the rollout until it's done.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs --watch
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre></li><li><p>Get the status of the latest rollout:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You cannot rollback a paused Deployment until you resume it.</div><h2 id="deployment-status">Deployment status</h2><p>A Deployment enters various states during its lifecycle. It can be <a href="#progressing-deployment">progressing</a> while
rolling out a new ReplicaSet, it can be <a href="#complete-deployment">complete</a>, or it can <a href="#failed-deployment">fail to progress</a>.</p><h3 id="progressing-deployment">Progressing Deployment</h3><p>Kubernetes marks a Deployment as <em>progressing</em> when one of the following tasks is performed:</p><ul><li>The Deployment creates a new ReplicaSet.</li><li>The Deployment is scaling up its newest ReplicaSet.</li><li>The Deployment is scaling down its older ReplicaSet(s).</li><li>New Pods become ready or available (ready for at least <a href="#min-ready-seconds">MinReadySeconds</a>).</li></ul><p>When the rollout becomes progressing, the Deployment controller adds a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetCreated</code> | <code>reason: FoundNewReplicaSet</code> | <code>reason: ReplicaSetUpdated</code></li></ul><p>You can monitor the progress for a Deployment by using <code>kubectl rollout status</code>.</p><h3 id="complete-deployment">Complete Deployment</h3><p>Kubernetes marks a Deployment as <em>complete</em> when it has the following characteristics:</p><ul><li>All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any
updates you've requested have been completed.</li><li>All of the replicas associated with the Deployment are available.</li><li>No old replicas for the Deployment are running.</li></ul><p>When the rollout becomes complete, the Deployment controller sets a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetAvailable</code></li></ul><p>This <code>Progressing</code> condition will retain a status value of <code>"True"</code> until a new rollout
is initiated. The condition holds even when availability of replicas changes (which
does instead affect the <code>Available</code> condition).</p><p>You can check if a Deployment has completed by using <code>kubectl rollout status</code>. If the rollout completed
successfully, <code>kubectl rollout status</code> returns a zero exit code.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment "nginx-deployment" successfully rolled out
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 0 (success):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b8860b">$?</span>
</span></span></code></pre></div><pre tabindex="0"><code>0
</code></pre><h3 id="failed-deployment">Failed Deployment</h3><p>Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:</p><ul><li>Insufficient quota</li><li>Readiness probe failures</li><li>Image pull errors</li><li>Insufficient permissions</li><li>Limit ranges</li><li>Application runtime misconfiguration</li></ul><p>One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
(<a href="#progress-deadline-seconds"><code>.spec.progressDeadlineSeconds</code></a>). <code>.spec.progressDeadlineSeconds</code> denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.</p><p>The following <code>kubectl</code> command sets the spec with <code>progressDeadlineSeconds</code> to make the controller report
lack of progress of a rollout for a Deployment after 10 minutes:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch deployment/nginx-deployment -p <span style="color:#b44">'{"spec":{"progressDeadlineSeconds":600}}'</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment patched
</code></pre><p>Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "False"</code></li><li><code>reason: ProgressDeadlineExceeded</code></li></ul><p>This condition can also fail early and is then set to status value of <code>"False"</code> due to reasons as <code>ReplicaSetCreateError</code>.
Also, the deadline is not taken into account anymore once the Deployment rollout completes.</p><p>See the <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties">Kubernetes API conventions</a> for more information on status conditions.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Kubernetes takes no action on a stalled Deployment other than to report a status condition with
<code>reason: ProgressDeadlineExceeded</code>. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.
You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering
the condition for exceeding the deadline.</div><p>You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let's suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><p>If you run <code>kubectl get deployment nginx-deployment -o yaml</code>, the Deployment status is similar to this:</p><pre tabindex="0"><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set "nginx-deployment-4262182780" is progressing.
    reason: ReplicaSetUpdated
    status: "True"
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: 'Error creating: pods "nginx-deployment-4262182780-" is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><p>Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:</p><pre tabindex="0"><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><p>You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you'll see the
Deployment's status update with a successful condition (<code>status: "True"</code> and <code>reason: NewReplicaSetAvailable</code>).</p><pre tabindex="0"><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><p><code>type: Available</code> with <code>status: "True"</code> means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. <code>type: Progressing</code> with <code>status: "True"</code> means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
<code>reason: NewReplicaSetAvailable</code> means that the Deployment is complete).</p><p>You can check if a Deployment has failed to progress by using <code>kubectl rollout status</code>. <code>kubectl rollout status</code>
returns a non-zero exit code if the Deployment has exceeded the progression deadline.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment "nginx" exceeded its progress deadline
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 1 (indicating an error):</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b8860b">$?</span>
</span></span></code></pre></div><pre tabindex="0"><code>1
</code></pre><h3 id="operating-on-a-failed-deployment">Operating on a failed deployment</h3><p>All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.</p><h2 id="clean-up-policy">Clean up Policy</h2><p>You can set <code>.spec.revisionHistoryLimit</code> field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.</div><p>The cleanup only starts <strong>after</strong> a Deployment reaches a
<a href="/docs/concepts/workloads/controllers/deployment/#complete-deployment">complete state</a>.
If you set <code>.spec.revisionHistoryLimit</code> to 0, any rollout nonetheless triggers creation of a new
ReplicaSet before Kubernetes removes the old one.</p><p>Even with a non-zero revision history limit, you can have more ReplicaSets than the limit
you configure. For example, if pods are crash looping, and there are multiple rolling updates
events triggered over time, you might end up with more ReplicaSets than the
<code>.spec.revisionHistoryLimit</code> because the Deployment never reaches a complete state.</p><h2 id="canary-deployment">Canary Deployment</h2><p>If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
<a href="/docs/concepts/workloads/management/#canary-deployments">managing resources</a>.</p><h2 id="writing-a-deployment-spec">Writing a Deployment Spec</h2><p>As with all other Kubernetes configs, a Deployment needs <code>.apiVersion</code>, <code>.kind</code>, and <code>.metadata</code> fields.
For general information about working with config files, see
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">deploying applications</a>,
configuring containers, and <a href="/docs/concepts/overview/working-with-objects/object-management/">using kubectl to manage resources</a> documents.</p><p>When the control plane creates new Pods for a Deployment, the <code>.metadata.name</code> of the
Deployment is part of the basis for naming those Pods. The name of a Deployment must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>A Deployment also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> and <code>.spec.selector</code> are the only required fields of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">Pod template</a>. It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href="#selector">selector</a>.</p><p>Only a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is
allowed, which is the default if not specified.</p><h3 id="replicas">Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a Deployment, example via <code>kubectl scale deployment deployment --replicas=X</code>, and then you update that Deployment based on a manifest
(for example: by running <code>kubectl apply -f deployment.yaml</code>),
then applying that manifest overwrites the manual scaling that you previously did.</p><p>If a <a href="/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a> (or any
similar API for horizontal scaling) is managing scaling for a Deployment, don't set <code>.spec.replicas</code>.</p><p>Instead, allow the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> to manage the
<code>.spec.replicas</code> field automatically.</p><h3 id="selector">Selector</h3><p><code>.spec.selector</code> is a required field that specifies a <a href="/docs/concepts/overview/working-with-objects/labels/">label selector</a>
for the Pods targeted by this Deployment.</p><p><code>.spec.selector</code> must match <code>.spec.template.metadata.labels</code>, or it will be rejected by the API.</p><p>In API version <code>apps/v1</code>, <code>.spec.selector</code> and <code>.metadata.labels</code> do not default to <code>.spec.template.metadata.labels</code> if not set. So they must be set explicitly. Also note that <code>.spec.selector</code> is immutable after creation of the Deployment in <code>apps/v1</code>.</p><p>A Deployment may terminate Pods whose labels match the selector if their template is different
from <code>.spec.template</code> or if the total number of such Pods exceeds <code>.spec.replicas</code>. It brings up new
Pods with <code>.spec.template</code> if the number of Pods is less than the desired number.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.</div><p>If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won't behave correctly.</p><h3 id="strategy">Strategy</h3><p><code>.spec.strategy</code> specifies the strategy used to replace old Pods by new ones.
<code>.spec.strategy.type</code> can be "Recreate" or "RollingUpdate". "RollingUpdate" is
the default value.</p><h4 id="recreate-deployment">Recreate Deployment</h4><p>All existing Pods are killed before new ones are created when <code>.spec.strategy.type==Recreate</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an
"at most" guarantee for your Pods, you should consider using a
<a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>.</div><h4 id="rolling-update-deployment">Rolling Update Deployment</h4><p>The Deployment updates Pods in a rolling update
fashion (gradually scale down the old ReplicaSets and scale up the new one) when <code>.spec.strategy.type==RollingUpdate</code>. You can specify <code>maxUnavailable</code> and <code>maxSurge</code> to control
the rolling update process.</p><h5 id="max-unavailable">Max Unavailable</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if <code>.spec.strategy.rollingUpdate.maxSurge</code> is 0. The default value is 25%.</p><p>For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.</p><h5 id="max-surge">Max Surge</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if <code>maxUnavailable</code> is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.</p><p>For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.</p><p>Here are some Rolling Update Deployment examples that use the <code>maxUnavailable</code> and <code>maxSurge</code>:</p><ul class="nav nav-tabs" id="tab-with-md" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-with-md-0" role="tab" aria-controls="tab-with-md-0" aria-selected="true">Max Unavailable</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-with-md-1" role="tab" aria-controls="tab-with-md-1">Max Surge</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-with-md-2" role="tab" aria-controls="tab-with-md-2">Hybrid</a></li></ul><div class="tab-content" id="tab-with-md"><div id="tab-with-md-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-with-md-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">strategy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">rollingUpdate</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="tab-with-md-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-with-md-1"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">strategy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">rollingUpdate</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">maxSurge</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div><div id="tab-with-md-2" class="tab-pane" role="tabpanel" aria-labelledby="tab-with-md-2"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">       </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">strategy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">rollingUpdate</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">maxSurge</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span></code></pre></div></p></div></div><h3 id="progress-deadline-seconds">Progress Deadline Seconds</h3><p><code>.spec.progressDeadlineSeconds</code> is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
<a href="#failed-deployment">failed progressing</a> - surfaced as a condition with <code>type: Progressing</code>, <code>status: "False"</code>.
and <code>reason: ProgressDeadlineExceeded</code> in the status of the resource. The Deployment controller will keep
retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.</p><p>If specified, this field needs to be greater than <code>.spec.minReadySeconds</code>.</p><h3 id="min-ready-seconds">Min Ready Seconds</h3><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</p><h3 id="terminating-pods">Terminating Pods</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DeploymentReplicaSetTerminatingReplicas"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>You can enable this feature by setting the <code>DeploymentReplicaSetTerminatingReplicas</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
on the <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">API server</a>
and on the <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></p><p>Pods that become terminating due to deletion or scale down may take a long time to terminate, and may consume
additional resources during that period. As a result, the total number of all pods can temporarily exceed
<code>.spec.replicas</code>. Terminating pods can be tracked using the <code>.status.terminatingReplicas</code> field of the Deployment.</p><h3 id="revision-history-limit">Revision History Limit</h3><p>A Deployment's revision history is stored in the ReplicaSets it controls.</p><p><code>.spec.revisionHistoryLimit</code> is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in <code>etcd</code> and crowd the output of <code>kubectl get rs</code>. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.</p><p>More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.</p><h3 id="paused">Paused</h3><p><code>.spec.paused</code> is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a stateless application using a Deployment</a>.</li><li>Read the
<a href="/docs/reference/kubernetes-api/workload-resources/deployment-v1/">Deployment</a> to understand the Deployment API.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li><li>Use kubectl to <a href="/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/">create a Deployment</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Lifecycle</h1><p>This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the <code>Pending</code> <a href="#pod-phase">phase</a>, moving through <code>Running</code> if at least one
of its primary containers starts OK, and then through either the <code>Succeeded</code> or
<code>Failed</code> phases depending on whether any container in the Pod terminated in failure.</p><p>Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID (<a href="/docs/concepts/overview/working-with-objects/names/#uids">UID</a>), and scheduled
to run on nodes where they remain until termination (according to restart policy) or
deletion.
If a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Node">Node</a> dies, the Pods running on (or scheduled
to run on) that node are <a href="#pod-garbage-collection">marked for deletion</a>. The control
plane marks the Pods for removal after a timeout period.</p><h2 id="pod-lifetime">Pod lifetime</h2><p>Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
<a href="#container-states">states</a> and determines what action to take to make the Pod
healthy again.</p><p>In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of <a href="#pod-conditions">Pod conditions</a>.
You can also inject <a href="#pod-readiness-gate">custom readiness information</a> into the
condition data for a Pod, if that is useful to your application.</p><p>Pods are only <a href="/docs/concepts/scheduling-eviction/">scheduled</a> once in their lifetime;
assigning a Pod to a specific node is called <em>binding</em>, and the process of selecting
which node to use is called <em>scheduling</em>.
Once a Pod has been scheduled and is bound to a node, Kubernetes tries
to run that Pod on the node. The Pod runs on that node until it stops, or until the Pod
is <a href="#pod-termination">terminated</a>; if Kubernetes isn't able to start the Pod on the selected
node (for example, if the node crashes before the Pod starts), then that particular Pod
never starts.</p><p>You can use <a href="/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">Pod Scheduling Readiness</a>
to delay scheduling for a Pod until all its <em>scheduling gates</em> are removed. For example,
you might want to define a set of Pods but only trigger scheduling once all the Pods
have been created.</p><h3 id="pod-fault-recovery">Pods and fault recovery</h3><p>If one of the containers in the Pod fails, then Kubernetes may try to restart that
specific container.
Read <a href="#container-restarts">How Pods handle problems with containers</a> to learn more.</p><p>Pods can however fail in a way that the cluster cannot recover from, and in that case
Kubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the
Pod and relies on other components to provide automatic healing.</p><p>If a Pod is scheduled to a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a> and that
node then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.
A Pod won't survive an <a class="glossary-tooltip" title="Process of terminating one or more Pods on Nodes" data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/" target="_blank" aria-label="eviction">eviction</a> due to
a lack of resources or Node maintenance.</p><p>Kubernetes uses a higher-level abstraction, called a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>, that handles the work of
managing the relatively disposable Pod instances.</p><p>A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead,
that Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can
even have same name (as in <code>.metadata.name</code>) that the old Pod had, but the replacement
would have a different <code>.metadata.uid</code> from the old Pod.</p><p>Kubernetes does not guarantee that a replacement for an existing Pod would be scheduled to
the same node as the old Pod that was being replaced.</p><h3 id="associated-lifetimes">Associated lifetimes</h3><p>When something is said to have the same lifetime as a Pod, such as a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volume">volume</a>,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.</p><figure class="diagram-medium"><img src="/images/docs/pod.svg" alt="A multi-container Pod that contains a file puller sidecar and a web server. The Pod uses an ephemeral emptyDir volume for shared storage between the containers."/><figcaption><h4>Figure 1.</h4><p>A multi-container Pod that contains a file puller <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar</a> and a web server. The Pod uses an <a href="/docs/concepts/storage/volumes/#emptydir">ephemeral <code>emptyDir</code> volume</a> for shared storage between the containers.</p></figcaption></figure><h2 id="pod-phase">Pod phase</h2><p>A Pod's <code>status</code> field is a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podstatus-v1-core">PodStatus</a>
object, which has a <code>phase</code> field.</p><p>The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.</p><p>The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given <code>phase</code> value.</p><p>Here are the possible values for <code>phase</code>:</p><table><thead><tr><th style="text-align:left">Value</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>Pending</code></td><td style="text-align:left">The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</td></tr><tr><td style="text-align:left"><code>Running</code></td><td style="text-align:left">The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.</td></tr><tr><td style="text-align:left"><code>Succeeded</code></td><td style="text-align:left">All containers in the Pod have terminated in success, and will not be restarted.</td></tr><tr><td style="text-align:left"><code>Failed</code></td><td style="text-align:left">All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.</td></tr><tr><td style="text-align:left"><code>Unknown</code></td><td style="text-align:left">For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.</td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>When a pod is failing to start repeatedly, <code>CrashLoopBackOff</code> may appear in the <code>Status</code> field of some kubectl commands.
Similarly, when a pod is being deleted, <code>Terminating</code> may appear in the <code>Status</code> field of some kubectl commands.</p><p>Make sure not to confuse <em>Status</em>, a kubectl display field for user intuition, with the pod's <code>phase</code>.
Pod phase is an explicit part of the Kubernetes data model and of the
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/">Pod API</a>.</p><pre tabindex="0"><code>  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE
  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h
</code></pre><hr/><p>A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.
You can use the flag <code>--force</code> to <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced">terminate a Pod by force</a>.</p></div><p>Since Kubernetes 1.27, the kubelet transitions deleted Pods, except for
<a href="/docs/tasks/configure-pod-container/static-pod/">static Pods</a> and
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced">force-deleted Pods</a>
without a finalizer, to a terminal phase (<code>Failed</code> or <code>Succeeded</code> depending on
the exit statuses of the pod containers) before their deletion from the API server.</p><p>If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the <code>phase</code> of all Pods on the lost node to Failed.</p><h2 id="container-states">Container states</h2><p>As well as the <a href="#pod-phase">phase</a> of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
<a href="/docs/concepts/containers/container-lifecycle-hooks/">container lifecycle hooks</a> to
trigger events to run at certain points in a container's lifecycle.</p><p>Once the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="scheduler">scheduler</a>
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>.
There are three possible container states: <code>Waiting</code>, <code>Running</code>, and <code>Terminated</code>.</p><p>To check the state of a Pod's containers, you can use
<code>kubectl describe pod &lt;name-of-pod&gt;</code>. The output shows the state for each container
within that Pod.</p><p>Each state has a specific meaning:</p><h3 id="container-state-waiting"><code>Waiting</code></h3><p>If a container is not in either the <code>Running</code> or <code>Terminated</code> state, it is <code>Waiting</code>.
A container in the <code>Waiting</code> state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a>
data.
When you use <code>kubectl</code> to query a Pod with a container that is <code>Waiting</code>, you also see
a Reason field to summarize why the container is in that state.</p><h3 id="container-state-running"><code>Running</code></h3><p>The <code>Running</code> status indicates that a container is executing without issues. If there
was a <code>postStart</code> hook configured, it has already executed and finished. When you use
<code>kubectl</code> to query a Pod with a container that is <code>Running</code>, you also see information
about when the container entered the <code>Running</code> state.</p><h3 id="container-state-terminated"><code>Terminated</code></h3><p>A container in the <code>Terminated</code> state began execution and then either ran to
completion or failed for some reason. When you use <code>kubectl</code> to query a Pod with
a container that is <code>Terminated</code>, you see a reason, an exit code, and the start and
finish time for that container's period of execution.</p><p>If a container has a <code>preStop</code> hook configured, this hook runs before the container enters
the <code>Terminated</code> state.</p><h2 id="container-restarts">How Pods handle problems with containers</h2><p>Kubernetes manages container failures within Pods using a <a href="#restart-policy"><code>restartPolicy</code></a> defined in the Pod <code>spec</code>. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:</p><ol><li><strong>Initial crash</strong>: Kubernetes attempts an immediate restart based on the Pod <code>restartPolicy</code>.</li><li><strong>Repeated crashes</strong>: After the initial crash Kubernetes applies an exponential
backoff delay for subsequent restarts, described in <a href="#restart-policy"><code>restartPolicy</code></a>.
This prevents rapid, repeated restart attempts from overloading the system.</li><li><strong>CrashLoopBackOff state</strong>: This indicates that the backoff delay mechanism is currently
in effect for a given container that is in a crash loop, failing and restarting repeatedly.</li><li><strong>Backoff reset</strong>: If a container runs successfully for a certain duration
(e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash
as the first one.</li></ol><p>In practice, a <code>CrashLoopBackOff</code> is a condition or event that might be seen as output
from the <code>kubectl</code> command, while describing or listing Pods, when a container in the Pod
fails to start properly and then continually tries and fails in a loop.</p><p>In other words, when a container enters the crash loop, Kubernetes applies the
exponential backoff delay mentioned in the <a href="#restart-policy">Container restart policy</a>.
This mechanism prevents a faulty container from overwhelming the system with continuous
failed start attempts.</p><p>The <code>CrashLoopBackOff</code> can be caused by issues like the following:</p><ul><li>Application errors that cause the container to exit.</li><li>Configuration errors, such as incorrect environment variables or missing
configuration files.</li><li>Resource constraints, where the container might not have enough memory or CPU
to start properly.</li><li>Health checks failing if the application doesn't start serving within the
expected time.</li><li>Container liveness probes or startup probes returning a <code>Failure</code> result
as mentioned in the <a href="#container-probes">probes section</a>.</li></ul><p>To investigate the root cause of a <code>CrashLoopBackOff</code> issue, a user can:</p><ol><li><strong>Check logs</strong>: Use <code>kubectl logs &lt;name-of-pod&gt;</code> to check the logs of the container.
This is often the most direct way to diagnose the issue causing the crashes.</li><li><strong>Inspect events</strong>: Use <code>kubectl describe pod &lt;name-of-pod&gt;</code> to see events
for the Pod, which can provide hints about configuration or resource issues.</li><li><strong>Review configuration</strong>: Ensure that the Pod configuration, including
environment variables and mounted volumes, is correct and that all required
external resources are available.</li><li><strong>Check resource limits</strong>: Make sure that the container has enough CPU
and memory allocated. Sometimes, increasing the resources in the Pod definition
can resolve the issue.</li><li><strong>Debug application</strong>: There might exist bugs or misconfigurations in the
application code. Running this container image locally or in a development
environment can help diagnose application specific issues.</li></ol><h3 id="restart-policy">Container restarts</h3><p>When a container in your Pod stops, or experiences failure, Kubernetes can restart it.
A restart isn't always appropriate; for example,
<a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/init-containers/" target="_blank" aria-label="init containers">init containers</a> run only once,
during Pod startup.</p><p>You can configure restarts as a policy that applies to all Pods, or using container-level configuration (for example: when you define a
<a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" aria-label="sidecar container">sidecar container</a>).</p><h4 id="container-restart-resilience">Container restarts and resilience</h4><p>The Kubernetes project recommends following cloud-native principles, including resilient
design that accounts for unannounced or arbitrary restarts. You can achieve this either
by failing the Pod and relying on automatic
<a href="/docs/concepts/workloads/controllers/">replacement</a>, or you can design for container-level resilience.
Either approach helps to ensure that your overall workload remains available despite
partial failure.</p><h4 id="pod-level-container-restart-policy">Pod-level container restart policy</h4><p>The <code>spec</code> of a Pod has a <code>restartPolicy</code> field with possible values Always, OnFailure,
and Never. The default value is Always.</p><p>The <code>restartPolicy</code> for a Pod applies to <a class="glossary-tooltip" title="A container used to run part of a workload. Compare with init container." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-app-container" target="_blank" aria-label="app containers">app containers</a>
in the Pod and to regular <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>.
<a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a>
ignore the Pod-level <code>restartPolicy</code> field: in Kubernetes, a sidecar is defined as an
entry inside <code>initContainers</code> that has its container-level <code>restartPolicy</code> set to <code>Always</code>.
For init containers that exit with an error, the kubelet restarts the init container if
the Pod level <code>restartPolicy</code> is either <code>OnFailure</code> or <code>Always</code>:</p><ul><li><code>Always</code>: Automatically restarts the container after any termination.</li><li><code>OnFailure</code>: Only restarts the container if it exits with an error (non-zero exit status).</li><li><code>Never</code>: Does not automatically restart the terminated container.</li></ul><p>When the kubelet is handling container restarts according to the configured restart
policy, that only applies to restarts that make replacement containers inside the
same Pod and running on the same node. After containers in a Pod exit, the kubelet
restarts them with an exponential backoff delay (10s, 20s, 40s, ), that is capped at
300 seconds (5 minutes). Once a container has executed for 10 minutes without any
problems, the kubelet resets the restart backoff timer for that container.
<a href="/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle">Sidecar containers and Pod lifecycle</a>
explains the behaviour of <code>init containers</code> when specify <code>restartpolicy</code> field on it.</p><h4 id="container-restart-rules">Individual container restart policy and rules</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: ContainerRestartRules"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>If your cluster has the feature gate <code>ContainerRestartRules</code> enabled, you can specify
<code>restartPolicy</code> and <code>restartPolicyRules</code> on <em>individual containers</em> to override the Pod
restart policy. Container restart policy and rules applies to <a class="glossary-tooltip" title="A container used to run part of a workload. Compare with init container." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-app-container" target="_blank" aria-label="app containers">app containers</a>
in the Pod and to regular <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>.</p><p>A Kubernetes-native <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a>
has its container-level <code>restartPolicy</code> set to <code>Always</code>, and does not support <code>restartPolicyRules</code>.</p><p>The container restarts will follow the same exponential backoff as pod restart policy described above.
Supported container restart policies:</p><ul><li><code>Always</code>: Automatically restarts the container after any termination.</li><li><code>OnFailure</code>: Only restarts the container if it exits with an error (non-zero exit status).</li><li><code>Never</code>: Does not automatically restart the terminated container.</li></ul><p>Additionally, <em>individual containers</em> can specify <code>restartPolicyRules</code>. If the <code>restartPolicyRules</code>
field is specified, then container <code>restartPolicy</code> <strong>must</strong> also be specified. The <code>restartPolicyRules</code>
define a list of rules to apply on container exit. Each rule will consist of a condition
and an action. The supported condition is <code>exitCodes</code>, which compares the exit code of the container
with a list of given values. The supported action is <code>Restart</code>, which means the container will be
restarted. The rules will be evaluated in order. On the first match, the action will be applied.
If none of the rules conditions matched, Kubernetes fallback to containers configured
<code>restartPolicy</code>.</p><p>For example, a Pod with OnFailure restart policy that have a <code>try-once</code> container. This allows
Pod to only restart certain containers:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">on</span>-failure-pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>try-once-container   <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This container will run only once because the restartPolicy is Never.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>docker.io/library/busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo "Only running once" &amp;&amp; sleep 10 &amp;&amp; exit 1'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never     <span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">on</span>-failure-container <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This container will be restarted on failure.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>docker.io/library/busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo "Keep restarting" &amp;&amp; sleep 1800 &amp;&amp; exit 1'</span>]<span style="color:#bbb">
</span></span></span></code></pre></div><p>A Pod with Always restart policy with an init container that only execute once. If the init
container fails, the Pod fails. This allows the Pod to fail if the initialization failed,
but also keep running once the initialization succeeds:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fail-pod-if-init-fails<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">initContainers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>init-once     <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This init container will only try once. If it fails, the pod will fail.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>docker.io/library/busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo "Failing initialization" &amp;&amp; sleep 10 &amp;&amp; exit 1'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>main-container<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This container will always be restarted once initialization succeeds.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>docker.io/library/busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'sleep 1800 &amp;&amp; exit 0'</span>]<span style="color:#bbb">
</span></span></span></code></pre></div><p>A Pod with Never restart policy with a container that ignores and restarts on specific exit codes.
This is useful to differentiate between restartable errors and non-restartable errors:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>restart-on-exit-codes<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>restart-on-exit-codes<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>docker.io/library/busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'sleep 60 &amp;&amp; exit 0'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never    <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Container restart policy must be specified if rules are specified</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">restartPolicyRules</span>:<span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Only restart the container if it exits with code 42</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">action</span>:<span style="color:#bbb"> </span>Restart<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">exitCodes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb"> </span>[<span style="color:#666">42</span>]<span style="color:#bbb">
</span></span></span></code></pre></div><p>Restart rules can be used for many more advanced lifecycle management scenarios. Note, restart rules
are affected by the same inconsistencies as the regular restart policy. Kubelet restarts, container
runtime garbage collection, intermitted connectivity issues with the control plane may cause the state
loss and containers may be re-run even when you expect a container not to be restarted.</p><h3 id="reduced-container-restart-delay">Reduced container restart delay</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: ReduceDefaultCrashLoopBackOffDecay"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>With the alpha feature gate <code>ReduceDefaultCrashLoopBackOffDecay</code> enabled,
container start retries across your cluster will be reduced to begin at 1s
(instead of 10s) and increase exponentially by 2x each restart until a maximum
delay of 60s (instead of 300s which is 5 minutes).</p><p>If you use this feature along with the alpha feature
<code>KubeletCrashLoopBackOffMax</code> (described below), individual nodes may have
different maximum delays.</p><h3 id="configurable-container-restart-delay">Configurable container restart delay</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: KubeletCrashLoopBackOffMax"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>With the alpha feature gate <code>KubeletCrashLoopBackOffMax</code> enabled, you can
reconfigure the maximum delay between container start retries from the default
of 300s (5 minutes). This configuration is set per node using kubelet
configuration. In your <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet
configuration</a>, under
<code>crashLoopBackOff</code> set the <code>maxContainerRestartPeriod</code> field between <code>"1s"</code> and
<code>"300s"</code>. As described above in <a href="#restart-policy">Container restart policy</a>,
delays on that node will still start at 10s and increase exponentially by 2x
each restart, but will now be capped at your configured maximum. If the
<code>maxContainerRestartPeriod</code> you configure is less than the default initial value
of 10s, the initial delay will instead be set to the configured maximum.</p><p>See the following kubelet configuration examples:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># container restart delays will start at 10s, increasing</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># 2x each time they are restarted, to a maximum of 100s</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">crashLoopBackOff</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">maxContainerRestartPeriod</span>:<span style="color:#bbb"> </span><span style="color:#b44">"100s"</span><span style="color:#bbb">
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#080;font-style:italic"># delays between container restarts will always be 2s</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">crashLoopBackOff</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">maxContainerRestartPeriod</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2s"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>If you use this feature along with the alpha feature
<code>ReduceDefaultCrashLoopBackOffDecay</code> (described above), your cluster defaults
for initial backoff and maximum backoff will no longer be 10s and 300s, but 1s
and 60s. Per node configuration takes precedence over the defaults set by
<code>ReduceDefaultCrashLoopBackOffDecay</code>, even if this would result in a node having
a longer maximum backoff than other nodes in the cluster.</p><h2 id="pod-conditions">Pod conditions</h2><p>A Pod has a PodStatus, which has an array of
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podcondition-v1-core">PodConditions</a>
through which the Pod has or has not passed. Kubelet manages the following
PodConditions:</p><ul><li><code>PodScheduled</code>: the Pod has been scheduled to a node.</li><li><code>PodReadyToStartContainers</code>: (beta feature; enabled by <a href="#pod-has-network">default</a>) the
Pod sandbox has been successfully created and networking configured.</li><li><code>ContainersReady</code>: all containers in the Pod are ready.</li><li><code>Initialized</code>: all <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>
have completed successfully.</li><li><code>Ready</code>: the Pod is able to serve requests and should be added to the load
balancing pools of all matching Services.</li><li><code>DisruptionTarget</code>: the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).</li><li><code>PodResizePending</code>: a pod resize was requested but cannot be applied. See <a href="/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status">Pod resize status</a>.</li><li><code>PodResizeInProgress</code>: the pod is in the process of resizing. See <a href="/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status">Pod resize status</a>.</li></ul><table><thead><tr><th style="text-align:left">Field name</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>type</code></td><td style="text-align:left">Name of this Pod condition.</td></tr><tr><td style="text-align:left"><code>status</code></td><td style="text-align:left">Indicates whether that condition is applicable, with possible values "<code>True</code>", "<code>False</code>", or "<code>Unknown</code>".</td></tr><tr><td style="text-align:left"><code>lastProbeTime</code></td><td style="text-align:left">Timestamp of when the Pod condition was last probed.</td></tr><tr><td style="text-align:left"><code>lastTransitionTime</code></td><td style="text-align:left">Timestamp for when the Pod last transitioned from one status to another.</td></tr><tr><td style="text-align:left"><code>reason</code></td><td style="text-align:left">Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.</td></tr><tr><td style="text-align:left"><code>message</code></td><td style="text-align:left">Human-readable message indicating details about the last status transition.</td></tr></tbody></table><h3 id="pod-readiness-gate">Pod readiness</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code></div><p>Your application can inject extra feedback or signals into PodStatus:
<em>Pod readiness</em>. To use this, set <code>readinessGates</code> in the Pod's <code>spec</code> to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.</p><p>Readiness gates are determined by the current state of <code>status.condition</code>
fields for the Pod. If Kubernetes cannot find such a condition in the
<code>status.conditions</code> field of a Pod, the status of the condition
is defaulted to "<code>False</code>".</p><p>Here is an example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">readinessGates</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">conditionType</span>:<span style="color:#bbb"> </span><span style="color:#b44">"www.example.com/feature-1"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">conditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Ready                             <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># a built-in PodCondition</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">"False"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">lastTransitionTime</span>:<span style="color:#bbb"> </span>2018-01-01T00:00:00Z<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span><span style="color:#b44">"www.example.com/feature-1"</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># an extra PodCondition</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">"False"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">null</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">lastTransitionTime</span>:<span style="color:#bbb"> </span>2018-01-01T00:00:00Z<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containerStatuses</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">containerID</span>:<span style="color:#bbb"> </span>docker://abcd...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The Pod conditions you add must have names that meet the Kubernetes
<a href="/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">label key format</a>.</p><h3 id="pod-readiness-status">Status for Pod readiness</h3><p>The <code>kubectl patch</code> command does not support patching object status.
To set these <code>status.conditions</code> for the Pod, applications and
<a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" data-toggle="tooltip" data-placement="top" href="/docs/concepts/extend-kubernetes/operator/" target="_blank" aria-label="operators">operators</a> should use
the <code>PATCH</code> action.
You can use a <a href="/docs/reference/using-api/client-libraries/">Kubernetes client library</a> to
write code that sets custom Pod conditions for Pod readiness.</p><p>For a Pod that uses custom conditions, that Pod is evaluated to be ready <strong>only</strong>
when both the following statements apply:</p><ul><li>All containers in the Pod are ready.</li><li>All conditions specified in <code>readinessGates</code> are <code>True</code>.</li></ul><p>When a Pod's containers are Ready but at least one custom condition is missing or
<code>False</code>, the kubelet sets the Pod's <a href="#pod-conditions">condition</a> to <code>ContainersReady</code>.</p><h3 id="pod-has-network">Pod network readiness</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [beta]</code></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>During its early development, this condition was named <code>PodHasNetwork</code>.</div><p>After a Pod gets scheduled on a node, it needs to be admitted by the kubelet and
to have any required storage volumes mounted. Once these phases are complete,
the kubelet works with
a container runtime (using <a class="glossary-tooltip" title="Protocol for communication between the kubelet and the local container runtime." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/cri" target="_blank" aria-label="Container Runtime Interface (CRI)">Container Runtime Interface (CRI)</a>) to set up a
runtime sandbox and configure networking for the Pod. If the
<code>PodReadyToStartContainersCondition</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled
(it is enabled by default for Kubernetes 1.34), the
<code>PodReadyToStartContainers</code> condition will be added to the <code>status.conditions</code> field of a Pod.</p><p>The <code>PodReadyToStartContainers</code> condition is set to <code>False</code> by the Kubelet when it detects a
Pod does not have a runtime sandbox with networking configured. This occurs in
the following scenarios:</p><ul><li>Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for
the Pod using the container runtime.</li><li>Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:<ul><li>the node rebooting, without the Pod getting evicted</li><li>for container runtimes that use virtual machines for isolation, the Pod
sandbox virtual machine rebooting, which then requires creating a new sandbox and
fresh container network configuration.</li></ul></li></ul><p>The <code>PodReadyToStartContainers</code> condition is set to <code>True</code> by the kubelet after the
successful completion of sandbox creation and network configuration for the Pod
by the runtime plugin. The kubelet can start pulling container images and create
containers after <code>PodReadyToStartContainers</code> condition has been set to <code>True</code>.</p><p>For a Pod with init containers, the kubelet sets the <code>Initialized</code> condition to
<code>True</code> after the init containers have successfully completed (which happens
after successful sandbox creation and network configuration by the runtime
plugin). For a Pod without init containers, the kubelet sets the <code>Initialized</code>
condition to <code>True</code> before sandbox creation and network configuration starts.</p><h2 id="container-probes">Container probes</h2><p>A <em>probe</em> is a diagnostic performed periodically by the <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
on a container. To perform a diagnostic, the kubelet either executes code within the container,
or makes a network request.</p><h3 id="probe-check-methods">Check mechanisms</h3><p>There are four different ways to check a container using a probe.
Each probe must define exactly one of these four mechanisms:</p><dl><dt><code>exec</code></dt><dd>Executes a specified command inside the container. The diagnostic
is considered successful if the command exits with a status code of 0.</dd><dt><code>grpc</code></dt><dd>Performs a remote procedure call using <a href="https://grpc.io/">gRPC</a>.
The target should implement
<a href="https://grpc.io/grpc/core/md_doc_health-checking.html">gRPC health checks</a>.
The diagnostic is considered successful if the <code>status</code>
of the response is <code>SERVING</code>.</dd><dt><code>httpGet</code></dt><dd>Performs an HTTP <code>GET</code> request against the Pod's IP
address on a specified port and path. The diagnostic is
considered successful if the response has a status code
greater than or equal to 200 and less than 400.</dd><dt><code>tcpSocket</code></dt><dd>Performs a TCP check against the Pod's IP address on
a specified port. The diagnostic is considered successful if
the port is open. If the remote system (the container) closes
the connection immediately after it opens, this counts as healthy.</dd></dl><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Unlike the other mechanisms, <code>exec</code> probe's implementation involves
the creation/forking of multiple processes each time when executed.
As a result, in case of the clusters having higher pod densities,
lower intervals of <code>initialDelaySeconds</code>, <code>periodSeconds</code>,
configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.
In such scenarios, consider using the alternative probe mechanisms to avoid the overhead.</div><h3 id="probe-outcome">Probe outcome</h3><p>Each probe has one of three results:</p><dl><dt><code>Success</code></dt><dd>The container passed the diagnostic.</dd><dt><code>Failure</code></dt><dd>The container failed the diagnostic.</dd><dt><code>Unknown</code></dt><dd>The diagnostic failed (no action should be taken, and the kubelet
will make further checks).</dd></dl><h3 id="types-of-probe">Types of probe</h3><p>The kubelet can optionally perform and react to three kinds of probes on running
containers:</p><dl><dt><code>livenessProbe</code></dt><dd>Indicates whether the container is running. If
the liveness probe fails, the kubelet kills the container, and the container
is subjected to its <a href="#restart-policy">restart policy</a>. If a container does not
provide a liveness probe, the default state is <code>Success</code>.</dd><dt><code>readinessProbe</code></dt><dd>Indicates whether the container is ready to respond to requests.
If the readiness probe fails, the <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/endpoint-slices/" target="_blank" aria-label="EndpointSlice">EndpointSlice</a>
controller removes the Pod's IP address from the EndpointSlices of all Services that match the Pod.
The default state of readiness before the initial delay is <code>Failure</code>. If a container does
not provide a readiness probe, the default state is <code>Success</code>.</dd><dt><code>startupProbe</code></dt><dd>Indicates whether the application within the container is started.
All other probes are disabled if a startup probe is provided, until it succeeds.
If the startup probe fails, the kubelet kills the container, and the container
is subjected to its <a href="#restart-policy">restart policy</a>. If a container does not
provide a startup probe, the default state is <code>Success</code>.</dd></dl><p>For more information about how to set up a liveness, readiness, or startup probe,
see <a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness, Readiness and Startup Probes</a>.</p><h4 id="when-should-you-use-a-liveness-probe">When should you use a liveness probe?</h4><p>If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod's <code>restartPolicy</code>.</p><p>If you'd like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a <code>restartPolicy</code> of Always or OnFailure.</p><h4 id="when-should-you-use-a-readiness-probe">When should you use a readiness probe?</h4><p>If you'd like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.</p><p>If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.</p><p>If your app has a strict dependency on back-end services, you can implement both
a liveness and a readiness probe. The liveness probe passes when the app itself
is healthy, but the readiness probe additionally checks that each required
back-end service is available. This helps you avoid directing traffic to Pods
that can only respond with error messages.</p><p>If your container needs to work on loading large data, configuration files, or
migrations during startup, you can use a
<a href="#when-should-you-use-a-startup-probe">startup probe</a>. However, if you want to
detect the difference between an app that has failed and an app that is still
processing its startup data, you might prefer a readiness probe.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; when the Pod is deleted, the corresponding endpoint
in the <code>EndpointSlice</code> will update its <a href="/docs/concepts/services-networking/endpoint-slices/#conditions">conditions</a>:
the endpoint <code>ready</code> condition will be set to <code>false</code>, so load balancers
will not use the Pod for regular traffic. See <a href="#pod-termination">Pod termination</a>
for more information about how the kubelet handles Pod deletion.</div><h4 id="when-should-you-use-a-startup-probe">When should you use a startup probe?</h4><p>Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.</p><p>If your container usually starts in more than
\( initialDelaySeconds + failureThreshold \times periodSeconds \), you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
<code>periodSeconds</code> is 10s. You should then set its <code>failureThreshold</code> high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.</p><h2 id="pod-termination">Termination of Pods</h2><p>Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a <code>KILL</code> signal and having no chance to clean up).</p><p>The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> attempts graceful
shutdown.</p><p>Typically, with this graceful termination of the pod, kubelet makes requests to the container runtime
to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal,
with a grace period timeout, to the main process in each container.
The requests to stop the containers are processed by the container runtime asynchronously.
There is no guarantee to the order of processing for these requests.
Many container runtimes respect the <code>STOPSIGNAL</code> value defined in the container image and,
if different, send the container image configured STOPSIGNAL instead of TERM.
Once the grace period has expired, the KILL signal is sent to any remaining
processes, and the Pod is then deleted from the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="API Server">API Server</a>. If the kubelet or the
container runtime's management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.</p><h3 id="pod-termination-stop-signals">Stop Signals</h3><p>The stop signal used to kill the container can be defined in the container image with the <code>STOPSIGNAL</code> instruction.
If no stop signal is defined in the image, the default signal of the container runtime
(SIGTERM for both containerd and CRI-O) would be used to kill the container.</p><h3 id="defining-custom-stop-signals">Defining custom stop signals</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: ContainerStopSignals"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>If the <code>ContainerStopSignals</code> feature gate is enabled, you can configure a custom stop signal
for your containers from the container Lifecycle. We require the Pod's <code>spec.os.name</code> field
to be present as a requirement for defining stop signals in the container lifecycle.
The list of signals that are valid depends on the OS the Pod is scheduled to.
For Pods scheduled to Windows nodes, we only support SIGTERM and SIGKILL as valid signals.</p><p>Here is an example Pod spec defining a custom stop signal:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">os</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>linux<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-container<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>container-image:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">lifecycle</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">stopSignal</span>:<span style="color:#bbb"> </span>SIGUSR1<span style="color:#bbb">
</span></span></span></code></pre></div><p>If a stop signal is defined in the lifecycle, this will override the signal defined in the container image.
If no stop signal is defined in the container spec, the container would fall back to the default behavior.</p><h3 id="pod-termination-flow">Pod Termination Flow</h3><p>Pod termination flow, illustrated with an example:</p><ol><li><p>You use the <code>kubectl</code> tool to manually delete a specific Pod, with the default grace period
(30 seconds).</p></li><li><p>The Pod in the API server is updated with the time beyond which the Pod is considered "dead"
along with the grace period.
If you use <code>kubectl describe</code> to check the Pod you're deleting, that Pod shows up as "Terminating".
On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
shutdown process.</p><ol><li><p>If one of the Pod's containers has defined a <code>preStop</code>
<a href="/docs/concepts/containers/container-lifecycle-hooks/">hook</a> and the <code>terminationGracePeriodSeconds</code>
in the Pod spec is not set to 0, the kubelet runs that hook inside of the container.
The default <code>terminationGracePeriodSeconds</code> setting is 30 seconds.</p><p>If the <code>preStop</code> hook is still running after the grace period expires, the kubelet requests
a small, one-off grace period extension of 2 seconds.<div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If the <code>preStop</code> hook needs longer to complete than the default grace period allows,
you must modify <code>terminationGracePeriodSeconds</code> to suit this.</div></p></li><li><p>The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
container.</p><p>There is <a href="#termination-with-sidecars">special ordering</a> if the Pod has any
<a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" aria-label="sidecar containers">sidecar containers</a> defined.
Otherwise, the containers in the Pod receive the TERM signal at different times and in
an arbitrary order. If the order of shutdowns matters, consider using a <code>preStop</code> hook
to synchronize (or switch to using sidecar containers).</p></li></ol></li><li><p>At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane
evaluates whether to remove that shutting-down Pod from EndpointSlice objects,
where those objects represent a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>
with a configured <a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels/" target="_blank" aria-label="selector">selector</a>.
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank" aria-label="ReplicaSets">ReplicaSets</a> and other workload resources
no longer treat the shutting-down Pod as a valid, in-service replica.</p><p>Pods that shut down slowly should not continue to serve regular traffic and should start
terminating and finish processing open connections. Some applications need to go beyond
finishing open connections and need more graceful termination, for example, session draining
and completion.</p><p>Any endpoints that represent the terminating Pods are not immediately removed from
EndpointSlices, and a status indicating <a href="/docs/concepts/services-networking/endpoint-slices/#conditions">terminating state</a>
is exposed from the EndpointSlice API.
Terminating endpoints always have their <code>ready</code> status as <code>false</code> (for backward compatibility
with versions before 1.26), so load balancers will not use it for regular traffic.</p><p>If traffic draining on terminating Pod is needed, the actual readiness can be checked as a
condition <code>serving</code>. You can find more details on how to implement connections draining in the
tutorial <a href="/docs/tutorials/services/pods-and-endpoint-termination-flow/">Pods And Endpoints Termination Flow</a></p><a id="pod-termination-beyond-grace-period"/></li><li><p>The kubelet ensures the Pod is shut down and terminated</p><ol><li>When the grace period expires, if there is still any container running in the Pod, the
kubelet triggers forcible shutdown.
The container runtime sends <code>SIGKILL</code> to any processes still running in any container in the Pod.
The kubelet also cleans up a hidden <code>pause</code> container if that container runtime uses one.</li><li>The kubelet transitions the Pod into a terminal phase (<code>Failed</code> or <code>Succeeded</code> depending on
the end state of its containers).</li><li>The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period
to 0 (immediate deletion).</li><li>The API server deletes the Pod's API object, which is then no longer visible from any client.</li></ol></li></ol><h3 id="pod-termination-forced">Forced Pod termination</h3><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Forced deletions can be potentially disruptive for some workloads and their Pods.</div><p>By default, all deletes are graceful within 30 seconds. The <code>kubectl delete</code> command supports
the <code>--grace-period=&lt;seconds&gt;</code> option which allows you to override the default and specify your
own value.</p><p>Setting the grace period to <code>0</code> forcibly and immediately deletes the Pod from the API
server. If the Pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.</p><p>Using kubectl, You must specify an additional flag <code>--force</code> along with <code>--grace-period=0</code>
in order to perform force deletions.</p><p>When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Immediate deletion does not wait for confirmation that the running resource has been terminated.
The resource may continue to run on the cluster indefinitely.</div><p>If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
<a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">deleting Pods from a StatefulSet</a>.</p><h3 id="termination-with-sidecars">Pod shutdown and sidecar containers</h3><p>If your Pod includes one or more
<a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>
(init containers with an Always restart policy), the kubelet will delay sending
the TERM signal to these sidecar containers until the last main container has fully terminated.
The sidecar containers will be terminated in the reverse order they are defined in the Pod spec.
This ensures that sidecar containers continue serving the other containers in the Pod until they
are no longer needed.</p><p>This means that slow termination of a main container will also delay the termination of the sidecar containers.
If the grace period expires before the termination process is complete, the Pod may enter <a href="#pod-termination-beyond-grace-period">forced termination</a>.
In this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.</p><p>Similarly, if the Pod has a <code>preStop</code> hook that exceeds the termination grace period, emergency termination may occur.
In general, if you have used <code>preStop</code> hooks to control the termination order without sidecar containers, you can now
remove them and allow the kubelet to manage sidecar termination automatically.</p><h3 id="pod-garbage-collection">Garbage collection of Pods</h3><p>For failed Pods, the API objects remain in the cluster's API until a human or
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> process
explicitly removes them.</p><p>The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up
terminated Pods (with a phase of <code>Succeeded</code> or <code>Failed</code>), when the number of Pods exceeds the
configured threshold (determined by <code>terminated-pod-gc-threshold</code> in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.</p><p>Additionally, PodGC cleans up any Pods which satisfy any of the following conditions:</p><ol><li>are orphan Pods - bound to a node which no longer exists,</li><li>are unscheduled terminating Pods,</li><li>are terminating Pods, bound to a non-ready node tainted with
<a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service"><code>node.kubernetes.io/out-of-service</code></a>.</li></ol><p>Along with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal
phase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.
See <a href="/docs/concepts/workloads/pods/disruptions/#pod-disruption-conditions">Pod disruption conditions</a>
for more details.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to container lifecycle events</a>.</p></li><li><p>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">configuring Liveness, Readiness and Startup Probes</a>.</p></li><li><p>Learn more about <a href="/docs/concepts/containers/container-lifecycle-hooks/">container lifecycle hooks</a>.</p></li><li><p>Learn more about <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>.</p></li><li><p>For detailed information about Pod and container status in the API, see
the API reference documentation covering
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus"><code>status</code></a> for Pod.</p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">ReplicationController</h1><div class="lead">Legacy API for managing workloads that can scale horizontally. Superseded by the Deployment and ReplicaSet APIs.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A <a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> that configures a <a href="/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> is now the recommended way to set up replication.</div><p>A <em>ReplicationController</em> ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.</p><h2 id="how-a-replicationcontroller-works">How a ReplicationController works</h2><p>If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.</p><p>ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in
kubectl commands.</p><p>A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely. A more complex use case is to run several identical replicas of a replicated
service, such as web servers.</p><h2 id="running-an-example-replicationcontroller">Running an example ReplicationController</h2><p>This example ReplicationController config runs three copies of the nginx web server.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/replication.yaml" download="controllers/replication.yaml"><code>controllers/replication.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-replication-yaml&quot;)" title="Copy controllers/replication.yaml to clipboard"/></div><div class="includecode" id="controllers-replication-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ReplicationController<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Run the example job by downloading the example file and then running this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>replicationcontroller/nginx created
</code></pre><p>Check on the status of the ReplicationController using this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe replicationcontrollers/nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><p>Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>Pods Status:    <span style="color:#666">3</span> Running / <span style="color:#666">0</span> Waiting / <span style="color:#666">0</span> Succeeded / <span style="color:#666">0</span> Failed
</span></span></code></pre></div><p>To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl get pods --selector<span style="color:#666">=</span><span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">={</span>.items..metadata.name<span style="color:#666">}</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b8860b">$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><p>Here, the selector is the same as the selector for the ReplicationController (seen in the
<code>kubectl describe</code> output), and in a different form in <code>replication.yaml</code>. The <code>--output=jsonpath</code> option
specifies an expression with the name from each pod in the returned list.</p><h2 id="writing-a-replicationcontroller-manifest">Writing a ReplicationController Manifest</h2><p>As with all other Kubernetes config, a ReplicationController needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.</p><p>When the control plane creates new Pods for a ReplicationController, the <code>.metadata.name</code> of the
ReplicationController is part of the basis for naming those Pods. The name of a ReplicationController must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>For general information about working with configuration files, see <a href="/docs/concepts/overview/working-with-objects/object-management/">object management</a>.</p><p>A ReplicationController also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>. It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href="#pod-selector">pod selector</a>.</p><p>Only a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is allowed, which is the default if not specified.</p><p>For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the <a href="/docs/reference/command-line-tools-reference/kubelet/">Kubelet</a>.</p><h3 id="labels-on-the-replicationcontroller">Labels on the ReplicationController</h3><p>The ReplicationController can itself have labels (<code>.metadata.labels</code>). Typically, you
would set these the same as the <code>.spec.template.metadata.labels</code>; if <code>.metadata.labels</code> is not specified
then it defaults to <code>.spec.template.metadata.labels</code>. However, they are allowed to be
different, and the <code>.metadata.labels</code> do not affect the behavior of the ReplicationController.</p><h3 id="pod-selector">Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selector</a>. A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.</p><p>If specified, the <code>.spec.template.metadata.labels</code> must be equal to the <code>.spec.selector</code>, or it will
be rejected by the API. If <code>.spec.selector</code> is unspecified, it will be defaulted to
<code>.spec.template.metadata.labels</code>.</p><p>Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods. Kubernetes does not stop you
from doing this.</p><p>If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see <a href="#working-with-replicationcontrollers">below</a>).</p><h3 id="multiple-replicas">Multiple Replicas</h3><p>You can specify how many pods should run concurrently by setting <code>.spec.replicas</code> to the number
of pods you would like to have running concurrently. The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id="working-with-replicationcontrollers">Working with ReplicationControllers</h2><h3 id="deleting-a-replicationcontroller-and-its-pods">Deleting a ReplicationController and its Pods</h3><p>To delete a ReplicationController and all its pods, use <a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>. Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself. If this kubectl
command is interrupted, it can be restarted.</p><p>When using the REST API or <a href="/docs/reference/using-api/client-libraries/">client library</a>, you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).</p><h3 id="deleting-only-a-replicationcontroller">Deleting only a ReplicationController</h3><p>You can delete a ReplicationController without affecting any of its pods.</p><p>Using kubectl, specify the <code>--cascade=orphan</code> option to <a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>.</p><p>When using the REST API or <a href="/docs/reference/using-api/client-libraries/">client library</a>, you can delete the ReplicationController object.</p><p>Once the original is deleted, you can create a new ReplicationController to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a <a href="#rolling-updates">rolling update</a>.</p><h3 id="isolating-pods-from-a-replicationcontroller">Isolating pods from a ReplicationController</h3><p>Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).</p><h2 id="common-usage-patterns">Common usage patterns</h2><h3 id="rescheduling">Rescheduling</h3><p>As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).</p><h3 id="scaling">Scaling</h3><p>The ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the <code>replicas</code> field.</p><h3 id="rolling-updates">Rolling updates</h3><p>The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.</p><p>As explained in <a href="https://issue.k8s.io/1353">#1353</a>, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.</p><p>Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.</p><p>The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.</p><h3 id="multiple-release-tracks">Multiple release tracks</h3><p>In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.</p><p>For instance, a service might target all pods with <code>tier in (frontend), environment in (prod)</code>. Now say you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a ReplicationController with <code>replicas</code> set to 9 for the bulk of the replicas, with labels <code>tier=frontend, environment=prod, track=stable</code>, and another ReplicationController with <code>replicas</code> set to 1 for the canary, with labels <code>tier=frontend, environment=prod, track=canary</code>. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.</p><h3 id="using-replicationcontrollers-with-services">Using ReplicationControllers with Services</h3><p>Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.</p><p>A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.</p><h2 id="writing-programs-for-replication">Writing programs for Replication</h2><p>Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the <a href="https://www.rabbitmq.com/tutorials/tutorial-two-python.html">RabbitMQ work queues</a>, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.</p><h2 id="responsibilities-of-the-replicationcontroller">Responsibilities of the ReplicationController</h2><p>The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, <a href="https://issue.k8s.io/620">readiness</a> and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.</p><p>The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in <a href="https://issue.k8s.io/492">#492</a>), which would change its <code>replicas</code> field. We will not add scheduling policies (for example, <a href="https://issue.k8s.io/367#issuecomment-48428019">spreading</a>) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (<a href="https://issue.k8s.io/170">#170</a>).</p><p>The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like <a href="https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1">Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h2 id="api-object">API Object</h2><p>Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
<a href="/docs/reference/generated/kubernetes-api/v1.34/#replicationcontroller-v1-core">ReplicationController API object</a>.</p><h2 id="alternatives-to-replicationcontroller">Alternatives to ReplicationController</h2><h3 id="replicaset">ReplicaSet</h3><p><a href="/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> is the next-generation ReplicationController that supports the new <a href="/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">set-based label selector</a>.
It's mainly used by <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> as a mechanism to orchestrate pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.</p><h3 id="deployment-recommended">Deployment (Recommended)</h3><p><a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality, because they are declarative, server-side, and have additional features.</p><h3 id="bare-pods">Bare Pods</h3><p>Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.</p><h3 id="job">Job</h3><p>Use a <a href="/docs/concepts/workloads/controllers/job/"><code>Job</code></a> instead of a ReplicationController for pods that are expected to terminate on their own
(that is, batch jobs).</p><h3 id="daemonset">DaemonSet</h3><p>Use a <a href="/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Learn about <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>, the replacement
for ReplicationController.</li><li><code>ReplicationController</code> is part of the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/replication-controller-v1/">ReplicationController</a>
object definition to understand the API for replication controllers.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Jobs</h1><div class="lead">Jobs represent one-off tasks that run to completion and then stop.</div><p>A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.
As pods successfully complete, the Job tracks the successful completions. When a specified number
of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up
the Pods it created. Suspending a Job will delete its active Pods until the Job
is resumed again.</p><p>A simple case is to create one Job object in order to reliably run one Pod to completion.
The Job object will start a new Pod if the first Pod fails or is deleted (for example
due to a node hardware failure or a node reboot).</p><p>You can also use a Job to run multiple Pods in parallel.</p><p>If you want to run a Job (either a single task, or several in parallel) on a schedule,
see <a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>.</p><h2 id="running-an-example-job">Running an example Job</h2><p>Here is an example Job config. It computes  to 2000 places and prints it out.
It takes around 10s to complete.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/job.yaml" download="controllers/job.yaml"><code>controllers/job.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-job-yaml&quot;)" title="Copy controllers/job.yaml to clipboard"/></div><div class="includecode" id="controllers-job-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>perl:5.34.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"perl"</span>,<span style="color:#bbb">  </span><span style="color:#b44">"-Mbignum=bpi"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-wle"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"print bpi(2000)"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>You can run the example with this command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>job.batch/pi created
</code></pre><p>Check on the status of the Job with <code>kubectl</code>:</p><ul class="nav nav-tabs" id="check-status-of-job" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#check-status-of-job-0" role="tab" aria-controls="check-status-of-job-0" aria-selected="true">kubectl describe job pi</a></li><li class="nav-item"><a data-toggle="tab" class="nav-link" href="#check-status-of-job-1" role="tab" aria-controls="check-status-of-job-1">kubectl get job pi -o yaml</a></li></ul><div class="tab-content" id="check-status-of-job"><div id="check-status-of-job-0" class="tab-pane show active" role="tabpanel" aria-labelledby="check-status-of-job-0"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>
</span></span><span style="display:flex"><span>Name:           pi
</span></span><span style="display:flex"><span>Namespace:      default
</span></span><span style="display:flex"><span>Selector:       batch.kubernetes.io/controller-uid<span style="color:#666">=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style="display:flex"><span>Labels:         batch.kubernetes.io/controller-uid<span style="color:#666">=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style="display:flex"><span>                batch.kubernetes.io/job-name<span style="color:#666">=</span>pi
</span></span><span style="display:flex"><span>                ...
</span></span><span style="display:flex"><span>Annotations:    batch.kubernetes.io/job-tracking: <span style="color:#b44">""</span>
</span></span><span style="display:flex"><span>Parallelism:    <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>Completions:    <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>Start Time:     Mon, <span style="color:#666">02</span> Dec <span style="color:#666">2019</span> 15:20:11 +0200
</span></span><span style="display:flex"><span>Completed At:   Mon, <span style="color:#666">02</span> Dec <span style="color:#666">2019</span> 15:21:16 +0200
</span></span><span style="display:flex"><span>Duration:       65s
</span></span><span style="display:flex"><span>Pods Statuses:  <span style="color:#666">0</span> Running / <span style="color:#666">1</span> Succeeded / <span style="color:#666">0</span> Failed
</span></span><span style="display:flex"><span>Pod Template:
</span></span><span style="display:flex"><span>  Labels:  batch.kubernetes.io/controller-uid<span style="color:#666">=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style="display:flex"><span>           batch.kubernetes.io/job-name<span style="color:#666">=</span>pi
</span></span><span style="display:flex"><span>  Containers:
</span></span><span style="display:flex"><span>   pi:
</span></span><span style="display:flex"><span>    Image:      perl:5.34.0
</span></span><span style="display:flex"><span>    Port:       &lt;none&gt;
</span></span><span style="display:flex"><span>    Host Port:  &lt;none&gt;
</span></span><span style="display:flex"><span>    Command:
</span></span><span style="display:flex"><span>      perl
</span></span><span style="display:flex"><span>      -Mbignum<span style="color:#666">=</span>bpi
</span></span><span style="display:flex"><span>      -wle
</span></span><span style="display:flex"><span>      print bpi<span style="color:#666">(</span>2000<span style="color:#666">)</span>
</span></span><span style="display:flex"><span>    Environment:  &lt;none&gt;
</span></span><span style="display:flex"><span>    Mounts:       &lt;none&gt;
</span></span><span style="display:flex"><span>  Volumes:        &lt;none&gt;
</span></span><span style="display:flex"><span>Events:
</span></span><span style="display:flex"><span>  Type    Reason            Age   From            Message
</span></span><span style="display:flex"><span>  ----    ------            ----  ----            -------
</span></span><span style="display:flex"><span>  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4
</span></span><span style="display:flex"><span>  Normal  Completed         18s   job-controller  Job completed
</span></span></code></pre></div></p></div><div id="check-status-of-job-1" class="tab-pane" role="tabpanel" aria-labelledby="check-status-of-job-1"><p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>
</span></span><span style="display:flex"><span>apiVersion: batch/v1
</span></span><span style="display:flex"><span>kind: Job
</span></span><span style="display:flex"><span>metadata:
</span></span><span style="display:flex"><span>  annotations: batch.kubernetes.io/job-tracking: <span style="color:#b44">""</span>
</span></span><span style="display:flex"><span>             ...  
</span></span><span style="display:flex"><span>  creationTimestamp: <span style="color:#b44">"2022-11-10T17:53:53Z"</span>
</span></span><span style="display:flex"><span>  generation: <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>  labels:
</span></span><span style="display:flex"><span>    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style="display:flex"><span>    batch.kubernetes.io/job-name: pi
</span></span><span style="display:flex"><span>  name: pi
</span></span><span style="display:flex"><span>  namespace: default
</span></span><span style="display:flex"><span>  resourceVersion: <span style="color:#b44">"4751"</span>
</span></span><span style="display:flex"><span>  uid: 204fb678-040b-497f-9266-35ffa8716d14
</span></span><span style="display:flex"><span>spec:
</span></span><span style="display:flex"><span>  backoffLimit: <span style="color:#666">4</span>
</span></span><span style="display:flex"><span>  completionMode: NonIndexed
</span></span><span style="display:flex"><span>  completions: <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>  parallelism: <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>  selector:
</span></span><span style="display:flex"><span>    matchLabels:
</span></span><span style="display:flex"><span>      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style="display:flex"><span>  suspend: <span style="color:#a2f">false</span>
</span></span><span style="display:flex"><span>  template:
</span></span><span style="display:flex"><span>    metadata:
</span></span><span style="display:flex"><span>      creationTimestamp: null
</span></span><span style="display:flex"><span>      labels:
</span></span><span style="display:flex"><span>        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style="display:flex"><span>        batch.kubernetes.io/job-name: pi
</span></span><span style="display:flex"><span>    spec:
</span></span><span style="display:flex"><span>      containers:
</span></span><span style="display:flex"><span>      - command:
</span></span><span style="display:flex"><span>        - perl
</span></span><span style="display:flex"><span>        - -Mbignum<span style="color:#666">=</span>bpi
</span></span><span style="display:flex"><span>        - -wle
</span></span><span style="display:flex"><span>        - print bpi<span style="color:#666">(</span>2000<span style="color:#666">)</span>
</span></span><span style="display:flex"><span>        image: perl:5.34.0
</span></span><span style="display:flex"><span>        imagePullPolicy: IfNotPresent
</span></span><span style="display:flex"><span>        name: pi
</span></span><span style="display:flex"><span>        resources: <span style="color:#666">{}</span>
</span></span><span style="display:flex"><span>        terminationMessagePath: /dev/termination-log
</span></span><span style="display:flex"><span>        terminationMessagePolicy: File
</span></span><span style="display:flex"><span>      dnsPolicy: ClusterFirst
</span></span><span style="display:flex"><span>      restartPolicy: Never
</span></span><span style="display:flex"><span>      schedulerName: default-scheduler
</span></span><span style="display:flex"><span>      securityContext: <span style="color:#666">{}</span>
</span></span><span style="display:flex"><span>      terminationGracePeriodSeconds: <span style="color:#666">30</span>
</span></span><span style="display:flex"><span>status:
</span></span><span style="display:flex"><span>  active: <span style="color:#666">1</span>
</span></span><span style="display:flex"><span>  ready: <span style="color:#666">0</span>
</span></span><span style="display:flex"><span>  startTime: <span style="color:#b44">"2022-11-10T17:53:57Z"</span>
</span></span><span style="display:flex"><span>  uncountedTerminatedPods: <span style="color:#666">{}</span>
</span></span></code></pre></div></p></div></div><p>To view completed Pods of a Job, use <code>kubectl get pods</code>.</p><p>To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:700">$(</span>kubectl get pods --selector<span style="color:#666">=</span>batch.kubernetes.io/job-name<span style="color:#666">=</span>pi --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">'{.items[*].metadata.name}'</span><span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span><span style="color:#a2f">echo</span> <span style="color:#b8860b">$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>pi-5rwd7
</code></pre><p>Here, the selector is the same as the selector for the Job. The <code>--output=jsonpath</code> option specifies an expression
with the name from each Pod in the returned list.</p><p>View the standard output of one of the pods:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs <span style="color:#b8860b">$pods</span>
</span></span></code></pre></div><p>Another way to view the logs of a Job:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl logs jobs/pi
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</code></pre><h2 id="writing-a-job-spec">Writing a Job spec</h2><p>As with all other Kubernetes config, a Job needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.</p><p>When the control plane creates new Pods for a Job, the <code>.metadata.name</code> of the
Job is part of the basis for naming those Pods. The name of a Job must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.
Even when the name is a DNS subdomain, the name must be no longer than 63
characters.</p><p>A Job also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="job-labels">Job Labels</h3><p>Job labels will have <code>batch.kubernetes.io/</code> prefix for <code>job-name</code> and <code>controller-uid</code>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>.
It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>,
except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see <a href="#pod-selector">pod selector</a>) and an appropriate restart policy.</p><p>Only a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>
equal to <code>Never</code> or <code>OnFailure</code> is allowed.</p><h3 id="pod-selector">Pod selector</h3><p>The <code>.spec.selector</code> field is optional. In almost all cases you should not specify it.
See section <a href="#specifying-your-own-pod-selector">specifying your own pod selector</a>.</p><h3 id="parallel-jobs">Parallel execution for Jobs</h3><p>There are three main types of task suitable to run as a Job:</p><ol><li>Non-parallel Jobs<ul><li>normally, only one Pod is started, unless the Pod fails.</li><li>the Job is complete as soon as its Pod terminates successfully.</li></ul></li><li>Parallel Jobs with a <em>fixed completion count</em>:<ul><li>specify a non-zero positive value for <code>.spec.completions</code>.</li><li>the Job represents the overall task, and is complete when there are <code>.spec.completions</code> successful Pods.</li><li>when using <code>.spec.completionMode="Indexed"</code>, each Pod gets a different index in the range 0 to <code>.spec.completions-1</code>.</li></ul></li><li>Parallel Jobs with a <em>work queue</em>:<ul><li>do not specify <code>.spec.completions</code>, default to <code>.spec.parallelism</code>.</li><li>the Pods must coordinate amongst themselves or an external service to determine
what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.</li><li>each Pod is independently capable of determining whether or not all its peers are done,
and thus that the entire Job is done.</li><li>when <em>any</em> Pod from the Job terminates with success, no new Pods are created.</li><li>once at least one Pod has terminated with success and all Pods are terminated,
then the Job is completed with success.</li><li>once any Pod has exited with success, no other Pod should still be doing any work
for this task or writing any output. They should all be in the process of exiting.</li></ul></li></ol><p>For a <em>non-parallel</em> Job, you can leave both <code>.spec.completions</code> and <code>.spec.parallelism</code> unset.
When both are unset, both are defaulted to 1.</p><p>For a <em>fixed completion count</em> Job, you should set <code>.spec.completions</code> to the number of completions needed.
You can set <code>.spec.parallelism</code>, or leave it unset and it will default to 1.</p><p>For a <em>work queue</em> Job, you must leave <code>.spec.completions</code> unset, and set <code>.spec.parallelism</code> to
a non-negative integer.</p><p>For more information about how to make use of the different types of job,
see the <a href="#job-patterns">job patterns</a> section.</p><h4 id="controlling-parallelism">Controlling parallelism</h4><p>The requested parallelism (<code>.spec.parallelism</code>) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.</p><p>Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:</p><ul><li>For <em>fixed completion count</em> Jobs, the actual number of pods running in parallel will not exceed the number of
remaining completions. Higher values of <code>.spec.parallelism</code> are effectively ignored.</li><li>For <em>work queue</em> Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.</li><li>If the Job <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="Controller">Controller</a> has not had time to react.</li><li>If the Job controller failed to create Pods for any reason (lack of <code>ResourceQuota</code>, lack of permission, etc.),
then there may be fewer pods than requested.</li><li>The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.</li><li>When a Pod is gracefully shut down, it takes time to stop.</li></ul><h3 id="completion-mode">Completion mode</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Jobs with <em>fixed completion count</em> - that is, jobs that have non null
<code>.spec.completions</code> - can have a completion mode that is specified in <code>.spec.completionMode</code>:</p><ul><li><p><code>NonIndexed</code> (default): the Job is considered complete when there have been
<code>.spec.completions</code> successfully completed Pods. In other words, each Pod
completion is homologous to each other. Note that Jobs that have null
<code>.spec.completions</code> are implicitly <code>NonIndexed</code>.</p></li><li><p><code>Indexed</code>: the Pods of a Job get an associated completion index from 0 to
<code>.spec.completions-1</code>. The index is available through four mechanisms:</p><ul><li>The Pod annotation <code>batch.kubernetes.io/job-completion-index</code>.</li><li>The Pod label <code>batch.kubernetes.io/job-completion-index</code> (for v1.28 and later). Note
the feature gate <code>PodIndexLabel</code> must be enabled to use this label, and it is enabled
by default.</li><li>As part of the Pod hostname, following the pattern <code>$(job-name)-$(index)</code>.
When you use an Indexed Job in combination with a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Service">Service</a>, Pods within the Job can use
the deterministic hostnames to address each other via DNS. For more information about
how to configure this, see <a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a>.</li><li>From the containerized task, in the environment variable <code>JOB_COMPLETION_INDEX</code>.</li></ul><p>The Job is considered complete when there is one successfully completed Pod
for each index. For more information about how to use this mode, see
<a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job for Parallel Processing with Static Work Assignment</a>.</p></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Although rare, more than one Pod could be started for the same index (due to various reasons such as node failures,
kubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will
count towards the completion count and update the status of the Job. The other Pods that are running
or completed for the same index will be deleted by the Job controller once they are detected.</div><h2 id="handling-pod-and-container-failures">Handling Pod and container failures</h2><p>A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this
happens, and the <code>.spec.template.spec.restartPolicy = "OnFailure"</code>, then the Pod stays
on the node, but the container is re-run. Therefore, your program needs to handle the case when it is
restarted locally, or else specify <code>.spec.template.spec.restartPolicy = "Never"</code>.
See <a href="/docs/concepts/workloads/pods/pod-lifecycle/#example-states">pod lifecycle</a> for more information on <code>restartPolicy</code>.</p><p>An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
<code>.spec.template.spec.restartPolicy = "Never"</code>. When a Pod fails, then the Job controller
starts a new Pod. This means that your application needs to handle the case when it is restarted in a new
pod. In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.</p><p>By default, each pod failure is counted towards the <code>.spec.backoffLimit</code> limit,
see <a href="#pod-backoff-failure-policy">pod backoff failure policy</a>. However, you can
customize handling of pod failures by setting the Job's <a href="#pod-failure-policy">pod failure policy</a>.</p><p>Additionally, you can choose to count the pod failures independently for each
index of an <a href="#completion-mode">Indexed</a> Job by setting the <code>.spec.backoffLimitPerIndex</code> field
(for more information, see <a href="#backoff-limit-per-index">backoff limit per index</a>).</p><p>Note that even if you specify <code>.spec.parallelism = 1</code> and <code>.spec.completions = 1</code> and
<code>.spec.template.spec.restartPolicy = "Never"</code>, the same program may
sometimes be started twice.</p><p>If you do specify <code>.spec.parallelism</code> and <code>.spec.completions</code> both greater than 1, then there may be
multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.</p><p>If you specify the <code>.spec.podFailurePolicy</code> field, the Job controller does not consider a terminating
Pod (a pod that has a <code>.metadata.deletionTimestamp</code> field set) as a failure until that Pod is
terminal (its <code>.status.phase</code> is <code>Failed</code> or <code>Succeeded</code>). However, the Job controller
creates a replacement Pod as soon as the termination becomes apparent. Once the
pod terminates, the Job controller evaluates <code>.backoffLimit</code> and <code>.podFailurePolicy</code>
for the relevant Job, taking this now-terminated Pod into consideration.</p><p>If either of these requirements is not satisfied, the Job controller counts
a terminating Pod as an immediate failure, even if that Pod later terminates
with <code>phase: "Succeeded"</code>.</p><h3 id="pod-backoff-failure-policy">Pod backoff failure policy</h3><p>There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set <code>.spec.backoffLimit</code> to specify the number of retries before
considering a Job as failed.</p><p>The <code>.spec.backoffLimit</code> is set by default to 6, unless the
<a href="#backoff-limit-per-index">backoff limit per index</a> (only Indexed Job) is specified.
When <code>.spec.backoffLimitPerIndex</code> is specified, then <code>.spec.backoffLimit</code> defaults
to 2147483647 (MaxInt32).</p><p>Failed Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes.</p><p>The number of retries is calculated in two ways:</p><ul><li>The number of Pods with <code>.status.phase = "Failed"</code>.</li><li>When using <code>restartPolicy = "OnFailure"</code>, the number of retries in all the
containers of Pods with <code>.status.phase</code> equal to <code>Pending</code> or <code>Running</code>.</li></ul><p>If either of the calculations reaches the <code>.spec.backoffLimit</code>, the Job is
considered failed.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If your Job has <code>restartPolicy = "OnFailure"</code>, keep in mind that your Pod running the job
will be terminated once the job backoff limit has been reached. This can make debugging
the Job's executable more difficult. We suggest setting
<code>restartPolicy = "Never"</code> when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.</div><h3 id="backoff-limit-per-index">Backoff limit per index</h3><div class="feature-state-notice feature-stable" title="Feature Gate: JobBackoffLimitPerIndex"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>When you run an <a href="#completion-mode">indexed</a> Job, you can choose to handle retries
for pod failures independently for each index. To do so, set the
<code>.spec.backoffLimitPerIndex</code> to specify the maximal number of pod failures
per index.</p><p>When the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the
<code>.status.failedIndexes</code> field. The succeeded indexes, those with a successfully
executed pods, are recorded in the <code>.status.completedIndexes</code> field, regardless of whether you set
the <code>backoffLimitPerIndex</code> field.</p><p>Note that a failing index does not interrupt execution of other indexes.
Once all indexes finish for a Job where you specified a backoff limit per index,
if at least one of those indexes did fail, the Job controller marks the overall
Job as failed, by setting the Failed condition in the status. The Job gets
marked as failed even if some, potentially nearly all, of the indexes were
processed successfully.</p><p>You can additionally limit the maximal number of indexes marked failed by
setting the <code>.spec.maxFailedIndexes</code> field.
When the number of failed indexes exceeds the <code>maxFailedIndexes</code> field, the
Job controller triggers termination of all remaining running Pods for that Job.
Once all pods are terminated, the entire Job is marked failed by the Job
controller, by setting the Failed condition in the Job status.</p><p>Here is an example manifest for a Job that defines a <code>backoffLimitPerIndex</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-backoff-limit-per-index-example.yaml" download="/controllers/job-backoff-limit-per-index-example.yaml"><code>/controllers/job-backoff-limit-per-index-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-job-backoff-limit-per-index-example-yaml&quot;)" title="Copy /controllers/job-backoff-limit-per-index-example.yaml to clipboard"/></div><div class="includecode" id="controllers-job-backoff-limit-per-index-example-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>job-backoff-limit-per-index-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">completionMode</span>:<span style="color:#bbb"> </span>Indexed <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># required for the feature</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">backoffLimitPerIndex</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># maximal number of failures per index</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">maxFailedIndexes</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># maximal number of failed indexes before terminating the Job execution</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># required for the feature</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>python<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">           </span><span style="color:#080;font-style:italic"># The jobs fails as there is at least one failed index</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                           </span><span style="color:#080;font-style:italic"># (all even indexes fail in here), yet all indexes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                           </span><span style="color:#080;font-style:italic"># are executed as maxFailedIndexes is not exceeded.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- python3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- |<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">          import os, sys
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">          print("Hello world")
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">          if int(os.environ.get("JOB_COMPLETION_INDEX")) % 2 == 0:
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            sys.exit(1)</span><span style="color:#bbb">          
</span></span></span></code></pre></div></div></div><p>In the example above, the Job controller allows for one restart for each
of the indexes. When the total number of failed indexes exceeds 5, then
the entire Job is terminated.</p><p>Once the job is finished, the Job status looks as follows:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>kubectl get -o yaml job job-backoff-limit-per-index-example
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">completedIndexes</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span>,<span style="color:#666">3</span>,<span style="color:#666">5</span>,<span style="color:#666">7</span>,<span style="color:#666">9</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">failedIndexes</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span>,<span style="color:#666">2</span>,<span style="color:#666">4</span>,<span style="color:#666">6</span>,<span style="color:#666">8</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">succeeded</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># 1 succeeded pod for each of 5 succeeded indexes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">failed</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># 2 failed pods (1 retry) for each of 5 failed indexes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">conditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">message</span>:<span style="color:#bbb"> </span>Job has failed indexes<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">reason</span>:<span style="color:#bbb"> </span>FailedIndexes<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">"True"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>FailureTarget<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">message</span>:<span style="color:#bbb"> </span>Job has failed indexes<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">reason</span>:<span style="color:#bbb"> </span>FailedIndexes<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">"True"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Failed<span style="color:#bbb">
</span></span></span></code></pre></div><p>The Job controller adds the <code>FailureTarget</code> Job condition to trigger
<a href="#job-termination-and-cleanup">Job termination and cleanup</a>. When all of the
Job Pods are terminated, the Job controller adds the <code>Failed</code> condition
with the same values for <code>reason</code> and <code>message</code> as the <code>FailureTarget</code> Job
condition. For details, see <a href="#termination-of-job-pods">Termination of Job Pods</a>.</p><p>Additionally, you may want to use the per-index backoff along with a
<a href="#pod-failure-policy">pod failure policy</a>. When using
per-index backoff, there is a new <code>FailIndex</code> action available which allows you to
avoid unnecessary retries within an index.</p><h3 id="pod-failure-policy">Pod failure policy</h3><div class="feature-state-notice feature-stable" title="Feature Gate: JobPodFailurePolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>A Pod failure policy, defined with the <code>.spec.podFailurePolicy</code> field, enables
your cluster to handle Pod failures based on the container exit codes and the
Pod conditions.</p><p>In some situations, you may want to have a better control when handling Pod
failures than the control provided by the <a href="#pod-backoff-failure-policy">Pod backoff failure policy</a>,
which is based on the Job's <code>.spec.backoffLimit</code>. These are some examples of use cases:</p><ul><li>To optimize costs of running workloads by avoiding unnecessary Pod restarts,
you can terminate a Job as soon as one of its Pods fails with an exit code
indicating a software bug.</li><li>To guarantee that your Job finishes even if there are disruptions, you can
ignore Pod failures caused by disruptions (such as <a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank" aria-label="preemption">preemption</a>,
<a class="glossary-tooltip" title="API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/api-eviction/" target="_blank" aria-label="API-initiated eviction">API-initiated eviction</a>
or <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="taint">taint</a>-based eviction) so
that they don't count towards the <code>.spec.backoffLimit</code> limit of retries.</li></ul><p>You can configure a Pod failure policy, in the <code>.spec.podFailurePolicy</code> field,
to meet the above use cases. This policy can handle Pod failures based on the
container exit codes and the Pod conditions.</p><p>Here is a manifest for a Job that defines a <code>podFailurePolicy</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-example.yaml" download="/controllers/job-pod-failure-policy-example.yaml"><code>/controllers/job-pod-failure-policy-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-job-pod-failure-policy-example-yaml&quot;)" title="Copy /controllers/job-pod-failure-policy-example.yaml to clipboard"/></div><div class="includecode" id="controllers-job-pod-failure-policy-example-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>job-pod-failure-policy-example<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">12</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>main<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>docker.io/library/bash:5<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"bash"</span>]<span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># example command simulating a bug which triggers the FailJob action</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">args</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- echo "Hello world!" &amp;&amp; sleep 5 &amp;&amp; exit 42<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">6</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podFailurePolicy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">action</span>:<span style="color:#bbb"> </span>FailJob<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">onExitCodes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">containerName</span>:<span style="color:#bbb"> </span>main     <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># optional</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator: In             # one of</span>:<span style="color:#bbb"> </span>In, NotIn<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb"> </span>[<span style="color:#666">42</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">action: Ignore             # one of</span>:<span style="color:#bbb"> </span>Ignore, FailJob, Count<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">onPodConditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>DisruptionTarget  <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># indicates Pod disruption</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In the example above, the first rule of the Pod failure policy specifies that
the Job should be marked failed if the <code>main</code> container fails with the 42 exit
code. The following are the rules for the <code>main</code> container specifically:</p><ul><li>an exit code of 0 means that the container succeeded</li><li>an exit code of 42 means that the <strong>entire Job</strong> failed</li><li>any other exit code represents that the container failed, and hence the entire
Pod. The Pod will be re-created if the total number of restarts is
below <code>backoffLimit</code>. If the <code>backoffLimit</code> is reached the <strong>entire Job</strong> failed.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Because the Pod template specifies a <code>restartPolicy: Never</code>,
the kubelet does not restart the <code>main</code> container in that particular Pod.</div><p>The second rule of the Pod failure policy, specifying the <code>Ignore</code> action for
failed Pods with condition <code>DisruptionTarget</code> excludes Pod disruptions from
being counted towards the <code>.spec.backoffLimit</code> limit of retries.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If the Job failed, either by the Pod failure policy or Pod backoff
failure policy, and the Job is running multiple Pods, Kubernetes terminates all
the Pods in that Job that are still Pending or Running.</div><p>These are some requirements and semantics of the API:</p><ul><li>if you want to use a <code>.spec.podFailurePolicy</code> field for a Job, you must
also define that Job's pod template with <code>.spec.restartPolicy</code> set to <code>Never</code>.</li><li>the Pod failure policy rules you specify under <code>spec.podFailurePolicy.rules</code>
are evaluated in order. Once a rule matches a Pod failure, the remaining rules
are ignored. When no rule matches the Pod failure, the default
handling applies.</li><li>you may want to restrict a rule to a specific container by specifying its name
in<code>spec.podFailurePolicy.rules[*].onExitCodes.containerName</code>. When not specified the rule
applies to all containers. When specified, it should match one the container
or <code>initContainer</code> names in the Pod template.</li><li>you may specify the action taken when a Pod failure policy is matched by
<code>spec.podFailurePolicy.rules[*].action</code>. Possible values are:<ul><li><code>FailJob</code>: use to indicate that the Pod's job should be marked as Failed and
all running Pods should be terminated.</li><li><code>Ignore</code>: use to indicate that the counter towards the <code>.spec.backoffLimit</code>
should not be incremented and a replacement Pod should be created.</li><li><code>Count</code>: use to indicate that the Pod should be handled in the default way.
The counter towards the <code>.spec.backoffLimit</code> should be incremented.</li><li><code>FailIndex</code>: use this action along with <a href="#backoff-limit-per-index">backoff limit per index</a>
to avoid unnecessary retries within the index of a failed pod.</li></ul></li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When you use a <code>podFailurePolicy</code>, the job controller only matches Pods in the
<code>Failed</code> phase. Pods with a deletion timestamp that are not in a terminal phase
(<code>Failed</code> or <code>Succeeded</code>) are considered still terminating. This implies that
terminating pods retain a <a href="#job-tracking-with-finalizers">tracking finalizer</a>
until they reach a terminal phase.
Since Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase
(see: <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">Pod Phase</a>). This
ensures that deleted pods have their finalizers removed by the Job controller.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Starting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates
terminating Pods only once these Pods reach the terminal <code>Failed</code> phase. This behavior is similar
to <code>podReplacementPolicy: Failed</code>. For more information, see <a href="#pod-replacement-policy">Pod replacement policy</a>.</div><p>When you use the <code>podFailurePolicy</code>, and the Job fails due to the pod
matching the rule with the <code>FailJob</code> action, then the Job controller triggers
the Job termination process by adding the <code>FailureTarget</code> condition.
For more details, see <a href="#job-termination-and-cleanup">Job termination and cleanup</a>.</p><h2 id="success-policy">Success policy</h2><p>When creating an Indexed Job, you can define when a Job can be declared as succeeded using a <code>.spec.successPolicy</code>,
based on the pods that succeeded.</p><p>By default, a Job succeeds when the number of succeeded Pods equals <code>.spec.completions</code>.
These are some situations where you might want additional control for declaring a Job succeeded:</p><ul><li>When running simulations with different parameters,
you might not need all the simulations to succeed for the overall Job to be successful.</li><li>When following a leader-worker pattern, only the success of the leader determines the success or
failure of a Job. Examples of this are frameworks like MPI and PyTorch etc.</li></ul><p>You can configure a success policy, in the <code>.spec.successPolicy</code> field,
to meet the above use cases. This policy can handle Job success based on the
succeeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods.
A success policy is defined by rules. Each rule can take one of the following forms:</p><ul><li>When you specify the <code>succeededIndexes</code> only,
once all indexes specified in the <code>succeededIndexes</code> succeed, the job controller marks the Job as succeeded.
The <code>succeededIndexes</code> must be a list of intervals between 0 and <code>.spec.completions-1</code>.</li><li>When you specify the <code>succeededCount</code> only,
once the number of succeeded indexes reaches the <code>succeededCount</code>, the job controller marks the Job as succeeded.</li><li>When you specify both <code>succeededIndexes</code> and <code>succeededCount</code>,
once the number of succeeded indexes from the subset of indexes specified in the <code>succeededIndexes</code> reaches the <code>succeededCount</code>,
the job controller marks the Job as succeeded.</li></ul><p>Note that when you specify multiple rules in the <code>.spec.successPolicy.rules</code>,
the job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.</p><p>Here is a manifest for a Job with <code>successPolicy</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-success-policy.yaml" download="/controllers/job-success-policy.yaml"><code>/controllers/job-success-policy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-job-success-policy-yaml&quot;)" title="Copy /controllers/job-success-policy.yaml to clipboard"/></div><div class="includecode" id="controllers-job-success-policy-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>job-success<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">completionMode</span>:<span style="color:#bbb"> </span>Indexed<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Required for the success policy</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">successPolicy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">rules</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">succeededIndexes</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span>,<span style="color:#666">2-3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">succeededCount</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>main<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>python<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded,</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">                          </span><span style="color:#080;font-style:italic"># the overall Job is a success.</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- python3<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- |<span style="color:#b44;font-style:italic">
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            import os, sys
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            if os.environ.get("JOB_COMPLETION_INDEX") == "2":
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">              sys.exit(0)
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">            else:
</span></span></span><span style="display:flex"><span><span style="color:#b44;font-style:italic">              sys.exit(1)</span><span style="color:#bbb">            
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>In the example above, both <code>succeededIndexes</code> and <code>succeededCount</code> have been specified.
Therefore, the job controller will mark the Job as succeeded and terminate the lingering Pods
when either of the specified indexes, 0, 2, or 3, succeed.
The Job that meets the success policy gets the <code>SuccessCriteriaMet</code> condition with a <code>SuccessPolicy</code> reason.
After the removal of the lingering Pods is issued, the Job gets the <code>Complete</code> condition.</p><p>Note that the <code>succeededIndexes</code> is represented as intervals separated by a hyphen.
The number are listed in represented by the first and last element of the series, separated by a hyphen.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When you specify both a success policy and some terminating policies such as <code>.spec.backoffLimit</code> and <code>.spec.podFailurePolicy</code>,
once the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.</div><h2 id="job-termination-and-cleanup">Job termination and cleanup</h2><p>When a Job completes, no more Pods are created, but the Pods are <a href="#pod-backoff-failure-policy">usually</a> not deleted either.
Keeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status. It is up to the user to delete
old jobs after noting their status. Delete the job with <code>kubectl</code> (e.g. <code>kubectl delete jobs/pi</code> or <code>kubectl delete -f ./job.yaml</code>).
When you delete the job using <code>kubectl</code>, all the pods it created are deleted too.</p><p>By default, a Job will run uninterrupted unless a Pod fails (<code>restartPolicy=Never</code>)
or a Container exits in error (<code>restartPolicy=OnFailure</code>), at which point the Job defers to the
<code>.spec.backoffLimit</code> described above. Once <code>.spec.backoffLimit</code> has been reached the Job will
be marked as failed and any running Pods will be terminated.</p><p>Another way to terminate a Job is by setting an active deadline.
Do this by setting the <code>.spec.activeDeadlineSeconds</code> field of the Job to a number of seconds.
The <code>activeDeadlineSeconds</code> applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches <code>activeDeadlineSeconds</code>, all of its running Pods are terminated and the Job status
will become <code>type: Failed</code> with <code>reason: DeadlineExceeded</code>.</p><p>Note that a Job's <code>.spec.activeDeadlineSeconds</code> takes precedence over its <code>.spec.backoffLimit</code>.
Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once
it reaches the time limit specified by <code>activeDeadlineSeconds</code>, even if the <code>backoffLimit</code> is not yet reached.</p><p>Example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pi-with-timeout<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">activeDeadlineSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>perl:5.34.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"perl"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-Mbignum=bpi"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-wle"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"print bpi(2000)"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span></code></pre></div><p>Note that both the Job spec and the <a href="/docs/concepts/workloads/pods/init-containers/#detailed-behavior">Pod template spec</a>
within the Job have an <code>activeDeadlineSeconds</code> field. Ensure that you set this field at the proper level.</p><p>Keep in mind that the <code>restartPolicy</code> applies to the Pod, and not to the Job itself:
there is no automatic Job restart once the Job status is <code>type: Failed</code>.
That is, the Job termination mechanisms activated with <code>.spec.activeDeadlineSeconds</code>
and <code>.spec.backoffLimit</code> result in a permanent Job failure that requires manual intervention to resolve.</p><h3 id="terminal-job-conditions">Terminal Job conditions</h3><p>A Job has two possible terminal states, each of which has a corresponding Job
condition:</p><ul><li>Succeeded: Job condition <code>Complete</code></li><li>Failed: Job condition <code>Failed</code></li></ul><p>Jobs fail for the following reasons:</p><ul><li>The number of Pod failures exceeded the specified <code>.spec.backoffLimit</code> in the Job
specification. For details, see <a href="#pod-backoff-failure-policy">Pod backoff failure policy</a>.</li><li>The Job runtime exceeded the specified <code>.spec.activeDeadlineSeconds</code></li><li>An indexed Job that used <code>.spec.backoffLimitPerIndex</code> has failed indexes.
For details, see <a href="#backoff-limit-per-index">Backoff limit per index</a>.</li><li>The number of failed indexes in the Job exceeded the specified
<code>spec.maxFailedIndexes</code>. For details, see <a href="#backoff-limit-per-index">Backoff limit per index</a></li><li>A failed Pod matches a rule in <code>.spec.podFailurePolicy</code> that has the <code>FailJob</code>
action. For details about how Pod failure policy rules might affect failure
evaluation, see <a href="#pod-failure-policy">Pod failure policy</a>.</li></ul><p>Jobs succeed for the following reasons:</p><ul><li>The number of succeeded Pods reached the specified <code>.spec.completions</code></li><li>The criteria specified in <code>.spec.successPolicy</code> are met. For details, see
<a href="#success-policy">Success policy</a>.</li></ul><p>In Kubernetes v1.31 and later the Job controller delays the addition of the
terminal conditions,<code>Failed</code> or <code>Complete</code>, until all of the Job Pods are terminated.</p><p>In Kubernetes v1.30 and earlier, the Job controller added the <code>Complete</code> or the
<code>Failed</code> Job terminal conditions as soon as the Job termination process was
triggered and all Pod finalizers were removed. However, some Pods would still
be running or terminating at the moment that the terminal condition was added.</p><p>In Kubernetes v1.31 and later, the controller only adds the Job terminal conditions
<em>after</em> all of the Pods are terminated. You can control this behavior by using the
<code>JobManagedBy</code> and the <code>JobPodReplacementPolicy</code> (both enabled by default)
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>.</p><h3 id="termination-of-job-pods">Termination of Job pods</h3><p>The Job controller adds the <code>FailureTarget</code> condition or the <code>SuccessCriteriaMet</code>
condition to the Job to trigger Pod termination after a Job meets either the
success or failure criteria.</p><p>Factors like <code>terminationGracePeriodSeconds</code> might increase the amount of time
from the moment that the Job controller adds the <code>FailureTarget</code> condition or the
<code>SuccessCriteriaMet</code> condition to the moment that all of the Job Pods terminate
and the Job controller adds a <a href="#terminal-job-conditions">terminal condition</a>
(<code>Failed</code> or <code>Complete</code>).</p><p>You can use the <code>FailureTarget</code> or the <code>SuccessCriteriaMet</code> condition to evaluate
whether the Job has failed or succeeded without having to wait for the controller
to add a terminal condition.</p><p>For example, you might want to decide when to create a replacement Job
that replaces a failed Job. If you replace the failed Job when the <code>FailureTarget</code>
condition appears, your replacement Job runs sooner, but could result in Pods
from the failed and the replacement Job running at the same time, using
extra compute resources.</p><p>Alternatively, if your cluster has limited resource capacity, you could choose to
wait until the <code>Failed</code> condition appears on the Job, which would delay your
replacement Job but would ensure that you conserve resources by waiting
until all of the failed Pods are removed.</p><h2 id="clean-up-finished-jobs-automatically">Clean up finished jobs automatically</h2><p>Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
<a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJobs</a>, the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.</p><h3 id="ttl-mechanism-for-finished-jobs">TTL mechanism for finished Jobs</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>Another way to clean up finished Jobs (either <code>Complete</code> or <code>Failed</code>)
automatically is to use a TTL mechanism provided by a
<a href="/docs/concepts/workloads/controllers/ttlafterfinished/">TTL controller</a> for
finished resources, by specifying the <code>.spec.ttlSecondsAfterFinished</code> field of
the Job.</p><p>When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.</p><p>For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pi-with-ttl<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ttlSecondsAfterFinished</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>perl:5.34.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">"perl"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-Mbignum=bpi"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"-wle"</span>,<span style="color:#bbb"> </span><span style="color:#b44">"print bpi(2000)"</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span></code></pre></div><p>The Job <code>pi-with-ttl</code> will be eligible to be automatically deleted, <code>100</code>
seconds after it finishes.</p><p>If the field is set to <code>0</code>, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won't be cleaned
up by the TTL controller after it finishes.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>It is recommended to set <code>ttlSecondsAfterFinished</code> field because unmanaged jobs
(Jobs that you created directly, and not indirectly through other workload APIs
such as CronJob) have a default deletion
policy of <code>orphanDependents</code> causing Pods created by an unmanaged Job to be left around
after that Job is fully deleted.
Even though the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> eventually
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">garbage collects</a>
the Pods from a deleted Job after they either fail or complete, sometimes those
lingering pods may cause cluster performance degradation or in worst case cause the
cluster to go offline due to this degradation.</p><p>You can use <a href="/docs/concepts/policy/limit-range/">LimitRanges</a> and
<a href="/docs/concepts/policy/resource-quotas/">ResourceQuotas</a> to place a
cap on the amount of resources that a particular namespace can
consume.</p></div><h2 id="job-patterns">Job patterns</h2><p>The Job object can be used to process a set of independent but related <em>work items</em>.
These might be emails to be sent, frames to be rendered, files to be transcoded,
ranges of keys in a NoSQL database to scan, and so on.</p><p>In a complex system, there may be multiple different sets of work items. Here we are just
considering one set of work items that the user wants to manage together  a <em>batch job</em>.</p><p>There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:</p><ul><li>One Job object for each work item, versus a single Job object for all work items.
One Job per work item creates some overhead for the user and for the system to manage
large numbers of Job objects.
A single Job for all work items is better for large numbers of items.</li><li>Number of Pods created equals number of work items, versus each Pod can process multiple work items.
When the number of Pods equals the number of work items, the Pods typically
requires less modification to existing code and containers. Having each Pod
process multiple work items is better for large numbers of items.</li><li>Several approaches use a work queue. This requires running a queue service,
and modifications to the existing program or container to make it use the work queue.
Other approaches are easier to adapt to an existing containerised application.</li><li>When the Job is associated with a
<a href="/docs/concepts/services-networking/service/#headless-services">headless Service</a>,
you can enable the Pods within a Job to communicate with each other to
collaborate in a computation.</li></ul><p>The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.</p><table><thead><tr><th>Pattern</th><th style="text-align:center">Single Job object</th><th style="text-align:center">Fewer pods than work items?</th><th style="text-align:center">Use app unmodified?</th></tr></thead><tbody><tr><td><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Queue with Pod Per Work Item</a></td><td style="text-align:center"></td><td style="text-align:center"/><td style="text-align:center">sometimes</td></tr><tr><td><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Queue with Variable Pod Count</a></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"/></tr><tr><td><a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job with Static Work Assignment</a></td><td style="text-align:center"></td><td style="text-align:center"/><td style="text-align:center"></td></tr><tr><td><a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a></td><td style="text-align:center"></td><td style="text-align:center">sometimes</td><td style="text-align:center">sometimes</td></tr><tr><td><a href="/docs/tasks/job/parallel-processing-expansion/">Job Template Expansion</a></td><td style="text-align:center"/><td style="text-align:center"/><td style="text-align:center"></td></tr></tbody></table><p>When you specify completions with <code>.spec.completions</code>, each Pod created by the Job controller
has an identical <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>spec</code></a>.
This means that all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables. These patterns
are different ways to arrange for pods to work on different things.</p><p>This table shows the required settings for <code>.spec.parallelism</code> and <code>.spec.completions</code> for each of the patterns.
Here, <code>W</code> is the number of work items.</p><table><thead><tr><th>Pattern</th><th style="text-align:center"><code>.spec.completions</code></th><th style="text-align:center"><code>.spec.parallelism</code></th></tr></thead><tbody><tr><td><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Queue with Pod Per Work Item</a></td><td style="text-align:center">W</td><td style="text-align:center">any</td></tr><tr><td><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Queue with Variable Pod Count</a></td><td style="text-align:center">null</td><td style="text-align:center">any</td></tr><tr><td><a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job with Static Work Assignment</a></td><td style="text-align:center">W</td><td style="text-align:center">any</td></tr><tr><td><a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a></td><td style="text-align:center">W</td><td style="text-align:center">W</td></tr><tr><td><a href="/docs/tasks/job/parallel-processing-expansion/">Job Template Expansion</a></td><td style="text-align:center">1</td><td style="text-align:center">should be 1</td></tr></tbody></table><h2 id="advanced-usage">Advanced usage</h2><h3 id="suspending-a-job">Suspending a Job</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>When a Job is created, the Job controller will immediately begin creating Pods
to satisfy the Job's requirements and will continue to do so until the Job is
complete. However, you may want to temporarily suspend a Job's execution and
resume it later, or start Jobs in suspended state and have a custom controller
decide later when to start them.</p><p>To suspend a Job, you can update the <code>.spec.suspend</code> field of
the Job to true; later, when you want to resume it again, update it to false.
Creating a Job with <code>.spec.suspend</code> set to true will create it in the suspended
state.</p><p>When a Job is resumed from suspension, its <code>.status.startTime</code> field will be
reset to the current time. This means that the <code>.spec.activeDeadlineSeconds</code>
timer will be stopped and reset when a Job is suspended and resumed.</p><p>When you suspend a Job, any running Pods that don't have a status of <code>Completed</code>
will be <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">terminated</a>
with a SIGTERM signal. The Pod's graceful termination period will be honored and
your Pod must handle this signal in this period. This may involve saving
progress for later or undoing changes. Pods terminated this way will not count
towards the Job's <code>completions</code> count.</p><p>An example Job definition in the suspended state can be like so:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get job myjob -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myjob<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">suspend</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>You can also toggle Job suspension by patching the Job using the command line.</p><p>Suspend an active Job:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch job/myjob --type<span style="color:#666">=</span>strategic --patch <span style="color:#b44">'{"spec":{"suspend":true}}'</span>
</span></span></code></pre></div><p>Resume a suspended Job:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch job/myjob --type<span style="color:#666">=</span>strategic --patch <span style="color:#b44">'{"spec":{"suspend":false}}'</span>
</span></span></code></pre></div><p>The Job's status can be used to determine if a Job is suspended or has been
suspended in the past:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get jobs/myjob -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># .metadata and .spec omitted</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">conditions</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2021-02-05T13:14:33Z"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">lastTransitionTime</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2021-02-05T13:14:33Z"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">"True"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>Suspended<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">startTime</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2021-02-05T13:13:48Z"</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The Job condition of type "Suspended" with status "True" means the Job is
suspended; the <code>lastTransitionTime</code> field can be used to determine how long the
Job has been suspended for. If the status of that condition is "False", then the
Job was previously suspended and is now running. If such a condition does not
exist in the Job's status, the Job has never been stopped.</p><p>Events are also created when the Job is suspended and resumed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe jobs/myjob
</span></span></code></pre></div><pre tabindex="0"><code>Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
</code></pre><p>The last four events, particularly the "Suspended" and "Resumed" events, are
directly a result of toggling the <code>.spec.suspend</code> field. In the time between
these two events, we see that no Pods were created, but Pod creation restarted
as soon as the Job was resumed.</p><h3 id="mutable-scheduling-directives">Mutable Scheduling Directives</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>In most cases, a parallel job will want the pods to run with constraints,
like all in the same zone, or all either on GPU model x or y but not a mix of both.</p><p>The <a href="#suspending-a-job">suspend</a> field is the first step towards achieving those semantics. Suspend allows a
custom queue controller to decide when a job should start; However, once a job is unsuspended,
a custom queue controller has no influence on where the pods of a job will actually land.</p><p>This feature allows updating a Job's scheduling directives before it starts, which gives custom queue
controllers the ability to influence pod placement while at the same time offloading actual
pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never
been unsuspended before.</p><p>The fields in a Job's pod template that can be updated are node affinity, node selector,
tolerations, labels, annotations and <a href="/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">scheduling gates</a>.</p><h3 id="specifying-your-own-pod-selector">Specifying your own Pod selector</h3><p>Normally, when you create a Job object, you do not specify <code>.spec.selector</code>.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.</p><p>However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the <code>.spec.selector</code> of the Job.</p><p>Be very careful when doing this. If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion. If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too. Kubernetes will not stop you from making a mistake when
specifying <code>.spec.selector</code>.</p><p>Here is an example of a case when you might want to use this feature.</p><p>Say Job <code>old</code> is already running. You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job <code>old</code> but <em>leave its pods
running</em>, using <code>kubectl delete jobs/old --cascade=orphan</code>.
Before deleting it, you make a note of what selector it uses:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get job old -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>old<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">batch.kubernetes.io/controller-uid</span>:<span style="color:#bbb"> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>Then you create a new Job with name <code>new</code> and you explicitly specify the same selector.
Since the existing Pods have label <code>batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>,
they are controlled by Job <code>new</code> as well.</p><p>You need to specify <code>manualSelector: true</code> in the new Job since you are not using
the selector that the system normally generates for you automatically.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">manualSelector</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">batch.kubernetes.io/controller-uid</span>:<span style="color:#bbb"> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>The new Job itself will have a different uid from <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code>. Setting
<code>manualSelector: true</code> tells the system that you know what you are doing and to allow this
mismatch.</p><h3 id="job-tracking-with-finalizers">Job tracking with finalizers</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>The control plane keeps track of the Pods that belong to any Job and notices if
any such Pod is removed from the API server. To do that, the Job controller
creates Pods with the finalizer <code>batch.kubernetes.io/job-tracking</code>. The
controller removes the finalizer only after the Pod has been accounted for in
the Job status, allowing the Pod to be removed by other controllers or users.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>See <a href="/docs/tasks/debug/debug-application/debug-pods/">My pod stays terminating</a> if you
observe that pods from a Job are stuck with the tracking finalizer.</div><h3 id="elastic-indexed-jobs">Elastic Indexed Jobs</h3><div class="feature-state-notice feature-stable" title="Feature Gate: ElasticIndexedJob"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>You can scale Indexed Jobs up or down by mutating both <code>.spec.parallelism</code>
and <code>.spec.completions</code> together such that <code>.spec.parallelism == .spec.completions</code>.
When scaling down, Kubernetes removes the Pods with higher indexes.</p><p>Use cases for elastic Indexed Jobs include batch workloads which require
scaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.</p><h3 id="pod-replacement-policy">Delayed creation of replacement pods</h3><div class="feature-state-notice feature-stable" title="Feature Gate: JobPodReplacementPolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>By default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp).
This means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job
can be greater than <code>parallelism</code> or greater than one Pod per index (if you are using an Indexed Job).</p><p>You may choose to create replacement Pods only when the terminating Pod is fully terminal (has <code>status.phase: Failed</code>).
To do this, set the <code>.spec.podReplacementPolicy: Failed</code>.
The default replacement policy depends on whether the Job has a <code>podFailurePolicy</code> set.
With no Pod failure policy defined for a Job, omitting the <code>podReplacementPolicy</code> field selects the
<code>TerminatingOrFailed</code> replacement policy:
the control plane creates replacement Pods immediately upon Pod deletion
(as soon as the control plane sees that a Pod for this Job has <code>deletionTimestamp</code> set).
For Jobs with a Pod failure policy set, the default <code>podReplacementPolicy</code> is <code>Failed</code>, and no other
value is permitted.
See <a href="#pod-failure-policy">Pod failure policy</a> to learn more about Pod failure policies for Jobs.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>new<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">podReplacementPolicy</span>:<span style="color:#bbb"> </span>Failed<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></span></span></code></pre></div><p>Provided your cluster has the feature gate enabled, you can inspect the <code>.status.terminating</code> field of a Job.
The value of the field is the number of Pods owned by the Job that are currently terminating.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get jobs/myjob -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#080;font-style:italic"># .metadata and .spec omitted</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">status</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">terminating</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># three Pods are terminating and have not yet reached the Failed phase</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="delegation-of-managing-a-job-object-to-external-controller">Delegation of managing a Job object to external controller</h3><div class="feature-state-notice feature-beta" title="Feature Gate: JobManagedBy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [beta]</code> (enabled by default: true)</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You can only set the <code>managedBy</code> field on Jobs if you enable the <code>JobManagedBy</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
(enabled by default).</div><p>This feature allows you to disable the built-in Job controller, for a specific
Job, and delegate reconciliation of the Job to an external controller.</p><p>You indicate the controller that reconciles the Job by setting a custom value
for the <code>spec.managedBy</code> field - any value
other than <code>kubernetes.io/job-controller</code>. The value of the field is immutable.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>When using this feature, make sure the controller indicated by the field is
installed, otherwise the Job may not be reconciled at all.</div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>When developing an external Job controller be aware that your controller needs
to operate in a fashion conformant with the definitions of the API spec and
status fields of the Job object.</p><p>Please review these in detail in the <a href="/docs/reference/kubernetes-api/workload-resources/job-v1/">Job API</a>.
We also recommend that you run the e2e conformance tests for the Job object to
verify your implementation.</p><p>Finally, when developing an external Job controller make sure it does not use the
<code>batch.kubernetes.io/job-tracking</code> finalizer, reserved for the built-in controller.</p></div><div class="alert alert-danger" role="alert"><h4 class="alert-heading">Warning:</h4>If you are considering to disable the <code>JobManagedBy</code> feature gate, or to
downgrade the cluster to a version without the feature gate enabled, check if
there are jobs with a custom value of the <code>spec.managedBy</code> field. If there
are such jobs, there is a risk that they might be reconciled by two controllers
after the operation: the built-in Job controller and the external controller
indicated by the field value.</div><h2 id="alternatives">Alternatives</h2><h3 id="bare-pods">Bare Pods</h3><p>When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted. However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.</p><h3 id="replication-controller">Replication Controller</h3><p>Jobs are complementary to <a href="/docs/concepts/workloads/controllers/replicationcontroller/">Replication Controllers</a>.
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).</p><p>As discussed in <a href="/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle</a>, <code>Job</code> is <em>only</em> appropriate
for pods with <code>RestartPolicy</code> equal to <code>OnFailure</code> or <code>Never</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>.</div><h3 id="single-job-starts-controller-pod">Single Job starts controller Pod</h3><p>Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods. This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.</p><p>An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Read about different ways of running Jobs:<ul><li><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Coarse Parallel Processing Using a Work Queue</a></li><li><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Fine Parallel Processing Using a Work Queue</a></li><li>Use an <a href="/docs/tasks/job/indexed-parallel-processing-static/">indexed Job for parallel processing with static work assignment</a></li><li>Create multiple Jobs based on a template: <a href="/docs/tasks/job/parallel-processing-expansion/">Parallel Processing using Expansions</a></li></ul></li><li>Follow the links within <a href="#clean-up-finished-jobs-automatically">Clean up finished jobs automatically</a>
to learn more about how your cluster can clean up completed and / or failed tasks.</li><li><code>Job</code> is part of the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/job-v1/">Job</a>
object definition to understand the API for jobs.</li><li>Read about <a href="/docs/concepts/workloads/controllers/cron-jobs/"><code>CronJob</code></a>, which you
can use to define a series of Jobs that will run based on a schedule, similar to
the UNIX tool <code>cron</code>.</li><li>Practice how to configure handling of retriable and non-retriable pod failures
using <code>podFailurePolicy</code>, based on the step-by-step <a href="/docs/tasks/job/pod-failure-policy/">examples</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Disruptions</h1><p>This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of disruptions can happen to Pods.</p><p>It is also for cluster administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.</p><h2 id="voluntary-and-involuntary-disruptions">Voluntary and involuntary disruptions</h2><p>Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.</p><p>We call these unavoidable cases <em>involuntary disruptions</em> to
an application. Examples are:</p><ul><li>a hardware failure of the physical machine backing the node</li><li>cluster administrator deletes VM (instance) by mistake</li><li>cloud provider or hypervisor failure makes VM disappear</li><li>a kernel panic</li><li>the node disappears from the cluster due to cluster network partition</li><li>eviction of a pod due to the node being <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">out-of-resources</a>.</li></ul><p>Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.</p><p>We call other cases <em>voluntary disruptions</em>. These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator. Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment's pod template causing a restart</li><li>directly deleting a pod (e.g. by accident)</li></ul><p>Cluster administrator actions include:</p><ul><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Draining a node</a> for repair or upgrade.</li><li>Draining a node from a cluster to scale the cluster down (learn about
<a href="/docs/concepts/cluster-administration/node-autoscaling/">Node Autoscaling</a>).</li><li>Removing a pod from a node to permit something else to fit on that node.</li></ul><p>These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.</p><p>Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.</div><h2 id="dealing-with-disruptions">Dealing with disruptions</h2><p>Here are some ways to mitigate involuntary disruptions:</p><ul><li>Ensure your pod <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">requests the resources</a> it needs.</li><li>Replicate your application if you need higher availability. (Learn about running replicated
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">stateless</a>
and <a href="/docs/tasks/run-application/run-replicated-stateful-application/">stateful</a> applications.)</li><li>For even higher availability when running replicated applications,
spread applications across racks (using
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">anti-affinity</a>)
or across zones (if using a
<a href="/docs/setup/multiple-zones">multi-zone cluster</a>.)</li></ul><p>The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are
no automated voluntary disruptions (only user-triggered ones). However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect. Certain configuration options, such as
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">using PriorityClasses</a>
in your pod spec can also cause voluntary (and involuntary) disruptions.</p><h2 id="pod-disruption-budgets">Pod disruption budgets</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.</p><p>As an application owner, you can create a PodDisruptionBudget (PDB) for each application.
A PDB limits the number of Pods of a replicated application that are down simultaneously from
voluntary disruptions. For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.</p><p>Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the <a href="/docs/tasks/administer-cluster/safely-drain-node/#eviction-api">Eviction API</a>
instead of directly deleting pods or deployments.</p><p>For example, the <code>kubectl drain</code> subcommand lets you mark a node as going out of
service. When you run <code>kubectl drain</code>, the tool tries to evict all of the Pods on
the Node you're taking out of service. The eviction request that <code>kubectl</code> submits on
your behalf may be temporarily rejected, so the tool periodically retries all failed
requests until all Pods on the target node are terminated, or until a configurable timeout
is reached.</p><p>A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have. For example, a Deployment which has a <code>.spec.replicas: 5</code> is
supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.</p><p>The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application's controller (deployment, stateful-set, etc).</p><p>The "intended" number of pods is computed from the <code>.spec.replicas</code> of the workload resource
that is managing those pods. The control plane discovers the owning workload resource by
examining the <code>.metadata.ownerReferences</code> of the Pod.</p><p><a href="#voluntary-and-involuntary-disruptions">Involuntary disruptions</a> cannot be prevented by PDBs; however they
do count against the budget.</p><p>Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but workload resources (such as Deployment and StatefulSet)
are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures
during application updates is configured in the spec for the specific workload resource.</p><p>It is recommended to set <code>AlwaysAllow</code> <a href="/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a>
to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.
The default behavior is to wait for the application pods to become <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>
before the drain can proceed.</p><p>When a pod is evicted using the eviction API, it is gracefully
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">terminated</a>, honoring the
<code>terminationGracePeriodSeconds</code> setting in its <a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec</a>.</p><h2 id="pdb-example">PodDisruptionBudget example</h2><p>Consider a cluster with 3 nodes, <code>node-1</code> through <code>node-3</code>.
The cluster is running several applications. One of them has 3 replicas initially called
<code>pod-a</code>, <code>pod-b</code>, and <code>pod-c</code>. Another, unrelated pod without a PDB, called <code>pod-x</code>, is also shown.
Initially, the pods are laid out as follows:</p><table><thead><tr><th style="text-align:center">node-1</th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th></tr></thead><tbody><tr><td style="text-align:center">pod-a <em>available</em></td><td style="text-align:center">pod-b <em>available</em></td><td style="text-align:center">pod-c <em>available</em></td></tr><tr><td style="text-align:center">pod-x <em>available</em></td><td style="text-align:center"/><td style="text-align:center"/></tr></tbody></table><p>All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.</p><p>For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain <code>node-1</code> using the <code>kubectl drain</code> command.
That tool tries to evict <code>pod-a</code> and <code>pod-x</code>. This succeeds immediately.
Both pods go into the <code>terminating</code> state at the same time.
This puts the cluster in this state:</p><table><thead><tr><th style="text-align:center">node-1 <em>draining</em></th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th></tr></thead><tbody><tr><td style="text-align:center">pod-a <em>terminating</em></td><td style="text-align:center">pod-b <em>available</em></td><td style="text-align:center">pod-c <em>available</em></td></tr><tr><td style="text-align:center">pod-x <em>terminating</em></td><td style="text-align:center"/><td style="text-align:center"/></tr></tbody></table><p>The deployment notices that one of the pods is terminating, so it creates a replacement
called <code>pod-d</code>. Since <code>node-1</code> is cordoned, it lands on another node. Something has
also created <code>pod-y</code> as a replacement for <code>pod-x</code>.</p><p>(Note: for a StatefulSet, <code>pod-a</code>, which would be called something like <code>pod-0</code>, would need
to terminate completely before its replacement, which is also called <code>pod-0</code> but has a
different UID, could be created. Otherwise, the example applies to a StatefulSet as well.)</p><p>Now the cluster is in this state:</p><table><thead><tr><th style="text-align:center">node-1 <em>draining</em></th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th></tr></thead><tbody><tr><td style="text-align:center">pod-a <em>terminating</em></td><td style="text-align:center">pod-b <em>available</em></td><td style="text-align:center">pod-c <em>available</em></td></tr><tr><td style="text-align:center">pod-x <em>terminating</em></td><td style="text-align:center">pod-d <em>starting</em></td><td style="text-align:center">pod-y</td></tr></tbody></table><p>At some point, the pods terminate, and the cluster looks like this:</p><table><thead><tr><th style="text-align:center">node-1 <em>drained</em></th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th></tr></thead><tbody><tr><td style="text-align:center"/><td style="text-align:center">pod-b <em>available</em></td><td style="text-align:center">pod-c <em>available</em></td></tr><tr><td style="text-align:center"/><td style="text-align:center">pod-d <em>starting</em></td><td style="text-align:center">pod-y</td></tr></tbody></table><p>At this point, if an impatient cluster administrator tries to drain <code>node-2</code> or
<code>node-3</code>, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2. After some time passes, <code>pod-d</code> becomes available.</p><p>The cluster state now looks like this:</p><table><thead><tr><th style="text-align:center">node-1 <em>drained</em></th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th></tr></thead><tbody><tr><td style="text-align:center"/><td style="text-align:center">pod-b <em>available</em></td><td style="text-align:center">pod-c <em>available</em></td></tr><tr><td style="text-align:center"/><td style="text-align:center">pod-d <em>available</em></td><td style="text-align:center">pod-y</td></tr></tbody></table><p>Now, the cluster administrator tries to drain <code>node-2</code>.
The drain command will try to evict the two pods in some order, say
<code>pod-b</code> first and then <code>pod-d</code>. It will succeed at evicting <code>pod-b</code>.
But, when it tries to evict <code>pod-d</code>, it will be refused because that would leave only
one pod available for the deployment.</p><p>The deployment creates a replacement for <code>pod-b</code> called <code>pod-e</code>.
Because there are not enough resources in the cluster to schedule
<code>pod-e</code> the drain will again block. The cluster may end up in this
state:</p><table><thead><tr><th style="text-align:center">node-1 <em>drained</em></th><th style="text-align:center">node-2</th><th style="text-align:center">node-3</th><th style="text-align:center"><em>no node</em></th></tr></thead><tbody><tr><td style="text-align:center"/><td style="text-align:center">pod-b <em>terminating</em></td><td style="text-align:center">pod-c <em>available</em></td><td style="text-align:center">pod-e <em>pending</em></td></tr><tr><td style="text-align:center"/><td style="text-align:center">pod-d <em>available</em></td><td style="text-align:center">pod-y</td><td style="text-align:center"/></tr></tbody></table><p>At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.</p><p>You can see how Kubernetes varies the rate at which disruptions
can happen, according to:</p><ul><li>how many replicas an application needs</li><li>how long it takes to gracefully shutdown an instance</li><li>how long it takes a new instance to start up</li><li>the type of controller</li><li>the cluster's resource capacity</li></ul><h2 id="pod-disruption-conditions">Pod disruption conditions</h2><div class="feature-state-notice feature-stable" title="Feature Gate: PodDisruptionConditions"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>A dedicated Pod <code>DisruptionTarget</code> <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions">condition</a>
is added to indicate
that the Pod is about to be deleted due to a <a class="glossary-tooltip" title="An event that leads to Pod(s) going out of service" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/disruptions/" target="_blank" aria-label="disruption">disruption</a>.
The <code>reason</code> field of the condition additionally
indicates one of the following reasons for the Pod termination:</p><dl><dt><code>PreemptionByScheduler</code></dt><dd>Pod is due to be <a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank" aria-label="preempted">preempted</a> by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod priority preemption</a>.</dd><dt><code>DeletionByTaintManager</code></dt><dd>Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within <code>kube-controller-manager</code>) due to a <code>NoExecute</code> taint that the Pod does not tolerate; see <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="taint">taint</a>-based evictions.</dd><dt><code>EvictionByEvictionAPI</code></dt><dd>Pod has been marked for <a class="glossary-tooltip" title="API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/api-eviction/" target="_blank" aria-label="eviction using the Kubernetes API">eviction using the Kubernetes API</a> .</dd><dt><code>DeletionByPodGC</code></dt><dd>Pod, that is bound to a no longer existing Node, is due to be deleted by <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">Pod garbage collection</a>.</dd><dt><code>TerminationByKubelet</code></dt><dd>Pod has been terminated by the kubelet, because of either <a class="glossary-tooltip" title="Node-pressure eviction is the process by which the kubelet proactively fails pods to reclaim resources on nodes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank" aria-label="node pressure eviction">node pressure eviction</a>,
the <a href="/docs/concepts/architecture/nodes/#graceful-node-shutdown">graceful node shutdown</a>,
or preemption for <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">system critical pods</a>.</dd></dl><p>In all other disruption scenarios, like eviction due to exceeding
<a href="/docs/concepts/configuration/manage-resources-containers/">Pod container limits</a>,
Pods don't receive the <code>DisruptionTarget</code> condition because the disruptions were
probably caused by the Pod and would reoccur on retry.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A Pod disruption might be interrupted. The control plane might re-attempt to
continue the disruption of the same Pod, but it is not guaranteed. As a result,
the <code>DisruptionTarget</code> condition might be added to a Pod, but that Pod might then not actually be
deleted. In such a situation, after some time, the
Pod disruption condition will be cleared.</div><p>Along with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal
phase (see also <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">Pod garbage collection</a>).</p><p>When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's
<a href="/docs/concepts/workloads/controllers/job/#pod-failure-policy">Pod failure policy</a>.</p><h2 id="separating-cluster-owner-and-application-owner-roles">Separating Cluster Owner and Application Owner Roles</h2><p>Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other. This separation of responsibilities
may make sense in these scenarios:</p><ul><li>when there are many application teams sharing a Kubernetes cluster, and
there is natural specialization of roles</li><li>when third-party tools or services are used to automate cluster management</li></ul><p>Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.</p><p>If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.</p><h2 id="how-to-perform-disruptive-actions-on-your-cluster">How to perform Disruptive Actions on your Cluster</h2><p>If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:</p><ul><li>Accept downtime during the upgrade.</li><li>Failover to another complete replica cluster.<ul><li>No downtime, but may be costly both for the duplicated nodes
and for human effort to orchestrate the switchover.</li></ul></li><li>Write disruption tolerant applications and use PDBs.<ul><li>No downtime.</li><li>Minimal resource duplication.</li><li>Allows more automation of cluster administration.</li><li>Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
disruptions largely overlaps with work to support autoscaling and tolerating
involuntary disruptions.</li></ul></li></ul><h2 id="what-s-next">What's next</h2><ul><li><p>Follow steps to protect your application by <a href="/docs/tasks/run-application/configure-pdb/">configuring a Pod Disruption Budget</a>.</p></li><li><p>Learn more about <a href="/docs/tasks/administer-cluster/safely-drain-node/">draining nodes</a></p></li><li><p>Learn about <a href="/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">updating a deployment</a>
including steps to maintain its availability during the rollout.</p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">User Namespaces</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code></div><p>This page explains how user namespaces are used in Kubernetes pods. A user
namespace isolates the user running inside the container from the one
in the host.</p><p>A process running as root in a container can run as a different (non-root) user
in the host; in other words, the process has full privileges for operations
inside the user namespace, but is unprivileged for operations outside the
namespace.</p><p>You can use this feature to reduce the damage a compromised container can do to
the host or other pods in the same node. There are <a href="https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation">several security
vulnerabilities</a> rated either <strong>HIGH</strong> or <strong>CRITICAL</strong> that were not
exploitable when user namespaces is active. It is expected user namespace will
mitigate some future vulnerabilities too.</p><h2 id="before-you-begin">Before you begin</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>This is a Linux-only feature and support is needed in Linux for idmap mounts on
the filesystems used. This means:</p><ul><li>On the node, the filesystem you use for <code>/var/lib/kubelet/pods/</code>, or the
custom directory you configure for this, needs idmap mount support.</li><li>All the filesystems used in the pod's volumes must support idmap mounts.</li></ul><p>In practice this means you need at least Linux 6.3, as tmpfs started supporting
idmap mounts in that version. This is usually needed as several Kubernetes
features use tmpfs (the service account token that is mounted by default uses a
tmpfs, Secrets use a tmpfs, etc.)</p><p>Some popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,
ext4, xfs, fat, tmpfs, overlayfs.</p><p>In addition, the container runtime and its underlying OCI runtime must support
user namespaces. The following OCI runtimes offer support:</p><ul><li><a href="https://github.com/containers/crun">crun</a> version 1.9 or greater (it's recommend version 1.13+).</li><li><a href="https://github.com/opencontainers/runc">runc</a> version 1.2 or greater</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Some OCI runtimes do not include the support needed for using user namespaces in
Linux pods. If you use a managed Kubernetes, or have downloaded it from packages
and set it up, it's possible that nodes in your cluster use a runtime that doesn't
include this support.</div><p>To use user namespaces with Kubernetes, you also need to use a CRI
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>
to use this feature with Kubernetes pods:</p><ul><li>containerd: version 2.0 (and later) supports user namespaces for containers.</li><li>CRI-O: version 1.25 (and later) supports user namespaces for containers.</li></ul><p>You can see the status of user namespaces support in cri-dockerd tracked in an <a href="https://github.com/Mirantis/cri-dockerd/issues/74">issue</a>
on GitHub.</p><h2 id="introduction">Introduction</h2><p>User namespaces is a Linux feature that allows to map users in the container to
different users in the host. Furthermore, the capabilities granted to a pod in
a user namespace are valid only in the namespace and void outside of it.</p><p>A pod can opt-in to use user namespaces by setting the <code>pod.spec.hostUsers</code> field
to <code>false</code>.</p><p>The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way
to guarantee that no two pods on the same node use the same mapping.</p><p>The <code>runAsUser</code>, <code>runAsGroup</code>, <code>fsGroup</code>, etc. fields in the <code>pod.spec</code> always
refer to the user inside the container. These users will be used for volume
mounts (specified in <code>pod.spec.volumes</code>) and therefore the host UID/GID will not
have any effect on writes/reads from volumes the pod can mount. In other words,
the inodes created/read in volumes mounted by the pod will be the same as if the
pod wasn't using user namespaces.</p><p>This way, a pod can easily enable and disable user namespaces (without affecting
its volume's file ownerships) and can also share volumes with pods without user
namespaces by just setting the appropriate users inside the container
(<code>RunAsUser</code>, <code>RunAsGroup</code>, <code>fsGroup</code>, etc.). This applies to any volume the pod
can mount, including <code>hostPath</code> (if the pod is allowed to mount <code>hostPath</code>
volumes).</p><p>By default, the valid UIDs/GIDs when this feature is enabled is the range 0-65535.
This applies to files and processes (<code>runAsUser</code>, <code>runAsGroup</code>, etc.).</p><p>Files using a UID/GID outside this range will be seen as belonging to the
overflow ID, usually 65534 (configured in <code>/proc/sys/kernel/overflowuid</code> and
<code>/proc/sys/kernel/overflowgid</code>). However, it is not possible to modify those
files, even by running as the 65534 user/group.</p><p>If the range 0-65535 is extended with a configuration knob, the aforementioned
restrictions apply to the extended range.</p><p>Most applications that need to run as root but don't access other host
namespaces or resources, should continue to run fine without any changes needed
if user namespaces is activated.</p><h2 id="pods-and-userns">Understanding user namespaces for pods</h2><p>Several container runtimes with their default configuration (like Docker Engine,
containerd, CRI-O) use Linux namespaces for isolation. Other technologies exist
and can be used with those runtimes too (e.g. Kata Containers uses VMs instead of
Linux namespaces). This page is applicable for container runtimes using Linux
namespaces for isolation.</p><p>When creating a pod, by default, several new namespaces are used for isolation:
a network namespace to isolate the network of the container, a PID namespace to
isolate the view of processes, etc. If a user namespace is used, this will
isolate the users in the container from the users in the node.</p><p>This means containers can run as root and be mapped to a non-root user on the
host. Inside the container the process will think it is running as root (and
therefore tools like <code>apt</code>, <code>yum</code>, etc. work fine), while in reality the process
doesn't have privileges on the host. You can verify this, for example, if you
check which user the container process is running by executing <code>ps aux</code> from
the host. The user <code>ps</code> shows is not the same as the user you see if you
execute inside the container the command <code>id</code>.</p><p>This abstraction limits what can happen, for example, if the container manages
to escape to the host. Given that the container is running as a non-privileged
user on the host, it is limited what it can do to the host.</p><p>Furthermore, as users on each pod will be mapped to different non-overlapping
users in the host, it is limited what they can do to other pods too.</p><p>Capabilities granted to a pod are also limited to the pod user namespace and
mostly invalid out of it, some are even completely void. Here are two examples:</p><ul><li><code>CAP_SYS_MODULE</code> does not have any effect if granted to a pod using user
namespaces, the pod isn't able to load kernel modules.</li><li><code>CAP_SYS_ADMIN</code> is limited to the pod's user namespace and invalid outside
of it.</li></ul><p>Without using a user namespace a container running as root, in the case of a
container breakout, has root privileges on the node. And if some capability were
granted to the container, the capabilities are valid on the host too. None of
this is true when we use user namespaces.</p><p>If you want to know more details about what changes when user namespaces are in
use, see <code>man 7 user_namespaces</code>.</p><h2 id="set-up-a-node-to-support-user-namespaces">Set up a node to support user namespaces</h2><p>By default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on
the assumption that the host's files and processes use UIDs/GIDs within this
range, which is standard for most Linux distributions. This approach prevents
any overlap between the UIDs/GIDs of the host and those of the pods.</p><p>Avoiding the overlap is important to mitigate the impact of vulnerabilities such
as <a href="https://github.com/kubernetes/kubernetes/issues/104980">CVE-2021-25741</a>, where a pod can potentially read arbitrary
files in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is
limited what a pod would be able to do: the pod UID/GID won't match the host's
file owner/group.</p><p>The kubelet can use a custom range for user IDs and group IDs for pods. To
configure a custom range, the node needs to have:</p><ul><li>A user <code>kubelet</code> in the system (you cannot use any other username here)</li><li>The binary <code>getsubids</code> installed (part of <a href="https://github.com/shadow-maint/shadow">shadow-utils</a>) and
in the <code>PATH</code> for the kubelet binary.</li><li>A configuration of subordinate UIDs/GIDs for the <code>kubelet</code> user (see
<a href="https://man7.org/linux/man-pages/man5/subuid.5.html"><code>man 5 subuid</code></a> and
<a href="https://man7.org/linux/man-pages/man5/subgid.5.html"><code>man 5 subgid</code></a>).</li></ul><p>This setting only gathers the UID/GID range configuration and does not change
the user executing the <code>kubelet</code>.</p><p>You must follow some constraints for the subordinate ID range that you assign
to the <code>kubelet</code> user:</p><ul><li><p>The subordinate user ID, that starts the UID range for Pods, <strong>must</strong> be a
multiple of 65536 and must also be greater than or equal to 65536. In other
words, you cannot use any ID from the range 0-65535 for Pods; the kubelet
imposes this restriction to make it difficult to create an accidentally insecure
configuration.</p></li><li><p>The subordinate ID count must be a multiple of 65536</p></li><li><p>The subordinate ID count must be at least <code>65536 x &lt;maxPods&gt;</code> where <code>&lt;maxPods&gt;</code>
is the maximum number of pods that can run on the node.</p></li><li><p>You must assign the same range for both user IDs and for group IDs, It doesn't
matter if other users have user ID ranges that don't align with the group ID
ranges.</p></li><li><p>None of the assigned ranges should overlap with any other assignment.</p></li><li><p>The subordinate configuration must be only one line. In other words, you can't
have multiple ranges.</p></li></ul><p>For example, you could define <code>/etc/subuid</code> and <code>/etc/subgid</code> to both have
these entries for the <code>kubelet</code> user:</p><pre tabindex="0"><code># The format is
#   name:firstID:count of IDs
# where
# - firstID is 65536 (the minimum value possible)
# - count of IDs is 110 * 65536
#   (110 is the default limit for number of pods on the node)

kubelet:65536:7208960
</code></pre><h2 id="id-count-for-each-of-pods">ID count for each of Pods</h2><p>Starting with Kubernetes v1.33, the ID count for each of Pods can be set in
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">userNamespaces</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">idsPerPod</span>:<span style="color:#bbb"> </span><span style="color:#666">1048576</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The value of <code>idsPerPod</code> (uint32) must be a multiple of 65536.
The default value is 65536.
This value only applies to containers created after the kubelet was started with
this <code>KubeletConfiguration</code>.
Running containers are not affected by this config.</p><p>In Kubernetes prior to v1.33, the ID count for each of Pods was hard-coded to
65536.</p><h2 id="integration-with-pod-security-admission-checks">Integration with Pod security admission checks</h2><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [alpha]</code></div><p>For Linux Pods that enable user namespaces, Kubernetes relaxes the application of
<a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> in a controlled way.
This behavior can be controlled by the <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a>
<code>UserNamespacesPodSecurityStandards</code>, which allows an early opt-in for end
users. Admins have to ensure that user namespaces are enabled by all nodes
within the cluster if using the feature gate.</p><p>If you enable the associated feature gate and create a Pod that uses user
namespaces, the following fields won't be constrained even in contexts that enforce the
<em>Baseline</em> or <em>Restricted</em> pod security standard. This behavior does not
present a security concern because <code>root</code> inside a Pod with user namespaces
actually refers to the user inside the container, that is never mapped to a
privileged user on the host. Here's the list of fields that are <strong>not</strong> checks for Pods in those
circumstances:</p><ul><li><code>spec.securityContext.runAsNonRoot</code></li><li><code>spec.containers[*].securityContext.runAsNonRoot</code></li><li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.initContainers[*].securityContext.runAsUser</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li></ul><h2 id="limitations">Limitations</h2><p>When using a user namespace for the pod, it is disallowed to use other host
namespaces. In particular, if you set <code>hostUsers: false</code> then you are not
allowed to set any of:</p><ul><li><code>hostNetwork: true</code></li><li><code>hostIPC: true</code></li><li><code>hostPID: true</code></li></ul><p>No container can use <code>volumeDevices</code> (raw block volumes, like /dev/sda) either.
This includes all the container arrays in the pod spec:</p><ul><li><code>containers</code></li><li><code>initContainers</code></li><li><code>ephemeralContainers</code></li></ul><h2 id="metrics-and-observability">Metrics and observability</h2><p>The kubelet exports two prometheus metrics specific to user-namespaces:</p><ul><li><code>started_user_namespaced_pods_total</code>: a counter that tracks the number of user namespaced pods that are attempted to be created.</li><li><code>started_user_namespaced_pods_errors_total</code>: a counter that tracks the number of errors creating user namespaced pods.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Take a look at <a href="/docs/tasks/configure-pod-container/user-namespaces/">Use a User Namespace With a Pod</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Windows in Kubernetes</h1><div class="lead">Kubernetes supports nodes that run Microsoft Windows.</div><p>Kubernetes supports worker <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a>
running either Linux or Microsoft Windows.</p><div class="alert alert-secondary callout third-party-content" role="alert"> This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>The CNCF and its parent the Linux Foundation take a vendor-neutral approach
towards compatibility. It is possible to join your <a href="https://www.microsoft.com/en-us/windows-server">Windows server</a>
as a worker node to a Kubernetes cluster.</p><p>You can <a href="/docs/tasks/tools/install-kubectl-windows/">install and set up kubectl on Windows</a>
no matter what operating system you use within your cluster.</p><p>If you are using Windows nodes, you can read:</p><ul><li><a href="/docs/concepts/services-networking/windows-networking/">Networking On Windows</a></li><li><a href="/docs/concepts/storage/windows-storage/">Windows Storage In Kubernetes</a></li><li><a href="/docs/concepts/configuration/windows-resource-management/">Resource Management for Windows Nodes</a></li><li><a href="/docs/tasks/configure-pod-container/configure-runasusername/">Configure RunAsUserName for Windows Pods and Containers</a></li><li><a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create A Windows HostProcess Pod</a></li><li><a href="/docs/tasks/configure-pod-container/configure-gmsa/">Configure Group Managed Service Accounts for Windows Pods and Containers</a></li><li><a href="/docs/concepts/security/windows-security/">Security For Windows Nodes</a></li><li><a href="/docs/tasks/debug/debug-cluster/windows/">Windows Debugging Tips</a></li><li><a href="/docs/concepts/windows/user-guide/">Guide for Scheduling Windows Containers in Kubernetes</a></li></ul><p>or, for an overview, read:</p><div class="section-index"><ul><li><a href="/docs/concepts/windows/intro/">Windows containers in Kubernetes</a></li><li><a href="/docs/concepts/windows/user-guide/">Guide for Running Windows Containers in Kubernetes</a></li></ul></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pods</h1><p><em>Pods</em> are the smallest deployable units of computing that you can create and manage in Kubernetes.</p><p>A <em>Pod</em> (as in a pod of whales or pea pod) is a group of one or more
<a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/" target="_blank" aria-label="containers">containers</a>, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and
co-scheduled, and run in a shared context. A Pod models an
application-specific "logical host": it contains one or more application
containers which are relatively tightly coupled.
In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.</p><p>As well as application containers, a Pod can contain
<a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/init-containers/" target="_blank" aria-label="init containers">init containers</a> that run
during Pod startup. You can also inject
<a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank" aria-label="ephemeral containers">ephemeral containers</a>
for debugging a running Pod.</p><h2 id="what-is-a-pod">What is a Pod?</h2><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You need to install a <a href="/docs/setup/production-environment/container-runtimes/">container runtime</a>
into each node in the cluster so that Pods can run there.</div><p>The shared context of a Pod is a set of Linux namespaces, cgroups, and
potentially other facets of isolation - the same things that isolate a <a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/" target="_blank" aria-label="container">container</a>. Within a Pod's context, the individual applications may have
further sub-isolations applied.</p><p>A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.</p><p>Pods in a Kubernetes cluster are used in two main ways:</p><ul><li><p><strong>Pods that run a single container</strong>. The "one-container-per-Pod" model is the
most common Kubernetes use case; in this case, you can think of a Pod as a
wrapper around a single container; Kubernetes manages Pods rather than managing
the containers directly.</p></li><li><p><strong>Pods that run multiple containers that need to work together</strong>. A Pod can
encapsulate an application composed of
<a href="#how-pods-manage-multiple-containers">multiple co-located containers</a> that are
tightly coupled and need to share resources. These co-located containers
form a single cohesive unit.</p><p>Grouping multiple co-located and co-managed containers in a single Pod is a
relatively advanced use case. You should use this pattern only in specific
instances in which your containers are tightly coupled.</p><p>You don't need to run multiple containers to provide replication (for resilience
or capacity); if you need multiple replicas, see
<a href="/docs/concepts/workloads/controllers/">Workload management</a>.</p></li></ul><h2 id="using-pods">Using Pods</h2><p>The following is an example of a Pod which consists of a container running the image <code>nginx:1.14.2</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/simple-pod.yaml" download="pods/simple-pod.yaml"><code>pods/simple-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-simple-pod-yaml&quot;)" title="Copy pods/simple-pod.yaml to clipboard"/></div><div class="includecode" id="pods-simple-pod-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>To create the Pod shown above, run the following command:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
</span></span></code></pre></div><p>Pods are generally not created directly and are created using workload resources.
See <a href="#working-with-pods">Working with Pods</a> for more information on how Pods are used
with workload resources.</p><h3 id="workload-resources-for-managing-pods">Workload resources for managing pods</h3><p>Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a> or <a class="glossary-tooltip" title="A finite or batch task that runs to completion." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/job/" target="_blank" aria-label="Job">Job</a>.
If your Pods need to track state, consider the
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a> resource.</p><p>Each Pod is meant to run a single instance of a given application. If you want to
scale your application horizontally (to provide more overall resources by running
more instances), you should use multiple Pods, one for each instance. In
Kubernetes, this is typically referred to as <em>replication</em>.
Replicated Pods are usually created and managed as a group by a workload resource
and its <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>.</p><p>See <a href="#pods-and-controllers">Pods and controllers</a> for more information on how
Kubernetes uses workload resources, and their controllers, to implement application
scaling and auto-healing.</p><p>Pods natively provide two kinds of shared resources for their constituent containers:
<a href="#pod-networking">networking</a> and <a href="#pod-storage">storage</a>.</p><h2 id="working-with-pods">Working with Pods</h2><p>You'll rarely create individual Pods directly in Kuberneteseven singleton Pods. This
is because Pods are designed as relatively ephemeral, disposable entities. When
a Pod gets created (directly by you, or indirectly by a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>), the new Pod is
scheduled to run on a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="Node">Node</a> in your cluster.
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,
the Pod is <em>evicted</em> for lack of resources, or the node fails.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Restarting a container in a Pod should not be confused with restarting a Pod. A Pod
is not a process, but an environment for running container(s). A Pod persists until
it is deleted.</div><p>The name of a Pod must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostname. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><h3 id="pod-os">Pod OS</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You should set the <code>.spec.os.name</code> field to either <code>windows</code> or <code>linux</code> to indicate the OS on
which you want the pod to run. These two are the only operating systems supported for now by
Kubernetes. In the future, this list may be expanded.</p><p>In Kubernetes v1.34, the value of <code>.spec.os.name</code> does not affect
how the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="kube-scheduler">kube-scheduler</a>
picks a node for the Pod to run on. In any cluster where there is more than one operating system for
running nodes, you should set the
<a href="/docs/reference/labels-annotations-taints/#kubernetes-io-os">kubernetes.io/os</a>
label correctly on each node, and define pods with a <code>nodeSelector</code> based on the operating system
label. The kube-scheduler assigns your pod to a node based on other criteria and may or may not
succeed in picking a suitable node placement where the node OS is right for the containers in that Pod.
The <a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a> also use this
field to avoid enforcing policies that aren't relevant to the operating system.</p><h3 id="pods-and-controllers">Pods and controllers</h3><p>You can use workload resources to create and manage multiple Pods for you. A controller
for the resource handles replication and rollout and automatic healing in case of
Pod failure. For example, if a Node fails, a controller notices that Pods on that
Node have stopped working and creates a replacement Pod. The scheduler places the
replacement Pod onto a healthy Node.</p><p>Here are some examples of workload resources that manage one or more Pods:</p><ul><li><a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a></li><li><a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSet">StatefulSet</a></li><li><a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/daemonset" target="_blank" aria-label="DaemonSet">DaemonSet</a></li></ul><h3 id="pod-templates">Pod templates</h3><p>Controllers for <a class="glossary-tooltip" title="A workload is an application running on Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/" target="_blank" aria-label="workload">workload</a> resources create Pods
from a <em>pod template</em> and manage those Pods on your behalf.</p><p>PodTemplates are specifications for creating Pods, and are included in workload resources such as
<a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>,
<a href="/docs/concepts/workloads/controllers/job/">Jobs</a>, and
<a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a>.</p><p>Each controller for a workload resource uses the <code>PodTemplate</code> inside the workload
object to make actual Pods. The <code>PodTemplate</code> is part of the desired state of whatever
workload resource you used to run your app.</p><p>When you create a Pod, you can include
<a href="/docs/tasks/inject-data-application/define-environment-variable-container/">environment variables</a>
in the Pod template for the containers that run in the Pod.</p><p>The sample below is a manifest for a simple Job with a <code>template</code> that starts one
container. The container in that Pod prints a message then pauses.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># This is the pod template</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># The pod template ends here</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>Modifying the pod template or switching to a new pod template has no direct effect
on the Pods that already exist. If you change the pod template for a workload
resource, that resource needs to create replacement Pods that use the updated template.</p><p>For example, the StatefulSet controller ensures that the running Pods match the current
pod template for each StatefulSet object. If you edit the StatefulSet to change its pod
template, the StatefulSet starts to create new Pods based on the updated template.
Eventually, all of the old Pods are replaced with new Pods, and the update is complete.</p><p>Each workload resource implements its own rules for handling changes to the Pod template.
If you want to read more about StatefulSet specifically, read
<a href="/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets">Update strategy</a> in the StatefulSet Basics tutorial.</p><p>On Nodes, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="kubelet">kubelet</a> does not
directly observe or manage any of the details around pod templates and updates; those
details are abstracted away. That abstraction and separation of concerns simplifies
system semantics, and makes it feasible to extend the cluster's behavior without
changing existing code.</p><h2 id="pod-update-and-replacement">Pod update and replacement</h2><p>As mentioned in the previous section, when the Pod template for a workload
resource is changed, the controller creates new Pods based on the updated
template instead of updating or patching the existing Pods.</p><p>Kubernetes doesn't prevent you from managing Pods directly. It is possible to
update some fields of a running Pod, in place. However, Pod update operations
like
<a href="/docs/reference/generated/kubernetes-api/v1.34/#patch-pod-v1-core"><code>patch</code></a>, and
<a href="/docs/reference/generated/kubernetes-api/v1.34/#replace-pod-v1-core"><code>replace</code></a>
have some limitations:</p><ul><li><p>Most of the metadata about a Pod is immutable. For example, you cannot
change the <code>namespace</code>, <code>name</code>, <code>uid</code>, or <code>creationTimestamp</code> fields.</p></li><li><p>If the <code>metadata.deletionTimestamp</code> is set, no new entry can be added to the
<code>metadata.finalizers</code> list.</p></li><li><p>Pod updates may not change fields other than <code>spec.containers[*].image</code>,
<code>spec.initContainers[*].image</code>, <code>spec.activeDeadlineSeconds</code>, <code>spec.terminationGracePeriodSeconds</code>,
<code>spec.tolerations</code> or <code>spec.schedulingGates</code>. For <code>spec.tolerations</code>, you can only add new entries.</p></li><li><p>When updating the <code>spec.activeDeadlineSeconds</code> field, two types of updates
are allowed:</p><ol><li>setting the unassigned field to a positive number;</li><li>updating the field from a positive number to a smaller, non-negative
number.</li></ol></li></ul><h3 id="pod-subresources">Pod subresources</h3><p>The above update rules apply to regular pod updates, but other pod fields can be updated through <em>subresources</em>.</p><ul><li><strong>Resize:</strong> The <code>resize</code> subresource allows container resources (<code>spec.containers[*].resources</code>) to be updated.
See <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources</a> for more details.</li><li><strong>Ephemeral Containers:</strong> The <code>ephemeralContainers</code> subresource allows
<a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank" aria-label="ephemeral containers">ephemeral containers</a>
to be added to a Pod.
See <a href="/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers</a> for more details.</li><li><strong>Status:</strong> The <code>status</code> subresource allows the pod status to be updated.
This is typically only used by the Kubelet and other system controllers.</li><li><strong>Binding:</strong> The <code>binding</code> subresource allows setting the pod's <code>spec.nodeName</code> via a <code>Binding</code> request.
This is typically only used by the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" aria-label="scheduler">scheduler</a>.</li></ul><h3 id="pod-generation">Pod generation</h3><ul><li>The <code>metadata.generation</code> field is unique. It will be automatically set by the
system such that new pods have a <code>metadata.generation</code> of 1, and every update to
mutable fields in the pod's spec will increment the <code>metadata.generation</code> by 1.</li></ul><div class="feature-state-notice feature-beta" title="Feature Gate: PodObservedGenerationTracking"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><ul><li><code>observedGeneration</code> is a field that is captured in the <code>status</code> section of the Pod
object. If the feature gate <code>PodObservedGenerationTracking</code> is set, the Kubelet will set <code>status.observedGeneration</code>
to track the pod state to the current pod status. The pod's <code>status.observedGeneration</code> will reflect the
<code>metadata.generation</code> of the pod at the point that the pod status is being reported.</li></ul><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>status.observedGeneration</code> field is managed by the kubelet and external controllers should <strong>not</strong> modify this field.</div><p>Different status fields may either be associated with the <code>metadata.generation</code> of the current sync loop, or with the
<code>metadata.generation</code> of the previous sync loop. The key distinction is whether a change in the <code>spec</code> is reflected
directly in the <code>status</code> or is an indirect result of a running process.</p><h4 id="direct-status-updates">Direct Status Updates</h4><p>For status fields where the allocated spec is directly reflected, the <code>observedGeneration</code> will
be associated with the current <code>metadata.generation</code> (Generation N).</p><p>This behavior applies to:</p><ul><li><strong>Resize Status</strong>: The status of a resource resize operation.</li><li><strong>Allocated Resources</strong>: The resources allocated to the Pod after a resize.</li><li><strong>Ephemeral Containers</strong>: When a new ephemeral container is added, and it is in <code>Waiting</code> state.</li></ul><h4 id="indirect-status-updates">Indirect Status Updates</h4><p>For status fields that are an indirect result of running the spec, the <code>observedGeneration</code> will be associated
with the <code>metadata.generation</code> of the previous sync loop (Generation N-1).</p><p>This behavior applies to:</p><ul><li><strong>Container Image</strong>: The <code>ContainerStatus.ImageID</code> reflects the image from the previous generation until the new image
is pulled and the container is updated.</li><li><strong>Actual Resources</strong>: During an in-progress resize, the actual resources in use still belong to the previous generation's
request.</li><li><strong>Container state</strong>: During an in-progress resize, with require restart policy reflects the previous generation's
request.</li><li><strong>activeDeadlineSeconds</strong> &amp; <strong>terminationGracePeriodSeconds</strong> &amp; <strong>deletionTimestamp</strong>: The effects of these fields on the
Pod's status are a result of the previously observed specification.</li></ul><h2 id="resource-sharing-and-communication">Resource sharing and communication</h2><p>Pods enable data sharing and communication among their constituent
containers.</p><h3 id="pod-storage">Storage in Pods</h3><p>A Pod can specify a set of shared storage
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/storage/volumes/" target="_blank" aria-label="volumes">volumes</a>. All containers
in the Pod can access the shared volumes, allowing those containers to
share data. Volumes also allow persistent data in a Pod to survive
in case one of the containers within needs to be restarted. See
<a href="/docs/concepts/storage/">Storage</a> for more information on how
Kubernetes implements shared storage and makes it available to Pods.</p><h3 id="pod-networking">Pod networking</h3><p>Each Pod is assigned a unique IP address for each address family. Every
container in a Pod shares the network namespace, including the IP address and
network ports. Inside a Pod (and <strong>only</strong> then), the containers that belong to the Pod
can communicate with one another using <code>localhost</code>. When containers in a Pod communicate
with entities <em>outside the Pod</em>,
they must coordinate how they use the shared network resources (such as ports).
Within a Pod, containers share an IP address and port space, and
can find each other via <code>localhost</code>. The containers in a Pod can also communicate
with each other using standard inter-process communications like SystemV semaphores
or POSIX shared memory. Containers in different Pods have distinct IP addresses
and can not communicate by OS-level IPC without special configuration.
Containers that want to interact with a container running in a different Pod can
use IP networking to communicate.</p><p>Containers within the Pod see the system hostname as being the same as the configured
<code>name</code> for the Pod. There's more about this in the <a href="/docs/concepts/cluster-administration/networking/">networking</a>
section.</p><h2 id="pod-security">Pod security settings</h2><p>To set security constraints on Pods and containers, you use the
<code>securityContext</code> field in the Pod specification. This field gives you
granular control over what a Pod or individual containers can do. For example:</p><ul><li>Drop specific Linux capabilities to avoid the impact of a CVE.</li><li>Force all processes in the Pod to run as a non-root user or as a specific
user or group ID.</li><li>Set a specific seccomp profile.</li><li>Set Windows security options, such as whether containers run as HostProcess.</li></ul><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>You can also use the Pod securityContext to enable
<a href="/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers"><em>privileged mode</em></a>
in Linux containers. Privileged mode overrides many of the other security
settings in the securityContext. Avoid using this setting unless you can't grant
the equivalent permissions by using other fields in the securityContext.
In Kubernetes 1.26 and later, you can run Windows containers in a similarly
privileged mode by setting the <code>windowsOptions.hostProcess</code> flag on the
security context of the Pod spec. For details and instructions, see
<a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod</a>.</div><ul><li>To learn about kernel-level security constraints that you can use,
see <a href="/docs/concepts/security/linux-kernel-security-constraints/">Linux kernel security constraints for Pods and containers</a>.</li><li>To learn more about the Pod security context, see
<a href="/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container</a>.</li></ul><h2 id="static-pods">Static Pods</h2><p><em>Static Pods</em> are managed directly by the kubelet daemon on a specific node,
without the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/#kube-apiserver" target="_blank" aria-label="API server">API server</a>
observing them.
Whereas most Pods are managed by the control plane (for example, a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>), for static
Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).</p><p>Static Pods are always bound to one <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kubelet" target="_blank" aria-label="Kubelet">Kubelet</a> on a specific node.
The main use for static Pods is to run a self-hosted control plane: in other words,
using the kubelet to supervise the individual <a href="/docs/concepts/architecture/#control-plane-components">control plane components</a>.</p><p>The kubelet automatically tries to create a <a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank" aria-label="mirror Pod">mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there. See the guide <a href="/docs/tasks/configure-pod-container/static-pod/">Create static Pods</a>
for more information.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>spec</code> of a static Pod cannot refer to other API objects
(e.g., <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" aria-label="ServiceAccount">ServiceAccount</a>,
<a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/configmap/" target="_blank" aria-label="ConfigMap">ConfigMap</a>,
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." data-toggle="tooltip" data-placement="top" href="/docs/concepts/configuration/secret/" target="_blank" aria-label="Secret">Secret</a>, etc).</div><h2 id="how-pods-manage-multiple-containers">Pods with multiple containers</h2><p>Pods are designed to support multiple cooperating processes (as containers) that form
a cohesive unit of service. The containers in a Pod are automatically co-located and
co-scheduled on the same physical or virtual machine in the cluster. The containers
can share resources and dependencies, communicate with one another, and coordinate
when and how they are terminated.</p><p>Pods in a Kubernetes cluster are used in two main ways:</p><ul><li><strong>Pods that run a single container</strong>. The "one-container-per-Pod" model is the
most common Kubernetes use case; in this case, you can think of a Pod as a
wrapper around a single container; Kubernetes manages Pods rather than managing
the containers directly.</li><li><strong>Pods that run multiple containers that need to work together</strong>. A Pod can
encapsulate an application composed of
multiple co-located containers that are
tightly coupled and need to share resources. These co-located containers
form a single cohesive unit of servicefor example, one container serving data
stored in a shared volume to the public, while a separate
<a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" aria-label="sidecar container">sidecar container</a>
refreshes or updates those files.
The Pod wraps these containers, storage resources, and an ephemeral network
identity together as a single unit.</li></ul><p>For example, you might have a container that
acts as a web server for files in a shared volume, and a separate
<a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a>
that updates those files from a remote source, as in the following diagram:</p><figure class="diagram-medium"><img src="/images/docs/pod.svg" alt="Pod creation diagram"/></figure><p>Some Pods have <a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/init-containers/" target="_blank" aria-label="init containers">init containers</a>
as well as <a class="glossary-tooltip" title="A container used to run part of a workload. Compare with init container." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-app-container" target="_blank" aria-label="app containers">app containers</a>.
By default, init containers run and complete before the app containers are started.</p><p>You can also have <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>
that provide auxiliary services to the main application Pod (for example: a service mesh).</p><div class="feature-state-notice feature-stable" title="Feature Gate: SidecarContainers"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Enabled by default, the <code>SidecarContainers</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
allows you to specify <code>restartPolicy: Always</code> for init containers.
Setting the <code>Always</code> restart policy ensures that the containers where you set it are
treated as <em>sidecars</em> that are kept running during the entire lifetime of the Pod.
Containers that you explicitly define as sidecar containers
start up before the main application Pod and remain running until the Pod is
shut down.</p><h2 id="container-probes">Container probes</h2><p>A <em>probe</em> is a diagnostic performed periodically by the kubelet on a container.
To perform a diagnostic, the kubelet can invoke different actions:</p><ul><li><code>ExecAction</code> (performed with the help of the container runtime)</li><li><code>TCPSocketAction</code> (checked directly by the kubelet)</li><li><code>HTTPGetAction</code> (checked directly by the kubelet)</li></ul><p>You can read more about <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">probes</a>
in the Pod Lifecycle documentation.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about the <a href="/docs/concepts/workloads/pods/pod-lifecycle/">lifecycle of a Pod</a>.</li><li>Learn about <a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a> and how you can use it to
configure different Pods with different container runtime configurations.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>
and how you can use it to manage application availability during disruptions.</li><li>Pod is a top-level resource in the Kubernetes REST API.
The
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/">Pod</a>
object definition describes the object in detail.</li><li><a href="/blog/2015/06/the-distributed-system-toolkit-patterns/">The Distributed System Toolkit: Patterns for Composite Containers</a> explains common layouts for Pods with more than one container.</li><li>Read about <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a></li></ul><p>To understand the context for why Kubernetes wraps a common Pod API in other resources
(such as <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/statefulset/" target="_blank" aria-label="StatefulSets">StatefulSets</a> or
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployments">Deployments</a>),
you can read about the prior art, including:</p><ul><li><a href="https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema">Aurora</a></li><li><a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a></li><li><a href="https://github.com/d2iq-archive/marathon">Marathon</a></li><li><a href="https://research.google/pubs/pub41684/">Omega</a></li><li><a href="https://engineering.fb.com/data-center-engineering/tupperware/">Tupperware</a>.</li></ul><div class="section-index"/></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Automatic Cleanup for Finished Jobs</h1><div class="lead">A time-to-live mechanism to clean up old Jobs that have finished execution.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>When your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)
so that you can tell whether the Job succeeded or failed.</p><p>Kubernetes' TTL-after-finished <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> provides a
TTL (time to live) mechanism to limit the lifetime of Job objects that
have finished execution.</p><h2 id="cleanup-for-finished-jobs">Cleanup for finished Jobs</h2><p>The TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean
up finished Jobs (either <code>Complete</code> or <code>Failed</code>) automatically by specifying the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job, as in this
<a href="/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">example</a>.</p><p>The TTL-after-finished controller assumes that a Job is eligible to be cleaned up
TTL seconds after the Job has finished. The timer starts once the
status condition of the Job changes to show that the Job is either <code>Complete</code> or <code>Failed</code>; once the TTL has
expired, that Job becomes eligible for
<a href="/docs/concepts/architecture/garbage-collection/#cascading-deletion">cascading</a> removal. When the
TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it.</p><p>Kubernetes honors object lifecycle guarantees on the Job, such as waiting for
<a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a>.</p><p>You can set the TTL seconds at any time. Here are some examples for setting the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job:</p><ul><li>Specify this field in the Job manifest, so that a Job can be cleaned up
automatically some time after it finishes.</li><li>Manually set this field of existing, already finished Jobs, so that they become eligible
for cleanup.</li><li>Use a
<a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating admission webhook</a>
to set this field dynamically at Job creation time. Cluster administrators can
use this to enforce a TTL policy for finished jobs.</li><li>Use a
<a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating admission webhook</a>
to set this field dynamically after the Job has finished, and choose
different TTL values based on job status, labels. For this case, the webhook needs
to detect changes to the <code>.status</code> of the Job and only set a TTL when the Job
is being marked as completed.</li><li>Write your own controller to manage the cleanup TTL for Jobs that match a particular
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels/" target="_blank" aria-label="selector">selector</a>.</li></ul><h2 id="caveats">Caveats</h2><h3 id="updating-ttl-for-finished-jobs">Updating TTL for finished Jobs</h3><p>You can modify the TTL period, e.g. <code>.spec.ttlSecondsAfterFinished</code> field of Jobs,
after the job is created or has finished. If you extend the TTL period after the
existing <code>ttlSecondsAfterFinished</code> period has expired, Kubernetes doesn't guarantee
to retain that Job, even if an update to extend the TTL returns a successful API
response.</p><h3 id="time-skew">Time skew</h3><p>Because the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in your cluster, which may cause the control plane to clean up Job objects
at the wrong time.</p><p>Clocks aren't always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Read <a href="/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">Clean up Jobs automatically</a></p></li><li><p>Refer to the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md">Kubernetes Enhancement Proposal</a>
(KEP) for adding this mechanism.</p></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">CronJob</h1><div class="lead">A CronJob starts one-time Jobs on a repeating schedule.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>A <em>CronJob</em> creates <a class="glossary-tooltip" title="A finite or batch task that runs to completion." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/job/" target="_blank" aria-label="Jobs">Jobs</a> on a repeating schedule.</p><p>CronJob is meant for performing regular scheduled actions such as backups, report generation,
and so on. One CronJob object is like one line of a <em>crontab</em> (cron table) file on a
Unix system. It runs a Job periodically on a given schedule, written in
<a href="https://en.wikipedia.org/wiki/Cron">Cron</a> format.</p><p>CronJobs have limitations and idiosyncrasies.
For example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the <a href="#cron-job-limitations">limitations</a> below.</p><p>When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the <code>.metadata.name</code>
of the CronJob is part of the basis for naming those Pods. The name of a CronJob must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.
Even when the name is a DNS subdomain, the name must be no longer than 52
characters. This is because the CronJob controller will automatically append
11 characters to the name you provide and there is a constraint that the
length of a Job name is no more than 63 characters.</p><h2 id="example">Example</h2><p>This example CronJob manifest prints the current time and a hello message every minute:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml" download="application/job/cronjob.yaml"><code>application/job/cronjob.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;application-job-cronjob-yaml&quot;)" title="Copy application/job/cronjob.yaml to clipboard"/></div><div class="includecode" id="application-job-cronjob-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>CronJob<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">schedule</span>:<span style="color:#bbb"> </span><span style="color:#b44">"* * * * *"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">jobTemplate</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- /bin/sh<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- date; echo Hello from the Kubernetes cluster<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>(<a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Running Automated Tasks with a CronJob</a>
takes you through this example in more detail).</p><h2 id="writing-a-cronjob-spec">Writing a CronJob spec</h2><h3 id="schedule-syntax">Schedule syntax</h3><p>The <code>.spec.schedule</code> field is required. The value of that field follows the <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> syntax:</p><pre tabindex="0"><code>#  minute (0 - 59)
#   hour (0 - 23)
#    day of the month (1 - 31)
#     month (1 - 12)
#      day of the week (0 - 6) (Sunday to Saturday)
#                                        OR sun, mon, tue, wed, thu, fri, sat
#     
#     
# * * * * *
</code></pre><p>For example, <code>0 3 * * 1</code> means this task is scheduled to run weekly on a Monday at 3 AM.</p><p>The format also includes extended "Vixie cron" step values. As explained in the
<a href="https://www.freebsd.org/cgi/man.cgi?crontab%285%29">FreeBSD manual</a>:</p><blockquote><p>Step values can be used in conjunction with ranges. Following a range
with <code>/&lt;number&gt;</code> specifies skips of the number's value through the
range. For example, <code>0-23/2</code> can be used in the hours field to specify
command execution every other hour (the alternative in the V7 standard is
<code>0,2,4,6,8,10,12,14,16,18,20,22</code>). Steps are also permitted after an
asterisk, so if you want to say "every two hours", just use <code>*/2</code>.</p></blockquote><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>A question mark (<code>?</code>) in the schedule has the same meaning as an asterisk <code>*</code>, that is,
it stands for any of available value for a given field.</div><p>Other than the standard syntax, some macros like <code>@monthly</code> can also be used:</p><table><thead><tr><th>Entry</th><th>Description</th><th>Equivalent to</th></tr></thead><tbody><tr><td>@yearly (or @annually)</td><td>Run once a year at midnight of 1 January</td><td>0 0 1 1 *</td></tr><tr><td>@monthly</td><td>Run once a month at midnight of the first day of the month</td><td>0 0 1 * *</td></tr><tr><td>@weekly</td><td>Run once a week at midnight on Sunday morning</td><td>0 0 * * 0</td></tr><tr><td>@daily (or @midnight)</td><td>Run once a day at midnight</td><td>0 0 * * *</td></tr><tr><td>@hourly</td><td>Run once an hour at the beginning of the hour</td><td>0 * * * *</td></tr></tbody></table><p>To generate CronJob schedule expressions, you can also use web tools like <a href="https://crontab.guru/">crontab.guru</a>.</p><h3 id="job-template">Job template</h3><p>The <code>.spec.jobTemplate</code> defines a template for the Jobs that the CronJob creates, and it is required.
It has exactly the same schema as a <a href="/docs/concepts/workloads/controllers/job/">Job</a>, except that
it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.
You can specify common metadata for the templated Jobs, such as
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="labels">labels</a> or
<a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/annotations" target="_blank" aria-label="annotations">annotations</a>.
For information about writing a Job <code>.spec</code>, see <a href="/docs/concepts/workloads/controllers/job/#writing-a-job-spec">Writing a Job Spec</a>.</p><h3 id="starting-deadline">Deadline for delayed Job start</h3><p>The <code>.spec.startingDeadlineSeconds</code> field is optional.
This field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time
for any reason.</p><p>After missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).
For example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late,
but no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for
the next scheduled run.</p><p>For Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.
If you don't specify <code>startingDeadlineSeconds</code> for a CronJob, the Job occurrences have no deadline.</p><p>If the <code>.spec.startingDeadlineSeconds</code> field is set (not null), the CronJob
controller measures the time between when a Job is expected to be created and
now. If the difference is higher than that limit, it will skip this execution.</p><p>For example, if it is set to <code>200</code>, it allows a Job to be created for up to 200
seconds after the actual schedule.</p><h3 id="concurrency-policy">Concurrency policy</h3><p>The <code>.spec.concurrencyPolicy</code> field is also optional.
It specifies how to treat concurrent executions of a Job that is created by this CronJob.
The spec may specify only one of the following concurrency policies:</p><ul><li><code>Allow</code> (default): The CronJob allows concurrently running Jobs</li><li><code>Forbid</code>: The CronJob does not allow concurrent runs; if it is time for a new Job run and the
previous Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the
previous Job run finishes, <code>.spec.startingDeadlineSeconds</code> is still taken into account and may
result in a new Job run.</li><li><code>Replace</code>: If it is time for a new Job run and the previous Job run hasn't finished yet, the
CronJob replaces the currently running Job run with a new Job run</li></ul><p>Note that concurrency policy only applies to the Jobs created by the same CronJob.
If there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.</p><h3 id="schedule-suspension">Schedule suspension</h3><p>You can suspend execution of Jobs for a CronJob, by setting the optional <code>.spec.suspend</code> field
to true. The field defaults to false.</p><p>This setting does <em>not</em> affect Jobs that the CronJob has already started.</p><p>If you do set that field to true, all subsequent executions are suspended (they remain
scheduled, but the CronJob controller does not start the Jobs to run the tasks) until
you unsuspend the CronJob.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>Executions that are suspended during their scheduled time count as missed Jobs.
When <code>.spec.suspend</code> changes from <code>true</code> to <code>false</code> on an existing CronJob without a
<a href="#starting-deadline">starting deadline</a>, the missed Jobs are scheduled immediately.</div><h3 id="jobs-history-limits">Jobs history limits</h3><p>The <code>.spec.successfulJobsHistoryLimit</code> and <code>.spec.failedJobsHistoryLimit</code> fields specify
how many completed and failed Jobs should be kept. Both fields are optional.</p><ul><li><p><code>.spec.successfulJobsHistoryLimit</code>: This field specifies the number of successful finished
jobs to keep. The default value is <code>3</code>. Setting this field to <code>0</code> will not keep any successful jobs.</p></li><li><p><code>.spec.failedJobsHistoryLimit</code>: This field specifies the number of failed finished jobs to keep.
The default value is <code>1</code>. Setting this field to <code>0</code> will not keep any failed jobs.</p></li></ul><p>For another way to clean up Jobs automatically, see
<a href="/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">Clean up finished Jobs automatically</a>.</p><h3 id="time-zones">Time zones</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>For CronJobs with no time zone specified, the <a class="glossary-tooltip" title="Control Plane component that runs controller processes." data-toggle="tooltip" data-placement="top" href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank" aria-label="kube-controller-manager">kube-controller-manager</a>
interprets schedules relative to its local time zone.</p><p>You can specify a time zone for a CronJob by setting <code>.spec.timeZone</code> to the name
of a valid <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">time zone</a>.
For example, setting <code>.spec.timeZone: "Etc/UTC"</code> instructs Kubernetes to interpret
the schedule relative to Coordinated Universal Time.</p><p>A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.</p><h2 id="cron-job-limitations">CronJob limitations</h2><h3 id="unsupported-timezone-specification">Unsupported TimeZone specification</h3><p>Specifying a timezone using <code>CRON_TZ</code> or <code>TZ</code> variables inside <code>.spec.schedule</code>
is <strong>not officially supported</strong> (and never has been). If you try to set a schedule
that includes <code>TZ</code> or <code>CRON_TZ</code> timezone specification, Kubernetes will fail to
create or update the resource with a validation error. You should specify time zones
using the <a href="#time-zones">time zone field</a>, instead.</p><h3 id="modifying-a-cronjob">Modifying a CronJob</h3><p>By design, a CronJob contains a template for <em>new</em> Jobs.
If you modify an existing CronJob, the changes you make will apply to new Jobs that
start to run after your modification is complete. Jobs (and their Pods) that have already
started continue to run without changes.
That is, the CronJob does <em>not</em> update existing Jobs, even if those remain running.</p><h3 id="job-creation">Job creation</h3><p>A CronJob creates a Job object approximately once per execution time of its schedule.
The scheduling is approximate because there
are certain circumstances where two Jobs might be created, or no Job might be created.
Kubernetes tries to avoid those situations, but does not completely prevent them. Therefore,
the Jobs that you define should be <em>idempotent</em>.</p><p>Starting with Kubernetes v1.32, CronJobs apply an annotation
<code>batch.kubernetes.io/cronjob-scheduled-timestamp</code> to their created Jobs. This annotation
indicates the originally scheduled creation time for the Job and is formatted in RFC3339.</p><p>If <code>startingDeadlineSeconds</code> is set to a large value or left unset (the default)
and if <code>concurrencyPolicy</code> is set to <code>Allow</code>, the Jobs will always run
at least once.</p><div class="alert alert-caution" role="alert"><h4 class="alert-heading">Caution:</h4>If <code>startingDeadlineSeconds</code> is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.</div><p>For every CronJob, the CronJob <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="Controller">Controller</a> checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.</p><pre tabindex="0"><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><p>It is important to note that if the <code>startingDeadlineSeconds</code> field is set (not <code>nil</code>), the controller counts how many missed Jobs occurred from the value of <code>startingDeadlineSeconds</code> until now rather than from the last scheduled time until now. For example, if <code>startingDeadlineSeconds</code> is <code>200</code>, the controller counts how many missed Jobs occurred in the last 200 seconds.</p><p>A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if <code>concurrencyPolicy</code> is set to <code>Forbid</code> and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.</p><p>For example, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> field is not set. If the CronJob controller happens to
be down from <code>08:29:00</code> to <code>10:21:00</code>, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.</p><p>To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (<code>08:29:00</code> to <code>10:21:00</code>,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.</p><p>The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a> and
<a href="/docs/concepts/workloads/controllers/job/">Jobs</a>, two concepts
that CronJobs rely upon.</li><li>Read about the detailed <a href="https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format">format</a>
of CronJob <code>.spec.schedule</code> fields.</li><li>For instructions on creating and working with CronJobs, and for an example
of a CronJob manifest,
see <a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Running automated tasks with CronJobs</a>.</li><li><code>CronJob</code> is part of the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/cron-job-v1/">CronJob</a>
API reference for more details.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Sidecar Containers</h1><div class="feature-state-notice feature-stable" title="Feature Gate: SidecarContainers"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Sidecar containers are the secondary containers that run along with the main
application container within the same <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>.
These containers are used to enhance or to extend the functionality of the primary <em>app
container</em> by providing additional services, or functionality such as logging, monitoring,
security, or data synchronization, without directly altering the primary application code.</p><p>Typically, you only have one app container in a Pod. For example, if you have a web
application that requires a local webserver, the local webserver is a sidecar and the
web application itself is the app container.</p><h2 id="pod-sidecar-containers">Sidecar containers in Kubernetes</h2><p>Kubernetes implements sidecar containers as a special case of
<a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>; sidecar containers remain
running after Pod startup. This document uses the term <em>regular init containers</em> to clearly
refer to containers that only run during Pod startup.</p><p>Provided that your cluster has the <code>SidecarContainers</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> enabled
(the feature is active by default since Kubernetes v1.29), you can specify a <code>restartPolicy</code>
for containers listed in a Pod's <code>initContainers</code> field.
These restartable <em>sidecar</em> containers are independent from other init containers and from
the main application container(s) within the same pod.
These can be started, stopped, or restarted without affecting the main application container
and other init containers.</p><p>You can also run a Pod with multiple containers that are not marked as init or sidecar
containers. This is appropriate if the containers within the Pod are required for the
Pod to work overall, but you don't need to control which containers start or stop first.
You could also do this if you need to support older versions of Kubernetes that don't
support a container-level <code>restartPolicy</code> field.</p><h3 id="sidecar-example">Example application</h3><p>Here's an example of a Deployment with two containers, one of which is a sidecar:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment-sidecar.yaml" download="application/deployment-sidecar.yaml"><code>application/deployment-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;application-deployment-sidecar-yaml&quot;)" title="Copy application/deployment-sidecar.yaml to clipboard"/></div><div class="includecode" id="application-deployment-sidecar-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>alpine:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'while true; do echo "logging" &gt;&gt; /opt/logs.txt; sleep 1; done'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/opt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">initContainers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>logshipper<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>alpine:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'tail -F /opt/logs.txt'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/opt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb"> </span>{}</span></span></code></pre></div></div></div><h2 id="sidecar-containers-and-pod-lifecycle">Sidecar containers and Pod lifecycle</h2><p>If an init container is created with its <code>restartPolicy</code> set to <code>Always</code>, it will
start and remain running during the entire life of the Pod. This can be helpful for
running supporting services separated from the main application containers.</p><p>If a <code>readinessProbe</code> is specified for this init container, its result will be used
to determine the <code>ready</code> state of the Pod.</p><p>Since these containers are defined as init containers, they benefit from the same
ordering and sequential guarantees as regular init containers, allowing you to mix
sidecar containers with regular init containers for complex Pod initialization flows.</p><p>Compared to regular init containers, sidecars defined within <code>initContainers</code> continue to
run after they have started. This is important when there is more than one entry inside
<code>.spec.initContainers</code> for a Pod. After a sidecar-style init container is running (the kubelet
has set the <code>started</code> status for that init container to true), the kubelet then starts the
next init container from the ordered <code>.spec.initContainers</code> list.
That status either becomes true because there is a process running in the
container and no startup probe defined, or as a result of its <code>startupProbe</code> succeeding.</p><p>Upon Pod <a href="/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars">termination</a>,
the kubelet postpones terminating sidecar containers until the main application container has fully stopped.
The sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.
This approach ensures that the sidecars remain operational, supporting other containers within the Pod,
until their service is no longer required.</p><h3 id="jobs-with-sidecar-containers">Jobs with sidecar containers</h3><p>If you define a Job that uses sidecar using Kubernetes-style init containers,
the sidecar container in each Pod does not prevent the Job from completing after the
main container has finished.</p><p>Here's an example of a Job with two containers, one of which is a sidecar:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/job-sidecar.yaml" download="application/job/job-sidecar.yaml"><code>application/job/job-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;application-job-job-sidecar-yaml&quot;)" title="Copy application/job/job-sidecar.yaml to clipboard"/></div><div class="includecode" id="application-job-job-sidecar-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myjob<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>myjob<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>alpine:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'echo "logging" &gt; /opt/logs.txt'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/opt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">initContainers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>logshipper<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>alpine:latest<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">'sh'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'-c'</span>,<span style="color:#bbb"> </span><span style="color:#b44">'tail -F /opt/logs.txt'</span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">              </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/opt<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">emptyDir</span>:<span style="color:#bbb"> </span>{}</span></span></code></pre></div></div></div><h2 id="differences-from-application-containers">Differences from application containers</h2><p>Sidecar containers run alongside <em>app containers</em> in the same pod. However, they do not
execute the primary application logic; instead, they provide supporting functionality to
the main application.</p><p>Sidecar containers have their own independent lifecycles. They can be started, stopped,
and restarted independently of app containers. This means you can update, scale, or
maintain sidecar containers without affecting the primary application.</p><p>Sidecar containers share the same network and storage namespaces with the primary
container. This co-location allows them to interact closely and share resources.</p><p>From a Kubernetes perspective, the sidecar container's graceful termination is less important.
When other containers take all allotted graceful termination time, the sidecar containers
will receive the <code>SIGTERM</code> signal, followed by the <code>SIGKILL</code> signal, before they have time to terminate gracefully.
So exit codes different from <code>0</code> (<code>0</code> indicates successful exit), for sidecar containers are normal
on Pod termination and should be generally ignored by the external tooling.</p><h2 id="differences-from-init-containers">Differences from init containers</h2><p>Sidecar containers work alongside the main container, extending its functionality and
providing additional services.</p><p>Sidecar containers run concurrently with the main application container. They are active
throughout the lifecycle of the pod and can be started and stopped independently of the
main container. Unlike <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>,
sidecar containers support <a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">probes</a> to control their lifecycle.</p><p>Sidecar containers can interact directly with the main application containers, because
like init containers they always share the same network, and can optionally also share
volumes (filesystems).</p><p>Init containers stop before the main containers start up, so init containers cannot
exchange messages with the app container in a Pod. Any data passing is one-way
(for example, an init container can put information inside an <code>emptyDir</code> volume).</p><p>Changing the image of a sidecar container will not cause the Pod to restart, but will
trigger a container restart.</p><h2 id="resource-sharing-within-containers">Resource sharing within containers</h2><p>Given the order of execution for init, sidecar and app containers, the following rules
for resource usage apply:</p><ul><li>The highest of any particular resource request or limit defined on all init
containers is the <em>effective init request/limit</em>. If any resource has no
resource limit specified this is considered as the highest limit.</li><li>The Pod's <em>effective request/limit</em> for a resource is the sum of
<a href="/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a> and the higher of:<ul><li>the sum of all non-init containers(app and sidecar containers) request/limit for a
resource</li><li>the effective init request/limit for a resource</li></ul></li><li>Scheduling is done based on effective requests/limits, which means
init containers can reserve resources for initialization that are not used
during the life of the Pod.</li><li>The QoS (quality of service) tier of the Pod's <em>effective QoS tier</em> is the
QoS tier for all init, sidecar and app containers alike.</li></ul><p>Quota and limits are applied based on the effective Pod request and
limit.</p><h3 id="cgroups">Sidecar containers and Linux cgroups</h3><p>On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod
request and limit, the same as the scheduler.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tutorials/configuration/pod-sidecar-containers/">Adopt Sidecar Containers</a></li><li>Read a blog post on <a href="/blog/2023/08/25/native-sidecar-containers/">native sidecar containers</a>.</li><li>Read about <a href="/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">creating a Pod that has an init container</a>.</li><li>Learn about the <a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">types of probes</a>: liveness, readiness, startup probe.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Workload Management</h1><p>Kubernetes provides several built-in APIs for declarative management of your
<a class="glossary-tooltip" title="A workload is an application running on Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/" target="_blank" aria-label="workloads">workloads</a>
and the components of those workloads.</p><p>Ultimately, your applications run as containers inside
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>; however, managing individual
Pods would be a lot of effort. For example, if a Pod fails, you probably want to
run a new Pod to replace it. Kubernetes can do that for you.</p><p>You use the Kubernetes API to create a workload
<a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank" aria-label="object">object</a> that represents a higher abstraction level
than a Pod, and then the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> automatically manages
Pod objects on your behalf, based on the specification for the workload object you defined.</p><p>The built-in APIs for managing workloads are:</p><p><a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> (and, indirectly, <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>),
the most common way to run an application on your cluster.
Deployment is a good fit for managing a stateless application workload on your cluster, where
any Pod in the Deployment is interchangeable and can be replaced if needed.
(Deployments are a replacement for the legacy
<a class="glossary-tooltip" title="A (deprecated) API object that manages a replicated application." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-replication-controller" target="_blank" aria-label="ReplicationController">ReplicationController</a> API).</p><p>A <a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> lets you
manage one or more Pods  all running the same application code  where the Pods rely
on having a distinct identity. This is different from a Deployment where the Pods are
expected to be interchangeable.
The most common use for a StatefulSet is to be able to make a link between its Pods and
their persistent storage. For example, you can run a StatefulSet that associates each Pod
with a <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a>. If one of the Pods
in the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the
same PersistentVolume.</p><p>A <a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> defines Pods that provide
facilities that are local to a specific <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a>;
for example, a driver that lets containers on that node access a storage system. You use a DaemonSet
when the driver, or other node-level service, has to run on the node where it's useful.
Each Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX
server.
A DaemonSet might be fundamental to the operation of your cluster,
such as a plugin to let that node access
<a href="/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model">cluster networking</a>,
it might help you to manage the node,
or it could provide less essential facilities that enhance the container platform you are running.
You can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example,
only install the GPU accelerator driver on nodes that have a GPU installed).</p><p>You can use a <a href="/docs/concepts/workloads/controllers/job/">Job</a> and / or
a <a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> to
define tasks that run to completion and then stop. A Job represents a one-off task,
whereas each CronJob repeats according to a schedule.</p><p>Other topics in this section:</p><div class="section-index"><ul><li><a href="/docs/concepts/workloads/controllers/ttlafterfinished/">Automatic Cleanup for Finished Jobs</a></li><li><a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a></li></ul></div></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Ephemeral Containers</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a> to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.</p><h2 id="understanding-ephemeral-containers">Understanding ephemeral containers</h2><p><a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="deployments">deployments</a>.</p><p>Sometimes it's necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.</p><h3 id="what-is-an-ephemeral-container">What is an ephemeral container?</h3><p>Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications. Ephemeral containers are
described using the same <code>ContainerSpec</code> as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.</p><ul><li>Ephemeral containers may not have ports, so fields such as <code>ports</code>,
<code>livenessProbe</code>, <code>readinessProbe</code> are disallowed.</li><li>Pod resource allocations are immutable, so setting <code>resources</code> is disallowed.</li><li>For a complete list of allowed fields, see the <a href="/docs/reference/generated/kubernetes-api/v1.34/#ephemeralcontainer-v1-core">EphemeralContainer reference
documentation</a>.</li></ul><p>Ephemeral containers are created using a special <code>ephemeralcontainers</code> handler
in the API rather than by adding them directly to <code>pod.spec</code>, so it's not
possible to add an ephemeral container using <code>kubectl edit</code>.</p><p>Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Ephemeral containers are not supported by <a href="/docs/tasks/configure-pod-container/static-pod/">static pods</a>.</div><h2 id="uses-for-ephemeral-containers">Uses for ephemeral containers</h2><p>Ephemeral containers are useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient because a container has crashed or a container image
doesn't include debugging utilities.</p><p>In particular, <a href="https://github.com/GoogleContainerTools/distroless">distroless images</a>
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it's difficult to troubleshoot distroless
images using <code>kubectl exec</code> alone.</p><p>When using ephemeral containers, it's helpful to enable <a href="/docs/tasks/configure-pod-container/share-process-namespace/">process namespace
sharing</a> so
you can view processes in other containers.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container">debug pods using ephemeral containers</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Hostname</h1><p>This page explains how to set a Pod's hostname,
potential side effects after configuration, and the underlying mechanics.</p><h2 id="default-pod-hostname">Default Pod hostname</h2><p>When a Pod is created, its hostname (as observed from within the Pod)
is derived from the Pod's metadata.name value.
Both the hostname and its corresponding fully qualified domain name (FQDN)
are set to the metadata.name value (from the Pod's perspective)</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox-1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"3600"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span></code></pre></div><p>The Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to <code>busybox-1</code>.</p><h2 id="hostname-with-pod-s-hostname-and-subdomain-fields">Hostname with pod's hostname and subdomain fields</h2><p>The Pod spec includes an optional <code>hostname</code> field.
When set, this value takes precedence over the Pod's <code>metadata.name</code> as the
hostname (observed from within the Pod).
For example, a Pod with spec.hostname set to <code>my-host</code> will have its hostname set to <code>my-host</code>.</p><p>The Pod spec also includes an optional <code>subdomain</code> field,
indicating the Pod belongs to a subdomain within its namespace.
If a Pod has <code>spec.hostname</code> set to "foo" and spec.subdomain set
to "bar" in the namespace <code>my-namespace</code>, its hostname becomes <code>foo</code> and its
fully qualified domain name (FQDN) becomes
<code>foo.bar.my-namespace.svc.cluster-domain.example</code> (observed from within the Pod).</p><p>When both hostname and subdomain are set, the cluster's DNS server will
create A and/or AAAA records based on these fields.
Refer to: <a href="/docs/concepts/services-networking/dns-pod-service/#pod-hostname-and-subdomain-field">Pod's hostname and subdomain fields</a>.</p><h2 id="hostname-with-pod-s-sethostnameasfqdn-fields">Hostname with pod's setHostnameAsFQDN fields</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [stable]</code></div><p>When a Pod is configured to have fully qualified domain name (FQDN), its
hostname is the short hostname. For example, if you have a Pod with the fully
qualified domain name <code>busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example</code>,
then by default the <code>hostname</code> command inside that Pod returns <code>busybox-1</code> and the
<code>hostname --fqdn</code> command returns the FQDN.</p><p>When both <code>setHostnameAsFQDN: true</code> and the subdomain field is set in the Pod spec,
the kubelet writes the Pod's FQDN
into the hostname for that Pod's namespace. In this case, both <code>hostname</code> and <code>hostname --fqdn</code>
return the Pod's FQDN.</p><p>The Pod's FQDN is constructed in the same manner as previously defined.
It is composed of the Pod's <code>spec.hostname</code> (if specified) or <code>metadata.name</code> field,
the <code>spec.subdomain</code>, the <code>namespace</code> name, and the cluster domain suffix.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><p>In Linux, the hostname field of the kernel (the <code>nodename</code> field of <code>struct utsname</code>) is limited to 64 characters.</p><p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start.
The Pod will remain in <code>Pending</code> status (<code>ContainerCreating</code> as seen by <code>kubectl</code>) generating
error events, such as "Failed to construct FQDN from Pod hostname and cluster domain".</p><p>This means that when using this field,
you must ensure the combined length of the Pod's <code>metadata.name</code> (or <code>spec.hostname</code>)
and <code>spec.subdomain</code> fields results in an FQDN that does not exceed 64 characters.</p></div><h2 id="hostname-with-pod-s-hostnameoverride">Hostname with pod's hostnameOverride</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: HostnameOverride"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>Setting a value for <code>hostnameOverride</code> in the Pod spec causes the kubelet
to unconditionally set both the Pod's hostname and fully qualified domain name (FQDN)
to the <code>hostnameOverride</code> value.</p><p>The <code>hostnameOverride</code> field has a length limitation of 64 characters
and must adhere to the DNS subdomain names standard defined in <a href="https://datatracker.ietf.org/doc/html/rfc1123">RFC 1123</a>.</p><p>Example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox-2-busybox-example-domain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">hostnameOverride</span>:<span style="color:#bbb"> </span>busybox-2.busybox.example.domain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:#b44">"3600"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This only affects the hostname within the Pod; it does not affect the Pod's A or AAAA records in the cluster DNS server.</div><p>If <code>hostnameOverride</code> is set alongside <code>hostname</code> and <code>subdomain</code> fields:</p><ul><li><p>The hostname inside the Pod is overridden to the <code>hostnameOverride</code> value.</p></li><li><p>The Pod's A and/or AAAA records in the cluster DNS server are still generated based on the <code>hostname</code> and <code>subdomain</code> fields.</p></li></ul><p>Note: If <code>hostnameOverride</code> is set, you cannot simultaneously set the <code>hostNetwork</code> and <code>setHostnameAsFQDN</code> fields.
The API server will explicitly reject any create request attempting this combination.</p><p>For details on behavior when <code>hostnameOverride</code> is set in combination with
other fields (hostname, subdomain, setHostnameAsFQDN, hostNetwork),
see the table in the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/4762-allow-arbitrary-fqdn-as-pod-hostname/README.md#design-details">KEP-4762 design details</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">ReplicaSet</h1><div class="lead">A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.</div><p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.</p><h2 id="how-a-replicaset-works">How a ReplicaSet works</h2><p>A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.</p><p>A ReplicaSet is linked to its Pods via the Pods' <a href="/docs/concepts/architecture/garbage-collection/#owners-dependents">metadata.ownerReferences</a>
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.</p><p>A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no
OwnerReference or the OwnerReference is not a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="Controller">Controller</a> and it
matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.</p><h2 id="when-to-use-a-replicaset">When to use a ReplicaSet</h2><p>A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to Pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.</p><p>This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.</p><h2 id="example">Example</h2><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/frontend.yaml" download="controllers/frontend.yaml"><code>controllers/frontend.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-frontend-yaml&quot;)" title="Copy controllers/frontend.yaml to clipboard"/></div><div class="includecode" id="controllers-frontend-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># modify replicas according to your case</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>php-redis<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>frontend.yaml</code> and submitting it to a Kubernetes cluster will
create the defined ReplicaSet and the Pods that it manages.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You can then get the current ReplicaSets deployed:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get rs
</span></span></code></pre></div><p>And see the frontend one you created:</p><pre tabindex="0"><code>NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s
</code></pre><p>You can also check on the state of the ReplicaSet:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>And you will see output similar to:</p><pre tabindex="0"><code>Name:         frontend
Namespace:    default
Selector:     tier=frontend
Labels:       app=guestbook
              tier=frontend
Annotations:  &lt;none&gt;
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-gbgfx
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-rwz57
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-wkl7w
</code></pre><p>And lastly you can check for the Pods brought up:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><p>You should see Pod information similar to:</p><pre tabindex="0"><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-gbgfx   1/1     Running   0          10m
frontend-rwz57   1/1     Running   0          10m
frontend-wkl7w   1/1     Running   0          10m
</code></pre><p>You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods frontend-gbgfx -o yaml
</span></span></code></pre></div><p>The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">"2024-02-28T22:30:44Z"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">generateName</span>:<span style="color:#bbb"> </span>frontend-<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend-gbgfx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ownerReferences</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">blockOwnerDeletion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">controller</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:700">true</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">uid</span>:<span style="color:#bbb"> </span>e129deca-f864-481b-bb16-b27abfd92292<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span></code></pre></div><h2 id="non-template-pod-acquisitions">Non-Template Pod acquisitions</h2><p>While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.</p><p>Take the previous frontend ReplicaSet example, and the Pods specified in the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-rs.yaml" download="pods/pod-rs.yaml"><code>pods/pod-rs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;pods-pod-rs-yaml&quot;)" title="Copy pods/pod-rs.yaml to clipboard"/></div><div class="includecode" id="pods-pod-rs-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/hello-app:2.0<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>pod2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>hello2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/hello-app:1.0<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.</p><p>Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.</p><p>Fetching the Pods:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the new Pods are either already terminated, or in the process of being terminated:</p><pre tabindex="0"><code>NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       0          10m
frontend-vcmts   1/1     Running       0          10m
frontend-wtsmm   1/1     Running       0          10m
pod1             0/1     Terminating   0          1s
pod2             0/1     Terminating   0          1s
</code></pre><p>If you create the Pods first:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>And then create the ReplicaSet however:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods
</span></span></code></pre></div><p>Will reveal in its output:</p><pre tabindex="0"><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
</code></pre><p>In this manner, a ReplicaSet can own a non-homogeneous set of Pods</p><h2 id="writing-a-replicaset-manifest">Writing a ReplicaSet manifest</h2><p>As with all other Kubernetes API objects, a ReplicaSet needs the <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
For ReplicaSets, the <code>kind</code> is always a ReplicaSet.</p><p>When the control plane creates new Pods for a ReplicaSet, the <code>.metadata.name</code> of the
ReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>A ReplicaSet also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a> which is also
required to have labels in place. In our <code>frontend.yaml</code> example we had one label: <code>tier: frontend</code>.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.</p><p>For the template's <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">restart policy</a> field,
<code>.spec.template.spec.restartPolicy</code>, the only allowed value is <code>Always</code>, which is the default.</p><h3 id="pod-selector">Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href="/docs/concepts/overview/working-with-objects/labels/">label selector</a>. As discussed
<a href="#how-a-replicaset-works">earlier</a> these are the labels used to identify potential Pods to acquire. In our
<code>frontend.yaml</code> example, the selector was:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span></code></pre></div><p>In the ReplicaSet, <code>.spec.template.metadata.labels</code> must match <code>spec.selector</code>, or it will
be rejected by the API.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>For 2 ReplicaSets specifying the same <code>.spec.selector</code> but different
<code>.spec.template.metadata.labels</code> and <code>.spec.template.spec</code> fields, each ReplicaSet ignores the
Pods created by the other ReplicaSet.</div><h3 id="replicas">Replicas</h3><p>You can specify how many Pods should run concurrently by setting <code>.spec.replicas</code>. The ReplicaSet will create/delete
its Pods to match this number.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id="working-with-replicasets">Working with ReplicaSets</h2><h3 id="deleting-a-replicaset-and-its-pods">Deleting a ReplicaSet and its Pods</h3><p>To delete a ReplicaSet and all of its Pods, use
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>. The
<a href="/docs/concepts/architecture/garbage-collection/">Garbage collector</a> automatically deletes all of
the dependent Pods by default.</p><p>When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to
<code>Background</code> or <code>Foreground</code> in the <code>-d</code> option. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span><span style="display:flex"><span>curl -X DELETE  <span style="color:#b44">'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  -d <span style="color:#b44">'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  -H <span style="color:#b44">"Content-Type: application/json"</span>
</span></span></code></pre></div><h3 id="deleting-just-a-replicaset">Deleting just a ReplicaSet</h3><p>You can delete a ReplicaSet without affecting any of its Pods using
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>
with the <code>--cascade=orphan</code> option.
When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to <code>Orphan</code>.
For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</span></span><span style="display:flex"><span>curl -X DELETE  <span style="color:#b44">'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  -d <span style="color:#b44">'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}'</span> <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  -H <span style="color:#b44">"Content-Type: application/json"</span>
</span></span></code></pre></div><p>Once the original is deleted, you can create a new ReplicaSet to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old Pods.
However, it will not make any effort to make existing Pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
<a href="/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">Deployment</a>, as
ReplicaSets do not support a rolling update directly.</p><h3 id="terminating-pods">Terminating Pods</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DeploymentReplicaSetTerminatingReplicas"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>You can enable this feature by setting the <code>DeploymentReplicaSetTerminatingReplicas</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
on the <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">API server</a>
and on the <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></p><p>Pods that become terminating due to deletion or scale down may take a long time to terminate, and may consume
additional resources during that period. As a result, the total number of all pods can temporarily exceed
<code>.spec.replicas</code>. Terminating pods can be tracked using the <code>.status.terminatingReplicas</code> field of the ReplicaSet.</p><h3 id="isolating-pods-from-a-replicaset">Isolating Pods from a ReplicaSet</h3><p>You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
assuming that the number of replicas is not also changed).</p><h3 id="scaling-a-replicaset">Scaling a ReplicaSet</h3><p>A ReplicaSet can be easily scaled up or down by simply updating the <code>.spec.replicas</code> field. The ReplicaSet controller
ensures that a desired number of Pods with a matching label selector are available and operational.</p><p>When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to
prioritize scaling down pods based on the following general algorithm:</p><ol><li>Pending (and unschedulable) pods are scaled down first</li><li>If <code>controller.kubernetes.io/pod-deletion-cost</code> annotation is set, then
the pod with the lower value will come first.</li><li>Pods on nodes with more replicas come before pods on nodes with fewer replicas.</li><li>If the pods' creation times differ, the pod that was created more recently
comes before the older pod (the creation times are bucketed on an integer log scale).</li></ol><p>If all of the above match, then selection is random.</p><h3 id="pod-deletion-cost">Pod deletion cost</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [beta]</code></div><p>Using the <a href="/docs/reference/labels-annotations-taints/#pod-deletion-cost"><code>controller.kubernetes.io/pod-deletion-cost</code></a>
annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.</p><p>The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of
deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion
cost are preferred to be deleted before pods with higher deletion cost.</p><p>The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.
Invalid values will be rejected by the API server.</p><p>This feature is beta and enabled by default. You can disable it using the
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
<code>PodDeletionCost</code> in both kube-apiserver and kube-controller-manager.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><ul><li>This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.</li><li>Users should avoid updating the annotation frequently, such as updating it based on a metric value,
because doing so will generate a significant number of pod updates on the apiserver.</li></ul></div><h4 id="example-use-case">Example Use Case</h4><p>The different pods of an application could have different utilization levels. On scale down, the application
may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application
should update <code>controller.kubernetes.io/pod-deletion-cost</code> once before issuing a scale down (setting the
annotation to a value proportional to pod utilization level). This works if the application itself controls
the down scaling; for example, the driver pod of a Spark deployment.</p><h3 id="replicaset-as-a-horizontal-pod-autoscaler-target">ReplicaSet as a Horizontal Pod Autoscaler Target</h3><p>A ReplicaSet can also be a target for
<a href="/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscalers (HPA)</a>. That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/hpa-rs.yaml" download="controllers/hpa-rs.yaml"><code>controllers/hpa-rs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-hpa-rs-yaml&quot;)" title="Copy controllers/hpa-rs.yaml to clipboard"/></div><div class="includecode" id="controllers-hpa-rs-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>autoscaling/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>HorizontalPodAutoscaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend-scaler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">scaleTargetRef</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">minReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">maxReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">targetCPUUtilizationPercentage</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>hpa-rs.yaml</code> and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated Pods.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Alternatively, you can use the <code>kubectl autoscale</code> command to accomplish the same
(and it's easier!)</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl autoscale rs frontend --max<span style="color:#666">=</span><span style="color:#666">10</span> --min<span style="color:#666">=</span><span style="color:#666">3</span> --cpu<span style="color:#666">=</span>50%
</span></span></code></pre></div><h2 id="alternatives-to-replicaset">Alternatives to ReplicaSet</h2><h3 id="deployment-recommended">Deployment (recommended)</h3><p><a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.</p><h3 id="bare-pods">Bare Pods</h3><p>Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or
terminated for any reason, such as in the case of node failure or disruptive node maintenance,
such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your
application requires only a single Pod. Think of it similarly to a process supervisor, only it
supervises multiple Pods across multiple nodes instead of individual processes on a single node. A
ReplicaSet delegates local container restarts to some agent on the node such as Kubelet.</p><h3 id="job">Job</h3><p>Use a <a href="/docs/concepts/workloads/controllers/job/"><code>Job</code></a> instead of a ReplicaSet for Pods that are
expected to terminate on their own (that is, batch jobs).</p><h3 id="daemonset">DaemonSet</h3><p>Use a <a href="/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> instead of a ReplicaSet for Pods that provide a
machine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied
to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h3 id="replicationcontroller">ReplicationController</h3><p>ReplicaSets are the successors to <a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationControllers</a>.
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">labels user guide</a>.
As such, ReplicaSets are preferred over ReplicationControllers</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Learn about <a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>.</li><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a Stateless Application Using a Deployment</a>,
which relies on ReplicaSets to work.</li><li><code>ReplicaSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/replica-set-v1/">ReplicaSet</a>
object definition to understand the API for replica sets.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Workloads</h1><div class="lead">Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.</div><p>A workload is an application running on Kubernetes.
Whether your workload is a single component or several that work together, on Kubernetes you run
it inside a set of <a href="/docs/concepts/workloads/pods/"><em>pods</em></a>.
In Kubernetes, a Pod represents a set of running
<a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/" target="_blank" aria-label="containers">containers</a> on your cluster.</p><p>Kubernetes pods have a <a href="/docs/concepts/workloads/pods/pod-lifecycle/">defined lifecycle</a>.
For example, once a pod is running in your cluster then a critical fault on the
<a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a> where that pod is running means that
all the pods on that node fail. Kubernetes treats that level of failure as final: you
would need to create a new Pod to recover, even if the node later becomes healthy.</p><p>However, to make life considerably easier, you don't need to manage each Pod directly.
Instead, you can use <em>workload resources</em> that manage a set of pods on your behalf.
These resources configure <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controllers">controllers</a>
that make sure the right number of the right kind of pod are running, to match the state
you specified.</p><p>Kubernetes provides several built-in workload resources:</p><ul><li><a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> and <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
(replacing the legacy resource
<a class="glossary-tooltip" title="A (deprecated) API object that manages a replicated application." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-replication-controller" target="_blank" aria-label="ReplicationController">ReplicationController</a>).
Deployment is a good fit for managing a stateless application workload on your cluster,
where any Pod in the Deployment is interchangeable and can be replaced if needed.</li><li><a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> lets you
run one or more related Pods that do track state somehow. For example, if your workload
records data persistently, you can run a StatefulSet that matches each Pod with a
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a>. Your code, running in the
Pods for that StatefulSet, can replicate data to other Pods in the same StatefulSet
to improve overall resilience.</li><li><a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> defines Pods that provide
facilities that are local to nodes.
Every time you add a node to your cluster that matches the specification in a DaemonSet,
the control plane schedules a Pod for that DaemonSet onto the new node.
Each pod in a DaemonSet performs a job similar to a system daemon on a classic Unix / POSIX
server. A DaemonSet might be fundamental to the operation of your cluster, such as
a plugin to run <a href="/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model">cluster networking</a>,
it might help you to manage the node,
or it could provide optional behavior that enhances the container platform you are running.</li><li><a href="/docs/concepts/workloads/controllers/job/">Job</a> and
<a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> provide different ways to
define tasks that run to completion and then stop.
You can use a <a href="/docs/concepts/workloads/controllers/job/">Job</a> to
define a task that runs to completion, just once. You can use a
<a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> to run
the same Job multiple times according a schedule.</li></ul><p>In the wider Kubernetes ecosystem, you can find third-party workload resources that provide
additional behaviors. Using a
<a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource definition</a>,
you can add in a third-party workload resource if you want a specific behavior that's not part
of Kubernetes' core. For example, if you wanted to run a group of Pods for your application but
stop work unless <em>all</em> the Pods are available (perhaps for some high-throughput distributed task),
then you can implement or install an extension that does provide that feature.</p><h2 id="what-s-next">What's next</h2><p>As well as reading about each API kind for workload management, you can read how to
do specific tasks:</p><ul><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a stateless application using a Deployment</a></li><li>Run a stateful application either as a <a href="/docs/tasks/run-application/run-single-instance-stateful-application/">single instance</a>
or as a <a href="/docs/tasks/run-application/run-replicated-stateful-application/">replicated set</a></li><li><a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Run automated tasks with a CronJob</a></li></ul><p>To learn about Kubernetes' mechanisms for separating code from configuration,
visit <a href="/docs/concepts/configuration/">Configuration</a>.</p><p>There are two supporting concepts that provide backgrounds about how Kubernetes manages pods
for applications:</p><ul><li><a href="/docs/concepts/architecture/garbage-collection/">Garbage collection</a> tidies up objects
from your cluster after their <em>owning resource</em> has been removed.</li><li>The <a href="/docs/concepts/workloads/controllers/ttlafterfinished/"><em>time-to-live after finished</em> controller</a>
removes Jobs once a defined time has passed since they completed.</li></ul><p>Once your application is running, you might want to make it available on the internet as
a <a href="/docs/concepts/services-networking/service/">Service</a> or, for web application only,
using an <a href="/docs/concepts/services-networking/ingress/">Ingress</a>.</p><div class="section-index"/></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">StatefulSets</h1><div class="lead">A StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.</div><p>StatefulSet is the workload API object used to manage stateful applications.</p><p>Manages the deployment and scaling of a set of <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a>, <em>and provides guarantees about the ordering and uniqueness</em> of these Pods.</p><p>Like a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p><h2 id="using-statefulsets">Using StatefulSets</h2><p>StatefulSets are valuable for applications that require one or more of the
following:</p><ul><li>Stable, unique network identifiers.</li><li>Stable, persistent storage.</li><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates.</li></ul><p>In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn't require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
<a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> or
<a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> may be better suited to your stateless needs.</p><h2 id="limitations">Limitations</h2><ul><li>The storage for a given Pod must either be provisioned by a
<a href="/docs/concepts/storage/dynamic-provisioning/">PersistentVolume Provisioner</a>
based on the requested <em>storage class</em>, or pre-provisioned by an admin.</li><li>Deleting and/or scaling a StatefulSet down will <em>not</em> delete the volumes associated with the
StatefulSet. This is done to ensure data safety, which is generally more valuable than an
automatic purge of all related StatefulSet resources.</li><li>StatefulSets currently require a <a href="/docs/concepts/services-networking/service/#headless-services">Headless Service</a>
to be responsible for the network identity of the Pods. You are responsible for creating this
Service.</li><li>StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is
deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is
possible to scale the StatefulSet down to 0 prior to deletion.</li><li>When using <a href="#rolling-updates">Rolling Updates</a> with the default
<a href="#pod-management-policies">Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires
<a href="#forced-rollback">manual intervention to repair</a>.</li></ul><h2 id="components">Components</h2><p>The example below demonstrates the components of a StatefulSet.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># has to match .spec.template.metadata.labels</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">"nginx"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># by default is 1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># by default is 0</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># has to match .spec.selector.matchLabels</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>registry.k8s.io/nginx-slim:0.24<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">volumeClaimTemplates</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">"ReadWriteOnce"</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">"my-storage-class"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>This example uses the <code>ReadWriteOnce</code> access mode, for simplicity. For
production use, the Kubernetes project recommends using the <code>ReadWriteOncePod</code>
access mode instead.</div><p>In the above example:</p><ul><li>A Headless Service, named <code>nginx</code>, is used to control the network domain.</li><li>The StatefulSet, named <code>web</code>, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.</li><li>The <code>volumeClaimTemplates</code> will provide stable storage using
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> provisioned by a
PersistentVolume Provisioner.</li></ul><p>The name of a StatefulSet object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><h3 id="pod-selector">Pod Selector</h3><p>You must set the <code>.spec.selector</code> field of a StatefulSet to match the labels of its
<code>.spec.template.metadata.labels</code>. Failing to specify a matching Pod Selector will result in a
validation error during StatefulSet creation.</p><h3 id="volume-claim-templates">Volume Claim Templates</h3><p>You can set the <code>.spec.volumeClaimTemplates</code> field to create a
<a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim</a>.
This will provide stable storage to the StatefulSet if either:</p><ul><li>The StorageClass specified for the volume claim is set up to use <a href="/docs/concepts/storage/dynamic-provisioning/">dynamic
provisioning</a>.</li><li>The cluster already contains a PersistentVolume with the correct StorageClass
and sufficient available storage space.</li></ul><h3 id="minimum-ready-seconds">Minimum ready seconds</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be running and ready without any of its containers crashing, for it to be considered available.
This is used to check progression of a rollout when using a <a href="#rolling-updates">Rolling Update</a> strategy.
This field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</p><h2 id="pod-identity">Pod Identity</h2><p>StatefulSet Pods have a unique identity that consists of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it's (re)scheduled on.</p><h3 id="ordinal-index">Ordinal Index</h3><p>For a StatefulSet with N <a href="#replicas">replicas</a>, each Pod in the StatefulSet
will be assigned an integer ordinal, that is unique over the Set. By default,
pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller
will also add a pod label with this index: <code>apps.kubernetes.io/pod-index</code>.</p><h3 id="start-ordinal">Start ordinal</h3><div class="feature-state-notice feature-stable" title="Feature Gate: StatefulSetStartOrdinal"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p><code>.spec.ordinals</code> is an optional field that allows you to configure the integer
ordinals assigned to each Pod. It defaults to nil. Within the field, you can
configure the following options:</p><ul><li><code>.spec.ordinals.start</code>: If the <code>.spec.ordinals.start</code> field is set, Pods will
be assigned ordinals from <code>.spec.ordinals.start</code> up through
<code>.spec.ordinals.start + .spec.replicas - 1</code>.</li></ul><h3 id="stable-network-id">Stable Network ID</h3><p>Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is <code>$(statefulset name)-$(ordinal)</code>. The example above will create three Pods
named <code>web-0,web-1,web-2</code>.
A StatefulSet can use a <a href="/docs/concepts/services-networking/service/#headless-services">Headless Service</a>
to control the domain of its Pods. The domain managed by this Service takes the form:
<code>$(service name).$(namespace).svc.cluster.local</code>, where "cluster.local" is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
<code>$(podname).$(governing service domain)</code>, where the governing service is defined
by the <code>serviceName</code> field on the StatefulSet.</p><p>Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.</p><p>If you need to discover Pods promptly after they are created, you have a few options:</p><ul><li>Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.</li><li>Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the
config map for CoreDNS, which currently caches for 30 seconds).</li></ul><p>As mentioned in the <a href="#limitations">limitations</a> section, you are responsible for
creating the <a href="/docs/concepts/services-networking/service/#headless-services">Headless Service</a>
responsible for the network identity of the pods.</p><p>Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/name)</th><th>StatefulSet (ns/name)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Cluster Domain will be set to <code>cluster.local</code> unless
<a href="/docs/concepts/services-networking/dns-pod-service/">otherwise configured</a>.</div><h3 id="stable-storage">Stable Storage</h3><p>For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one
PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume
with a StorageClass of <code>my-storage-class</code> and 1 GiB of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its <code>volumeMounts</code> mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.</p><h3 id="pod-name-label">Pod Name Label</h3><p>When the StatefulSet <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> creates a Pod,
it adds a label, <code>statefulset.kubernetes.io/pod-name</code>, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.</p><h3 id="pod-index-label">Pod index label</h3><div class="feature-state-notice feature-stable" title="Feature Gate: PodIndexLabel"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>When the StatefulSet <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> creates a Pod,
the new Pod is labelled with <code>apps.kubernetes.io/pod-index</code>. The value of this label is the ordinal index of
the Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics
using the pod index label, and more. Note the feature gate <code>PodIndexLabel</code> is enabled and locked by default for this
feature, in order to disable it, users will have to use server emulated version v1.31.</p><h2 id="deployment-and-scaling-guarantees">Deployment and Scaling Guarantees</h2><ul><li>For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.</li><li>When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.</li><li>Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.</li><li>Before a Pod is terminated, all of its successors must be completely shutdown.</li></ul><p>The StatefulSet should not specify a <code>pod.Spec.TerminationGracePeriodSeconds</code> of 0. This practice
is unsafe and strongly discouraged. For further explanation, please refer to
<a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">force deleting StatefulSet Pods</a>.</p><p>When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
<a href="/docs/concepts/workloads/pods/pod-lifecycle/">Running and Ready</a>, and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.</p><p>If a user were to scale the deployed example by patching the StatefulSet such that
<code>replicas=1</code>, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1's termination, web-1 would not be terminated
until web-0 is Running and Ready.</p><h3 id="pod-management-policies">Pod Management Policies</h3><p>StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its <code>.spec.podManagementPolicy</code> field.</p><h4 id="orderedready-pod-management">OrderedReady Pod Management</h4><p><code>OrderedReady</code> pod management is the default for StatefulSets. It implements the behavior
described in <a href="#deployment-and-scaling-guarantees">Deployment and Scaling Guarantees</a>.</p><h4 id="parallel-pod-management">Parallel Pod Management</h4><p><code>Parallel</code> pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not
affected.</p><h2 id="update-strategies">Update strategies</h2><p>A StatefulSet's <code>.spec.updateStrategy</code> field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet. There are two possible values:</p><dl><dt><code>OnDelete</code></dt><dd>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>OnDelete</code>,
the StatefulSet controller will not automatically update the Pods in a
StatefulSet. Users must manually delete Pods to cause the controller to
create new Pods that reflect modifications made to a StatefulSet's <code>.spec.template</code>.</dd><dt><code>RollingUpdate</code></dt><dd>The <code>RollingUpdate</code> update strategy implements automated, rolling updates for the Pods in a
StatefulSet. This is the default update strategy.</dd></dl><h2 id="rolling-updates">Rolling Updates</h2><p>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>RollingUpdate</code>, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time.</p><p>The Kubernetes control plane waits until an updated Pod is Running and Ready prior
to updating its predecessor. If you have set <code>.spec.minReadySeconds</code> (see
<a href="#minimum-ready-seconds">Minimum Ready Seconds</a>), the control plane additionally waits that
amount of time after the Pod turns ready, before moving on.</p><h3 id="partitions">Partitioned rolling updates</h3><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a
<code>.spec.updateStrategy.rollingUpdate.partition</code>. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet's
<code>.spec.template</code> is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet's <code>.spec.updateStrategy.rollingUpdate.partition</code> is greater than its <code>.spec.replicas</code>,
updates to its <code>.spec.template</code> will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.</p><h3 id="maximum-unavailable-pods">Maximum unavailable Pods</h3><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [alpha]</code></div><p>You can control the maximum number of Pods that can be unavailable during an update
by specifying the <code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code> field.
The value can be an absolute number (for example, <code>5</code>) or a percentage of desired
Pods (for example, <code>10%</code>). Absolute number is calculated from the percentage value
by rounding it up. This field cannot be 0. The default setting is 1.</p><p>This field applies to all Pods in the range <code>0</code> to <code>replicas - 1</code>. If there is any
unavailable Pod in the range <code>0</code> to <code>replicas - 1</code>, it will be counted towards
<code>maxUnavailable</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The <code>maxUnavailable</code> field is in Alpha stage and it is honored only by API servers
that are running with the <code>MaxUnavailableStatefulSet</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
enabled.</div><h3 id="forced-rollback">Forced rollback</h3><p>When using <a href="#rolling-updates">Rolling Updates</a> with the default
<a href="#pod-management-policies">Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires manual intervention to repair.</p><p>If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.</p><p>In this state, it's not enough to revert the Pod template to a good configuration.
Due to a <a href="https://github.com/kubernetes/kubernetes/issues/67250">known issue</a>,
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.</p><p>After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.</p><h2 id="revision-history">Revision history</h2><p>ControllerRevision is a Kubernetes API resource used by controllers, such as the StatefulSet controller, to track historical configuration changes.</p><p>StatefulSets use ControllerRevisions to maintain a revision history, enabling rollbacks and version tracking.</p><h3 id="how-statefulsets-track-changes-using-controllerrevisions">How StatefulSets track changes using ControllerRevisions</h3><p>When you update a StatefulSet's Pod template (<code>spec.template</code>), the StatefulSet controller:</p><ol><li>Prepares a new ControllerRevision object</li><li>Stores a snapshot of the Pod template and metadata</li><li>Assigns an incremental revision number</li></ol><h4 id="key-properties">Key Properties</h4><p>See <a href="/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/">ControllerRevision</a> to learn more about key properties and other details.</p><hr/><h3 id="managing-revision-history">Managing Revision History</h3><p>Control retained revisions with <code>.spec.revisionHistoryLimit</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>webapp<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">revisionHistoryLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Keep last 5 revisions</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ... other spec fields ...</span><span style="color:#bbb">
</span></span></span></code></pre></div><ul><li><strong>Default</strong>: 10 revisions retained if unspecified</li><li><strong>Cleanup</strong>: Oldest revisions are garbage-collected when exceeding the limit</li></ul><h4 id="performing-rollbacks">Performing Rollbacks</h4><p>You can revert to a previous configuration using:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># View revision history</span>
</span></span><span style="display:flex"><span>kubectl rollout <span style="color:#a2f">history</span> statefulset/webapp
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Rollback to a specific revision</span>
</span></span><span style="display:flex"><span>kubectl rollout undo statefulset/webapp --to-revision<span style="color:#666">=</span><span style="color:#666">3</span>
</span></span></code></pre></div><p>This will:</p><ul><li>Apply the Pod template from revision 3</li><li>Create a new ControllerRevision with an updated revision number</li></ul><h4 id="inspecting-controllerrevisions">Inspecting ControllerRevisions</h4><p>To view associated ControllerRevisions:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span><span style="color:#080;font-style:italic"># List all revisions for the StatefulSet</span>
</span></span><span style="display:flex"><span>kubectl get controllerrevisions -l app.kubernetes.io/name<span style="color:#666">=</span>webapp
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># View detailed configuration of a specific revision</span>
</span></span><span style="display:flex"><span>kubectl get controllerrevision/webapp-3 -o yaml
</span></span></code></pre></div><h4 id="best-practices">Best Practices</h4><h5 id="retention-policy">Retention Policy</h5><ul><li>Set <code>revisionHistoryLimit</code> between <strong>510</strong> for most workloads.</li><li>Increase only if <strong>deep rollback history</strong> is required.</li></ul><h5 id="monitoring">Monitoring</h5><ul><li><p>Regularly check revisions with:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get controllerrevisions
</span></span></code></pre></div></li><li><p>Alert on <strong>rapid revision count growth</strong>.</p></li></ul><h5 id="avoid">Avoid</h5><ul><li>Manual edits to ControllerRevision objects.</li><li>Using revisions as a backup mechanism (use actual backup tools).</li><li>Setting <code>revisionHistoryLimit: 0</code> (disables rollback capability).</li></ul><h2 id="persistentvolumeclaim-retention">PersistentVolumeClaim retention</h2><div class="feature-state-notice feature-stable" title="Feature Gate: StatefulSetAutoDeletePVC"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The optional <code>.spec.persistentVolumeClaimRetentionPolicy</code> field controls if
and how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the
<code>StatefulSetAutoDeletePVC</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
on the API server and the controller manager to use this field.
Once enabled, there are two policies you can configure for each StatefulSet:</p><dl><dt><code>whenDeleted</code></dt><dd>Configures the volume retention behavior that applies when the StatefulSet is deleted.</dd><dt><code>whenScaled</code></dt><dd>Configures the volume retention behavior that applies when the replica count of
the StatefulSet is reduced; for example, when scaling down the set.</dd></dl><p>For each policy that you can configure, you can set the value to either <code>Delete</code> or <code>Retain</code>.</p><dl><dt><code>Delete</code></dt><dd>The PVCs created from the StatefulSet <code>volumeClaimTemplate</code> are deleted for each Pod
affected by the policy. With the <code>whenDeleted</code> policy all PVCs from the
<code>volumeClaimTemplate</code> are deleted after their Pods have been deleted. With the
<code>whenScaled</code> policy, only PVCs corresponding to Pod replicas being scaled down are
deleted, after their Pods have been deleted.</dd><dt><code>Retain</code> (default)</dt><dd>PVCs from the <code>volumeClaimTemplate</code> are not affected when their Pod is
deleted. This is the behavior before this new feature.</dd></dl><p>Bear in mind that these policies <strong>only</strong> apply when Pods are being removed due to the
StatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet
fails due to node failure, and the control plane creates a replacement Pod, the StatefulSet
retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to
the node where the new Pod is about to launch.</p><p>The default for policies is <code>Retain</code>, matching the StatefulSet behavior before this new feature.</p><p>Here is an example policy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">persistentVolumeClaimRetentionPolicy</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenDeleted</span>:<span style="color:#bbb"> </span>Retain<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">whenScaled</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">...</span><span style="color:#bbb">
</span></span></span></code></pre></div><p>The StatefulSet <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a> adds
<a href="/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications">owner references</a>
to its PVCs, which are then deleted by the <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/garbage-collection/" target="_blank" aria-label="garbage collector">garbage collector</a> after the Pod is terminated. This enables the Pod to
cleanly unmount all volumes before the PVCs are deleted (and before the backing PV and
volume are deleted, depending on the retain policy). When you set the <code>whenDeleted</code>
policy to <code>Delete</code>, an owner reference to the StatefulSet instance is placed on all PVCs
associated with that StatefulSet.</p><p>The <code>whenScaled</code> policy must delete PVCs only when a Pod is scaled down, and not when a
Pod is deleted for another reason. When reconciling, the StatefulSet controller compares
its desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod
whose id greater than the replica count is condemned and marked for deletion. If the
<code>whenScaled</code> policy is <code>Delete</code>, the condemned Pods are first set as owners to the
associated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs
to be garbage collected after only the condemned Pods have terminated.</p><p>This means that if the controller crashes and restarts, no Pod will be deleted before its
owner reference has been updated appropriate to the policy. If a condemned Pod is
force-deleted while the controller is down, the owner reference may or may not have been
set up, depending on when the controller crashed. It may take several reconcile loops to
update the owner references, so some condemned Pods may have set up owner references and
others may not. For this reason we recommend waiting for the controller to come back up,
which will verify owner references before terminating Pods. If that is not possible, the
operator should verify the owner references on PVCs to ensure the expected objects are
deleted when Pods are force-deleted.</p><h3 id="replicas">Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a deployment, example via <code>kubectl scale statefulset statefulset --replicas=X</code>, and then you update that StatefulSet
based on a manifest (for example: by running <code>kubectl apply -f statefulset.yaml</code>), then applying that manifest overwrites the manual scaling
that you previously did.</p><p>If a <a href="/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a>
(or any similar API for horizontal scaling) is managing scaling for a
Statefulset, don't set <code>.spec.replicas</code>. Instead, allow the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> to manage
the <code>.spec.replicas</code> field automatically.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Find out how to use StatefulSets<ul><li>Follow an example of <a href="/docs/tutorials/stateful-application/basic-stateful-set/">deploying a stateful application</a>.</li><li>Follow an example of <a href="/docs/tutorials/stateful-application/cassandra/">deploying Cassandra with Stateful Sets</a>.</li><li>Follow an example of <a href="/docs/tasks/run-application/run-replicated-stateful-application/">running a replicated stateful application</a>.</li><li>Learn how to <a href="/docs/tasks/run-application/scale-stateful-set/">scale a StatefulSet</a>.</li><li>Learn what's involved when you <a href="/docs/tasks/run-application/delete-stateful-set/">delete a StatefulSet</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/configure-volume-storage/">configure a Pod to use a volume for storage</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">configure a Pod to use a PersistentVolume for storage</a>.</li></ul></li><li><code>StatefulSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/">StatefulSet</a>
object definition to understand the API for stateful sets.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Autoscaling Workloads</h1><div class="lead">With autoscaling, you can automatically update your workloads in one way or another. This allows your cluster to react to changes in resource demand more elastically and efficiently.</div><p>In Kubernetes, you can <em>scale</em> a workload depending on the current demand of resources.
This allows your cluster to react to changes in resource demand more elastically and efficiently.</p><p>When you scale a workload, you can either increase or decrease the number of replicas managed by
the workload, or adjust the resources available to the replicas in-place.</p><p>The first approach is referred to as <em>horizontal scaling</em>, while the second is referred to as
<em>vertical scaling</em>.</p><p>There are manual and automatic ways to scale your workloads, depending on your use case.</p><h2 id="scaling-workloads-manually">Scaling workloads manually</h2><p>Kubernetes supports <em>manual scaling</em> of workloads. Horizontal scaling can be done
using the <code>kubectl</code> CLI.
For vertical scaling, you need to <em>patch</em> the resource definition of your workload.</p><p>See below for examples of both strategies.</p><ul><li><strong>Horizontal scaling</strong>: <a href="/docs/tutorials/kubernetes-basics/scale/scale-intro/">Running multiple instances of your app</a></li><li><strong>Vertical scaling</strong>: <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resizing CPU and memory resources assigned to containers</a></li></ul><h2 id="scaling-workloads-automatically">Scaling workloads automatically</h2><p>Kubernetes also supports <em>automatic scaling</em> of workloads, which is the focus of this page.</p><p>The concept of <em>Autoscaling</em> in Kubernetes refers to the ability to automatically update an
object that manages a set of Pods (for example a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/controllers/deployment/" target="_blank" aria-label="Deployment">Deployment</a>).</p><h3 id="scaling-workloads-horizontally">Scaling workloads horizontally</h3><p>In Kubernetes, you can automatically scale a workload horizontally using a <em>HorizontalPodAutoscaler</em> (HPA).</p><p>It is implemented as a Kubernetes API resource and a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/controller/" target="_blank" aria-label="controller">controller</a>
and periodically adjusts the number of <a class="glossary-tooltip" title="Replicas are copies of pods, ensuring availability, scalability, and fault tolerance by maintaining identical instances." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-replica" target="_blank" aria-label="replicas">replicas</a>
in a workload to match observed resource utilization such as CPU or memory usage.</p><p>There is a <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough tutorial</a> of configuring a HorizontalPodAutoscaler for a Deployment.</p><h3 id="scaling-workloads-vertically">Scaling workloads vertically</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You can automatically scale a workload vertically using a <em>VerticalPodAutoscaler</em> (VPA).
Unlike the HPA, the VPA doesn't come with Kubernetes by default, but is a separate project
that can be found <a href="https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler">on GitHub</a>.</p><p>Once installed, it allows you to create <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." data-toggle="tooltip" data-placement="top" href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank" aria-label="CustomResourceDefinitions">CustomResourceDefinitions</a>
(CRDs) for your workloads which define <em>how</em> and <em>when</em> to scale the resources of the managed replicas.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>You will need to have the <a href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server</a>
installed to your cluster for the VPA to work.</div><p>At the moment, the VPA can operate in four different modes:</p><table><caption style="display:none">Different modes of the VPA</caption><thead><tr><th style="text-align:left">Mode</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>Auto</code></td><td style="text-align:left">Currently <code>Recreate</code>. This might change to in-place updates in the future.</td></tr><tr><td style="text-align:left"><code>Recreate</code></td><td style="text-align:left">The VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendation</td></tr><tr><td style="text-align:left"><code>Initial</code></td><td style="text-align:left">The VPA only assigns resource requests on pod creation and never changes them later.</td></tr><tr><td style="text-align:left"><code>Off</code></td><td style="text-align:left">The VPA does not automatically change the resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.</td></tr></tbody></table><h4 id="in-place-pod-vertical-scaling">In-place pod vertical scaling</h4><div class="feature-state-notice feature-beta" title="Feature Gate: InPlacePodVerticalScaling"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>As of Kubernetes 1.34, VPA does not support resizing pods in-place,
but this integration is being worked on.
For manually resizing pods in-place, see <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources In-Place</a>.</p><h3 id="autoscaling-based-on-cluster-size">Autoscaling based on cluster size</h3><p>For workloads that need to be scaled based on the size of the cluster (for example
<code>cluster-dns</code> or other system components), you can use the
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler"><em>Cluster Proportional Autoscaler</em></a>.
Just like the VPA, it is not part of the Kubernetes core, but hosted as its
own project on GitHub.</p><p>The Cluster Proportional Autoscaler watches the number of schedulable <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a>
and cores and scales the number of replicas of the target workload accordingly.</p><p>If the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using
the <a href="https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler"><em>Cluster Proportional Vertical Autoscaler</em></a>.
The project is <strong>currently in beta</strong> and can be found on GitHub.</p><p>While the Cluster Proportional Autoscaler scales the number of replicas of a workload,
the Cluster Proportional Vertical Autoscaler adjusts the resource requests for a workload
(for example a Deployment or DaemonSet) based on the number of nodes and/or cores in the cluster.</p><h3 id="event-driven-autoscaling">Event driven Autoscaling</h3><p>It is also possible to scale workloads based on events, for example using the
<a href="https://keda.sh/"><em>Kubernetes Event Driven Autoscaler</em> (<strong>KEDA</strong>)</a>.</p><p>KEDA is a CNCF-graduated project enabling you to scale your workloads based on the number
of events to be processed, for example the amount of messages in a queue. There exists
a wide range of adapters for different event sources to choose from.</p><h3 id="autoscaling-based-on-schedules">Autoscaling based on schedules</h3><p>Another strategy for scaling your workloads is to <strong>schedule</strong> the scaling operations, for example in order to
reduce resource consumption during off-peak hours.</p><p>Similar to event driven autoscaling, such behavior can be achieved using KEDA in conjunction with
its <a href="https://keda.sh/docs/latest/scalers/cron/"><code>Cron</code> scaler</a>.
The <code>Cron</code> scaler allows you to define schedules (and time zones) for scaling your workloads in or out.</p><h2 id="scaling-cluster-infrastructure">Scaling cluster infrastructure</h2><p>If scaling workloads isn't enough to meet your needs, you can also scale your cluster infrastructure itself.</p><p>Scaling the cluster infrastructure normally means adding or removing <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a>.
Read <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a>
for more information.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about scaling horizontally<ul><li><a href="/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a></li><li><a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">HorizontalPodAutoscaler Walkthrough</a></li></ul></li><li><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources In-Place</a></li><li><a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></li><li>Learn about <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Managing Workloads</h1><p>You've deployed your application and exposed it via a Service. Now what? Kubernetes provides a
number of tools to help you manage your application deployment, including scaling and updating.</p><h2 id="organizing-resource-configurations">Organizing resource configurations</h2><p>Many applications require multiple resources to be created, such as a Deployment along with a Service.
Management of multiple resources can be simplified by grouping them together in the same file
(separated by <code>---</code> in YAML). For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx-app.yaml" download="application/nginx-app.yaml"><code>application/nginx-app.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;application-nginx-app-yaml&quot;)" title="Copy application/nginx-app.yaml to clipboard"/></div><div class="includecode" id="application-nginx-app-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-nginx-svc<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Multiple resources can be created the same way as a single resource:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">service/my-nginx-svc created
deployment.apps/my-nginx created
</code></pre><p>The resources will be created in the order they appear in the manifest. Therefore, it's best to
specify the Service first, since that will ensure the scheduler can spread the pods associated
with the Service as they are created by the controller(s), such as Deployment.</p><p><code>kubectl apply</code> also accepts multiple <code>-f</code> arguments:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml <span style="color:#b62;font-weight:700">\
</span></span></span><span style="display:flex"><span><span style="color:#b62;font-weight:700"/>  -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><p>It is a recommended practice to put resources related to the same microservice or application tier
into the same file, and to group all of the files associated with your application in the same
directory. If the tiers of your application bind to each other using DNS, you can deploy all of
the components of your stack together.</p><p>A URL can also be specified as a configuration source, which is handy for deploying directly from
manifests in your source control system:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx created
</code></pre><p>If you need to define more manifests, such as adding a ConfigMap, you can do that too.</p><h3 id="external-tools">External tools</h3><p>This section lists only the most common tools used for managing workloads on Kubernetes. To see a larger list, view
<a href="https://landscape.cncf.io/guide#app-definition-and-development--application-definition-image-build">Application definition and image build</a>
in the <a class="glossary-tooltip" title="Cloud Native Computing Foundation" data-toggle="tooltip" data-placement="top" href="https://cncf.io/" target="_blank" aria-label="CNCF">CNCF</a> Landscape.</p><h4 id="external-tool-helm">Helm</h4><div class="alert alert-secondary callout third-party-content" role="alert"> This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p><a href="https://helm.sh/">Helm</a> is a tool for managing packages of pre-configured
Kubernetes resources. These packages are known as <em>Helm charts</em>.</p><h4 id="external-tool-kustomize">Kustomize</h4><p><a href="https://kustomize.io/">Kustomize</a> traverses a Kubernetes manifest to add, remove or update configuration options.
It is available both as a standalone binary and as a <a href="/docs/tasks/manage-kubernetes-objects/kustomization/">native feature</a>
of kubectl.</p><h2 id="bulk-operations-in-kubectl">Bulk operations in kubectl</h2><p>Resource creation isn't the only operation that <code>kubectl</code> can perform in bulk. It can also extract
resource names from configuration files in order to perform other operations, in particular to
delete the same resources you created:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps "my-nginx" deleted
service "my-nginx-svc" deleted
</code></pre><p>In the case of two resources, you can specify both resources on the command line using the
resource/name syntax:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployments/my-nginx services/my-nginx-svc
</span></span></code></pre></div><p>For larger numbers of resources, you'll find it easier to specify the selector (label query)
specified using <code>-l</code> or <code>--selector</code>, to filter resources by their labels:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl delete deployment,services -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps "my-nginx" deleted
service "my-nginx-svc" deleted
</code></pre><h3 id="chaining-and-filtering">Chaining and filtering</h3><p>Because <code>kubectl</code> outputs resource names in the same syntax it accepts, you can chain operations
using <code>$()</code> or <code>xargs</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get <span style="color:#a2f;font-weight:700">$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ <span style="color:#a2f;font-weight:700">)</span>
</span></span><span style="display:flex"><span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ | xargs -i kubectl get <span style="color:#b44">'{}'</span>
</span></span></code></pre></div><p>The output might be similar to:</p><pre tabindex="0"><code class="language-none" data-lang="none">NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGE
my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</code></pre><p>With the above commands, first you create resources under <code>docs/concepts/cluster-administration/nginx/</code> and print
the resources created with <code>-o name</code> output format (print each resource as resource/name).
Then you <code>grep</code> only the Service, and then print it with <a href="/docs/reference/kubectl/generated/kubectl_get/"><code>kubectl get</code></a>.</p><h3 id="recursive-operations-on-local-files">Recursive operations on local files</h3><p>If you happen to organize your resources across several subdirectories within a particular
directory, you can recursively perform the operations on the subdirectories also, by specifying
<code>--recursive</code> or <code>-R</code> alongside the <code>--filename</code>/<code>-f</code> argument.</p><p>For instance, assume there is a directory <code>project/k8s/development</code> that holds all of the
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-manifest" target="_blank" aria-label="manifests">manifests</a> needed for the development environment,
organized by resource type:</p><pre tabindex="0"><code class="language-none" data-lang="none">project/k8s/development
 configmap
  my-configmap.yaml
 deployment
  my-deployment.yaml
 pvc
     my-pvc.yaml
</code></pre><p>By default, performing a bulk operation on <code>project/k8s/development</code> will stop at the first level
of the directory, not processing any subdirectories. If you had tried to create the resources in
this directory using the following command, we would have encountered an error:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f project/k8s/development
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)
</code></pre><p>Instead, specify the <code>--recursive</code> or <code>-R</code> command line argument along with the <code>--filename</code>/<code>-f</code> argument:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f project/k8s/development --recursive
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre><p>The <code>--recursive</code> argument works with any operation that accepts the <code>--filename</code>/<code>-f</code> argument such as:
<code>kubectl create</code>, <code>kubectl get</code>, <code>kubectl delete</code>, <code>kubectl describe</code>, or even <code>kubectl rollout</code>.</p><p>The <code>--recursive</code> argument also works when multiple <code>-f</code> arguments are provided:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">namespace/development created
namespace/staging created
configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre><p>If you're interested in learning more about <code>kubectl</code>, go ahead and read
<a href="/docs/reference/kubectl/">Command line tool (kubectl)</a>.</p><h2 id="updating-your-application-without-an-outage">Updating your application without an outage</h2><p>At some point, you'll eventually need to update your deployed application, typically by specifying
a new image or image tag. <code>kubectl</code> supports several update operations, each of which is applicable
to different scenarios.</p><p>You can run multiple copies of your app, and use a <em>rollout</em> to gradually shift the traffic to
new healthy Pods. Eventually, all the running Pods would have the new software.</p><p>This section of the page guides you through how to create and update applications with Deployments.</p><p>Let's say you were running version 1.14.2 of nginx:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl create deployment my-nginx --image<span style="color:#666">=</span>nginx:1.14.2
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx created
</code></pre><p>Ensure that there is 1 replica:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale --replicas <span style="color:#666">1</span> deployments/my-nginx --subresource<span style="color:#666">=</span><span style="color:#b44">'scale'</span> --type<span style="color:#666">=</span><span style="color:#b44">'merge'</span> -p <span style="color:#b44">'{"spec":{"replicas": 1}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx scaled
</code></pre><p>and allow Kubernetes to add more temporary replicas during a rollout, by setting a <em>surge maximum</em> of
100%:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl patch --type<span style="color:#666">=</span><span style="color:#b44">'merge'</span> -p <span style="color:#b44">'{"spec":{"strategy":{"rollingUpdate":{"maxSurge": "100%" }}}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx patched
</code></pre><p>To update to version 1.16.1, change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code>
to <code>nginx:1.16.1</code> using <code>kubectl edit</code>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit deployment/my-nginx
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># Change the manifest to use the newer container image, then save your changes</span>
</span></span></code></pre></div><p>That's it! The Deployment will declaratively update the deployed nginx application progressively
behind the scene. It ensures that only a certain number of old replicas may be down while they are
being updated, and only a certain number of new replicas may be created above the desired number
of pods. To learn more details about how this happens,
visit <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>.</p><p>You can use rollouts with DaemonSets, Deployments, or StatefulSets.</p><h3 id="managing-rollouts">Managing rollouts</h3><p>You can use <a href="/docs/reference/kubectl/generated/kubectl_rollout/"><code>kubectl rollout</code></a> to manage a
progressive update of an existing application.</p><p>For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f my-deployment.yaml
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># wait for rollout to finish</span>
</span></span><span style="display:flex"><span>kubectl rollout status deployment/my-deployment --timeout 10m <span style="color:#080;font-style:italic"># 10 minute timeout</span>
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f backing-stateful-component.yaml
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># don't wait for rollout to finish, just check the status</span>
</span></span><span style="display:flex"><span>kubectl rollout status statefulsets/backing-stateful-component --watch<span style="color:#666">=</span><span style="color:#a2f">false</span>
</span></span></code></pre></div><p>You can also pause, resume or cancel a rollout.
Visit <a href="/docs/reference/kubectl/generated/kubectl_rollout/"><code>kubectl rollout</code></a> to learn more.</p><h2 id="canary-deployments">Canary deployments</h2><p>Another scenario where multiple labels are needed is to distinguish deployments of different
releases or configurations of the same component. It is common practice to deploy a <em>canary</em> of a
new application release (specified via image tag in the pod template) side by side with the
previous release so that the new release can receive live production traffic before fully rolling
it out.</p><p>For instance, you can use a <code>track</code> label to differentiate different releases.</p><p>The primary, stable release would have a <code>track</code> label with value as <code>stable</code>:</p><pre tabindex="0"><code class="language-none" data-lang="none">name: frontend
replicas: 3
...
labels:
   app: guestbook
   tier: frontend
   track: stable
...
image: gb-frontend:v3
</code></pre><p>and then you can create a new release of the guestbook frontend that carries the <code>track</code> label
with different value (i.e. <code>canary</code>), so that two sets of pods would not overlap:</p><pre tabindex="0"><code class="language-none" data-lang="none">name: frontend-canary
replicas: 1
...
labels:
   app: guestbook
   tier: frontend
   track: canary
...
image: gb-frontend:v4
</code></pre><p>The frontend service would span both sets of replicas by selecting the common subset of their
labels (i.e. omitting the <code>track</code> label), so that the traffic will be redirected to both
applications:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">   </span><span style="color:green;font-weight:700">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></span></span></code></pre></div><p>You can tweak the number of replicas of the stable and canary releases to determine the ratio of
each release that will receive live production traffic (in this case, 3:1).
Once you're confident, you can update the stable track to the new application release and remove
the canary one.</p><h2 id="updating-annotations">Updating annotations</h2><p>Sometimes you would want to attach annotations to resources. Annotations are arbitrary
non-identifying metadata for retrieval by API clients such as tools or libraries.
This can be done with <code>kubectl annotate</code>. For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl annotate pods my-nginx-v4-9gw19 <span style="color:#b8860b">description</span><span style="color:#666">=</span><span style="color:#b44">'my frontend running nginx'</span>
</span></span><span style="display:flex"><span>kubectl get pods my-nginx-v4-9gw19 -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>apiVersion: v1
</span></span><span style="display:flex"><span>kind: pod
</span></span><span style="display:flex"><span>metadata:
</span></span><span style="display:flex"><span>  annotations:
</span></span><span style="display:flex"><span>    description: my frontend running nginx
</span></span><span style="display:flex"><span>...
</span></span></code></pre></div><p>For more information, see <a href="/docs/concepts/overview/working-with-objects/annotations/">annotations</a>
and <a href="/docs/reference/kubectl/generated/kubectl_annotate/">kubectl annotate</a>.</p><h2 id="scaling-your-application">Scaling your application</h2><p>When load on your application grows or shrinks, use <code>kubectl</code> to scale your application.
For instance, to decrease the number of nginx replicas from 3 to 1, do:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl scale deployment/my-nginx --replicas<span style="color:#666">=</span><span style="color:#666">1</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx scaled
</code></pre><p>Now you only have one pod managed by the deployment.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>my-nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME                        READY     STATUS    RESTARTS   AGE
my-nginx-2035384211-j5fhi   1/1       Running   0          30m
</code></pre><p>To have the system automatically choose the number of nginx replicas as needed,
ranging from 1 to 3, do:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span><span style="color:#080;font-style:italic"># This requires an existing source of container and Pod metrics</span>
</span></span><span style="display:flex"><span>kubectl autoscale deployment/my-nginx --min<span style="color:#666">=</span><span style="color:#666">1</span> --max<span style="color:#666">=</span><span style="color:#666">3</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</code></pre><p>Now your nginx replicas will be scaled up and down as needed, automatically.</p><p>For more information, please see <a href="/docs/reference/kubectl/generated/kubectl_scale/">kubectl scale</a>,
<a href="/docs/reference/kubectl/generated/kubectl_autoscale/">kubectl autoscale</a> and
<a href="/docs/tasks/run-application/horizontal-pod-autoscale/">horizontal pod autoscaler</a> document.</p><h2 id="in-place-updates-of-resources">In-place updates of resources</h2><p>Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.</p><h3 id="kubectl-apply">kubectl apply</h3><p>It is suggested to maintain a set of configuration files in source control
(see <a href="https://martinfowler.com/bliki/InfrastructureAsCode.html">configuration as code</a>),
so that they can be maintained and versioned along with the code for the resources they configure.
Then, you can use <a href="/docs/reference/kubectl/generated/kubectl_apply/"><code>kubectl apply</code></a>
to push your configuration changes to the cluster.</p><p>This command will compare the version of the configuration that you're pushing with the previous
version and apply the changes you've made, without overwriting any automated changes to properties
you haven't specified.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx configured
</code></pre><p>To learn more about the underlying mechanism, read <a href="/docs/reference/using-api/server-side-apply/">server-side apply</a>.</p><h3 id="kubectl-edit">kubectl edit</h3><p>Alternatively, you may also update resources with <a href="/docs/reference/kubectl/generated/kubectl_edit/"><code>kubectl edit</code></a>:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>This is equivalent to first <code>get</code> the resource, edit it in text editor, and then <code>apply</code> the
resource with the updated version:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
</span></span><span style="display:flex"><span>vi /tmp/nginx.yaml
</span></span><span style="display:flex"><span><span style="color:#080;font-style:italic"># do some edit, and then save the file</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>kubectl apply -f /tmp/nginx.yaml
</span></span><span style="display:flex"><span>deployment.apps/my-nginx configured
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>rm /tmp/nginx.yaml
</span></span></code></pre></div><p>This allows you to do more significant changes more easily. Note that you can specify the editor
with your <code>EDITOR</code> or <code>KUBE_EDITOR</code> environment variables.</p><p>For more information, please see <a href="/docs/reference/kubectl/generated/kubectl_edit/">kubectl edit</a>.</p><h3 id="kubectl-patch">kubectl patch</h3><p>You can use <a href="/docs/reference/kubectl/generated/kubectl_patch/"><code>kubectl patch</code></a> to update API objects in place.
This subcommand supports JSON patch,
JSON merge patch, and strategic merge patch.</p><p>See
<a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch</a>
for more details.</p><h2 id="disruptive-updates">Disruptive updates</h2><p>In some cases, you may need to update resource fields that cannot be updated once initialized, or
you may want to make a recursive change immediately, such as to fix broken pods created by a
Deployment. To change such fields, use <code>replace --force</code>, which deletes and re-creates the
resource. In this case, you can modify your original configuration file:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="display:flex"><span>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</span></span></code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/my-nginx deleted
deployment.apps/my-nginx replaced
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/tasks/debug/debug-application/debug-running-pod/">how to use <code>kubectl</code> for application introspection and debugging</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Guide for Running Windows Containers in Kubernetes</h1><p>This page provides a walkthrough for some steps you can follow to run
Windows containers using Kubernetes.
The page also highlights some Windows specific functionality within Kubernetes.</p><p>It is important to note that creating and deploying services and workloads on Kubernetes
behaves in much the same way for Linux and Windows containers.
The <a href="/docs/reference/kubectl/">kubectl commands</a> to interface with the cluster are identical.
The examples in this page are provided to jumpstart your experience with Windows containers.</p><h2 id="objectives">Objectives</h2><p>Configure an example deployment to run Windows containers on a Windows node.</p><h2 id="before-you-begin">Before you begin</h2><p>You should already have access to a Kubernetes cluster that includes a
worker node running Windows Server.</p><h2 id="getting-started-deploying-a-windows-workload">Getting Started: Deploying a Windows workload</h2><p>The example YAML file below deploys a simple webserver application running inside a Windows container.</p><p>Create a manifest named <code>win-webserver.yaml</code> with the contents below:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># the port that this service should serve on</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>NodePort<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>windowswebserver<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">command</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- powershell.exe<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- -command<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:#b44">"&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add('http://*:80/') ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host('Listening at http://*:80/') ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host '' ;Write-Host('&gt; {0}' -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header='&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;' ;$$callerCountsString='' ;$$callerCounts.Keys | % { $$callerCountsString+='&lt;p&gt;IP {0} callerCount {1} ' -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer='&lt;/body&gt;&lt;/html&gt;' ;$$content='{0}{1}{2}' -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host('&lt; {0}' -f $$responseStatus)  } ; "</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">     </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></span></span></code></pre></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Port mapping is also supported, but for simplicity this example exposes
port 80 of the container directly to the Service.</div><ol><li><p>Check that all nodes are healthy:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl get nodes
</span></span></code></pre></div></li><li><p>Deploy the service and watch for pod updates:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="display:flex"><span>kubectl apply -f win-webserver.yaml
</span></span><span style="display:flex"><span>kubectl get pods -o wide -w
</span></span></code></pre></div><p>When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.</p></li><li><p>Check that the deployment succeeded. To verify:</p><ul><li>Several pods listed from the Linux control plane node, use <code>kubectl get pods</code></li><li>Node-to-pod communication across the network, <code>curl</code> port 80 of your pod IPs from the Linux control plane node
to check for a web server response</li><li>Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)
using <code>kubectl exec</code></li><li>Service-to-pod communication, <code>curl</code> the virtual service IP (seen under <code>kubectl get services</code>)
from the Linux control plane node and from individual pods</li><li>Service discovery, <code>curl</code> the service name with the Kubernetes <a href="/docs/concepts/services-networking/dns-pod-service/#services">default DNS suffix</a></li><li>Inbound connectivity, <code>curl</code> the NodePort from the Linux control plane node or machines outside of the cluster</li><li>Outbound connectivity, <code>curl</code> external IPs from inside the pod using <code>kubectl exec</code></li></ul></li></ol><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.
Only Windows pods are able to access service IPs.</div><h2 id="observability">Observability</h2><h3 id="capturing-logs-from-workloads">Capturing logs from workloads</h3><p>Logs are an important element of observability; they enable users to gain insights
into the operational aspect of workloads and are a key ingredient to troubleshooting issues.
Because Windows containers and workloads inside Windows containers behave differently from Linux containers,
users had a hard time collecting logs, limiting operational visibility.
Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows)
or push entries to the application event log.
<a href="https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor">LogMonitor</a>, an open source tool by Microsoft,
is the recommended way to monitor configured log sources inside a Windows container.
LogMonitor supports monitoring event logs, ETW providers, and custom application logs,
piping them to STDOUT for consumption by <code>kubectl logs &lt;pod&gt;</code>.</p><p>Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files
to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.</p><h2 id="configuring-container-user">Configuring container user</h2><h3 id="using-configurable-container-usernames">Using configurable Container usernames</h3><p>Windows containers can be configured to run their entrypoints and processes
with different usernames than the image defaults.
Learn more about it <a href="/docs/tasks/configure-pod-container/configure-runasusername/">here</a>.</p><h3 id="managing-workload-identity-with-group-managed-service-accounts">Managing Workload Identity with Group Managed Service Accounts</h3><p>Windows container workloads can be configured to use Group Managed Service Accounts (GMSA).
Group Managed Service Accounts are a specific type of Active Directory account that provide automatic password management,
simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.
Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.
Learn more about configuring and using GMSA for Windows containers <a href="/docs/tasks/configure-pod-container/configure-gmsa/">here</a>.</p><h2 id="taints-and-tolerations">Taints and tolerations</h2><p>Users need to use some combination of <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="taint">taint</a>
and node selectors in order to schedule Linux and Windows workloads to their respective OS-specific nodes.
The recommended approach is outlined below,
with one of its main goals being that this approach should not break compatibility for existing Linux workloads.</p><p>You can (and should) set <code>.spec.os.name</code> for each Pod, to indicate the operating system
that the containers in that Pod are designed for. For Pods that run Linux containers, set
<code>.spec.os.name</code> to <code>linux</code>. For Pods that run Windows containers, set <code>.spec.os.name</code>
to <code>windows</code>.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If you are running a version of Kubernetes older than 1.24, you may need to enable
the <code>IdentifyPodOS</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
to be able to set a value for <code>.spec.pod.os</code>.</div><p>The scheduler does not use the value of <code>.spec.os.name</code> when assigning Pods to nodes. You should
use normal Kubernetes mechanisms for
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">assigning pods to nodes</a>
to ensure that the control plane for your cluster places pods onto nodes that are running the
appropriate operating system.</p><p>The <code>.spec.os.name</code> value has no effect on the scheduling of the Windows pods,
so taints and tolerations (or node selectors) are still required
to ensure that the Windows pods land onto appropriate Windows nodes.</p><h3 id="ensuring-os-specific-workloads-land-on-the-appropriate-container-host">Ensuring OS-specific workloads land on the appropriate container host</h3><p>Users can ensure Windows containers can be scheduled on the appropriate host using taints and tolerations.
All Kubernetes nodes running Kubernetes 1.34 have the following default labels:</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>If a Pod specification does not specify a <code>nodeSelector</code> such as <code>"kubernetes.io/os": windows</code>,
it is possible the Pod can be scheduled on any host, Windows or Linux.
This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.
The best practice for Kubernetes 1.34 is to use a <code>nodeSelector</code>.</p><p>However, in many cases users have a pre-existing large number of deployments for Linux containers,
as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with operators.
In those situations, you may be hesitant to make the configuration change to add <code>nodeSelector</code> fields to all Pods and Pod templates.
The alternative is to use taints. Because the kubelet can set taints during registration,
it could easily be modified to automatically add a taint when running on Windows only.</p><p>For example: <code>--register-with-taints='os=windows:NoSchedule'</code></p><p>By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).
In order for a Windows Pod to be scheduled on a Windows node,
it would need both the <code>nodeSelector</code> and the appropriate matching toleration to choose Windows.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">node.kubernetes.io/windows-build</span>:<span style="color:#bbb"> </span><span style="color:#b44">'10.0.17763'</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">"os"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">"Equal"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"windows"</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">"NoSchedule"</span><span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="handling-multiple-windows-versions-in-the-same-cluster">Handling multiple Windows versions in the same cluster</h3><p>The Windows Server version used by each pod must match that of the node. If you want to use multiple Windows
Server versions in the same cluster, then you should set additional node labels and <code>nodeSelector</code> fields.</p><p>Kubernetes automatically adds a label,
<a href="/docs/reference/labels-annotations-taints/#nodekubernetesiowindows-build"><code>node.kubernetes.io/windows-build</code></a>
to simplify this.</p><p>This label reflects the Windows major, minor, and build number that need to match for compatibility.
Here are values used for each Windows Server version:</p><table><thead><tr><th>Product Name</th><th>Version</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server 2022</td><td>10.0.20348</td></tr></tbody></table><h3 id="simplifying-with-runtimeclass">Simplifying with RuntimeClass</h3><p><a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a> can be used to simplify the process of using taints and tolerations.
A cluster administrator can create a <code>RuntimeClass</code> object which is used to encapsulate these taints and tolerations.</p><ol><li><p>Save this file to <code>runtimeClasses.yml</code>. It includes the appropriate <code>nodeSelector</code>
for the Windows OS, architecture, and version.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>node.k8s.io/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>RuntimeClass<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>windows-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">handler</span>:<span style="color:#bbb"> </span>example-container-runtime-handler<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">scheduling</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">nodeSelector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/os</span>:<span style="color:#bbb"> </span><span style="color:#b44">'windows'</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">kubernetes.io/arch</span>:<span style="color:#bbb"> </span><span style="color:#b44">'amd64'</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">node.kubernetes.io/windows-build</span>:<span style="color:#bbb"> </span><span style="color:#b44">'10.0.17763'</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>os<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>Equal<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">"windows"</span><span style="color:#bbb">
</span></span></span></code></pre></div></li><li><p>Run <code>kubectl create -f runtimeClasses.yml</code> using as a cluster administrator</p></li><li><p>Add <code>runtimeClassName: windows-2019</code> as appropriate to Pod specs</p><p>For example:</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">runtimeClassName</span>:<span style="color:#bbb"> </span>windows-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>iis<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>800Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>.1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>300Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span>- <span style="color:green;font-weight:700">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"> </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:#00f;font-weight:700">---</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>iis<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">ports</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span>- <span style="color:green;font-weight:700">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></span></span></code></pre></div></li></ol></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Downward API</h1><div class="lead">There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API.</div><p>It is sometimes useful for a container to have information about itself, without
being overly coupled to Kubernetes. The <em>downward API</em> allows containers to consume
information about themselves or the cluster without using the Kubernetes client
or API server.</p><p>An example is an existing application that assumes a particular well-known
environment variable holds a unique identifier. One possibility is to wrap the
application, but that is tedious and error-prone, and it violates the goal of low
coupling. A better option would be to use the Pod's name as an identifier, and
inject the Pod's name into the well-known environment variable.</p><p>In Kubernetes, there are two ways to expose Pod and container fields to a running container:</p><ul><li>as <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a></li><li>as <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">files in a <code>downwardAPI</code> volume</a></li></ul><p>Together, these two ways of exposing Pod and container fields are called the
<em>downward API</em>.</p><h2 id="available-fields">Available fields</h2><p>Only some Kubernetes API fields are available through the downward API. This
section lists which fields you can make available.</p><p>You can pass information from available Pod-level fields using <code>fieldRef</code>.
At the API level, the <code>spec</code> for a Pod always defines at least one
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">Container</a>.
You can pass information from available Container-level fields using
<code>resourceFieldRef</code>.</p><h3 id="downwardapi-fieldRef">Information available via <code>fieldRef</code></h3><p>For some Pod-level fields, you can provide them to a container either as
an environment variable or using a <code>downwardAPI</code> volume. The fields available
via either mechanism are:</p><dl><dt><code>metadata.name</code></dt><dd>the pod's name</dd><dt><code>metadata.namespace</code></dt><dd>the pod's <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank" aria-label="namespace">namespace</a></dd><dt><code>metadata.uid</code></dt><dd>the pod's unique ID</dd><dt><code>metadata.annotations['&lt;KEY&gt;']</code></dt><dd>the value of the pod's <a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/annotations" target="_blank" aria-label="annotation">annotation</a> named <code>&lt;KEY&gt;</code> (for example, <code>metadata.annotations['myannotation']</code>)</dd><dt><code>metadata.labels['&lt;KEY&gt;']</code></dt><dd>the text value of the pod's <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle="tooltip" data-placement="top" href="/docs/concepts/overview/working-with-objects/labels" target="_blank" aria-label="label">label</a> named <code>&lt;KEY&gt;</code> (for example, <code>metadata.labels['mylabel']</code>)</dd></dl><p>The following information is available through environment variables
<strong>but not as a downwardAPI volume fieldRef</strong>:</p><dl><dt><code>spec.serviceAccountName</code></dt><dd>the name of the pod's <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." data-toggle="tooltip" data-placement="top" href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" aria-label="service account">service account</a></dd><dt><code>spec.nodeName</code></dt><dd>the name of the <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="node">node</a> where the Pod is executing</dd><dt><code>status.hostIP</code></dt><dd>the primary IP address of the node to which the Pod is assigned</dd><dt><code>status.hostIPs</code></dt><dd>the IP addresses is a dual-stack version of <code>status.hostIP</code>, the first is always the same as <code>status.hostIP</code>.</dd><dt><code>status.podIP</code></dt><dd>the pod's primary IP address (usually, its IPv4 address)</dd><dt><code>status.podIPs</code></dt><dd>the IP addresses is a dual-stack version of <code>status.podIP</code>, the first is always the same as <code>status.podIP</code></dd></dl><p>The following information is available through a <code>downwardAPI</code> volume
<code>fieldRef</code>, <strong>but not as environment variables</strong>:</p><dl><dt><code>metadata.labels</code></dt><dd>all of the pod's labels, formatted as <code>label-key="escaped-label-value"</code> with one label per line</dd><dt><code>metadata.annotations</code></dt><dd>all of the pod's annotations, formatted as <code>annotation-key="escaped-annotation-value"</code> with one annotation per line</dd></dl><h3 id="downwardapi-resourceFieldRef">Information available via <code>resourceFieldRef</code></h3><p>These container-level fields allow you to provide information about
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">requests and limits</a>
for resources such as CPU and memory.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-beta" title="Feature Gate: InPlacePodVerticalScaling"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>Container CPU and memory resources can be resized while the container is running.
If this happens, a downward API volume will be updated,
but environment variables will not be updated unless the container restarts.
See <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a>
for more details.</p></div><dl><dt><code>resource: limits.cpu</code></dt><dd>A container's CPU limit</dd><dt><code>resource: requests.cpu</code></dt><dd>A container's CPU request</dd><dt><code>resource: limits.memory</code></dt><dd>A container's memory limit</dd><dt><code>resource: requests.memory</code></dt><dd>A container's memory request</dd><dt><code>resource: limits.hugepages-*</code></dt><dd>A container's hugepages limit</dd><dt><code>resource: requests.hugepages-*</code></dt><dd>A container's hugepages request</dd><dt><code>resource: limits.ephemeral-storage</code></dt><dd>A container's ephemeral-storage limit</dd><dt><code>resource: requests.ephemeral-storage</code></dt><dd>A container's ephemeral-storage request</dd></dl><h4 id="fallback-information-for-resource-limits">Fallback information for resource limits</h4><p>If CPU and memory limits are not specified for a container, and you use the
downward API to try to expose that information, then the
kubelet defaults to exposing the maximum allocatable value for CPU and memory
based on the <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">node allocatable</a>
calculation.</p><h2 id="what-s-next">What's next</h2><p>You can read about <a href="/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code> volumes</a>.</p><p>You can try using the downward API to expose container- or Pod-level information:</p><ul><li>as <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a></li><li>as <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">files in <code>downwardAPI</code> volume</a></li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Windows containers in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that
run in many organizations. <a href="https://aka.ms/windowscontainers">Windows containers</a>
provide a way to encapsulate processes and package dependencies, making it easier
to use DevOps practices and follow cloud native patterns for Windows applications.</p><p>Organizations with investments in Windows-based applications and Linux-based
applications don't have to look for separate orchestrators to manage their workloads,
leading to increased operational efficiencies across their deployments, regardless
of operating system.</p><h2 id="windows-nodes-in-kubernetes">Windows nodes in Kubernetes</h2><p>To enable the orchestration of Windows containers in Kubernetes, include Windows nodes
in your existing Linux cluster. Scheduling Windows containers in
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pods">Pods</a> on Kubernetes is similar to
scheduling Linux-based containers.</p><p>In order to run Windows containers, your Kubernetes cluster must include
multiple operating systems.
While you can only run the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> on Linux,
you can deploy worker nodes running either Windows or Linux.</p><p>Windows <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." data-toggle="tooltip" data-placement="top" href="/docs/concepts/architecture/nodes/" target="_blank" aria-label="nodes">nodes</a> are
<a href="#windows-os-version-support">supported</a> provided that the operating system is
Windows Server 2019 or Windows Server 2022.</p><p>This document uses the term <em>Windows containers</em> to mean Windows containers with
process isolation. Kubernetes does not support running Windows containers with
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container">Hyper-V isolation</a>.</p><h2 id="limitations">Compatibility and limitations</h2><p>Some node features are only available if you use a specific
<a href="#container-runtime">container runtime</a>; others are not available on Windows nodes,
including:</p><ul><li>HugePages: not supported for Windows containers</li><li>Privileged containers: not supported for Windows containers.
<a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess Containers</a> offer similar functionality.</li><li>TerminationGracePeriod: requires containerD</li></ul><p>Not all features of shared namespaces are supported. See <a href="#api">API compatibility</a>
for more details.</p><p>See <a href="#windows-os-version-support">Windows OS version compatibility</a> for details on
the Windows versions that Kubernetes is tested against.</p><p>From an API and kubectl perspective, Windows containers behave in much the same
way as Linux-based containers. However, there are some notable differences in key
functionality which are outlined in this section.</p><h3 id="compatibility-linux-similarities">Comparison with Linux</h3><p>Key Kubernetes elements work the same way in Windows as they do in Linux. This
section refers to several key workload abstractions and how they map to Windows.</p><ul><li><p><a href="/docs/concepts/workloads/pods/">Pods</a></p><p>A Pod is the basic building block of Kubernetesthe smallest and simplest unit in
the Kubernetes object model that you create or deploy. You may not deploy Windows and
Linux containers in the same Pod. All containers in a Pod are scheduled onto a single
Node where each Node represents a specific platform and architecture. The following
Pod capabilities, properties and events are supported with Windows containers:</p><ul><li><p>Single or multiple containers per Pod with process isolation and volume sharing</p></li><li><p>Pod <code>status</code> fields</p></li><li><p>Readiness, liveness, and startup probes</p></li><li><p>postStart &amp; preStop container lifecycle hooks</p></li><li><p>ConfigMap, Secrets: as environment variables or volumes</p></li><li><p><code>emptyDir</code> volumes</p></li><li><p>Named pipe host mounts</p></li><li><p>Resource limits</p></li><li><p>OS field:</p><p>The <code>.spec.os.name</code> field should be set to <code>windows</code> to indicate that the current Pod uses Windows containers.</p><p>If you set the <code>.spec.os.name</code> field to <code>windows</code>,
you must not set the following fields in the <code>.spec</code> of that Pod:</p><ul><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li><li><code>spec.securityContext.seLinuxOptions</code></li><li><code>spec.securityContext.seccompProfile</code></li><li><code>spec.securityContext.fsGroup</code></li><li><code>spec.securityContext.fsGroupChangePolicy</code></li><li><code>spec.securityContext.sysctls</code></li><li><code>spec.shareProcessNamespace</code></li><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.securityContext.runAsGroup</code></li><li><code>spec.securityContext.supplementalGroups</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions</code></li><li><code>spec.containers[*].securityContext.seccompProfile</code></li><li><code>spec.containers[*].securityContext.capabilities</code></li><li><code>spec.containers[*].securityContext.readOnlyRootFilesystem</code></li><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsGroup</code></li></ul><p>In the above list, wildcards (<code>*</code>) indicate all elements in a list.
For example, <code>spec.containers[*].securityContext</code> refers to the SecurityContext object
for all containers. If any of these fields is specified, the Pod will
not be admitted by the API server.</p></li></ul></li><li><p><a href="/docs/concepts/workloads/controllers/">Workload resources</a> including:</p><ul><li>ReplicaSet</li><li>Deployment</li><li>StatefulSet</li><li>DaemonSet</li><li>Job</li><li>CronJob</li><li>ReplicationController</li></ul></li><li><p><a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." data-toggle="tooltip" data-placement="top" href="/docs/concepts/services-networking/service/" target="_blank" aria-label="Services">Services</a>
See <a href="/docs/concepts/services-networking/windows-networking/#load-balancing-and-services">Load balancing and Services</a> for more details.</p></li></ul><p>Pods, workload resources, and Services are critical elements to managing Windows
workloads on Kubernetes. However, on their own they are not enough to enable
the proper lifecycle management of Windows workloads in a dynamic cloud native
environment.</p><ul><li><code>kubectl exec</code></li><li>Pod and container metrics</li><li><a class="glossary-tooltip" title="Object that automatically scales the number of pod replicas based on targeted resource utilization or custom metric targets." data-toggle="tooltip" data-placement="top" href="/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" aria-label="Horizontal pod autoscaling">Horizontal pod autoscaling</a></li><li><a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." data-toggle="tooltip" data-placement="top" href="/docs/concepts/policy/resource-quotas/" target="_blank" aria-label="Resource quotas">Resource quotas</a></li><li>Scheduler preemption</li></ul><h3 id="kubelet-compatibility">Command line options for the kubelet</h3><p>Some kubelet command line options behave differently on Windows, as described below:</p><ul><li>The <code>--windows-priorityclass</code> lets you set the scheduling priority of the kubelet process
(see <a href="/docs/concepts/configuration/windows-resource-management/#resource-management-cpu">CPU resource management</a>)</li><li>The <code>--kube-reserved</code>, <code>--system-reserved</code> , and <code>--eviction-hard</code> flags update
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">NodeAllocatable</a></li><li>Eviction by using <code>--enforce-node-allocable</code> is not implemented</li><li>When running on a Windows node the kubelet does not have memory or CPU
restrictions. <code>--kube-reserved</code> and <code>--system-reserved</code> only subtract from <code>NodeAllocatable</code>
and do not guarantee resource provided for workloads.
See <a href="/docs/concepts/configuration/windows-resource-management/#resource-reservation">Resource Management for Windows nodes</a>
for more information.</li><li>The <code>PIDPressure</code> Condition is not implemented</li><li>The kubelet does not take OOM eviction actions</li></ul><h3 id="api">API compatibility</h3><p>There are subtle differences in the way the Kubernetes APIs work for Windows due to the OS
and container runtime. Some workload properties were designed for Linux, and fail to run on Windows.</p><p>At a high level, these OS concepts are different:</p><ul><li>Identity - Linux uses userID (UID) and groupID (GID) which
are represented as integer types. User and group names
are not canonical - they are just an alias in <code>/etc/groups</code>
or <code>/etc/passwd</code> back to UID+GID. Windows uses a larger binary
<a href="https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/security-identifiers">security identifier</a> (SID)
which is stored in the Windows Security Access Manager (SAM) database. This
database is not shared between the host and containers, or between containers.</li><li>File permissions - Windows uses an access control list based on (SIDs), whereas
POSIX systems such as Linux use a bitmask based on object permissions and UID+GID,
plus <em>optional</em> access control lists.</li><li>File paths - the convention on Windows is to use <code>\</code> instead of <code>/</code>. The Go IO
libraries typically accept both and just make it work, but when you're setting a
path or command line that's interpreted inside a container, <code>\</code> may be needed.</li><li>Signals - Windows interactive apps handle termination differently, and can
implement one or more of these:<ul><li>A UI thread handles well-defined messages including <code>WM_CLOSE</code>.</li><li>Console apps handle Ctrl-C or Ctrl-break using a Control Handler.</li><li>Services register a Service Control Handler function that can accept
<code>SERVICE_CONTROL_STOP</code> control codes.</li></ul></li></ul><p>Container exit codes follow the same convention where 0 is success, and nonzero is failure.
The specific error codes may differ across Windows and Linux. However, exit codes
passed from the Kubernetes components (kubelet, kube-proxy) are unchanged.</p><h4 id="compatibility-v1-pod-spec-containers">Field compatibility for container specifications</h4><p>The following list documents differences between how Pod container specifications
work between Windows and Linux:</p><ul><li>Huge pages are not implemented in the Windows container
runtime, and are not available. They require <a href="https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support">asserting a user
privilege</a>
that's not configurable for containers.</li><li><code>requests.cpu</code> and <code>requests.memory</code> - requests are subtracted
from node available resources, so they can be used to avoid overprovisioning a
node. However, they cannot be used to guarantee resources in an overprovisioned
node. They should be applied to all containers as a best practice if the operator
wants to avoid overprovisioning entirely.</li><li><code>securityContext.allowPrivilegeEscalation</code> -
not possible on Windows; none of the capabilities are hooked up</li><li><code>securityContext.capabilities</code> -
POSIX capabilities are not implemented on Windows</li><li><code>securityContext.privileged</code> -
Windows doesn't support privileged containers, use <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess Containers</a> instead</li><li><code>securityContext.procMount</code> -
Windows doesn't have a <code>/proc</code> filesystem</li><li><code>securityContext.readOnlyRootFilesystem</code> -
not possible on Windows; write access is required for registry &amp; system
processes to run inside the container</li><li><code>securityContext.runAsGroup</code> -
not possible on Windows as there is no GID support</li><li><code>securityContext.runAsNonRoot</code> -
this setting will prevent containers from running as <code>ContainerAdministrator</code>
which is the closest equivalent to a root user on Windows.</li><li><code>securityContext.runAsUser</code> -
use <a href="/docs/tasks/configure-pod-container/configure-runasusername/"><code>runAsUserName</code></a>
instead</li><li><code>securityContext.seLinuxOptions</code> -
not possible on Windows as SELinux is Linux-specific</li><li><code>terminationMessagePath</code> -
this has some limitations in that Windows doesn't support mapping single files. The
default value is <code>/dev/termination-log</code>, which does work because it does not
exist on Windows by default.</li></ul><h4 id="compatibility-v1-pod">Field compatibility for Pod specifications</h4><p>The following list documents differences between how Pod specifications work between Windows and Linux:</p><ul><li><code>hostIPC</code> and <code>hostpid</code> - host namespace sharing is not possible on Windows</li><li><code>hostNetwork</code> - host networking is not possible on Windows</li><li><code>dnsPolicy</code> - setting the Pod <code>dnsPolicy</code> to <code>ClusterFirstWithHostNet</code> is
not supported on Windows because host networking is not provided. Pods always
run with a container network.</li><li><code>podSecurityContext</code> <a href="#compatibility-v1-pod-spec-containers-securitycontext">see below</a></li><li><code>shareProcessNamespace</code> - this is a beta feature, and depends on Linux namespaces
which are not implemented on Windows. Windows cannot share process namespaces or
the container's root filesystem. Only the network can be shared.</li><li><code>terminationGracePeriodSeconds</code> - this is not fully implemented in Docker on Windows,
see the <a href="https://github.com/moby/moby/issues/25982">GitHub issue</a>.
The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT,
then Windows waits 5 seconds by default, and finally shuts down
all processes using the normal Windows shutdown behavior. The 5
second default is actually in the Windows registry
<a href="https://github.com/moby/moby/issues/25982#issuecomment-426441183">inside the container</a>,
so it can be overridden when the container is built.</li><li><code>volumeDevices</code> - this is a beta feature, and is not implemented on Windows.
Windows cannot attach raw block devices to pods.</li><li><code>volumes</code><ul><li>If you define an <code>emptyDir</code> volume, you cannot set its volume source to <code>memory</code>.</li></ul></li><li>You cannot enable <code>mountPropagation</code> for volume mounts as this is not
supported on Windows.</li></ul><h4 id="compatibility-v1-pod-sec-containers-hostnetwork">Host network access</h4><p>Kubernetes v1.26 to v1.32 included alpha support for running Windows Pods in the host's network namespace.</p><p>Kubernetes v1.34 does <strong>not</strong> include the <code>WindowsHostNetwork</code> feature gate
or support for running Windows Pods in the host's network namespace.</p><h4 id="compatibility-v1-pod-spec-containers-securitycontext">Field compatibility for Pod security context</h4><p>Only the <code>securityContext.runAsNonRoot</code> and <code>securityContext.windowsOptions</code> from the Pod
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>securityContext</code></a> fields work on Windows.</p><h2 id="node-problem-detector">Node problem detector</h2><p>The node problem detector (see
<a href="/docs/tasks/debug/debug-cluster/monitor-node-health/">Monitor Node Health</a>)
has preliminary support for Windows.
For more information, visit the project's <a href="https://github.com/kubernetes/node-problem-detector#windows">GitHub page</a>.</p><h2 id="pause-container">Pause container</h2><p>In a Kubernetes Pod, an infrastructure or pause container is first created
to host the container. In Linux, the cgroups and namespaces that make up a pod
need a process to maintain their continued existence; the pause process provides
this. Containers that belong to the same pod, including infrastructure and worker
containers, share a common network endpoint (same IPv4 and / or IPv6 address, same
network port spaces). Kubernetes uses pause containers to allow for worker containers
crashing or restarting without losing any of the networking configuration.</p><p>Kubernetes maintains a multi-architecture image that includes support for Windows.
For Kubernetes v1.34.0 the recommended pause image is <code>registry.k8s.io/pause:3.6</code>.
The <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause">source code</a>
is available on GitHub.</p><p>Microsoft maintains a different multi-architecture image, with Linux and Windows
amd64 support, that you can find as <code>mcr.microsoft.com/oss/kubernetes/pause:3.6</code>.
This image is built from the same source as the Kubernetes maintained image but
all of the Windows binaries are <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode">authenticode signed</a> by Microsoft.
The Kubernetes project recommends using the Microsoft maintained image if you are
deploying to a production or production-like environment that requires signed
binaries.</p><h2 id="container-runtime">Container runtimes</h2><p>You need to install a
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." data-toggle="tooltip" data-placement="top" href="/docs/setup/production-environment/container-runtimes" target="_blank" aria-label="container runtime">container runtime</a>
into each node in the cluster so that Pods can run there.</p><p>The following container runtimes work with Windows:</p><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h3 id="containerd">ContainerD</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>You can use <a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" data-toggle="tooltip" data-placement="top" href="https://containerd.io/docs/" target="_blank" aria-label="ContainerD">ContainerD</a> 1.4.0+
as the container runtime for Kubernetes nodes that run Windows.</p><p>Learn how to <a href="/docs/setup/production-environment/container-runtimes/#containerd">install ContainerD on a Windows node</a>.<div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>There is a <a href="/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations">known limitation</a>
when using GMSA with containerd to access Windows network shares, which requires a
kernel patch.</div></p><h3 id="mcr">Mirantis Container Runtime</h3><p><a href="https://docs.mirantis.com/mcr/25.0/overview.html">Mirantis Container Runtime</a> (MCR)
is available as a container runtime for all Windows Server 2019 and later versions.</p><p>See <a href="https://docs.mirantis.com/mcr/25.0/install/mcr-windows.html">Install MCR on Windows Servers</a> for more information.</p><h2 id="windows-os-version-support">Windows OS version compatibility</h2><p>On Windows nodes, strict compatibility rules apply where the host OS version must
match the container base image OS version. Only Windows containers with a container
operating system of Windows Server 2019 are fully supported.</p><p>For Kubernetes v1.34, operating system compatibility for Windows nodes (and Pods)
is as follows:</p><dl><dt>Windows Server LTSC release</dt><dd>Windows Server 2019</dd><dd>Windows Server 2022</dd><dt>Windows Server SAC release</dt><dd>Windows Server version 20H2</dd></dl><p>The Kubernetes <a href="/docs/setup/release/version-skew-policy/">version-skew policy</a> also applies.</p><h2 id="windows-hardware-recommendations">Hardware recommendations and considerations</h2><div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>The following hardware specifications outlined here should be regarded as sensible default values.
They are not intended to represent minimum requirements or specific recommendations for production environments.
Depending on the requirements for your workload these values may need to be adjusted.</div><ul><li>64-bit processor 4 CPU cores or more, capable of supporting virtualization</li><li>8GB or more of RAM</li><li>50GB or more of free disk space</li></ul><p>Refer to
<a href="https://learn.microsoft.com/en-us/windows-server/get-started/hardware-requirements">Hardware requirements for Windows Server Microsoft documentation</a>
for the most up-to-date information on minimum hardware requirements. For guidance on deciding on resources for
production worker nodes refer to <a href="/docs/setup/production-environment/#production-worker-nodes">Production worker nodes Kubernetes documentation</a>.</p><p>To optimize system resources, if a graphical user interface is not required,
it may be preferable to use a Windows Server OS installation that excludes
the <a href="https://learn.microsoft.com/en-us/windows-server/get-started/install-options-server-core-desktop-experience">Windows Desktop Experience</a>
installation option, as this configuration typically frees up more system
resources.</p><p>In assessing disk space for Windows worker nodes, take note that Windows container images are typically larger than
Linux container images, with container image sizes ranging
from <a href="https://techcommunity.microsoft.com/t5/containers/nano-server-x-server-core-x-server-which-base-image-is-the-right/ba-p/2835785">300MB to over 10GB</a>
for a single image. Additionally, take note that the <code>C:</code> drive in Windows containers represents a virtual free size of
20GB by default, which is not the actual consumed space, but rather the disk size for which a single container can grow
to occupy when using local storage on the host.
See <a href="https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-storage#storage-limits">Containers on Windows - Container Storage Documentation</a>
for more detail.</p><h2 id="troubleshooting">Getting help and troubleshooting</h2><p>Your main source of help for troubleshooting your Kubernetes cluster should start
with the <a href="/docs/tasks/debug/">Troubleshooting</a>
page.</p><p>Some additional, Windows-specific troubleshooting help is included
in this section. Logs are an important element of troubleshooting
issues in Kubernetes. Make sure to include them any time you seek
troubleshooting assistance from other contributors. Follow the
instructions in the
SIG Windows <a href="https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs">contributing guide on gathering logs</a>.</p><h3 id="reporting-issues-and-feature-requests">Reporting issues and feature requests</h3><p>If you have what looks like a bug, or you would like to
make a feature request, please follow the <a href="https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#reporting-issues-and-feature-requests">SIG Windows contributing guide</a> to create a new issue.
You should first search the list of issues in case it was
reported previously and comment with your experience on the issue and add additional
logs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support and
troubleshooting ideas prior to creating a ticket.</p><h3 id="validating-the-windows-cluster-operability">Validating the Windows cluster operability</h3><p>The Kubernetes project provides a <em>Windows Operational Readiness</em> specification,
accompanied by a structured test suite. This suite is split into two sets of tests,
core and extended, each containing categories aimed at testing specific areas.
It can be used to validate all the functionalities of a Windows and hybrid system
(mixed with Linux nodes) with full coverage.</p><p>To set up the project on a newly created cluster, refer to the instructions in the
<a href="https://github.com/kubernetes-sigs/windows-operational-readiness/blob/main/README.md">project guide</a>.</p><h2 id="deployment-tools">Deployment tools</h2><p>The kubeadm tool helps you to deploy a Kubernetes cluster, providing the control
plane to manage the cluster it, and nodes to run your workloads.</p><p>The Kubernetes <a href="https://cluster-api.sigs.k8s.io/">cluster API</a> project also provides means to automate deployment of Windows nodes.</p><h2 id="windows-distribution-channels">Windows distribution channels</h2><p>For a detailed explanation of Windows distribution channels see the
<a href="https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19">Microsoft documentation</a>.</p><p>Information on the different Windows Server servicing channels
including their support models can be found at
<a href="https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison">Windows Server servicing channels</a>.</p></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">Pod Quality of Service Classes</h1><p>This page introduces <em>Quality of Service (QoS) classes</em> in Kubernetes, and explains
how Kubernetes assigns a QoS class to each Pod as a consequence of the resource
constraints that you specify for the containers in that Pod. Kubernetes relies on this
classification to make decisions about which Pods to evict when there are not enough
available resources on a Node.</p><h2 id="quality-of-service-classes">Quality of Service classes</h2><p>Kubernetes classifies the Pods that you run and allocates each Pod into a specific
<em>quality of service (QoS) class</em>. Kubernetes uses that classification to influence how different
pods are handled. Kubernetes does this classification based on the
<a href="/docs/concepts/configuration/manage-resources-containers/">resource requests</a>
of the <a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." data-toggle="tooltip" data-placement="top" href="/docs/concepts/containers/" target="_blank" aria-label="Containers">Containers</a> in that Pod, along with
how those requests relate to resource limits.
This is known as <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/pod-qos/" target="_blank" aria-label="Quality of Service">Quality of Service</a>
(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests
and limits of its component Containers. QoS classes are used by Kubernetes to decide
which Pods to evict from a Node experiencing
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node Pressure</a>. The possible
QoS classes are <code>Guaranteed</code>, <code>Burstable</code>, and <code>BestEffort</code>. When a Node runs out of resources,
Kubernetes will first evict <code>BestEffort</code> Pods running on that Node, followed by <code>Burstable</code> and
finally <code>Guaranteed</code> Pods. When this eviction is due to resource pressure, only Pods exceeding
resource requests are candidates for eviction.</p><h3 id="guaranteed">Guaranteed</h3><p>Pods that are <code>Guaranteed</code> have the strictest resource limits and are least likely
to face eviction. They are guaranteed not to be killed until they exceed their limits
or there are no lower-priority Pods that can be preempted from the Node. They may
not acquire resources beyond their specified limits. These Pods can also make
use of exclusive CPUs using the
<a href="/docs/tasks/administer-cluster/cpu-management-policies/#static-policy"><code>static</code></a> CPU management policy.</p><h4 id="criteria">Criteria</h4><p>For a Pod to be given a QoS class of <code>Guaranteed</code>:</p><ul><li>Every Container in the Pod must have a memory limit and a memory request.</li><li>For every Container in the Pod, the memory limit must equal the memory request.</li><li>Every Container in the Pod must have a CPU limit and a CPU request.</li><li>For every Container in the Pod, the CPU limit must equal the CPU request.</li></ul><h3 id="burstable">Burstable</h3><p>Pods that are <code>Burstable</code> have some lower-bound resource guarantees based on the request, but
do not require a specific limit. If a limit is not specified, it defaults to a
limit equivalent to the capacity of the Node, which allows the Pods to flexibly increase
their resources if resources are available. In the event of Pod eviction due to Node
resource pressure, these Pods are evicted only after all <code>BestEffort</code> Pods are evicted.
Because a <code>Burstable</code> Pod can include a Container that has no resource limits or requests, a Pod
that is <code>Burstable</code> can try to use any amount of node resources.</p><h4 id="criteria-1">Criteria</h4><p>A Pod is given a QoS class of <code>Burstable</code> if:</p><ul><li>The Pod does not meet the criteria for QoS class <code>Guaranteed</code>.</li><li>At least one Container in the Pod has a memory or CPU request or limit.</li></ul><h3 id="besteffort">BestEffort</h3><p>Pods in the <code>BestEffort</code> QoS class can use node resources that aren't specifically assigned
to Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the
kubelet, and you assign 4 CPU cores to a <code>Guaranteed</code> Pod, then a Pod in the <code>BestEffort</code>
QoS class can try to use any amount of the remaining 12 CPU cores.</p><p>The kubelet prefers to evict <code>BestEffort</code> Pods if the node comes under resource pressure.</p><h4 id="criteria-2">Criteria</h4><p>A Pod has a QoS class of <code>BestEffort</code> if it doesn't meet the criteria for either <code>Guaranteed</code>
or <code>Burstable</code>. In other words, a Pod is <code>BestEffort</code> only if none of the Containers in the Pod have a
memory limit or a memory request, and none of the Containers in the Pod have a
CPU limit or a CPU request.
Containers in a Pod can request other resources (not CPU or memory) and still be classified as
<code>BestEffort</code>.</p><h2 id="memory-qos-with-cgroup-v2">Memory QoS with cgroup v2</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: MemoryQoS"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [alpha]</code> (enabled by default: false)</div><p>Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.
Memory requests and limits of containers in pod are used to set specific interfaces <code>memory.min</code>
and <code>memory.high</code> provided by the memory controller. When <code>memory.min</code> is set to memory requests,
memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures
memory availability for Kubernetes pods. And if memory limits are set in the container,
this means that the system needs to limit container memory usage; Memory QoS uses <code>memory.high</code>
to throttle workload approaching its memory limit, ensuring that the system is not overwhelmed
by instantaneous memory allocation.</p><p>Memory QoS relies on QoS class to determine which settings to apply; however, these are different
mechanisms that both provide controls over quality of service.</p><h2 id="class-independent-behavior">Some behavior is independent of QoS class</h2><p>Certain behavior is independent of the QoS class assigned by Kubernetes. For example:</p><ul><li><p>Any Container exceeding a resource limit will be killed and restarted by the kubelet without
affecting other Containers in that Pod.</p></li><li><p>If a Container exceeds its resource request and the node it runs on faces
resource pressure, the Pod it is in becomes a candidate for <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">eviction</a>.
If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a
replacement Pod, usually on a different node.</p></li><li><p>The resource request of a Pod is equal to the sum of the resource requests of
its component Containers, and the resource limit of a Pod is equal to the sum of
the resource limits of its component Containers.</p></li><li><p>The kube-scheduler does not consider QoS class when selecting which Pods to
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption">preempt</a>.
Preemption can occur when a cluster does not have enough resources to run all the Pods
you defined.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/configuration/manage-resources-containers/">resource management for Pods and Containers</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure eviction</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod priority and preemption</a>.</li><li>Learn about <a href="/docs/concepts/workloads/pods/disruptions/">Pod disruptions</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">assign memory resources to containers and pods</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">assign CPU resources to containers and pods</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/quality-service-pod/">configure Quality of Service for Pods</a>.</li></ul></div>
<hr>
<div class="td-content"><h1 data-pagefind-weight="10">DaemonSet</h1><div class="lead">A DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.</div><p>A <em>DaemonSet</em> ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the
cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage
collected. Deleting a DaemonSet will clean up the Pods it created.</p><p>Some typical uses of a DaemonSet are:</p><ul><li>running a cluster storage daemon on every node</li><li>running a logs collection daemon on every node</li><li>running a node monitoring daemon on every node</li></ul><p>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.</p><h2 id="writing-a-daemonset-spec">Writing a DaemonSet Spec</h2><h3 id="create-a-daemonset">Create a DaemonSet</h3><p>You can describe a DaemonSet in a YAML file. For example, the <code>daemonset.yaml</code> file below
describes a DaemonSet that runs the fluentd-elasticsearch Docker image:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/daemonset.yaml" download="controllers/daemonset.yaml"><code>controllers/daemonset.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" onclick="copyCode(&quot;controllers-daemonset-yaml&quot;)" title="Copy controllers/daemonset.yaml to clipboard"/></div><div class="includecode" id="controllers-daemonset-yaml"><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">k8s-app</span>:<span style="color:#bbb"> </span>fluentd-logging<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb"/><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">selector</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">matchLabels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">template</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">metadata</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">labels</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">spec</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">tolerations</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># these tolerations are to have the daemonset runnable on control plane nodes</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># remove them if your control plane nodes should not run pods</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/control-plane<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">containers</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">image</span>:<span style="color:#bbb"> </span>quay.io/fluentd_elasticsearch/fluentd:v5.0.1<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">resources</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">limits</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">requests</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">            </span><span style="color:green;font-weight:700">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">volumeMounts</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># it may be desirable to set a high priority class to ensure that a DaemonSet Pod</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># preempts running Pods</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># priorityClassName: important</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span><span style="color:green;font-weight:700">volumes</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">hostPath</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">          </span><span style="color:green;font-weight:700">path</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span></span></span></code></pre></div></div></div><p>Create a DaemonSet based on the YAML file:</p><pre tabindex="0"><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><h3 id="required-fields">Required Fields</h3><p>As with all other Kubernetes config, a DaemonSet needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields. For
general information about working with config files, see
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">running stateless applications</a>
and <a href="/docs/concepts/overview/working-with-objects/object-management/">object management using kubectl</a>.</p><p>The name of a DaemonSet object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>A DaemonSet also needs a
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code></a>
section.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is one of the required fields in <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>.
It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." data-toggle="tooltip" data-placement="top" href="/docs/concepts/workloads/pods/" target="_blank" aria-label="Pod">Pod</a>,
except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see <a href="#pod-selector">pod selector</a>).</p><p>A Pod Template in a DaemonSet must have a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>
equal to <code>Always</code>, or be unspecified, which defaults to <code>Always</code>.</p><h3 id="pod-selector">Pod Selector</h3><p>The <code>.spec.selector</code> field is a pod selector. It works the same as the <code>.spec.selector</code> of
a <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p>You must specify a pod selector that matches the labels of the
<code>.spec.template</code>.
Also, once a DaemonSet is created,
its <code>.spec.selector</code> can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.</p><p>The <code>.spec.selector</code> is an object consisting of two fields:</p><ul><li><code>matchLabels</code> - works the same as the <code>.spec.selector</code> of a
<a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>.</li><li><code>matchExpressions</code> - allows to build more sophisticated selectors by specifying key,
list of values and an operator that relates the key and values.</li></ul><p>When the two are specified the result is ANDed.</p><p>The <code>.spec.selector</code> must match the <code>.spec.template.metadata.labels</code>.
Config with these two not matching will be rejected by the API.</p><h3 id="running-pods-on-select-nodes">Running Pods on select Nodes</h3><p>If you specify a <code>.spec.template.spec.nodeSelector</code>, then the DaemonSet controller will
create Pods on nodes which match that <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">node selector</a>.
Likewise if you specify a <code>.spec.template.spec.affinity</code>,
then DaemonSet controller will create Pods on nodes which match that
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">node affinity</a>.
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</p><h2 id="how-daemon-pods-are-scheduled">How Daemon Pods are scheduled</h2><p>A DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.
The DaemonSet controller creates a Pod for each eligible node and adds the
<code>spec.affinity.nodeAffinity</code> field of the Pod to match the target host. After
the Pod is created, the default scheduler typically takes over and then binds
the Pod to the target host by setting the <code>.spec.nodeName</code> field. If the new
Pod cannot fit on the node, the default scheduler may preempt (evict) some of
the existing Pods based on the
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority">priority</a>
of the new Pod.</p><div class="alert alert-info" role="alert"><h4 class="alert-heading">Note:</h4>If it's important that the DaemonSet pod run on each node, it's often desirable
to set the <code>.spec.template.spec.priorityClassName</code> of the DaemonSet to a
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClass</a>
with a higher priority to ensure that this eviction occurs.</div><p>The user can specify a different scheduler for the Pods of the DaemonSet, by
setting the <code>.spec.template.spec.schedulerName</code> field of the DaemonSet.</p><p>The original node affinity specified at the
<code>.spec.template.spec.affinity.nodeAffinity</code> field (if specified) is taken into
consideration by the DaemonSet controller when evaluating the eligible nodes,
but is replaced on the created Pod with the node affinity that matches the name
of the eligible node.</p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="display:flex"><span><span style="color:green;font-weight:700">nodeAffinity</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">  </span><span style="color:green;font-weight:700">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span><span style="color:green;font-weight:700">nodeSelectorTerms</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">    </span>- <span style="color:green;font-weight:700">matchFields</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">      </span>- <span style="color:green;font-weight:700">key</span>:<span style="color:#bbb"> </span>metadata.name<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span><span style="color:green;font-weight:700">values</span>:<span style="color:#bbb">
</span></span></span><span style="display:flex"><span><span style="color:#bbb">        </span>- target-host-name<span style="color:#bbb">
</span></span></span></code></pre></div><h3 id="taints-and-tolerations">Taints and tolerations</h3><p>The DaemonSet controller automatically adds a set of <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." data-toggle="tooltip" data-placement="top" href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" aria-label="tolerations">tolerations</a> to DaemonSet Pods:</p><table><caption style="display:none">Tolerations for DaemonSet pods</caption><thead><tr><th>Toleration key</th><th>Effect</th><th>Details</th></tr></thead><tbody><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready"><code>node.kubernetes.io/not-ready</code></a></td><td><code>NoExecute</code></td><td>DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable"><code>node.kubernetes.io/unreachable</code></a></td><td><code>NoExecute</code></td><td>DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure"><code>node.kubernetes.io/disk-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with disk pressure issues.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure"><code>node.kubernetes.io/memory-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with memory pressure issues.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure"><code>node.kubernetes.io/pid-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with process pressure issues.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable"><code>node.kubernetes.io/unschedulable</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes that are unschedulable.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable"><code>node.kubernetes.io/network-unavailable</code></a></td><td><code>NoSchedule</code></td><td><strong>Only added for DaemonSet Pods that request host networking</strong>, i.e., Pods having <code>spec.hostNetwork: true</code>. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.</td></tr></tbody></table><p>You can add your own tolerations to the Pods of a DaemonSet as well, by
defining these in the Pod template of the DaemonSet.</p><p>Because the DaemonSet controller sets the
<code>node.kubernetes.io/unschedulable:NoSchedule</code> toleration automatically,
Kubernetes can run DaemonSet Pods on nodes that are marked as <em>unschedulable</em>.</p><p>If you use a DaemonSet to provide an important node-level function, such as
<a href="/docs/concepts/cluster-administration/networking/">cluster networking</a>, it is
helpful that Kubernetes places DaemonSet Pods on nodes before they are ready.
For example, without that special toleration, you could end up in a deadlock
situation where the node is not marked as ready because the network plugin is
not running there, and at the same time the network plugin is not running on
that node because the node is not yet ready.</p><h2 id="communicating-with-daemon-pods">Communicating with Daemon Pods</h2><p>Some possible patterns for communicating with Pods in a DaemonSet are:</p><ul><li><strong>Push</strong>: Pods in the DaemonSet are configured to send updates to another service, such
as a stats database. They do not have clients.</li><li><strong>NodeIP and Known Port</strong>: Pods in the DaemonSet can use a <code>hostPort</code>, so that the pods
are reachable via the node IPs.
Clients know the list of node IPs somehow, and know the port by convention.</li><li><strong>DNS</strong>: Create a <a href="/docs/concepts/services-networking/service/#headless-services">headless service</a>
with the same pod selector, and then discover DaemonSets using the <code>endpoints</code>
resource or retrieve multiple A records from DNS.</li><li><strong>Service</strong>: Create a service with the same Pod selector, and use the service to reach a
daemon on a random node. Use <a href="/docs/concepts/services-networking/service-traffic-policy/">Service Internal Traffic Policy</a>
to limit to pods on the same node.</li></ul><h2 id="updating-a-daemonset">Updating a DaemonSet</h2><p>If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.</p><p>You can modify the Pods that a DaemonSet creates. However, Pods do not allow all
fields to be updated. Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.</p><p>You can delete a DaemonSet. If you specify <code>--cascade=orphan</code> with <code>kubectl</code>, then the Pods
will be left on the nodes. If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its <code>updateStrategy</code>.</p><p>You can <a href="/docs/tasks/manage-daemon/update-daemon-set/">perform a rolling update</a> on a DaemonSet.</p><h2 id="alternatives-to-daemonset">Alternatives to DaemonSet</h2><h3 id="init-scripts">Init scripts</h3><p>It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
<code>init</code>, <code>upstartd</code>, or <code>systemd</code>). This is perfectly fine. However, there are several advantages to
running such processes via a DaemonSet:</p><ul><li>Ability to monitor and manage logs for daemons in the same way as applications.</li><li>Same config language and tools (e.g. Pod templates, <code>kubectl</code>) for daemons and applications.</li><li>Running daemons in containers with resource limits increases isolation between daemons from app
containers. However, this can also be accomplished by running the daemons in a container but not in a Pod.</li></ul><h3 id="bare-pods">Bare Pods</h3><p>It is possible to create Pods directly which specify a particular node to run on. However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.</p><h3 id="static-pods">Static Pods</h3><p>It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These
are called <a href="/docs/tasks/configure-pod-container/static-pod/">static pods</a>.
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.</p><h3 id="deployments">Deployments</h3><p>DaemonSets are similar to <a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a> in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).</p><p>Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.</p><p>For example, <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugins</a> often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>:<ul><li>Learn about <a href="/docs/tasks/configure-pod-container/static-pod/">static Pods</a>, which are useful for running Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle="tooltip" data-placement="top" href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank" aria-label="control plane">control plane</a> components.</li></ul></li><li>Find out how to use DaemonSets:<ul><li><a href="/docs/tasks/manage-daemon/update-daemon-set/">Perform a rolling update on a DaemonSet</a>.</li><li><a href="/docs/tasks/manage-daemon/rollback-daemon-set/">Perform a rollback on a DaemonSet</a>
(for example, if a roll out didn't work how you expected).</li></ul></li><li>Understand <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">how Kubernetes assigns Pods to Nodes</a>.</li><li>Learn about <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugins</a> and
<a href="/docs/concepts/cluster-administration/addons/">add ons</a>, which often run as DaemonSets.</li><li><code>DaemonSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/">DaemonSet</a>
object definition to understand the API for daemon sets.</li></ul></div>